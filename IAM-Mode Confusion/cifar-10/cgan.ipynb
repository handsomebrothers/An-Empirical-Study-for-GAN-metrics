{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f99ac073160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# #指定使用那块GUP训练\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.ConfigProto()\n",
    "# 设置最大占有GPU不超过显存的70%\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n",
    "# # 重点：设置动态分配GPU\n",
    "config.gpu_options.allow_growth = True\n",
    "# 创建session时\n",
    "tf.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 4,310,400\n",
      "Trainable params: 4,308,480\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 4, 4, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 8, 8, 128)         819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 32, 32, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 4,522,883\n",
      "Trainable params: 4,517,891\n",
      "Non-trainable params: 4,992\n",
      "_________________________________________________________________\n",
      "epoch:0 step:1 [D loss: 4.152684, acc.: 33.59%] [G loss: 0.316296]\n",
      "epoch:0 step:2 [D loss: 0.991663, acc.: 50.00%] [G loss: 3.755607]\n",
      "epoch:0 step:3 [D loss: 0.951699, acc.: 50.00%] [G loss: 2.709499]\n",
      "epoch:0 step:4 [D loss: 1.081482, acc.: 33.59%] [G loss: 3.554532]\n",
      "epoch:0 step:5 [D loss: 0.360410, acc.: 83.59%] [G loss: 3.161462]\n",
      "epoch:0 step:6 [D loss: 0.531125, acc.: 67.97%] [G loss: 4.298576]\n",
      "epoch:0 step:7 [D loss: 0.179507, acc.: 95.31%] [G loss: 4.005746]\n",
      "epoch:0 step:8 [D loss: 0.428110, acc.: 79.69%] [G loss: 5.083507]\n",
      "epoch:0 step:9 [D loss: 0.142152, acc.: 93.75%] [G loss: 4.678815]\n",
      "epoch:0 step:10 [D loss: 0.083965, acc.: 100.00%] [G loss: 4.409805]\n",
      "epoch:0 step:11 [D loss: 0.092227, acc.: 99.22%] [G loss: 5.587786]\n",
      "epoch:0 step:12 [D loss: 0.070041, acc.: 98.44%] [G loss: 5.089712]\n",
      "epoch:0 step:13 [D loss: 0.634162, acc.: 50.78%] [G loss: 7.901289]\n",
      "epoch:0 step:14 [D loss: 0.317653, acc.: 84.38%] [G loss: 6.441988]\n",
      "epoch:0 step:15 [D loss: 0.029921, acc.: 100.00%] [G loss: 5.184913]\n",
      "epoch:0 step:16 [D loss: 0.035729, acc.: 100.00%] [G loss: 4.310612]\n",
      "epoch:0 step:17 [D loss: 0.073527, acc.: 100.00%] [G loss: 6.050846]\n",
      "epoch:0 step:18 [D loss: 0.010991, acc.: 100.00%] [G loss: 5.646429]\n",
      "epoch:0 step:19 [D loss: 0.426470, acc.: 76.56%] [G loss: 9.299594]\n",
      "epoch:0 step:20 [D loss: 0.273790, acc.: 85.16%] [G loss: 8.566582]\n",
      "epoch:0 step:21 [D loss: 0.072155, acc.: 96.88%] [G loss: 6.169305]\n",
      "epoch:0 step:22 [D loss: 0.031986, acc.: 99.22%] [G loss: 3.047902]\n",
      "epoch:0 step:23 [D loss: 0.127949, acc.: 97.66%] [G loss: 5.301085]\n",
      "epoch:0 step:24 [D loss: 0.051521, acc.: 98.44%] [G loss: 3.869535]\n",
      "epoch:0 step:25 [D loss: 0.576851, acc.: 81.25%] [G loss: 6.814863]\n",
      "epoch:0 step:26 [D loss: 0.842937, acc.: 78.12%] [G loss: 3.604163]\n",
      "epoch:0 step:27 [D loss: 0.068730, acc.: 99.22%] [G loss: 1.121122]\n",
      "epoch:0 step:28 [D loss: 0.525484, acc.: 70.31%] [G loss: 6.571347]\n",
      "epoch:0 step:29 [D loss: 0.809095, acc.: 72.66%] [G loss: 3.612592]\n",
      "epoch:0 step:30 [D loss: 0.074796, acc.: 100.00%] [G loss: 1.322535]\n",
      "epoch:0 step:31 [D loss: 1.065292, acc.: 50.00%] [G loss: 6.272769]\n",
      "epoch:0 step:32 [D loss: 0.692667, acc.: 70.31%] [G loss: 4.952377]\n",
      "epoch:0 step:33 [D loss: 0.322084, acc.: 81.25%] [G loss: 2.040294]\n",
      "epoch:0 step:34 [D loss: 0.169823, acc.: 93.75%] [G loss: 3.966729]\n",
      "epoch:0 step:35 [D loss: 0.125144, acc.: 100.00%] [G loss: 4.809593]\n",
      "epoch:0 step:36 [D loss: 0.119259, acc.: 96.09%] [G loss: 3.182154]\n",
      "epoch:0 step:37 [D loss: 2.516019, acc.: 49.22%] [G loss: 6.073090]\n",
      "epoch:0 step:38 [D loss: 0.884466, acc.: 63.28%] [G loss: 3.952372]\n",
      "epoch:0 step:39 [D loss: 0.406553, acc.: 78.12%] [G loss: 1.704335]\n",
      "epoch:0 step:40 [D loss: 0.131279, acc.: 99.22%] [G loss: 1.293490]\n",
      "epoch:0 step:41 [D loss: 0.087557, acc.: 100.00%] [G loss: 1.278594]\n",
      "epoch:0 step:42 [D loss: 0.053570, acc.: 100.00%] [G loss: 1.033471]\n",
      "epoch:0 step:43 [D loss: 0.091297, acc.: 100.00%] [G loss: 1.423782]\n",
      "epoch:0 step:44 [D loss: 0.160159, acc.: 98.44%] [G loss: 3.832836]\n",
      "epoch:0 step:45 [D loss: 0.263403, acc.: 91.41%] [G loss: 4.096697]\n",
      "epoch:0 step:46 [D loss: 0.070471, acc.: 99.22%] [G loss: 3.053856]\n",
      "epoch:0 step:47 [D loss: 0.148624, acc.: 100.00%] [G loss: 5.111369]\n",
      "epoch:0 step:48 [D loss: 0.095345, acc.: 96.88%] [G loss: 5.059632]\n",
      "epoch:0 step:49 [D loss: 0.061397, acc.: 99.22%] [G loss: 3.354620]\n",
      "epoch:0 step:50 [D loss: 0.111018, acc.: 99.22%] [G loss: 5.254462]\n",
      "epoch:0 step:51 [D loss: 0.035713, acc.: 100.00%] [G loss: 5.444176]\n",
      "epoch:0 step:52 [D loss: 0.086147, acc.: 97.66%] [G loss: 2.721087]\n",
      "epoch:0 step:53 [D loss: 0.412267, acc.: 64.84%] [G loss: 7.159805]\n",
      "epoch:0 step:54 [D loss: 0.500402, acc.: 80.47%] [G loss: 4.547148]\n",
      "epoch:0 step:55 [D loss: 0.111583, acc.: 96.88%] [G loss: 4.506502]\n",
      "epoch:0 step:56 [D loss: 0.049491, acc.: 100.00%] [G loss: 4.670545]\n",
      "epoch:0 step:57 [D loss: 0.033426, acc.: 100.00%] [G loss: 4.472514]\n",
      "epoch:0 step:58 [D loss: 0.039633, acc.: 100.00%] [G loss: 4.348873]\n",
      "epoch:0 step:59 [D loss: 0.037112, acc.: 100.00%] [G loss: 4.300553]\n",
      "epoch:0 step:60 [D loss: 0.030212, acc.: 100.00%] [G loss: 5.054688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:61 [D loss: 0.032550, acc.: 100.00%] [G loss: 4.321854]\n",
      "epoch:0 step:62 [D loss: 0.041061, acc.: 100.00%] [G loss: 4.958233]\n",
      "epoch:0 step:63 [D loss: 0.016504, acc.: 100.00%] [G loss: 4.675683]\n",
      "epoch:0 step:64 [D loss: 0.040117, acc.: 100.00%] [G loss: 4.357265]\n",
      "epoch:0 step:65 [D loss: 0.152232, acc.: 100.00%] [G loss: 6.951140]\n",
      "epoch:0 step:66 [D loss: 0.116403, acc.: 93.75%] [G loss: 6.715162]\n",
      "epoch:0 step:67 [D loss: 0.047150, acc.: 99.22%] [G loss: 5.231528]\n",
      "epoch:0 step:68 [D loss: 0.250142, acc.: 89.84%] [G loss: 7.656771]\n",
      "epoch:0 step:69 [D loss: 0.196772, acc.: 92.97%] [G loss: 6.381342]\n",
      "epoch:0 step:70 [D loss: 0.096018, acc.: 96.09%] [G loss: 4.477276]\n",
      "epoch:0 step:71 [D loss: 0.098724, acc.: 100.00%] [G loss: 5.651394]\n",
      "epoch:0 step:72 [D loss: 0.012283, acc.: 100.00%] [G loss: 6.087322]\n",
      "epoch:0 step:73 [D loss: 0.024550, acc.: 100.00%] [G loss: 5.240605]\n",
      "epoch:0 step:74 [D loss: 0.054299, acc.: 100.00%] [G loss: 4.727752]\n",
      "epoch:0 step:75 [D loss: 0.040376, acc.: 99.22%] [G loss: 4.489807]\n",
      "epoch:0 step:76 [D loss: 0.114052, acc.: 98.44%] [G loss: 5.689536]\n",
      "epoch:0 step:77 [D loss: 0.084393, acc.: 98.44%] [G loss: 4.007257]\n",
      "epoch:0 step:78 [D loss: 0.295421, acc.: 84.38%] [G loss: 8.918606]\n",
      "epoch:0 step:79 [D loss: 0.721701, acc.: 76.56%] [G loss: 6.987784]\n",
      "epoch:0 step:80 [D loss: 0.083209, acc.: 98.44%] [G loss: 4.983362]\n",
      "epoch:0 step:81 [D loss: 0.060696, acc.: 99.22%] [G loss: 2.052746]\n",
      "epoch:0 step:82 [D loss: 1.282129, acc.: 57.03%] [G loss: 9.605685]\n",
      "epoch:0 step:83 [D loss: 2.132088, acc.: 59.38%] [G loss: 3.954547]\n",
      "epoch:0 step:84 [D loss: 0.656611, acc.: 75.00%] [G loss: 3.288803]\n",
      "epoch:0 step:85 [D loss: 0.132678, acc.: 98.44%] [G loss: 4.517645]\n",
      "epoch:0 step:86 [D loss: 0.046901, acc.: 100.00%] [G loss: 4.829444]\n",
      "epoch:0 step:87 [D loss: 0.053517, acc.: 99.22%] [G loss: 4.264824]\n",
      "epoch:0 step:88 [D loss: 0.089517, acc.: 99.22%] [G loss: 3.441580]\n",
      "epoch:0 step:89 [D loss: 0.124326, acc.: 96.09%] [G loss: 5.259486]\n",
      "epoch:0 step:90 [D loss: 0.113057, acc.: 96.09%] [G loss: 5.020688]\n",
      "epoch:0 step:91 [D loss: 0.099723, acc.: 96.09%] [G loss: 4.522537]\n",
      "epoch:0 step:92 [D loss: 0.057762, acc.: 98.44%] [G loss: 4.179924]\n",
      "epoch:0 step:93 [D loss: 0.048817, acc.: 100.00%] [G loss: 4.416767]\n",
      "epoch:0 step:94 [D loss: 0.038435, acc.: 100.00%] [G loss: 3.788434]\n",
      "epoch:0 step:95 [D loss: 0.094616, acc.: 100.00%] [G loss: 5.712981]\n",
      "epoch:0 step:96 [D loss: 0.116854, acc.: 96.09%] [G loss: 4.404965]\n",
      "epoch:0 step:97 [D loss: 1.117394, acc.: 48.44%] [G loss: 7.720479]\n",
      "epoch:0 step:98 [D loss: 2.222582, acc.: 50.78%] [G loss: 4.010759]\n",
      "epoch:0 step:99 [D loss: 0.544984, acc.: 78.91%] [G loss: 2.389518]\n",
      "epoch:0 step:100 [D loss: 0.156136, acc.: 99.22%] [G loss: 3.121676]\n",
      "epoch:0 step:101 [D loss: 0.066523, acc.: 99.22%] [G loss: 3.171281]\n",
      "epoch:0 step:102 [D loss: 0.098841, acc.: 98.44%] [G loss: 3.354805]\n",
      "epoch:0 step:103 [D loss: 0.083980, acc.: 100.00%] [G loss: 3.634338]\n",
      "epoch:0 step:104 [D loss: 0.113997, acc.: 98.44%] [G loss: 3.669874]\n",
      "epoch:0 step:105 [D loss: 0.101549, acc.: 99.22%] [G loss: 3.141982]\n",
      "epoch:0 step:106 [D loss: 0.218824, acc.: 92.97%] [G loss: 4.409507]\n",
      "epoch:0 step:107 [D loss: 0.103269, acc.: 95.31%] [G loss: 4.037180]\n",
      "epoch:0 step:108 [D loss: 0.341547, acc.: 88.28%] [G loss: 2.600108]\n",
      "epoch:0 step:109 [D loss: 0.136432, acc.: 96.88%] [G loss: 3.826331]\n",
      "epoch:0 step:110 [D loss: 0.096058, acc.: 99.22%] [G loss: 4.586669]\n",
      "epoch:0 step:111 [D loss: 0.121189, acc.: 97.66%] [G loss: 3.679453]\n",
      "epoch:0 step:112 [D loss: 0.120950, acc.: 99.22%] [G loss: 4.383347]\n",
      "epoch:0 step:113 [D loss: 0.270755, acc.: 94.53%] [G loss: 3.029139]\n",
      "epoch:0 step:114 [D loss: 0.038941, acc.: 100.00%] [G loss: 3.228003]\n",
      "epoch:0 step:115 [D loss: 0.038688, acc.: 99.22%] [G loss: 3.761754]\n",
      "epoch:0 step:116 [D loss: 0.065297, acc.: 99.22%] [G loss: 3.955523]\n",
      "epoch:0 step:117 [D loss: 0.275147, acc.: 91.41%] [G loss: 6.514210]\n",
      "epoch:0 step:118 [D loss: 0.618777, acc.: 70.31%] [G loss: 3.825416]\n",
      "epoch:0 step:119 [D loss: 0.326926, acc.: 78.12%] [G loss: 5.978178]\n",
      "epoch:0 step:120 [D loss: 0.057055, acc.: 99.22%] [G loss: 6.219983]\n",
      "epoch:0 step:121 [D loss: 0.133943, acc.: 93.75%] [G loss: 6.525007]\n",
      "epoch:0 step:122 [D loss: 0.068778, acc.: 98.44%] [G loss: 5.623213]\n",
      "epoch:0 step:123 [D loss: 0.072391, acc.: 98.44%] [G loss: 4.763425]\n",
      "epoch:0 step:124 [D loss: 0.042607, acc.: 100.00%] [G loss: 4.400142]\n",
      "epoch:0 step:125 [D loss: 0.088405, acc.: 100.00%] [G loss: 6.629835]\n",
      "epoch:0 step:126 [D loss: 0.114582, acc.: 95.31%] [G loss: 5.364233]\n",
      "epoch:0 step:127 [D loss: 0.092907, acc.: 96.88%] [G loss: 4.571953]\n",
      "epoch:0 step:128 [D loss: 0.068436, acc.: 99.22%] [G loss: 5.556154]\n",
      "epoch:0 step:129 [D loss: 0.063920, acc.: 100.00%] [G loss: 4.046597]\n",
      "epoch:0 step:130 [D loss: 1.929157, acc.: 49.22%] [G loss: 9.550112]\n",
      "epoch:0 step:131 [D loss: 3.264548, acc.: 50.00%] [G loss: 3.635574]\n",
      "epoch:0 step:132 [D loss: 1.054098, acc.: 53.91%] [G loss: 2.067941]\n",
      "epoch:0 step:133 [D loss: 0.287530, acc.: 92.97%] [G loss: 2.338000]\n",
      "epoch:0 step:134 [D loss: 0.172230, acc.: 99.22%] [G loss: 2.524538]\n",
      "epoch:0 step:135 [D loss: 0.174591, acc.: 96.88%] [G loss: 2.632047]\n",
      "epoch:0 step:136 [D loss: 0.108834, acc.: 98.44%] [G loss: 2.945426]\n",
      "epoch:0 step:137 [D loss: 0.126947, acc.: 100.00%] [G loss: 2.894880]\n",
      "epoch:0 step:138 [D loss: 0.076187, acc.: 100.00%] [G loss: 2.456781]\n",
      "epoch:0 step:139 [D loss: 0.202491, acc.: 94.53%] [G loss: 3.972721]\n",
      "epoch:0 step:140 [D loss: 0.126517, acc.: 96.09%] [G loss: 4.161928]\n",
      "epoch:0 step:141 [D loss: 0.147554, acc.: 96.88%] [G loss: 3.312303]\n",
      "epoch:0 step:142 [D loss: 0.124049, acc.: 99.22%] [G loss: 3.752608]\n",
      "epoch:0 step:143 [D loss: 0.091849, acc.: 98.44%] [G loss: 3.896834]\n",
      "epoch:0 step:144 [D loss: 0.215633, acc.: 96.09%] [G loss: 3.763819]\n",
      "epoch:0 step:145 [D loss: 0.085536, acc.: 99.22%] [G loss: 4.073874]\n",
      "epoch:0 step:146 [D loss: 0.085574, acc.: 99.22%] [G loss: 3.726658]\n",
      "epoch:0 step:147 [D loss: 0.130217, acc.: 100.00%] [G loss: 4.816222]\n",
      "epoch:0 step:148 [D loss: 0.277303, acc.: 89.84%] [G loss: 4.561656]\n",
      "epoch:0 step:149 [D loss: 0.165678, acc.: 96.88%] [G loss: 3.787110]\n",
      "epoch:0 step:150 [D loss: 0.068803, acc.: 99.22%] [G loss: 2.982296]\n",
      "epoch:0 step:151 [D loss: 0.195354, acc.: 95.31%] [G loss: 5.393886]\n",
      "epoch:0 step:152 [D loss: 0.166407, acc.: 94.53%] [G loss: 4.624657]\n",
      "epoch:0 step:153 [D loss: 0.068531, acc.: 97.66%] [G loss: 2.752770]\n",
      "epoch:0 step:154 [D loss: 0.163361, acc.: 92.19%] [G loss: 4.207054]\n",
      "epoch:0 step:155 [D loss: 0.206776, acc.: 90.62%] [G loss: 3.890727]\n",
      "epoch:0 step:156 [D loss: 0.124774, acc.: 99.22%] [G loss: 4.350569]\n",
      "epoch:0 step:157 [D loss: 0.111422, acc.: 97.66%] [G loss: 2.862822]\n",
      "epoch:0 step:158 [D loss: 0.289308, acc.: 89.06%] [G loss: 6.730844]\n",
      "epoch:0 step:159 [D loss: 0.687779, acc.: 71.88%] [G loss: 4.350122]\n",
      "epoch:0 step:160 [D loss: 0.059327, acc.: 99.22%] [G loss: 2.994002]\n",
      "epoch:0 step:161 [D loss: 0.037235, acc.: 100.00%] [G loss: 2.804410]\n",
      "epoch:0 step:162 [D loss: 0.320484, acc.: 85.16%] [G loss: 5.246350]\n",
      "epoch:0 step:163 [D loss: 0.067104, acc.: 98.44%] [G loss: 5.689178]\n",
      "epoch:0 step:164 [D loss: 0.147094, acc.: 93.75%] [G loss: 4.601014]\n",
      "epoch:0 step:165 [D loss: 0.439140, acc.: 75.00%] [G loss: 7.101509]\n",
      "epoch:0 step:166 [D loss: 0.453414, acc.: 72.66%] [G loss: 4.995553]\n",
      "epoch:0 step:167 [D loss: 0.071728, acc.: 99.22%] [G loss: 3.366223]\n",
      "epoch:0 step:168 [D loss: 0.086693, acc.: 96.88%] [G loss: 3.624735]\n",
      "epoch:0 step:169 [D loss: 0.059952, acc.: 100.00%] [G loss: 3.887174]\n",
      "epoch:0 step:170 [D loss: 0.088653, acc.: 100.00%] [G loss: 5.101198]\n",
      "epoch:0 step:171 [D loss: 0.046478, acc.: 99.22%] [G loss: 4.724392]\n",
      "epoch:0 step:172 [D loss: 0.196649, acc.: 96.09%] [G loss: 6.721158]\n",
      "epoch:0 step:173 [D loss: 0.226900, acc.: 87.50%] [G loss: 4.905824]\n",
      "epoch:0 step:174 [D loss: 0.401154, acc.: 84.38%] [G loss: 8.228399]\n",
      "epoch:0 step:175 [D loss: 0.770081, acc.: 67.97%] [G loss: 4.709742]\n",
      "epoch:0 step:176 [D loss: 0.147597, acc.: 96.88%] [G loss: 4.956176]\n",
      "epoch:0 step:177 [D loss: 0.020533, acc.: 100.00%] [G loss: 4.815649]\n",
      "epoch:0 step:178 [D loss: 0.336489, acc.: 85.16%] [G loss: 6.902299]\n",
      "epoch:0 step:179 [D loss: 0.516837, acc.: 74.22%] [G loss: 4.067709]\n",
      "epoch:0 step:180 [D loss: 0.102809, acc.: 99.22%] [G loss: 4.556602]\n",
      "epoch:0 step:181 [D loss: 0.042113, acc.: 100.00%] [G loss: 4.683756]\n",
      "epoch:0 step:182 [D loss: 0.039074, acc.: 99.22%] [G loss: 3.616987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:183 [D loss: 0.075122, acc.: 99.22%] [G loss: 2.863273]\n",
      "epoch:0 step:184 [D loss: 0.130491, acc.: 96.88%] [G loss: 4.953936]\n",
      "epoch:0 step:185 [D loss: 0.198770, acc.: 91.41%] [G loss: 3.244072]\n",
      "epoch:0 step:186 [D loss: 0.231380, acc.: 92.97%] [G loss: 6.257140]\n",
      "epoch:0 step:187 [D loss: 1.014951, acc.: 54.69%] [G loss: 2.828717]\n",
      "epoch:0 step:188 [D loss: 0.048003, acc.: 99.22%] [G loss: 4.090246]\n",
      "epoch:0 step:189 [D loss: 0.052829, acc.: 100.00%] [G loss: 3.275062]\n",
      "epoch:0 step:190 [D loss: 0.151636, acc.: 95.31%] [G loss: 1.439602]\n",
      "epoch:0 step:191 [D loss: 0.303601, acc.: 89.84%] [G loss: 5.756842]\n",
      "epoch:0 step:192 [D loss: 0.563518, acc.: 78.12%] [G loss: 2.644011]\n",
      "epoch:0 step:193 [D loss: 0.268356, acc.: 89.06%] [G loss: 3.834568]\n",
      "epoch:0 step:194 [D loss: 0.121962, acc.: 98.44%] [G loss: 3.821657]\n",
      "epoch:0 step:195 [D loss: 0.231395, acc.: 93.75%] [G loss: 3.271947]\n",
      "epoch:0 step:196 [D loss: 0.303338, acc.: 87.50%] [G loss: 5.878960]\n",
      "epoch:0 step:197 [D loss: 0.458395, acc.: 76.56%] [G loss: 2.513527]\n",
      "epoch:0 step:198 [D loss: 0.699550, acc.: 69.53%] [G loss: 6.736086]\n",
      "epoch:0 step:199 [D loss: 0.978107, acc.: 61.72%] [G loss: 3.258026]\n",
      "epoch:0 step:200 [D loss: 0.203034, acc.: 90.62%] [G loss: 2.741002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:41: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:201 [D loss: 0.071633, acc.: 100.00%] [G loss: 2.920048]\n",
      "epoch:0 step:202 [D loss: 0.174370, acc.: 96.88%] [G loss: 3.039822]\n",
      "epoch:0 step:203 [D loss: 0.081078, acc.: 97.66%] [G loss: 2.552064]\n",
      "epoch:0 step:204 [D loss: 0.432396, acc.: 82.03%] [G loss: 5.567200]\n",
      "epoch:0 step:205 [D loss: 0.400435, acc.: 80.47%] [G loss: 4.017076]\n",
      "epoch:0 step:206 [D loss: 0.268250, acc.: 89.84%] [G loss: 3.124529]\n",
      "epoch:0 step:207 [D loss: 0.110992, acc.: 97.66%] [G loss: 3.860549]\n",
      "epoch:0 step:208 [D loss: 0.436597, acc.: 83.59%] [G loss: 4.925383]\n",
      "epoch:0 step:209 [D loss: 0.279190, acc.: 85.94%] [G loss: 3.524256]\n",
      "epoch:0 step:210 [D loss: 0.400532, acc.: 81.25%] [G loss: 4.616728]\n",
      "epoch:0 step:211 [D loss: 0.121699, acc.: 97.66%] [G loss: 5.014831]\n",
      "epoch:0 step:212 [D loss: 0.127157, acc.: 96.09%] [G loss: 3.016573]\n",
      "epoch:0 step:213 [D loss: 0.447872, acc.: 79.69%] [G loss: 5.845903]\n",
      "epoch:0 step:214 [D loss: 0.311982, acc.: 82.81%] [G loss: 4.995003]\n",
      "epoch:0 step:215 [D loss: 0.122307, acc.: 96.09%] [G loss: 3.069873]\n",
      "epoch:0 step:216 [D loss: 0.256341, acc.: 92.19%] [G loss: 5.365542]\n",
      "epoch:0 step:217 [D loss: 0.138282, acc.: 92.97%] [G loss: 4.771447]\n",
      "epoch:0 step:218 [D loss: 0.161106, acc.: 96.88%] [G loss: 3.460388]\n",
      "epoch:0 step:219 [D loss: 0.142268, acc.: 96.88%] [G loss: 2.460940]\n",
      "epoch:0 step:220 [D loss: 0.251421, acc.: 91.41%] [G loss: 4.685126]\n",
      "epoch:0 step:221 [D loss: 0.312402, acc.: 86.72%] [G loss: 2.259090]\n",
      "epoch:0 step:222 [D loss: 0.338628, acc.: 80.47%] [G loss: 5.061527]\n",
      "epoch:0 step:223 [D loss: 0.301835, acc.: 85.16%] [G loss: 3.107317]\n",
      "epoch:0 step:224 [D loss: 0.174522, acc.: 93.75%] [G loss: 1.096965]\n",
      "epoch:0 step:225 [D loss: 0.100318, acc.: 100.00%] [G loss: 0.742725]\n",
      "epoch:0 step:226 [D loss: 0.426164, acc.: 77.34%] [G loss: 5.568569]\n",
      "epoch:0 step:227 [D loss: 0.379990, acc.: 81.25%] [G loss: 3.364058]\n",
      "epoch:0 step:228 [D loss: 0.268460, acc.: 89.84%] [G loss: 6.050337]\n",
      "epoch:0 step:229 [D loss: 0.167757, acc.: 94.53%] [G loss: 4.705218]\n",
      "epoch:0 step:230 [D loss: 0.306854, acc.: 85.16%] [G loss: 4.585648]\n",
      "epoch:0 step:231 [D loss: 0.045837, acc.: 99.22%] [G loss: 4.018332]\n",
      "epoch:0 step:232 [D loss: 0.639545, acc.: 67.19%] [G loss: 8.170748]\n",
      "epoch:0 step:233 [D loss: 1.702952, acc.: 51.56%] [G loss: 2.327306]\n",
      "epoch:0 step:234 [D loss: 0.411909, acc.: 86.72%] [G loss: 3.848686]\n",
      "epoch:0 step:235 [D loss: 0.045246, acc.: 100.00%] [G loss: 4.297225]\n",
      "epoch:0 step:236 [D loss: 0.075883, acc.: 98.44%] [G loss: 3.447311]\n",
      "epoch:0 step:237 [D loss: 0.328324, acc.: 83.59%] [G loss: 4.423501]\n",
      "epoch:0 step:238 [D loss: 0.195682, acc.: 91.41%] [G loss: 3.951696]\n",
      "epoch:0 step:239 [D loss: 0.187131, acc.: 92.97%] [G loss: 2.763973]\n",
      "epoch:0 step:240 [D loss: 0.483799, acc.: 75.00%] [G loss: 4.843701]\n",
      "epoch:0 step:241 [D loss: 0.724096, acc.: 67.19%] [G loss: 3.076234]\n",
      "epoch:0 step:242 [D loss: 0.191639, acc.: 94.53%] [G loss: 4.013843]\n",
      "epoch:0 step:243 [D loss: 0.054756, acc.: 100.00%] [G loss: 4.388356]\n",
      "epoch:0 step:244 [D loss: 0.088536, acc.: 97.66%] [G loss: 3.671153]\n",
      "epoch:0 step:245 [D loss: 0.203751, acc.: 89.84%] [G loss: 5.361897]\n",
      "epoch:0 step:246 [D loss: 0.115897, acc.: 91.41%] [G loss: 5.272566]\n",
      "epoch:0 step:247 [D loss: 0.056423, acc.: 99.22%] [G loss: 4.211722]\n",
      "epoch:0 step:248 [D loss: 0.076500, acc.: 99.22%] [G loss: 4.058764]\n",
      "epoch:0 step:249 [D loss: 0.102998, acc.: 99.22%] [G loss: 4.693639]\n",
      "epoch:0 step:250 [D loss: 0.098867, acc.: 98.44%] [G loss: 4.660519]\n",
      "epoch:0 step:251 [D loss: 0.051953, acc.: 99.22%] [G loss: 4.130068]\n",
      "epoch:0 step:252 [D loss: 0.128882, acc.: 97.66%] [G loss: 5.765141]\n",
      "epoch:0 step:253 [D loss: 0.082027, acc.: 96.88%] [G loss: 6.118098]\n",
      "epoch:0 step:254 [D loss: 0.076469, acc.: 98.44%] [G loss: 4.593674]\n",
      "epoch:0 step:255 [D loss: 0.036866, acc.: 100.00%] [G loss: 3.020051]\n",
      "epoch:0 step:256 [D loss: 0.426736, acc.: 71.88%] [G loss: 8.588142]\n",
      "epoch:0 step:257 [D loss: 0.991844, acc.: 63.28%] [G loss: 6.347498]\n",
      "epoch:0 step:258 [D loss: 0.051567, acc.: 98.44%] [G loss: 5.492502]\n",
      "epoch:0 step:259 [D loss: 0.011023, acc.: 100.00%] [G loss: 5.009422]\n",
      "epoch:0 step:260 [D loss: 0.010674, acc.: 100.00%] [G loss: 4.687563]\n",
      "epoch:0 step:261 [D loss: 0.015991, acc.: 100.00%] [G loss: 4.099582]\n",
      "epoch:0 step:262 [D loss: 0.034692, acc.: 100.00%] [G loss: 3.409397]\n",
      "epoch:0 step:263 [D loss: 0.093093, acc.: 100.00%] [G loss: 3.640393]\n",
      "epoch:0 step:264 [D loss: 0.203543, acc.: 94.53%] [G loss: 5.661306]\n",
      "epoch:0 step:265 [D loss: 0.074964, acc.: 97.66%] [G loss: 5.911218]\n",
      "epoch:0 step:266 [D loss: 0.091706, acc.: 96.88%] [G loss: 4.661990]\n",
      "epoch:0 step:267 [D loss: 0.074160, acc.: 98.44%] [G loss: 3.062541]\n",
      "epoch:0 step:268 [D loss: 0.049866, acc.: 100.00%] [G loss: 1.937722]\n",
      "epoch:0 step:269 [D loss: 0.039225, acc.: 99.22%] [G loss: 0.498877]\n",
      "epoch:0 step:270 [D loss: 0.017797, acc.: 100.00%] [G loss: 0.313044]\n",
      "epoch:0 step:271 [D loss: 0.045721, acc.: 100.00%] [G loss: 0.108524]\n",
      "epoch:0 step:272 [D loss: 0.010080, acc.: 100.00%] [G loss: 0.093983]\n",
      "epoch:0 step:273 [D loss: 0.023156, acc.: 100.00%] [G loss: 0.037632]\n",
      "epoch:0 step:274 [D loss: 0.039764, acc.: 100.00%] [G loss: 0.079770]\n",
      "epoch:0 step:275 [D loss: 0.008321, acc.: 100.00%] [G loss: 0.052760]\n",
      "epoch:0 step:276 [D loss: 0.036825, acc.: 100.00%] [G loss: 0.065942]\n",
      "epoch:0 step:277 [D loss: 0.014582, acc.: 100.00%] [G loss: 0.192511]\n",
      "epoch:0 step:278 [D loss: 0.020362, acc.: 100.00%] [G loss: 0.535781]\n",
      "epoch:0 step:279 [D loss: 0.048667, acc.: 99.22%] [G loss: 0.497915]\n",
      "epoch:0 step:280 [D loss: 1.458912, acc.: 46.09%] [G loss: 9.817686]\n",
      "epoch:0 step:281 [D loss: 2.802860, acc.: 51.56%] [G loss: 4.687243]\n",
      "epoch:0 step:282 [D loss: 1.179577, acc.: 57.81%] [G loss: 2.615946]\n",
      "epoch:0 step:283 [D loss: 0.418352, acc.: 86.72%] [G loss: 2.456626]\n",
      "epoch:0 step:284 [D loss: 0.168448, acc.: 97.66%] [G loss: 2.541592]\n",
      "epoch:0 step:285 [D loss: 0.136753, acc.: 99.22%] [G loss: 2.278555]\n",
      "epoch:0 step:286 [D loss: 0.098189, acc.: 99.22%] [G loss: 2.427392]\n",
      "epoch:0 step:287 [D loss: 0.079979, acc.: 100.00%] [G loss: 2.338117]\n",
      "epoch:0 step:288 [D loss: 0.176670, acc.: 97.66%] [G loss: 2.319349]\n",
      "epoch:0 step:289 [D loss: 0.176979, acc.: 96.88%] [G loss: 1.855335]\n",
      "epoch:0 step:290 [D loss: 0.470304, acc.: 76.56%] [G loss: 4.612730]\n",
      "epoch:0 step:291 [D loss: 0.895295, acc.: 59.38%] [G loss: 1.278741]\n",
      "epoch:0 step:292 [D loss: 0.311068, acc.: 78.91%] [G loss: 2.773779]\n",
      "epoch:0 step:293 [D loss: 0.101986, acc.: 96.88%] [G loss: 3.211230]\n",
      "epoch:0 step:294 [D loss: 0.112300, acc.: 98.44%] [G loss: 2.124929]\n",
      "epoch:0 step:295 [D loss: 0.060423, acc.: 100.00%] [G loss: 1.275409]\n",
      "epoch:0 step:296 [D loss: 0.424919, acc.: 71.09%] [G loss: 5.340334]\n",
      "epoch:0 step:297 [D loss: 0.573981, acc.: 77.34%] [G loss: 4.033950]\n",
      "epoch:0 step:298 [D loss: 0.075862, acc.: 99.22%] [G loss: 3.541309]\n",
      "epoch:0 step:299 [D loss: 0.060034, acc.: 100.00%] [G loss: 3.444532]\n",
      "epoch:0 step:300 [D loss: 0.056227, acc.: 100.00%] [G loss: 3.376418]\n",
      "epoch:0 step:301 [D loss: 0.113826, acc.: 99.22%] [G loss: 3.520141]\n",
      "epoch:0 step:302 [D loss: 0.107419, acc.: 99.22%] [G loss: 4.083947]\n",
      "epoch:0 step:303 [D loss: 0.210465, acc.: 94.53%] [G loss: 3.457449]\n",
      "epoch:0 step:304 [D loss: 0.168816, acc.: 95.31%] [G loss: 5.167357]\n",
      "epoch:0 step:305 [D loss: 0.131320, acc.: 95.31%] [G loss: 4.780924]\n",
      "epoch:0 step:306 [D loss: 0.608323, acc.: 71.88%] [G loss: 5.538729]\n",
      "epoch:0 step:307 [D loss: 0.092207, acc.: 98.44%] [G loss: 5.524653]\n",
      "epoch:0 step:308 [D loss: 0.159588, acc.: 92.97%] [G loss: 2.964556]\n",
      "epoch:0 step:309 [D loss: 0.093558, acc.: 96.88%] [G loss: 2.779025]\n",
      "epoch:0 step:310 [D loss: 0.067649, acc.: 99.22%] [G loss: 2.995645]\n",
      "epoch:0 step:311 [D loss: 0.029916, acc.: 100.00%] [G loss: 2.483852]\n",
      "epoch:0 step:312 [D loss: 0.069160, acc.: 100.00%] [G loss: 2.211970]\n",
      "epoch:0 step:313 [D loss: 0.148153, acc.: 97.66%] [G loss: 2.223663]\n",
      "epoch:0 step:314 [D loss: 0.050542, acc.: 99.22%] [G loss: 1.864420]\n",
      "epoch:0 step:315 [D loss: 0.098327, acc.: 97.66%] [G loss: 1.150193]\n",
      "epoch:0 step:316 [D loss: 0.131829, acc.: 93.75%] [G loss: 4.751348]\n",
      "epoch:0 step:317 [D loss: 0.182797, acc.: 90.62%] [G loss: 3.525737]\n",
      "epoch:0 step:318 [D loss: 0.052376, acc.: 100.00%] [G loss: 2.611537]\n",
      "epoch:0 step:319 [D loss: 0.084154, acc.: 98.44%] [G loss: 3.073996]\n",
      "epoch:0 step:320 [D loss: 0.224810, acc.: 92.97%] [G loss: 4.037663]\n",
      "epoch:0 step:321 [D loss: 0.086247, acc.: 96.88%] [G loss: 4.220229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:322 [D loss: 0.170602, acc.: 94.53%] [G loss: 3.845188]\n",
      "epoch:0 step:323 [D loss: 0.107223, acc.: 97.66%] [G loss: 4.732882]\n",
      "epoch:0 step:324 [D loss: 0.152303, acc.: 94.53%] [G loss: 4.293324]\n",
      "epoch:0 step:325 [D loss: 0.248748, acc.: 92.19%] [G loss: 9.054130]\n",
      "epoch:0 step:326 [D loss: 0.683832, acc.: 78.91%] [G loss: 3.137986]\n",
      "epoch:0 step:327 [D loss: 0.511999, acc.: 71.88%] [G loss: 6.994500]\n",
      "epoch:0 step:328 [D loss: 0.119641, acc.: 93.75%] [G loss: 7.473141]\n",
      "epoch:0 step:329 [D loss: 0.399475, acc.: 87.50%] [G loss: 4.972366]\n",
      "epoch:0 step:330 [D loss: 0.066396, acc.: 98.44%] [G loss: 4.332680]\n",
      "epoch:0 step:331 [D loss: 0.107802, acc.: 96.09%] [G loss: 6.038680]\n",
      "epoch:0 step:332 [D loss: 0.076476, acc.: 99.22%] [G loss: 5.200812]\n",
      "epoch:0 step:333 [D loss: 0.173145, acc.: 96.09%] [G loss: 6.421764]\n",
      "epoch:0 step:334 [D loss: 0.326363, acc.: 89.06%] [G loss: 3.107828]\n",
      "epoch:0 step:335 [D loss: 0.114028, acc.: 99.22%] [G loss: 5.594959]\n",
      "epoch:0 step:336 [D loss: 0.082414, acc.: 97.66%] [G loss: 5.148125]\n",
      "epoch:0 step:337 [D loss: 0.085907, acc.: 100.00%] [G loss: 3.516861]\n",
      "epoch:0 step:338 [D loss: 0.170837, acc.: 91.41%] [G loss: 6.024665]\n",
      "epoch:0 step:339 [D loss: 0.856028, acc.: 64.84%] [G loss: 0.621596]\n",
      "epoch:0 step:340 [D loss: 0.020532, acc.: 100.00%] [G loss: 1.099203]\n",
      "epoch:0 step:341 [D loss: 0.039001, acc.: 100.00%] [G loss: 0.702231]\n",
      "epoch:0 step:342 [D loss: 0.006675, acc.: 100.00%] [G loss: 0.521263]\n",
      "epoch:0 step:343 [D loss: 0.021807, acc.: 99.22%] [G loss: 0.137353]\n",
      "epoch:0 step:344 [D loss: 0.012359, acc.: 100.00%] [G loss: 0.060591]\n",
      "epoch:0 step:345 [D loss: 0.041738, acc.: 100.00%] [G loss: 0.256904]\n",
      "epoch:0 step:346 [D loss: 0.019797, acc.: 100.00%] [G loss: 0.247974]\n",
      "epoch:0 step:347 [D loss: 0.022047, acc.: 100.00%] [G loss: 0.055495]\n",
      "epoch:0 step:348 [D loss: 0.035680, acc.: 100.00%] [G loss: 0.133294]\n",
      "epoch:0 step:349 [D loss: 0.030539, acc.: 99.22%] [G loss: 0.043406]\n",
      "epoch:0 step:350 [D loss: 0.066820, acc.: 100.00%] [G loss: 1.665117]\n",
      "epoch:0 step:351 [D loss: 1.379641, acc.: 39.84%] [G loss: 9.080675]\n",
      "epoch:0 step:352 [D loss: 1.985436, acc.: 42.19%] [G loss: 4.612834]\n",
      "epoch:0 step:353 [D loss: 0.124780, acc.: 96.09%] [G loss: 2.106814]\n",
      "epoch:0 step:354 [D loss: 0.663094, acc.: 65.62%] [G loss: 3.934566]\n",
      "epoch:0 step:355 [D loss: 0.232802, acc.: 86.72%] [G loss: 3.024934]\n",
      "epoch:0 step:356 [D loss: 0.297097, acc.: 88.28%] [G loss: 1.307057]\n",
      "epoch:0 step:357 [D loss: 0.118938, acc.: 99.22%] [G loss: 0.951509]\n",
      "epoch:0 step:358 [D loss: 0.149964, acc.: 98.44%] [G loss: 1.330975]\n",
      "epoch:0 step:359 [D loss: 0.092692, acc.: 99.22%] [G loss: 0.617943]\n",
      "epoch:0 step:360 [D loss: 0.207210, acc.: 92.97%] [G loss: 1.728207]\n",
      "epoch:0 step:361 [D loss: 0.092881, acc.: 96.88%] [G loss: 1.051658]\n",
      "epoch:0 step:362 [D loss: 0.200432, acc.: 96.88%] [G loss: 0.603483]\n",
      "epoch:0 step:363 [D loss: 0.177122, acc.: 95.31%] [G loss: 1.016902]\n",
      "epoch:0 step:364 [D loss: 0.390617, acc.: 82.81%] [G loss: 6.317939]\n",
      "epoch:0 step:365 [D loss: 0.294487, acc.: 85.16%] [G loss: 4.917830]\n",
      "epoch:0 step:366 [D loss: 0.266436, acc.: 87.50%] [G loss: 3.679566]\n",
      "epoch:0 step:367 [D loss: 0.941103, acc.: 60.94%] [G loss: 6.370714]\n",
      "epoch:0 step:368 [D loss: 1.185720, acc.: 54.69%] [G loss: 3.062447]\n",
      "epoch:0 step:369 [D loss: 0.321165, acc.: 83.59%] [G loss: 3.128846]\n",
      "epoch:0 step:370 [D loss: 0.169417, acc.: 96.09%] [G loss: 3.222211]\n",
      "epoch:0 step:371 [D loss: 0.223699, acc.: 94.53%] [G loss: 4.142227]\n",
      "epoch:0 step:372 [D loss: 0.133289, acc.: 96.09%] [G loss: 3.863810]\n",
      "epoch:0 step:373 [D loss: 0.217279, acc.: 96.09%] [G loss: 3.658888]\n",
      "epoch:0 step:374 [D loss: 0.181587, acc.: 95.31%] [G loss: 3.424886]\n",
      "epoch:0 step:375 [D loss: 0.088820, acc.: 98.44%] [G loss: 2.404214]\n",
      "epoch:0 step:376 [D loss: 0.420783, acc.: 78.12%] [G loss: 6.460804]\n",
      "epoch:0 step:377 [D loss: 0.663879, acc.: 73.44%] [G loss: 2.356262]\n",
      "epoch:0 step:378 [D loss: 0.181912, acc.: 94.53%] [G loss: 1.862804]\n",
      "epoch:0 step:379 [D loss: 0.046907, acc.: 100.00%] [G loss: 2.135544]\n",
      "epoch:0 step:380 [D loss: 0.140140, acc.: 96.88%] [G loss: 2.232769]\n",
      "epoch:0 step:381 [D loss: 0.148865, acc.: 93.75%] [G loss: 0.628359]\n",
      "epoch:0 step:382 [D loss: 0.116361, acc.: 97.66%] [G loss: 1.154988]\n",
      "epoch:0 step:383 [D loss: 0.106074, acc.: 95.31%] [G loss: 0.355029]\n",
      "epoch:0 step:384 [D loss: 0.082335, acc.: 99.22%] [G loss: 1.011617]\n",
      "epoch:0 step:385 [D loss: 0.142978, acc.: 99.22%] [G loss: 2.699699]\n",
      "epoch:0 step:386 [D loss: 0.743163, acc.: 65.62%] [G loss: 6.475743]\n",
      "epoch:0 step:387 [D loss: 1.228945, acc.: 67.97%] [G loss: 1.842626]\n",
      "epoch:0 step:388 [D loss: 0.099315, acc.: 98.44%] [G loss: 1.672859]\n",
      "epoch:0 step:389 [D loss: 0.065878, acc.: 98.44%] [G loss: 1.592676]\n",
      "epoch:0 step:390 [D loss: 0.078623, acc.: 100.00%] [G loss: 1.633971]\n",
      "epoch:0 step:391 [D loss: 0.076029, acc.: 100.00%] [G loss: 1.232496]\n",
      "epoch:0 step:392 [D loss: 0.183016, acc.: 92.97%] [G loss: 4.196065]\n",
      "epoch:0 step:393 [D loss: 0.148070, acc.: 96.09%] [G loss: 2.968798]\n",
      "epoch:0 step:394 [D loss: 0.465194, acc.: 75.78%] [G loss: 4.725937]\n",
      "epoch:0 step:395 [D loss: 0.619235, acc.: 77.34%] [G loss: 1.504342]\n",
      "epoch:0 step:396 [D loss: 0.113321, acc.: 97.66%] [G loss: 1.361886]\n",
      "epoch:0 step:397 [D loss: 0.077779, acc.: 99.22%] [G loss: 1.697681]\n",
      "epoch:0 step:398 [D loss: 0.048643, acc.: 100.00%] [G loss: 1.888596]\n",
      "epoch:0 step:399 [D loss: 0.067181, acc.: 99.22%] [G loss: 1.568828]\n",
      "epoch:0 step:400 [D loss: 0.102902, acc.: 98.44%] [G loss: 1.384078]\n",
      "epoch:0 step:401 [D loss: 0.191030, acc.: 95.31%] [G loss: 1.811796]\n",
      "epoch:0 step:402 [D loss: 0.062167, acc.: 98.44%] [G loss: 2.432617]\n",
      "epoch:0 step:403 [D loss: 0.051730, acc.: 99.22%] [G loss: 1.703525]\n",
      "epoch:0 step:404 [D loss: 0.103404, acc.: 97.66%] [G loss: 3.689206]\n",
      "epoch:0 step:405 [D loss: 0.115294, acc.: 97.66%] [G loss: 3.480828]\n",
      "epoch:0 step:406 [D loss: 0.070960, acc.: 99.22%] [G loss: 3.199082]\n",
      "epoch:0 step:407 [D loss: 0.140103, acc.: 99.22%] [G loss: 5.653732]\n",
      "epoch:0 step:408 [D loss: 0.095767, acc.: 96.09%] [G loss: 5.495435]\n",
      "epoch:0 step:409 [D loss: 0.032827, acc.: 99.22%] [G loss: 4.891177]\n",
      "epoch:0 step:410 [D loss: 0.032554, acc.: 100.00%] [G loss: 4.002525]\n",
      "epoch:0 step:411 [D loss: 0.057194, acc.: 100.00%] [G loss: 3.736408]\n",
      "epoch:0 step:412 [D loss: 0.113220, acc.: 98.44%] [G loss: 5.869121]\n",
      "epoch:0 step:413 [D loss: 0.157793, acc.: 93.75%] [G loss: 3.846128]\n",
      "epoch:0 step:414 [D loss: 0.047650, acc.: 99.22%] [G loss: 3.782715]\n",
      "epoch:0 step:415 [D loss: 0.034106, acc.: 100.00%] [G loss: 3.294093]\n",
      "epoch:0 step:416 [D loss: 0.042123, acc.: 100.00%] [G loss: 2.648834]\n",
      "epoch:0 step:417 [D loss: 0.246170, acc.: 88.28%] [G loss: 7.729285]\n",
      "epoch:0 step:418 [D loss: 0.523904, acc.: 75.78%] [G loss: 4.678456]\n",
      "epoch:0 step:419 [D loss: 0.021719, acc.: 100.00%] [G loss: 1.470761]\n",
      "epoch:0 step:420 [D loss: 0.369238, acc.: 82.81%] [G loss: 6.266101]\n",
      "epoch:0 step:421 [D loss: 0.052716, acc.: 99.22%] [G loss: 7.163428]\n",
      "epoch:0 step:422 [D loss: 0.201775, acc.: 91.41%] [G loss: 3.190766]\n",
      "epoch:0 step:423 [D loss: 0.834785, acc.: 62.50%] [G loss: 7.700686]\n",
      "epoch:0 step:424 [D loss: 0.919708, acc.: 58.59%] [G loss: 3.685837]\n",
      "epoch:0 step:425 [D loss: 0.113169, acc.: 95.31%] [G loss: 3.368556]\n",
      "epoch:0 step:426 [D loss: 0.056769, acc.: 100.00%] [G loss: 3.403437]\n",
      "epoch:0 step:427 [D loss: 0.099284, acc.: 98.44%] [G loss: 4.051060]\n",
      "epoch:0 step:428 [D loss: 0.044297, acc.: 100.00%] [G loss: 3.822671]\n",
      "epoch:0 step:429 [D loss: 0.169088, acc.: 97.66%] [G loss: 4.369331]\n",
      "epoch:0 step:430 [D loss: 0.175924, acc.: 96.88%] [G loss: 4.516963]\n",
      "epoch:0 step:431 [D loss: 0.182292, acc.: 95.31%] [G loss: 4.301947]\n",
      "epoch:0 step:432 [D loss: 0.087376, acc.: 99.22%] [G loss: 3.626262]\n",
      "epoch:0 step:433 [D loss: 0.339711, acc.: 90.62%] [G loss: 6.975982]\n",
      "epoch:0 step:434 [D loss: 0.341366, acc.: 82.03%] [G loss: 6.113038]\n",
      "epoch:0 step:435 [D loss: 0.032391, acc.: 99.22%] [G loss: 5.038994]\n",
      "epoch:0 step:436 [D loss: 0.031333, acc.: 99.22%] [G loss: 4.466483]\n",
      "epoch:0 step:437 [D loss: 0.051962, acc.: 100.00%] [G loss: 3.648857]\n",
      "epoch:0 step:438 [D loss: 0.149446, acc.: 96.88%] [G loss: 4.849862]\n",
      "epoch:0 step:439 [D loss: 0.132104, acc.: 96.88%] [G loss: 4.377922]\n",
      "epoch:0 step:440 [D loss: 0.550128, acc.: 69.53%] [G loss: 8.742030]\n",
      "epoch:0 step:441 [D loss: 1.700760, acc.: 50.78%] [G loss: 4.193002]\n",
      "epoch:0 step:442 [D loss: 0.063162, acc.: 100.00%] [G loss: 3.095539]\n",
      "epoch:0 step:443 [D loss: 0.115895, acc.: 99.22%] [G loss: 3.844044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:444 [D loss: 0.054886, acc.: 100.00%] [G loss: 3.878782]\n",
      "epoch:0 step:445 [D loss: 0.092133, acc.: 100.00%] [G loss: 3.519429]\n",
      "epoch:0 step:446 [D loss: 0.452566, acc.: 75.00%] [G loss: 5.350393]\n",
      "epoch:0 step:447 [D loss: 0.368880, acc.: 82.81%] [G loss: 3.880275]\n",
      "epoch:0 step:448 [D loss: 0.195948, acc.: 96.88%] [G loss: 3.404858]\n",
      "epoch:0 step:449 [D loss: 0.298384, acc.: 85.16%] [G loss: 4.990879]\n",
      "epoch:0 step:450 [D loss: 0.161585, acc.: 94.53%] [G loss: 4.423162]\n",
      "epoch:0 step:451 [D loss: 0.229884, acc.: 92.19%] [G loss: 4.836423]\n",
      "epoch:0 step:452 [D loss: 0.139090, acc.: 96.88%] [G loss: 4.152638]\n",
      "epoch:0 step:453 [D loss: 0.282928, acc.: 89.84%] [G loss: 5.949427]\n",
      "epoch:0 step:454 [D loss: 0.080058, acc.: 96.88%] [G loss: 6.430911]\n",
      "epoch:0 step:455 [D loss: 0.169489, acc.: 94.53%] [G loss: 5.821420]\n",
      "epoch:0 step:456 [D loss: 0.055913, acc.: 100.00%] [G loss: 4.653841]\n",
      "epoch:0 step:457 [D loss: 0.302804, acc.: 89.06%] [G loss: 8.091278]\n",
      "epoch:0 step:458 [D loss: 0.842274, acc.: 71.88%] [G loss: 3.425786]\n",
      "epoch:0 step:459 [D loss: 0.329911, acc.: 84.38%] [G loss: 4.496147]\n",
      "epoch:0 step:460 [D loss: 0.070414, acc.: 99.22%] [G loss: 3.962696]\n",
      "epoch:0 step:461 [D loss: 0.694470, acc.: 69.53%] [G loss: 6.342132]\n",
      "epoch:0 step:462 [D loss: 1.094553, acc.: 62.50%] [G loss: 2.865661]\n",
      "epoch:0 step:463 [D loss: 0.140564, acc.: 98.44%] [G loss: 2.104753]\n",
      "epoch:0 step:464 [D loss: 0.106016, acc.: 99.22%] [G loss: 2.377408]\n",
      "epoch:0 step:465 [D loss: 0.090449, acc.: 99.22%] [G loss: 2.162966]\n",
      "epoch:0 step:466 [D loss: 0.229456, acc.: 93.75%] [G loss: 3.757340]\n",
      "epoch:0 step:467 [D loss: 0.138153, acc.: 95.31%] [G loss: 3.818901]\n",
      "epoch:0 step:468 [D loss: 0.162902, acc.: 95.31%] [G loss: 3.544601]\n",
      "epoch:0 step:469 [D loss: 0.304688, acc.: 85.16%] [G loss: 4.421941]\n",
      "epoch:0 step:470 [D loss: 0.106947, acc.: 96.09%] [G loss: 4.797080]\n",
      "epoch:0 step:471 [D loss: 0.082759, acc.: 96.88%] [G loss: 3.371032]\n",
      "epoch:0 step:472 [D loss: 0.076653, acc.: 99.22%] [G loss: 3.014215]\n",
      "epoch:0 step:473 [D loss: 0.047193, acc.: 100.00%] [G loss: 2.438046]\n",
      "epoch:0 step:474 [D loss: 0.135686, acc.: 95.31%] [G loss: 1.487243]\n",
      "epoch:0 step:475 [D loss: 0.105753, acc.: 98.44%] [G loss: 0.968462]\n",
      "epoch:0 step:476 [D loss: 0.073034, acc.: 100.00%] [G loss: 0.532495]\n",
      "epoch:0 step:477 [D loss: 0.027512, acc.: 100.00%] [G loss: 0.261209]\n",
      "epoch:0 step:478 [D loss: 0.068118, acc.: 100.00%] [G loss: 0.428107]\n",
      "epoch:0 step:479 [D loss: 0.014017, acc.: 100.00%] [G loss: 0.640250]\n",
      "epoch:0 step:480 [D loss: 0.026782, acc.: 99.22%] [G loss: 0.195126]\n",
      "epoch:0 step:481 [D loss: 0.121876, acc.: 96.88%] [G loss: 0.642463]\n",
      "epoch:0 step:482 [D loss: 0.069553, acc.: 97.66%] [G loss: 0.570288]\n",
      "epoch:0 step:483 [D loss: 0.098350, acc.: 97.66%] [G loss: 0.240770]\n",
      "epoch:0 step:484 [D loss: 0.019649, acc.: 100.00%] [G loss: 1.257148]\n",
      "epoch:0 step:485 [D loss: 0.042757, acc.: 100.00%] [G loss: 0.452348]\n",
      "epoch:0 step:486 [D loss: 0.103853, acc.: 98.44%] [G loss: 2.536718]\n",
      "epoch:0 step:487 [D loss: 0.215192, acc.: 92.97%] [G loss: 2.694971]\n",
      "epoch:0 step:488 [D loss: 0.078657, acc.: 99.22%] [G loss: 4.359829]\n",
      "epoch:0 step:489 [D loss: 0.631061, acc.: 67.97%] [G loss: 7.728627]\n",
      "epoch:0 step:490 [D loss: 2.060360, acc.: 44.53%] [G loss: 2.174774]\n",
      "epoch:0 step:491 [D loss: 0.276052, acc.: 89.84%] [G loss: 3.835050]\n",
      "epoch:0 step:492 [D loss: 0.114154, acc.: 95.31%] [G loss: 3.016947]\n",
      "epoch:0 step:493 [D loss: 0.161787, acc.: 95.31%] [G loss: 2.287642]\n",
      "epoch:0 step:494 [D loss: 0.104410, acc.: 98.44%] [G loss: 2.906005]\n",
      "epoch:0 step:495 [D loss: 0.085233, acc.: 98.44%] [G loss: 2.252916]\n",
      "epoch:0 step:496 [D loss: 0.180254, acc.: 97.66%] [G loss: 1.354073]\n",
      "epoch:0 step:497 [D loss: 0.279638, acc.: 89.06%] [G loss: 4.024344]\n",
      "epoch:0 step:498 [D loss: 0.235777, acc.: 87.50%] [G loss: 3.263932]\n",
      "epoch:0 step:499 [D loss: 0.115972, acc.: 97.66%] [G loss: 2.827065]\n",
      "epoch:0 step:500 [D loss: 0.077158, acc.: 97.66%] [G loss: 2.016888]\n",
      "epoch:0 step:501 [D loss: 0.057252, acc.: 100.00%] [G loss: 1.779022]\n",
      "epoch:0 step:502 [D loss: 0.061115, acc.: 100.00%] [G loss: 1.521077]\n",
      "epoch:0 step:503 [D loss: 0.059251, acc.: 99.22%] [G loss: 0.652494]\n",
      "epoch:0 step:504 [D loss: 0.096644, acc.: 98.44%] [G loss: 0.481993]\n",
      "epoch:0 step:505 [D loss: 0.025245, acc.: 100.00%] [G loss: 0.437412]\n",
      "epoch:0 step:506 [D loss: 0.142517, acc.: 97.66%] [G loss: 5.632448]\n",
      "epoch:0 step:507 [D loss: 0.263487, acc.: 85.94%] [G loss: 1.255749]\n",
      "epoch:0 step:508 [D loss: 0.230844, acc.: 89.06%] [G loss: 4.955477]\n",
      "epoch:0 step:509 [D loss: 0.226728, acc.: 91.41%] [G loss: 4.168839]\n",
      "epoch:0 step:510 [D loss: 0.179207, acc.: 92.97%] [G loss: 1.686648]\n",
      "epoch:0 step:511 [D loss: 0.280151, acc.: 89.06%] [G loss: 4.177133]\n",
      "epoch:0 step:512 [D loss: 0.090785, acc.: 97.66%] [G loss: 2.488921]\n",
      "epoch:0 step:513 [D loss: 0.655571, acc.: 75.78%] [G loss: 7.545625]\n",
      "epoch:0 step:514 [D loss: 0.871246, acc.: 77.34%] [G loss: 3.159653]\n",
      "epoch:0 step:515 [D loss: 0.161652, acc.: 93.75%] [G loss: 3.112249]\n",
      "epoch:0 step:516 [D loss: 0.018084, acc.: 100.00%] [G loss: 2.814704]\n",
      "epoch:0 step:517 [D loss: 0.069564, acc.: 98.44%] [G loss: 2.433139]\n",
      "epoch:0 step:518 [D loss: 0.079608, acc.: 99.22%] [G loss: 1.359033]\n",
      "epoch:0 step:519 [D loss: 0.311344, acc.: 88.28%] [G loss: 4.877753]\n",
      "epoch:0 step:520 [D loss: 0.101402, acc.: 95.31%] [G loss: 4.344363]\n",
      "epoch:0 step:521 [D loss: 0.098990, acc.: 97.66%] [G loss: 3.101405]\n",
      "epoch:0 step:522 [D loss: 0.196010, acc.: 96.09%] [G loss: 4.526878]\n",
      "epoch:0 step:523 [D loss: 0.082714, acc.: 98.44%] [G loss: 3.906536]\n",
      "epoch:0 step:524 [D loss: 0.064331, acc.: 100.00%] [G loss: 3.243630]\n",
      "epoch:0 step:525 [D loss: 0.093706, acc.: 98.44%] [G loss: 5.560529]\n",
      "epoch:0 step:526 [D loss: 0.105228, acc.: 97.66%] [G loss: 3.697260]\n",
      "epoch:0 step:527 [D loss: 0.092366, acc.: 96.09%] [G loss: 5.106094]\n",
      "epoch:0 step:528 [D loss: 0.112115, acc.: 95.31%] [G loss: 3.658387]\n",
      "epoch:0 step:529 [D loss: 0.064863, acc.: 100.00%] [G loss: 4.820088]\n",
      "epoch:0 step:530 [D loss: 0.053579, acc.: 99.22%] [G loss: 3.883109]\n",
      "epoch:0 step:531 [D loss: 0.067993, acc.: 100.00%] [G loss: 4.979488]\n",
      "epoch:0 step:532 [D loss: 0.300865, acc.: 89.84%] [G loss: 9.003456]\n",
      "epoch:0 step:533 [D loss: 0.214787, acc.: 93.75%] [G loss: 7.681347]\n",
      "epoch:0 step:534 [D loss: 0.031069, acc.: 99.22%] [G loss: 6.102658]\n",
      "epoch:0 step:535 [D loss: 0.043126, acc.: 99.22%] [G loss: 5.710563]\n",
      "epoch:0 step:536 [D loss: 0.026430, acc.: 100.00%] [G loss: 4.708202]\n",
      "epoch:0 step:537 [D loss: 0.900254, acc.: 60.94%] [G loss: 10.444036]\n",
      "epoch:0 step:538 [D loss: 2.037931, acc.: 50.78%] [G loss: 4.428920]\n",
      "epoch:0 step:539 [D loss: 0.077895, acc.: 98.44%] [G loss: 3.152160]\n",
      "epoch:0 step:540 [D loss: 0.105764, acc.: 98.44%] [G loss: 2.617064]\n",
      "epoch:0 step:541 [D loss: 0.108946, acc.: 98.44%] [G loss: 3.654353]\n",
      "epoch:0 step:542 [D loss: 0.058336, acc.: 100.00%] [G loss: 3.790391]\n",
      "epoch:0 step:543 [D loss: 0.105312, acc.: 99.22%] [G loss: 2.109386]\n",
      "epoch:0 step:544 [D loss: 0.118767, acc.: 99.22%] [G loss: 2.915223]\n",
      "epoch:0 step:545 [D loss: 0.236000, acc.: 93.75%] [G loss: 3.415248]\n",
      "epoch:0 step:546 [D loss: 0.354849, acc.: 82.81%] [G loss: 1.108878]\n",
      "epoch:0 step:547 [D loss: 0.240451, acc.: 90.62%] [G loss: 3.763684]\n",
      "epoch:0 step:548 [D loss: 0.149703, acc.: 95.31%] [G loss: 3.366818]\n",
      "epoch:0 step:549 [D loss: 0.181160, acc.: 95.31%] [G loss: 2.954977]\n",
      "epoch:0 step:550 [D loss: 0.284553, acc.: 87.50%] [G loss: 4.459561]\n",
      "epoch:0 step:551 [D loss: 0.201806, acc.: 95.31%] [G loss: 4.774946]\n",
      "epoch:0 step:552 [D loss: 0.176135, acc.: 94.53%] [G loss: 3.070520]\n",
      "epoch:0 step:553 [D loss: 0.139834, acc.: 96.88%] [G loss: 4.460857]\n",
      "epoch:0 step:554 [D loss: 0.099430, acc.: 97.66%] [G loss: 4.184573]\n",
      "epoch:0 step:555 [D loss: 0.080236, acc.: 100.00%] [G loss: 3.457424]\n",
      "epoch:0 step:556 [D loss: 0.266540, acc.: 85.94%] [G loss: 6.410387]\n",
      "epoch:0 step:557 [D loss: 0.760366, acc.: 59.38%] [G loss: 2.558165]\n",
      "epoch:0 step:558 [D loss: 0.198922, acc.: 92.97%] [G loss: 5.420980]\n",
      "epoch:0 step:559 [D loss: 0.100343, acc.: 98.44%] [G loss: 4.772260]\n",
      "epoch:0 step:560 [D loss: 0.153045, acc.: 94.53%] [G loss: 3.912998]\n",
      "epoch:0 step:561 [D loss: 0.373386, acc.: 84.38%] [G loss: 6.430594]\n",
      "epoch:0 step:562 [D loss: 0.631224, acc.: 71.88%] [G loss: 3.234153]\n",
      "epoch:0 step:563 [D loss: 0.227428, acc.: 89.84%] [G loss: 5.548824]\n",
      "epoch:0 step:564 [D loss: 0.096109, acc.: 96.88%] [G loss: 5.032069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:565 [D loss: 0.083342, acc.: 99.22%] [G loss: 4.628212]\n",
      "epoch:0 step:566 [D loss: 0.192986, acc.: 94.53%] [G loss: 5.758165]\n",
      "epoch:0 step:567 [D loss: 0.189187, acc.: 92.19%] [G loss: 4.801458]\n",
      "epoch:0 step:568 [D loss: 0.109278, acc.: 100.00%] [G loss: 5.491735]\n",
      "epoch:0 step:569 [D loss: 0.120794, acc.: 97.66%] [G loss: 3.594989]\n",
      "epoch:0 step:570 [D loss: 0.331707, acc.: 83.59%] [G loss: 8.940877]\n",
      "epoch:0 step:571 [D loss: 1.168166, acc.: 57.81%] [G loss: 3.790070]\n",
      "epoch:0 step:572 [D loss: 0.070015, acc.: 96.88%] [G loss: 2.433923]\n",
      "epoch:0 step:573 [D loss: 0.133887, acc.: 94.53%] [G loss: 3.137884]\n",
      "epoch:0 step:574 [D loss: 0.029669, acc.: 100.00%] [G loss: 3.771735]\n",
      "epoch:0 step:575 [D loss: 0.155954, acc.: 96.09%] [G loss: 2.402558]\n",
      "epoch:0 step:576 [D loss: 0.120386, acc.: 96.88%] [G loss: 3.038908]\n",
      "epoch:0 step:577 [D loss: 0.226932, acc.: 93.75%] [G loss: 3.566009]\n",
      "epoch:0 step:578 [D loss: 0.100126, acc.: 99.22%] [G loss: 3.399413]\n",
      "epoch:0 step:579 [D loss: 0.273103, acc.: 89.06%] [G loss: 5.322248]\n",
      "epoch:0 step:580 [D loss: 0.428426, acc.: 82.03%] [G loss: 2.976805]\n",
      "epoch:0 step:581 [D loss: 0.061184, acc.: 99.22%] [G loss: 3.252980]\n",
      "epoch:0 step:582 [D loss: 0.080746, acc.: 98.44%] [G loss: 4.671557]\n",
      "epoch:0 step:583 [D loss: 0.048217, acc.: 100.00%] [G loss: 4.768475]\n",
      "epoch:0 step:584 [D loss: 0.114143, acc.: 99.22%] [G loss: 6.054536]\n",
      "epoch:0 step:585 [D loss: 0.193981, acc.: 91.41%] [G loss: 4.667595]\n",
      "epoch:0 step:586 [D loss: 0.121377, acc.: 95.31%] [G loss: 7.015803]\n",
      "epoch:0 step:587 [D loss: 0.030077, acc.: 99.22%] [G loss: 7.648234]\n",
      "epoch:0 step:588 [D loss: 0.083320, acc.: 97.66%] [G loss: 5.898905]\n",
      "epoch:0 step:589 [D loss: 0.111333, acc.: 96.88%] [G loss: 6.804253]\n",
      "epoch:0 step:590 [D loss: 0.010416, acc.: 100.00%] [G loss: 6.840764]\n",
      "epoch:0 step:591 [D loss: 0.042182, acc.: 100.00%] [G loss: 4.055627]\n",
      "epoch:0 step:592 [D loss: 0.650553, acc.: 77.34%] [G loss: 10.196988]\n",
      "epoch:0 step:593 [D loss: 1.563138, acc.: 53.91%] [G loss: 6.362996]\n",
      "epoch:0 step:594 [D loss: 0.007029, acc.: 100.00%] [G loss: 4.381924]\n",
      "epoch:0 step:595 [D loss: 0.023624, acc.: 100.00%] [G loss: 2.952813]\n",
      "epoch:0 step:596 [D loss: 0.102297, acc.: 96.88%] [G loss: 2.838419]\n",
      "epoch:0 step:597 [D loss: 0.039957, acc.: 100.00%] [G loss: 1.964080]\n",
      "epoch:0 step:598 [D loss: 0.044987, acc.: 100.00%] [G loss: 1.307070]\n",
      "epoch:0 step:599 [D loss: 0.250199, acc.: 85.94%] [G loss: 4.841910]\n",
      "epoch:0 step:600 [D loss: 0.124003, acc.: 95.31%] [G loss: 4.421392]\n",
      "epoch:0 step:601 [D loss: 0.129405, acc.: 93.75%] [G loss: 0.641242]\n",
      "epoch:0 step:602 [D loss: 0.030054, acc.: 100.00%] [G loss: 0.035294]\n",
      "epoch:0 step:603 [D loss: 0.018852, acc.: 100.00%] [G loss: 0.132967]\n",
      "epoch:0 step:604 [D loss: 0.072794, acc.: 99.22%] [G loss: 0.438573]\n",
      "epoch:0 step:605 [D loss: 0.034605, acc.: 100.00%] [G loss: 0.589920]\n",
      "epoch:0 step:606 [D loss: 0.023333, acc.: 100.00%] [G loss: 0.118131]\n",
      "epoch:0 step:607 [D loss: 0.094214, acc.: 100.00%] [G loss: 0.720425]\n",
      "epoch:0 step:608 [D loss: 0.008616, acc.: 100.00%] [G loss: 0.848168]\n",
      "epoch:0 step:609 [D loss: 0.147491, acc.: 94.53%] [G loss: 0.680759]\n",
      "epoch:0 step:610 [D loss: 0.023706, acc.: 100.00%] [G loss: 0.569550]\n",
      "epoch:0 step:611 [D loss: 0.155937, acc.: 96.09%] [G loss: 1.300977]\n",
      "epoch:0 step:612 [D loss: 0.045657, acc.: 100.00%] [G loss: 1.465807]\n",
      "epoch:0 step:613 [D loss: 0.661281, acc.: 57.03%] [G loss: 9.399133]\n",
      "epoch:0 step:614 [D loss: 1.440247, acc.: 57.81%] [G loss: 4.643257]\n",
      "epoch:0 step:615 [D loss: 0.230648, acc.: 90.62%] [G loss: 2.412648]\n",
      "epoch:0 step:616 [D loss: 0.069454, acc.: 98.44%] [G loss: 2.339640]\n",
      "epoch:0 step:617 [D loss: 0.188767, acc.: 95.31%] [G loss: 3.532949]\n",
      "epoch:0 step:618 [D loss: 0.068804, acc.: 100.00%] [G loss: 4.570349]\n",
      "epoch:0 step:619 [D loss: 0.121201, acc.: 96.88%] [G loss: 2.499555]\n",
      "epoch:0 step:620 [D loss: 0.635364, acc.: 65.62%] [G loss: 6.075915]\n",
      "epoch:0 step:621 [D loss: 0.964673, acc.: 64.06%] [G loss: 4.760550]\n",
      "epoch:0 step:622 [D loss: 0.153644, acc.: 95.31%] [G loss: 3.276898]\n",
      "epoch:0 step:623 [D loss: 0.189437, acc.: 95.31%] [G loss: 3.850804]\n",
      "epoch:0 step:624 [D loss: 0.105268, acc.: 98.44%] [G loss: 4.749690]\n",
      "epoch:0 step:625 [D loss: 0.085851, acc.: 99.22%] [G loss: 3.561603]\n",
      "epoch:0 step:626 [D loss: 0.113242, acc.: 98.44%] [G loss: 3.210924]\n",
      "epoch:0 step:627 [D loss: 0.123497, acc.: 99.22%] [G loss: 3.065637]\n",
      "epoch:0 step:628 [D loss: 0.185488, acc.: 95.31%] [G loss: 3.306264]\n",
      "epoch:0 step:629 [D loss: 0.310171, acc.: 87.50%] [G loss: 3.562551]\n",
      "epoch:0 step:630 [D loss: 0.034261, acc.: 100.00%] [G loss: 3.631522]\n",
      "epoch:0 step:631 [D loss: 0.233846, acc.: 89.84%] [G loss: 0.956133]\n",
      "epoch:0 step:632 [D loss: 0.092357, acc.: 98.44%] [G loss: 1.323076]\n",
      "epoch:0 step:633 [D loss: 0.068632, acc.: 98.44%] [G loss: 0.311677]\n",
      "epoch:0 step:634 [D loss: 0.176757, acc.: 92.19%] [G loss: 4.448737]\n",
      "epoch:0 step:635 [D loss: 0.569790, acc.: 74.22%] [G loss: 0.139888]\n",
      "epoch:0 step:636 [D loss: 0.385664, acc.: 75.78%] [G loss: 5.406939]\n",
      "epoch:0 step:637 [D loss: 0.252314, acc.: 86.72%] [G loss: 5.117331]\n",
      "epoch:0 step:638 [D loss: 0.197385, acc.: 92.19%] [G loss: 1.228071]\n",
      "epoch:0 step:639 [D loss: 0.106920, acc.: 98.44%] [G loss: 1.330866]\n",
      "epoch:0 step:640 [D loss: 0.094745, acc.: 99.22%] [G loss: 2.127082]\n",
      "epoch:0 step:641 [D loss: 0.035303, acc.: 100.00%] [G loss: 2.404811]\n",
      "epoch:0 step:642 [D loss: 0.377725, acc.: 85.16%] [G loss: 5.468725]\n",
      "epoch:0 step:643 [D loss: 0.569015, acc.: 75.78%] [G loss: 1.210580]\n",
      "epoch:0 step:644 [D loss: 0.449374, acc.: 75.00%] [G loss: 5.542390]\n",
      "epoch:0 step:645 [D loss: 0.146514, acc.: 95.31%] [G loss: 5.475782]\n",
      "epoch:0 step:646 [D loss: 0.279953, acc.: 88.28%] [G loss: 2.349064]\n",
      "epoch:0 step:647 [D loss: 0.526069, acc.: 75.78%] [G loss: 4.876106]\n",
      "epoch:0 step:648 [D loss: 0.260444, acc.: 89.06%] [G loss: 4.520391]\n",
      "epoch:0 step:649 [D loss: 0.496352, acc.: 78.91%] [G loss: 2.659161]\n",
      "epoch:0 step:650 [D loss: 0.215924, acc.: 91.41%] [G loss: 3.823050]\n",
      "epoch:0 step:651 [D loss: 0.118385, acc.: 96.09%] [G loss: 3.241123]\n",
      "epoch:0 step:652 [D loss: 0.256600, acc.: 90.62%] [G loss: 3.659714]\n",
      "epoch:0 step:653 [D loss: 0.069707, acc.: 98.44%] [G loss: 4.082844]\n",
      "epoch:0 step:654 [D loss: 0.244519, acc.: 91.41%] [G loss: 5.768765]\n",
      "epoch:0 step:655 [D loss: 0.241473, acc.: 89.06%] [G loss: 3.906382]\n",
      "epoch:0 step:656 [D loss: 0.142319, acc.: 95.31%] [G loss: 5.247322]\n",
      "epoch:0 step:657 [D loss: 0.117279, acc.: 96.09%] [G loss: 4.948518]\n",
      "epoch:0 step:658 [D loss: 0.117945, acc.: 98.44%] [G loss: 4.653159]\n",
      "epoch:0 step:659 [D loss: 0.073969, acc.: 98.44%] [G loss: 3.818949]\n",
      "epoch:0 step:660 [D loss: 0.223995, acc.: 91.41%] [G loss: 7.044291]\n",
      "epoch:0 step:661 [D loss: 0.215413, acc.: 90.62%] [G loss: 5.853802]\n",
      "epoch:0 step:662 [D loss: 0.054206, acc.: 100.00%] [G loss: 4.872558]\n",
      "epoch:0 step:663 [D loss: 0.104803, acc.: 95.31%] [G loss: 7.269979]\n",
      "epoch:0 step:664 [D loss: 0.083164, acc.: 98.44%] [G loss: 6.426048]\n",
      "epoch:0 step:665 [D loss: 0.099704, acc.: 99.22%] [G loss: 5.927149]\n",
      "epoch:0 step:666 [D loss: 0.062687, acc.: 99.22%] [G loss: 6.043893]\n",
      "epoch:0 step:667 [D loss: 0.037096, acc.: 99.22%] [G loss: 5.425624]\n",
      "epoch:0 step:668 [D loss: 0.328731, acc.: 85.16%] [G loss: 10.386894]\n",
      "epoch:0 step:669 [D loss: 0.912866, acc.: 67.19%] [G loss: 4.509548]\n",
      "epoch:0 step:670 [D loss: 0.313841, acc.: 89.84%] [G loss: 5.230002]\n",
      "epoch:0 step:671 [D loss: 0.027342, acc.: 100.00%] [G loss: 5.467475]\n",
      "epoch:0 step:672 [D loss: 0.295465, acc.: 88.28%] [G loss: 5.282077]\n",
      "epoch:0 step:673 [D loss: 0.206855, acc.: 92.97%] [G loss: 4.047624]\n",
      "epoch:0 step:674 [D loss: 0.302818, acc.: 87.50%] [G loss: 4.514256]\n",
      "epoch:0 step:675 [D loss: 0.219975, acc.: 90.62%] [G loss: 3.003523]\n",
      "epoch:0 step:676 [D loss: 0.087839, acc.: 100.00%] [G loss: 2.611165]\n",
      "epoch:0 step:677 [D loss: 0.049991, acc.: 99.22%] [G loss: 2.589910]\n",
      "epoch:0 step:678 [D loss: 0.040393, acc.: 100.00%] [G loss: 2.404319]\n",
      "epoch:0 step:679 [D loss: 0.057187, acc.: 98.44%] [G loss: 1.371024]\n",
      "epoch:0 step:680 [D loss: 0.105889, acc.: 98.44%] [G loss: 2.468753]\n",
      "epoch:0 step:681 [D loss: 0.037386, acc.: 100.00%] [G loss: 2.146788]\n",
      "epoch:0 step:682 [D loss: 0.210908, acc.: 91.41%] [G loss: 1.858241]\n",
      "epoch:0 step:683 [D loss: 0.055444, acc.: 100.00%] [G loss: 1.576626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:684 [D loss: 0.251001, acc.: 90.62%] [G loss: 4.534317]\n",
      "epoch:0 step:685 [D loss: 0.427367, acc.: 81.25%] [G loss: 0.393509]\n",
      "epoch:0 step:686 [D loss: 1.446852, acc.: 58.59%] [G loss: 7.920042]\n",
      "epoch:0 step:687 [D loss: 1.017020, acc.: 65.62%] [G loss: 2.871424]\n",
      "epoch:0 step:688 [D loss: 0.201951, acc.: 93.75%] [G loss: 4.387515]\n",
      "epoch:0 step:689 [D loss: 0.075910, acc.: 97.66%] [G loss: 5.044633]\n",
      "epoch:0 step:690 [D loss: 0.149433, acc.: 95.31%] [G loss: 3.507978]\n",
      "epoch:0 step:691 [D loss: 0.078353, acc.: 98.44%] [G loss: 3.611836]\n",
      "epoch:0 step:692 [D loss: 0.260738, acc.: 90.62%] [G loss: 4.382777]\n",
      "epoch:0 step:693 [D loss: 0.130738, acc.: 96.09%] [G loss: 4.982692]\n",
      "epoch:0 step:694 [D loss: 0.231546, acc.: 90.62%] [G loss: 3.276031]\n",
      "epoch:0 step:695 [D loss: 0.340482, acc.: 76.56%] [G loss: 6.567455]\n",
      "epoch:0 step:696 [D loss: 0.268206, acc.: 85.94%] [G loss: 5.996731]\n",
      "epoch:0 step:697 [D loss: 0.218141, acc.: 92.19%] [G loss: 4.095828]\n",
      "epoch:0 step:698 [D loss: 0.099387, acc.: 95.31%] [G loss: 4.076416]\n",
      "epoch:0 step:699 [D loss: 0.051355, acc.: 100.00%] [G loss: 4.590416]\n",
      "epoch:0 step:700 [D loss: 0.070549, acc.: 100.00%] [G loss: 4.249881]\n",
      "epoch:0 step:701 [D loss: 0.038549, acc.: 100.00%] [G loss: 3.450264]\n",
      "epoch:0 step:702 [D loss: 0.115113, acc.: 96.88%] [G loss: 3.558124]\n",
      "epoch:0 step:703 [D loss: 0.120166, acc.: 98.44%] [G loss: 4.671195]\n",
      "epoch:0 step:704 [D loss: 0.127378, acc.: 96.88%] [G loss: 3.602642]\n",
      "epoch:0 step:705 [D loss: 0.174709, acc.: 95.31%] [G loss: 5.088923]\n",
      "epoch:0 step:706 [D loss: 0.099308, acc.: 96.88%] [G loss: 3.851355]\n",
      "epoch:0 step:707 [D loss: 0.044867, acc.: 100.00%] [G loss: 0.864820]\n",
      "epoch:0 step:708 [D loss: 0.041025, acc.: 100.00%] [G loss: 0.629547]\n",
      "epoch:0 step:709 [D loss: 0.084153, acc.: 100.00%] [G loss: 1.923962]\n",
      "epoch:0 step:710 [D loss: 0.077828, acc.: 97.66%] [G loss: 0.434018]\n",
      "epoch:0 step:711 [D loss: 0.055907, acc.: 99.22%] [G loss: 0.560739]\n",
      "epoch:0 step:712 [D loss: 0.099667, acc.: 97.66%] [G loss: 0.957738]\n",
      "epoch:0 step:713 [D loss: 0.032448, acc.: 100.00%] [G loss: 0.273984]\n",
      "epoch:0 step:714 [D loss: 0.318600, acc.: 86.72%] [G loss: 7.966566]\n",
      "epoch:0 step:715 [D loss: 0.608793, acc.: 71.09%] [G loss: 2.544390]\n",
      "epoch:0 step:716 [D loss: 0.296177, acc.: 89.84%] [G loss: 3.257823]\n",
      "epoch:0 step:717 [D loss: 0.043054, acc.: 100.00%] [G loss: 3.380382]\n",
      "epoch:0 step:718 [D loss: 0.435851, acc.: 80.47%] [G loss: 2.715969]\n",
      "epoch:0 step:719 [D loss: 0.410048, acc.: 77.34%] [G loss: 6.820690]\n",
      "epoch:0 step:720 [D loss: 1.595563, acc.: 44.53%] [G loss: 3.371749]\n",
      "epoch:0 step:721 [D loss: 0.080433, acc.: 100.00%] [G loss: 3.219238]\n",
      "epoch:0 step:722 [D loss: 0.131727, acc.: 96.09%] [G loss: 3.556953]\n",
      "epoch:0 step:723 [D loss: 0.114207, acc.: 100.00%] [G loss: 3.473165]\n",
      "epoch:0 step:724 [D loss: 0.175185, acc.: 96.88%] [G loss: 3.140541]\n",
      "epoch:0 step:725 [D loss: 0.146599, acc.: 96.88%] [G loss: 2.562502]\n",
      "epoch:0 step:726 [D loss: 0.192198, acc.: 96.09%] [G loss: 2.821236]\n",
      "epoch:0 step:727 [D loss: 0.407073, acc.: 82.81%] [G loss: 6.653008]\n",
      "epoch:0 step:728 [D loss: 0.584322, acc.: 73.44%] [G loss: 3.671041]\n",
      "epoch:0 step:729 [D loss: 0.080043, acc.: 99.22%] [G loss: 1.584737]\n",
      "epoch:0 step:730 [D loss: 0.182049, acc.: 94.53%] [G loss: 4.924977]\n",
      "epoch:0 step:731 [D loss: 0.111084, acc.: 96.09%] [G loss: 5.494974]\n",
      "epoch:0 step:732 [D loss: 0.071082, acc.: 97.66%] [G loss: 2.740935]\n",
      "epoch:0 step:733 [D loss: 0.078035, acc.: 99.22%] [G loss: 2.768120]\n",
      "epoch:0 step:734 [D loss: 0.085313, acc.: 98.44%] [G loss: 1.728948]\n",
      "epoch:0 step:735 [D loss: 0.228234, acc.: 90.62%] [G loss: 5.903627]\n",
      "epoch:0 step:736 [D loss: 0.197149, acc.: 90.62%] [G loss: 5.293207]\n",
      "epoch:0 step:737 [D loss: 0.174851, acc.: 95.31%] [G loss: 2.896372]\n",
      "epoch:0 step:738 [D loss: 0.408279, acc.: 76.56%] [G loss: 7.395717]\n",
      "epoch:0 step:739 [D loss: 0.469616, acc.: 82.03%] [G loss: 6.707746]\n",
      "epoch:0 step:740 [D loss: 0.082774, acc.: 98.44%] [G loss: 5.280012]\n",
      "epoch:0 step:741 [D loss: 0.019666, acc.: 100.00%] [G loss: 4.600215]\n",
      "epoch:0 step:742 [D loss: 0.026079, acc.: 100.00%] [G loss: 3.735309]\n",
      "epoch:0 step:743 [D loss: 0.141126, acc.: 92.97%] [G loss: 4.650926]\n",
      "epoch:0 step:744 [D loss: 0.027850, acc.: 100.00%] [G loss: 4.794991]\n",
      "epoch:0 step:745 [D loss: 0.235672, acc.: 90.62%] [G loss: 5.477489]\n",
      "epoch:0 step:746 [D loss: 0.171874, acc.: 95.31%] [G loss: 4.971963]\n",
      "epoch:0 step:747 [D loss: 0.065789, acc.: 99.22%] [G loss: 4.654439]\n",
      "epoch:0 step:748 [D loss: 0.213542, acc.: 92.97%] [G loss: 6.872854]\n",
      "epoch:0 step:749 [D loss: 0.271283, acc.: 87.50%] [G loss: 5.548459]\n",
      "epoch:0 step:750 [D loss: 0.079487, acc.: 98.44%] [G loss: 4.388297]\n",
      "epoch:0 step:751 [D loss: 0.035810, acc.: 100.00%] [G loss: 3.812930]\n",
      "epoch:0 step:752 [D loss: 0.047722, acc.: 99.22%] [G loss: 4.867101]\n",
      "epoch:0 step:753 [D loss: 0.127577, acc.: 96.09%] [G loss: 4.420792]\n",
      "epoch:0 step:754 [D loss: 0.108708, acc.: 96.88%] [G loss: 4.005376]\n",
      "epoch:0 step:755 [D loss: 0.087324, acc.: 99.22%] [G loss: 3.724928]\n",
      "epoch:0 step:756 [D loss: 0.194849, acc.: 91.41%] [G loss: 6.002539]\n",
      "epoch:0 step:757 [D loss: 0.164685, acc.: 91.41%] [G loss: 2.748971]\n",
      "epoch:0 step:758 [D loss: 0.136647, acc.: 95.31%] [G loss: 5.626862]\n",
      "epoch:0 step:759 [D loss: 0.112376, acc.: 96.88%] [G loss: 2.732446]\n",
      "epoch:0 step:760 [D loss: 0.141787, acc.: 94.53%] [G loss: 5.105343]\n",
      "epoch:0 step:761 [D loss: 0.264913, acc.: 85.94%] [G loss: 6.250237]\n",
      "epoch:0 step:762 [D loss: 0.552182, acc.: 74.22%] [G loss: 3.465500]\n",
      "epoch:0 step:763 [D loss: 0.051276, acc.: 96.88%] [G loss: 3.634060]\n",
      "epoch:0 step:764 [D loss: 0.026888, acc.: 100.00%] [G loss: 1.068111]\n",
      "epoch:0 step:765 [D loss: 0.935606, acc.: 62.50%] [G loss: 10.456736]\n",
      "epoch:0 step:766 [D loss: 2.876252, acc.: 50.00%] [G loss: 4.101061]\n",
      "epoch:0 step:767 [D loss: 0.531810, acc.: 77.34%] [G loss: 1.560381]\n",
      "epoch:0 step:768 [D loss: 0.246195, acc.: 91.41%] [G loss: 2.494900]\n",
      "epoch:0 step:769 [D loss: 0.080624, acc.: 99.22%] [G loss: 2.763896]\n",
      "epoch:0 step:770 [D loss: 0.137451, acc.: 98.44%] [G loss: 2.056785]\n",
      "epoch:0 step:771 [D loss: 0.149437, acc.: 100.00%] [G loss: 2.905069]\n",
      "epoch:0 step:772 [D loss: 0.109024, acc.: 98.44%] [G loss: 2.763325]\n",
      "epoch:0 step:773 [D loss: 0.185323, acc.: 96.88%] [G loss: 3.469477]\n",
      "epoch:0 step:774 [D loss: 0.123525, acc.: 99.22%] [G loss: 3.512730]\n",
      "epoch:0 step:775 [D loss: 0.224417, acc.: 92.19%] [G loss: 3.755855]\n",
      "epoch:0 step:776 [D loss: 0.208378, acc.: 95.31%] [G loss: 3.808588]\n",
      "epoch:0 step:777 [D loss: 0.266622, acc.: 90.62%] [G loss: 4.813608]\n",
      "epoch:0 step:778 [D loss: 0.142036, acc.: 96.09%] [G loss: 4.129136]\n",
      "epoch:0 step:779 [D loss: 0.250872, acc.: 92.19%] [G loss: 4.383513]\n",
      "epoch:0 step:780 [D loss: 0.062542, acc.: 99.22%] [G loss: 4.318129]\n",
      "epoch:0 step:781 [D loss: 0.124232, acc.: 96.09%] [G loss: 3.627130]\n",
      "epoch:1 step:782 [D loss: 0.093706, acc.: 99.22%] [G loss: 3.291006]\n",
      "epoch:1 step:783 [D loss: 0.061507, acc.: 99.22%] [G loss: 3.275580]\n",
      "epoch:1 step:784 [D loss: 0.195855, acc.: 93.75%] [G loss: 5.080973]\n",
      "epoch:1 step:785 [D loss: 0.170536, acc.: 92.97%] [G loss: 3.274984]\n",
      "epoch:1 step:786 [D loss: 0.101627, acc.: 100.00%] [G loss: 3.300882]\n",
      "epoch:1 step:787 [D loss: 0.230653, acc.: 92.97%] [G loss: 3.450251]\n",
      "epoch:1 step:788 [D loss: 0.133966, acc.: 96.88%] [G loss: 3.755753]\n",
      "epoch:1 step:789 [D loss: 0.496707, acc.: 78.91%] [G loss: 4.757377]\n",
      "epoch:1 step:790 [D loss: 0.083442, acc.: 96.09%] [G loss: 4.487659]\n",
      "epoch:1 step:791 [D loss: 0.080877, acc.: 98.44%] [G loss: 2.868251]\n",
      "epoch:1 step:792 [D loss: 0.235792, acc.: 91.41%] [G loss: 5.985955]\n",
      "epoch:1 step:793 [D loss: 0.263917, acc.: 88.28%] [G loss: 4.687631]\n",
      "epoch:1 step:794 [D loss: 0.344447, acc.: 81.25%] [G loss: 7.057588]\n",
      "epoch:1 step:795 [D loss: 0.464864, acc.: 84.38%] [G loss: 5.878045]\n",
      "epoch:1 step:796 [D loss: 0.053202, acc.: 99.22%] [G loss: 4.444256]\n",
      "epoch:1 step:797 [D loss: 0.096575, acc.: 96.09%] [G loss: 4.414493]\n",
      "epoch:1 step:798 [D loss: 0.031472, acc.: 100.00%] [G loss: 4.304968]\n",
      "epoch:1 step:799 [D loss: 0.131667, acc.: 96.09%] [G loss: 5.523224]\n",
      "epoch:1 step:800 [D loss: 0.095975, acc.: 98.44%] [G loss: 5.429022]\n",
      "epoch:1 step:801 [D loss: 0.158112, acc.: 95.31%] [G loss: 5.589799]\n",
      "epoch:1 step:802 [D loss: 0.325618, acc.: 87.50%] [G loss: 4.422387]\n",
      "epoch:1 step:803 [D loss: 0.067538, acc.: 99.22%] [G loss: 4.799170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:804 [D loss: 0.282984, acc.: 90.62%] [G loss: 6.615199]\n",
      "epoch:1 step:805 [D loss: 0.242749, acc.: 88.28%] [G loss: 3.883224]\n",
      "epoch:1 step:806 [D loss: 0.250942, acc.: 89.84%] [G loss: 5.770864]\n",
      "epoch:1 step:807 [D loss: 0.149339, acc.: 96.88%] [G loss: 4.831666]\n",
      "epoch:1 step:808 [D loss: 0.343114, acc.: 80.47%] [G loss: 6.852909]\n",
      "epoch:1 step:809 [D loss: 0.257716, acc.: 87.50%] [G loss: 4.095545]\n",
      "epoch:1 step:810 [D loss: 0.166581, acc.: 92.19%] [G loss: 6.116457]\n",
      "epoch:1 step:811 [D loss: 0.061659, acc.: 99.22%] [G loss: 5.470811]\n",
      "epoch:1 step:812 [D loss: 0.078687, acc.: 97.66%] [G loss: 3.544095]\n",
      "epoch:1 step:813 [D loss: 0.193257, acc.: 92.19%] [G loss: 5.948470]\n",
      "epoch:1 step:814 [D loss: 0.269860, acc.: 86.72%] [G loss: 4.678470]\n",
      "epoch:1 step:815 [D loss: 0.083130, acc.: 99.22%] [G loss: 4.250087]\n",
      "epoch:1 step:816 [D loss: 0.291675, acc.: 86.72%] [G loss: 7.463307]\n",
      "epoch:1 step:817 [D loss: 1.687416, acc.: 34.38%] [G loss: 4.943566]\n",
      "epoch:1 step:818 [D loss: 0.041834, acc.: 100.00%] [G loss: 5.471028]\n",
      "epoch:1 step:819 [D loss: 0.222807, acc.: 89.84%] [G loss: 3.332984]\n",
      "epoch:1 step:820 [D loss: 0.116925, acc.: 97.66%] [G loss: 3.605572]\n",
      "epoch:1 step:821 [D loss: 0.020062, acc.: 100.00%] [G loss: 4.260171]\n",
      "epoch:1 step:822 [D loss: 0.170843, acc.: 94.53%] [G loss: 4.503983]\n",
      "epoch:1 step:823 [D loss: 0.073431, acc.: 98.44%] [G loss: 4.415124]\n",
      "epoch:1 step:824 [D loss: 0.253240, acc.: 91.41%] [G loss: 5.452744]\n",
      "epoch:1 step:825 [D loss: 0.107845, acc.: 97.66%] [G loss: 4.395110]\n",
      "epoch:1 step:826 [D loss: 0.218564, acc.: 93.75%] [G loss: 5.685814]\n",
      "epoch:1 step:827 [D loss: 0.137680, acc.: 93.75%] [G loss: 4.158375]\n",
      "epoch:1 step:828 [D loss: 0.319330, acc.: 84.38%] [G loss: 6.522922]\n",
      "epoch:1 step:829 [D loss: 0.542673, acc.: 70.31%] [G loss: 4.039083]\n",
      "epoch:1 step:830 [D loss: 0.617595, acc.: 66.41%] [G loss: 7.872707]\n",
      "epoch:1 step:831 [D loss: 0.788690, acc.: 64.84%] [G loss: 5.629489]\n",
      "epoch:1 step:832 [D loss: 0.082208, acc.: 96.09%] [G loss: 4.732843]\n",
      "epoch:1 step:833 [D loss: 0.027397, acc.: 100.00%] [G loss: 4.247404]\n",
      "epoch:1 step:834 [D loss: 0.053111, acc.: 100.00%] [G loss: 4.333543]\n",
      "epoch:1 step:835 [D loss: 0.083725, acc.: 98.44%] [G loss: 4.938951]\n",
      "epoch:1 step:836 [D loss: 0.055229, acc.: 100.00%] [G loss: 4.546852]\n",
      "epoch:1 step:837 [D loss: 0.100867, acc.: 98.44%] [G loss: 4.295282]\n",
      "epoch:1 step:838 [D loss: 0.283315, acc.: 89.06%] [G loss: 4.712265]\n",
      "epoch:1 step:839 [D loss: 0.142663, acc.: 95.31%] [G loss: 4.416650]\n",
      "epoch:1 step:840 [D loss: 0.130314, acc.: 95.31%] [G loss: 5.011937]\n",
      "epoch:1 step:841 [D loss: 0.117770, acc.: 96.88%] [G loss: 5.135304]\n",
      "epoch:1 step:842 [D loss: 0.841845, acc.: 59.38%] [G loss: 8.967545]\n",
      "epoch:1 step:843 [D loss: 1.001991, acc.: 60.94%] [G loss: 5.489453]\n",
      "epoch:1 step:844 [D loss: 0.128340, acc.: 92.97%] [G loss: 3.613976]\n",
      "epoch:1 step:845 [D loss: 0.102740, acc.: 98.44%] [G loss: 3.636817]\n",
      "epoch:1 step:846 [D loss: 0.132241, acc.: 97.66%] [G loss: 4.463612]\n",
      "epoch:1 step:847 [D loss: 0.088037, acc.: 97.66%] [G loss: 4.592836]\n",
      "epoch:1 step:848 [D loss: 0.161138, acc.: 95.31%] [G loss: 3.796165]\n",
      "epoch:1 step:849 [D loss: 0.258636, acc.: 87.50%] [G loss: 5.524313]\n",
      "epoch:1 step:850 [D loss: 0.180904, acc.: 92.19%] [G loss: 4.827045]\n",
      "epoch:1 step:851 [D loss: 0.280115, acc.: 90.62%] [G loss: 5.097086]\n",
      "epoch:1 step:852 [D loss: 0.043386, acc.: 99.22%] [G loss: 4.771998]\n",
      "epoch:1 step:853 [D loss: 0.056961, acc.: 99.22%] [G loss: 3.686941]\n",
      "epoch:1 step:854 [D loss: 0.140564, acc.: 95.31%] [G loss: 5.822604]\n",
      "epoch:1 step:855 [D loss: 0.203973, acc.: 92.19%] [G loss: 5.145572]\n",
      "epoch:1 step:856 [D loss: 0.045947, acc.: 99.22%] [G loss: 3.838049]\n",
      "epoch:1 step:857 [D loss: 0.179716, acc.: 97.66%] [G loss: 7.202103]\n",
      "epoch:1 step:858 [D loss: 0.594211, acc.: 71.88%] [G loss: 3.069792]\n",
      "epoch:1 step:859 [D loss: 0.292505, acc.: 87.50%] [G loss: 7.510281]\n",
      "epoch:1 step:860 [D loss: 0.326424, acc.: 87.50%] [G loss: 6.100603]\n",
      "epoch:1 step:861 [D loss: 0.103329, acc.: 95.31%] [G loss: 5.106489]\n",
      "epoch:1 step:862 [D loss: 0.050963, acc.: 99.22%] [G loss: 4.138605]\n",
      "epoch:1 step:863 [D loss: 0.195590, acc.: 92.19%] [G loss: 5.719196]\n",
      "epoch:1 step:864 [D loss: 0.132917, acc.: 94.53%] [G loss: 5.036316]\n",
      "epoch:1 step:865 [D loss: 0.273204, acc.: 89.06%] [G loss: 5.145285]\n",
      "epoch:1 step:866 [D loss: 0.043893, acc.: 99.22%] [G loss: 4.476002]\n",
      "epoch:1 step:867 [D loss: 0.178538, acc.: 92.97%] [G loss: 4.792236]\n",
      "epoch:1 step:868 [D loss: 0.075010, acc.: 98.44%] [G loss: 4.153689]\n",
      "epoch:1 step:869 [D loss: 0.195354, acc.: 94.53%] [G loss: 5.513868]\n",
      "epoch:1 step:870 [D loss: 0.216405, acc.: 89.84%] [G loss: 5.173701]\n",
      "epoch:1 step:871 [D loss: 0.080384, acc.: 97.66%] [G loss: 4.593384]\n",
      "epoch:1 step:872 [D loss: 0.197499, acc.: 92.19%] [G loss: 7.645576]\n",
      "epoch:1 step:873 [D loss: 0.209553, acc.: 89.06%] [G loss: 6.077498]\n",
      "epoch:1 step:874 [D loss: 0.066412, acc.: 99.22%] [G loss: 5.839149]\n",
      "epoch:1 step:875 [D loss: 0.029438, acc.: 100.00%] [G loss: 4.153912]\n",
      "epoch:1 step:876 [D loss: 0.443002, acc.: 81.25%] [G loss: 9.137671]\n",
      "epoch:1 step:877 [D loss: 0.860553, acc.: 62.50%] [G loss: 4.284431]\n",
      "epoch:1 step:878 [D loss: 0.190623, acc.: 90.62%] [G loss: 4.478013]\n",
      "epoch:1 step:879 [D loss: 0.007957, acc.: 100.00%] [G loss: 4.409721]\n",
      "epoch:1 step:880 [D loss: 0.060658, acc.: 100.00%] [G loss: 3.606283]\n",
      "epoch:1 step:881 [D loss: 0.129142, acc.: 96.88%] [G loss: 2.927226]\n",
      "epoch:1 step:882 [D loss: 0.429143, acc.: 80.47%] [G loss: 6.809547]\n",
      "epoch:1 step:883 [D loss: 0.385849, acc.: 82.81%] [G loss: 5.922644]\n",
      "epoch:1 step:884 [D loss: 0.152681, acc.: 94.53%] [G loss: 3.539429]\n",
      "epoch:1 step:885 [D loss: 0.094112, acc.: 99.22%] [G loss: 3.578224]\n",
      "epoch:1 step:886 [D loss: 0.061981, acc.: 98.44%] [G loss: 3.718096]\n",
      "epoch:1 step:887 [D loss: 0.096854, acc.: 98.44%] [G loss: 4.838960]\n",
      "epoch:1 step:888 [D loss: 0.049138, acc.: 99.22%] [G loss: 3.929370]\n",
      "epoch:1 step:889 [D loss: 0.212412, acc.: 90.62%] [G loss: 6.526464]\n",
      "epoch:1 step:890 [D loss: 0.342558, acc.: 82.81%] [G loss: 4.174785]\n",
      "epoch:1 step:891 [D loss: 0.217959, acc.: 90.62%] [G loss: 6.587667]\n",
      "epoch:1 step:892 [D loss: 0.019834, acc.: 100.00%] [G loss: 7.045614]\n",
      "epoch:1 step:893 [D loss: 0.136110, acc.: 93.75%] [G loss: 4.329459]\n",
      "epoch:1 step:894 [D loss: 0.084419, acc.: 98.44%] [G loss: 4.463729]\n",
      "epoch:1 step:895 [D loss: 0.043992, acc.: 100.00%] [G loss: 3.366920]\n",
      "epoch:1 step:896 [D loss: 0.121305, acc.: 96.88%] [G loss: 4.693318]\n",
      "epoch:1 step:897 [D loss: 0.717205, acc.: 56.25%] [G loss: 8.005924]\n",
      "epoch:1 step:898 [D loss: 0.860669, acc.: 62.50%] [G loss: 4.929097]\n",
      "epoch:1 step:899 [D loss: 0.047123, acc.: 98.44%] [G loss: 2.944366]\n",
      "epoch:1 step:900 [D loss: 0.044574, acc.: 100.00%] [G loss: 3.517508]\n",
      "epoch:1 step:901 [D loss: 0.021315, acc.: 100.00%] [G loss: 2.326373]\n",
      "epoch:1 step:902 [D loss: 0.062201, acc.: 98.44%] [G loss: 2.206871]\n",
      "epoch:1 step:903 [D loss: 0.171908, acc.: 96.88%] [G loss: 4.494396]\n",
      "epoch:1 step:904 [D loss: 0.643212, acc.: 68.75%] [G loss: 2.887577]\n",
      "epoch:1 step:905 [D loss: 0.272584, acc.: 89.06%] [G loss: 3.369626]\n",
      "epoch:1 step:906 [D loss: 0.059376, acc.: 97.66%] [G loss: 1.482139]\n",
      "epoch:1 step:907 [D loss: 0.137831, acc.: 95.31%] [G loss: 2.144758]\n",
      "epoch:1 step:908 [D loss: 0.051595, acc.: 99.22%] [G loss: 2.070331]\n",
      "epoch:1 step:909 [D loss: 0.152865, acc.: 95.31%] [G loss: 1.668408]\n",
      "epoch:1 step:910 [D loss: 0.487295, acc.: 74.22%] [G loss: 7.295395]\n",
      "epoch:1 step:911 [D loss: 0.591613, acc.: 67.97%] [G loss: 4.754021]\n",
      "epoch:1 step:912 [D loss: 0.044598, acc.: 100.00%] [G loss: 3.402463]\n",
      "epoch:1 step:913 [D loss: 0.125102, acc.: 98.44%] [G loss: 2.912944]\n",
      "epoch:1 step:914 [D loss: 0.024407, acc.: 100.00%] [G loss: 3.314404]\n",
      "epoch:1 step:915 [D loss: 0.204778, acc.: 93.75%] [G loss: 5.378882]\n",
      "epoch:1 step:916 [D loss: 0.115186, acc.: 96.88%] [G loss: 5.636888]\n",
      "epoch:1 step:917 [D loss: 0.056390, acc.: 99.22%] [G loss: 4.451337]\n",
      "epoch:1 step:918 [D loss: 0.912085, acc.: 68.75%] [G loss: 9.057460]\n",
      "epoch:1 step:919 [D loss: 1.711328, acc.: 50.78%] [G loss: 6.018020]\n",
      "epoch:1 step:920 [D loss: 0.081404, acc.: 98.44%] [G loss: 4.728049]\n",
      "epoch:1 step:921 [D loss: 0.050984, acc.: 100.00%] [G loss: 4.181167]\n",
      "epoch:1 step:922 [D loss: 0.048127, acc.: 99.22%] [G loss: 3.738181]\n",
      "epoch:1 step:923 [D loss: 0.111112, acc.: 97.66%] [G loss: 4.273757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:924 [D loss: 0.062306, acc.: 100.00%] [G loss: 3.974897]\n",
      "epoch:1 step:925 [D loss: 0.078543, acc.: 100.00%] [G loss: 3.848712]\n",
      "epoch:1 step:926 [D loss: 0.109762, acc.: 98.44%] [G loss: 3.730502]\n",
      "epoch:1 step:927 [D loss: 0.283739, acc.: 88.28%] [G loss: 4.291890]\n",
      "epoch:1 step:928 [D loss: 0.117030, acc.: 97.66%] [G loss: 4.905314]\n",
      "epoch:1 step:929 [D loss: 0.387317, acc.: 84.38%] [G loss: 4.546912]\n",
      "epoch:1 step:930 [D loss: 0.143974, acc.: 96.09%] [G loss: 3.866346]\n",
      "epoch:1 step:931 [D loss: 0.102718, acc.: 96.88%] [G loss: 2.947165]\n",
      "epoch:1 step:932 [D loss: 0.054072, acc.: 100.00%] [G loss: 1.877224]\n",
      "epoch:1 step:933 [D loss: 0.090310, acc.: 99.22%] [G loss: 3.079802]\n",
      "epoch:1 step:934 [D loss: 0.124860, acc.: 96.88%] [G loss: 2.581053]\n",
      "epoch:1 step:935 [D loss: 0.087492, acc.: 99.22%] [G loss: 2.518666]\n",
      "epoch:1 step:936 [D loss: 0.135071, acc.: 97.66%] [G loss: 1.060451]\n",
      "epoch:1 step:937 [D loss: 0.144433, acc.: 96.09%] [G loss: 1.361366]\n",
      "epoch:1 step:938 [D loss: 0.073012, acc.: 99.22%] [G loss: 1.106998]\n",
      "epoch:1 step:939 [D loss: 0.224264, acc.: 93.75%] [G loss: 3.349808]\n",
      "epoch:1 step:940 [D loss: 0.180157, acc.: 93.75%] [G loss: 0.696334]\n",
      "epoch:1 step:941 [D loss: 0.051585, acc.: 99.22%] [G loss: 0.491114]\n",
      "epoch:1 step:942 [D loss: 0.027827, acc.: 100.00%] [G loss: 0.262498]\n",
      "epoch:1 step:943 [D loss: 0.043209, acc.: 99.22%] [G loss: 0.220806]\n",
      "epoch:1 step:944 [D loss: 0.061236, acc.: 100.00%] [G loss: 0.627120]\n",
      "epoch:1 step:945 [D loss: 0.167062, acc.: 94.53%] [G loss: 0.483214]\n",
      "epoch:1 step:946 [D loss: 0.017416, acc.: 100.00%] [G loss: 0.432164]\n",
      "epoch:1 step:947 [D loss: 0.152260, acc.: 94.53%] [G loss: 6.735843]\n",
      "epoch:1 step:948 [D loss: 0.586375, acc.: 75.78%] [G loss: 1.330156]\n",
      "epoch:1 step:949 [D loss: 0.071096, acc.: 97.66%] [G loss: 3.164240]\n",
      "epoch:1 step:950 [D loss: 0.036863, acc.: 99.22%] [G loss: 2.705709]\n",
      "epoch:1 step:951 [D loss: 0.102641, acc.: 98.44%] [G loss: 4.066193]\n",
      "epoch:1 step:952 [D loss: 0.207272, acc.: 92.19%] [G loss: 5.204103]\n",
      "epoch:1 step:953 [D loss: 1.768514, acc.: 40.62%] [G loss: 9.380199]\n",
      "epoch:1 step:954 [D loss: 2.489753, acc.: 50.78%] [G loss: 4.151232]\n",
      "epoch:1 step:955 [D loss: 0.321319, acc.: 87.50%] [G loss: 2.574098]\n",
      "epoch:1 step:956 [D loss: 0.128504, acc.: 99.22%] [G loss: 2.538123]\n",
      "epoch:1 step:957 [D loss: 0.113167, acc.: 99.22%] [G loss: 2.883067]\n",
      "epoch:1 step:958 [D loss: 0.105035, acc.: 97.66%] [G loss: 3.485018]\n",
      "epoch:1 step:959 [D loss: 0.167900, acc.: 96.88%] [G loss: 3.206442]\n",
      "epoch:1 step:960 [D loss: 0.155839, acc.: 96.09%] [G loss: 3.912802]\n",
      "epoch:1 step:961 [D loss: 0.119965, acc.: 96.88%] [G loss: 3.888571]\n",
      "epoch:1 step:962 [D loss: 0.154573, acc.: 96.88%] [G loss: 4.418607]\n",
      "epoch:1 step:963 [D loss: 0.159414, acc.: 95.31%] [G loss: 4.153716]\n",
      "epoch:1 step:964 [D loss: 0.204335, acc.: 92.97%] [G loss: 4.368555]\n",
      "epoch:1 step:965 [D loss: 0.137002, acc.: 98.44%] [G loss: 4.609483]\n",
      "epoch:1 step:966 [D loss: 0.144206, acc.: 98.44%] [G loss: 4.148080]\n",
      "epoch:1 step:967 [D loss: 0.154269, acc.: 96.09%] [G loss: 5.439987]\n",
      "epoch:1 step:968 [D loss: 0.098148, acc.: 97.66%] [G loss: 5.462828]\n",
      "epoch:1 step:969 [D loss: 0.094708, acc.: 98.44%] [G loss: 5.471114]\n",
      "epoch:1 step:970 [D loss: 0.098198, acc.: 99.22%] [G loss: 5.015885]\n",
      "epoch:1 step:971 [D loss: 0.120247, acc.: 96.09%] [G loss: 5.520782]\n",
      "epoch:1 step:972 [D loss: 0.088129, acc.: 99.22%] [G loss: 4.727658]\n",
      "epoch:1 step:973 [D loss: 0.207305, acc.: 92.97%] [G loss: 5.337432]\n",
      "epoch:1 step:974 [D loss: 0.107190, acc.: 98.44%] [G loss: 3.129909]\n",
      "epoch:1 step:975 [D loss: 0.079022, acc.: 96.88%] [G loss: 3.909770]\n",
      "epoch:1 step:976 [D loss: 0.130765, acc.: 96.09%] [G loss: 7.542032]\n",
      "epoch:1 step:977 [D loss: 0.088042, acc.: 97.66%] [G loss: 7.943300]\n",
      "epoch:1 step:978 [D loss: 0.955750, acc.: 57.81%] [G loss: 3.526800]\n",
      "epoch:1 step:979 [D loss: 0.034814, acc.: 98.44%] [G loss: 4.451284]\n",
      "epoch:1 step:980 [D loss: 0.010803, acc.: 100.00%] [G loss: 4.241200]\n",
      "epoch:1 step:981 [D loss: 0.057081, acc.: 98.44%] [G loss: 3.734451]\n",
      "epoch:1 step:982 [D loss: 0.036547, acc.: 100.00%] [G loss: 3.091529]\n",
      "epoch:1 step:983 [D loss: 0.096310, acc.: 97.66%] [G loss: 3.603357]\n",
      "epoch:1 step:984 [D loss: 0.031797, acc.: 98.44%] [G loss: 2.290094]\n",
      "epoch:1 step:985 [D loss: 0.052212, acc.: 100.00%] [G loss: 0.914908]\n",
      "epoch:1 step:986 [D loss: 0.408041, acc.: 82.03%] [G loss: 6.809195]\n",
      "epoch:1 step:987 [D loss: 1.064925, acc.: 61.72%] [G loss: 1.703291]\n",
      "epoch:1 step:988 [D loss: 0.165796, acc.: 92.97%] [G loss: 0.688319]\n",
      "epoch:1 step:989 [D loss: 0.042818, acc.: 99.22%] [G loss: 1.213927]\n",
      "epoch:1 step:990 [D loss: 0.029908, acc.: 100.00%] [G loss: 0.819106]\n",
      "epoch:1 step:991 [D loss: 0.462836, acc.: 74.22%] [G loss: 3.534248]\n",
      "epoch:1 step:992 [D loss: 0.160555, acc.: 94.53%] [G loss: 3.869439]\n",
      "epoch:1 step:993 [D loss: 0.321762, acc.: 87.50%] [G loss: 0.871989]\n",
      "epoch:1 step:994 [D loss: 0.134772, acc.: 95.31%] [G loss: 0.815711]\n",
      "epoch:1 step:995 [D loss: 0.192843, acc.: 92.97%] [G loss: 0.347324]\n",
      "epoch:1 step:996 [D loss: 0.037752, acc.: 100.00%] [G loss: 0.367471]\n",
      "epoch:1 step:997 [D loss: 0.088236, acc.: 98.44%] [G loss: 0.802673]\n",
      "epoch:1 step:998 [D loss: 0.018779, acc.: 100.00%] [G loss: 2.032346]\n",
      "epoch:1 step:999 [D loss: 0.038561, acc.: 100.00%] [G loss: 1.199178]\n",
      "epoch:1 step:1000 [D loss: 0.118758, acc.: 95.31%] [G loss: 3.365684]\n",
      "epoch:1 step:1001 [D loss: 0.072667, acc.: 98.44%] [G loss: 3.418618]\n",
      "epoch:1 step:1002 [D loss: 0.069706, acc.: 100.00%] [G loss: 1.417556]\n",
      "epoch:1 step:1003 [D loss: 0.168485, acc.: 95.31%] [G loss: 5.215541]\n",
      "epoch:1 step:1004 [D loss: 0.227932, acc.: 89.84%] [G loss: 2.331974]\n",
      "epoch:1 step:1005 [D loss: 0.318690, acc.: 82.81%] [G loss: 6.620136]\n",
      "epoch:1 step:1006 [D loss: 0.388634, acc.: 82.03%] [G loss: 5.476873]\n",
      "epoch:1 step:1007 [D loss: 0.037275, acc.: 99.22%] [G loss: 4.830284]\n",
      "epoch:1 step:1008 [D loss: 0.069531, acc.: 99.22%] [G loss: 2.916080]\n",
      "epoch:1 step:1009 [D loss: 0.221606, acc.: 87.50%] [G loss: 6.812280]\n",
      "epoch:1 step:1010 [D loss: 0.252383, acc.: 88.28%] [G loss: 5.268878]\n",
      "epoch:1 step:1011 [D loss: 0.070572, acc.: 100.00%] [G loss: 4.269120]\n",
      "epoch:1 step:1012 [D loss: 0.049559, acc.: 100.00%] [G loss: 4.480479]\n",
      "epoch:1 step:1013 [D loss: 0.477454, acc.: 72.66%] [G loss: 8.561058]\n",
      "epoch:1 step:1014 [D loss: 0.667829, acc.: 65.62%] [G loss: 5.281691]\n",
      "epoch:1 step:1015 [D loss: 0.040983, acc.: 99.22%] [G loss: 4.356463]\n",
      "epoch:1 step:1016 [D loss: 0.021009, acc.: 100.00%] [G loss: 3.421093]\n",
      "epoch:1 step:1017 [D loss: 0.035975, acc.: 100.00%] [G loss: 3.523122]\n",
      "epoch:1 step:1018 [D loss: 0.086567, acc.: 100.00%] [G loss: 4.097649]\n",
      "epoch:1 step:1019 [D loss: 0.056541, acc.: 98.44%] [G loss: 3.347075]\n",
      "epoch:1 step:1020 [D loss: 0.136658, acc.: 96.88%] [G loss: 4.304162]\n",
      "epoch:1 step:1021 [D loss: 0.112383, acc.: 95.31%] [G loss: 3.043583]\n",
      "epoch:1 step:1022 [D loss: 0.205493, acc.: 89.06%] [G loss: 4.769832]\n",
      "epoch:1 step:1023 [D loss: 0.310715, acc.: 89.06%] [G loss: 1.801085]\n",
      "epoch:1 step:1024 [D loss: 0.185683, acc.: 93.75%] [G loss: 3.806675]\n",
      "epoch:1 step:1025 [D loss: 0.170024, acc.: 91.41%] [G loss: 1.826742]\n",
      "epoch:1 step:1026 [D loss: 0.110742, acc.: 97.66%] [G loss: 2.574888]\n",
      "epoch:1 step:1027 [D loss: 0.241982, acc.: 90.62%] [G loss: 4.261496]\n",
      "epoch:1 step:1028 [D loss: 0.423848, acc.: 82.03%] [G loss: 1.944972]\n",
      "epoch:1 step:1029 [D loss: 0.031772, acc.: 100.00%] [G loss: 1.773506]\n",
      "epoch:1 step:1030 [D loss: 0.053267, acc.: 99.22%] [G loss: 0.859029]\n",
      "epoch:1 step:1031 [D loss: 0.174867, acc.: 94.53%] [G loss: 4.541206]\n",
      "epoch:1 step:1032 [D loss: 0.442242, acc.: 79.69%] [G loss: 0.663273]\n",
      "epoch:1 step:1033 [D loss: 0.138834, acc.: 94.53%] [G loss: 3.021883]\n",
      "epoch:1 step:1034 [D loss: 0.078779, acc.: 96.09%] [G loss: 1.882198]\n",
      "epoch:1 step:1035 [D loss: 0.038163, acc.: 99.22%] [G loss: 0.374424]\n",
      "epoch:1 step:1036 [D loss: 0.529409, acc.: 71.09%] [G loss: 9.038511]\n",
      "epoch:1 step:1037 [D loss: 2.006207, acc.: 53.91%] [G loss: 2.999883]\n",
      "epoch:1 step:1038 [D loss: 0.209018, acc.: 90.62%] [G loss: 3.695432]\n",
      "epoch:1 step:1039 [D loss: 0.078155, acc.: 98.44%] [G loss: 3.713731]\n",
      "epoch:1 step:1040 [D loss: 0.078770, acc.: 100.00%] [G loss: 3.382036]\n",
      "epoch:1 step:1041 [D loss: 0.101842, acc.: 100.00%] [G loss: 3.843622]\n",
      "epoch:1 step:1042 [D loss: 0.344084, acc.: 87.50%] [G loss: 4.989766]\n",
      "epoch:1 step:1043 [D loss: 0.410988, acc.: 79.69%] [G loss: 3.906285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1044 [D loss: 0.204540, acc.: 96.09%] [G loss: 4.108197]\n",
      "epoch:1 step:1045 [D loss: 0.253746, acc.: 91.41%] [G loss: 4.042883]\n",
      "epoch:1 step:1046 [D loss: 0.279639, acc.: 89.84%] [G loss: 5.490474]\n",
      "epoch:1 step:1047 [D loss: 0.183848, acc.: 94.53%] [G loss: 4.935865]\n",
      "epoch:1 step:1048 [D loss: 0.512410, acc.: 73.44%] [G loss: 8.164240]\n",
      "epoch:1 step:1049 [D loss: 0.677308, acc.: 75.00%] [G loss: 5.272966]\n",
      "epoch:1 step:1050 [D loss: 0.084445, acc.: 98.44%] [G loss: 3.806751]\n",
      "epoch:1 step:1051 [D loss: 0.069383, acc.: 97.66%] [G loss: 3.501609]\n",
      "epoch:1 step:1052 [D loss: 0.027959, acc.: 100.00%] [G loss: 3.065548]\n",
      "epoch:1 step:1053 [D loss: 0.107248, acc.: 96.88%] [G loss: 3.497698]\n",
      "epoch:1 step:1054 [D loss: 0.040667, acc.: 100.00%] [G loss: 2.980463]\n",
      "epoch:1 step:1055 [D loss: 0.312767, acc.: 85.94%] [G loss: 6.151281]\n",
      "epoch:1 step:1056 [D loss: 1.043331, acc.: 60.16%] [G loss: 1.273155]\n",
      "epoch:1 step:1057 [D loss: 0.196235, acc.: 89.06%] [G loss: 3.185947]\n",
      "epoch:1 step:1058 [D loss: 0.119967, acc.: 96.09%] [G loss: 2.592895]\n",
      "epoch:1 step:1059 [D loss: 0.055832, acc.: 98.44%] [G loss: 1.425134]\n",
      "epoch:1 step:1060 [D loss: 0.035393, acc.: 100.00%] [G loss: 1.045460]\n",
      "epoch:1 step:1061 [D loss: 0.058562, acc.: 100.00%] [G loss: 1.336799]\n",
      "epoch:1 step:1062 [D loss: 0.082794, acc.: 98.44%] [G loss: 2.794533]\n",
      "epoch:1 step:1063 [D loss: 0.190306, acc.: 94.53%] [G loss: 1.266978]\n",
      "epoch:1 step:1064 [D loss: 0.065272, acc.: 100.00%] [G loss: 1.432764]\n",
      "epoch:1 step:1065 [D loss: 0.204743, acc.: 95.31%] [G loss: 4.541187]\n",
      "epoch:1 step:1066 [D loss: 0.086687, acc.: 97.66%] [G loss: 4.236787]\n",
      "epoch:1 step:1067 [D loss: 0.325979, acc.: 82.81%] [G loss: 3.060425]\n",
      "epoch:1 step:1068 [D loss: 0.093201, acc.: 98.44%] [G loss: 4.287564]\n",
      "epoch:1 step:1069 [D loss: 0.140497, acc.: 93.75%] [G loss: 5.003911]\n",
      "epoch:1 step:1070 [D loss: 0.148568, acc.: 95.31%] [G loss: 4.064698]\n",
      "epoch:1 step:1071 [D loss: 0.471925, acc.: 74.22%] [G loss: 8.237080]\n",
      "epoch:1 step:1072 [D loss: 0.593940, acc.: 67.97%] [G loss: 5.569666]\n",
      "epoch:1 step:1073 [D loss: 0.053651, acc.: 99.22%] [G loss: 3.341389]\n",
      "epoch:1 step:1074 [D loss: 0.136636, acc.: 93.75%] [G loss: 4.754996]\n",
      "epoch:1 step:1075 [D loss: 0.009936, acc.: 100.00%] [G loss: 4.749189]\n",
      "epoch:1 step:1076 [D loss: 0.068676, acc.: 98.44%] [G loss: 3.062772]\n",
      "epoch:1 step:1077 [D loss: 0.281885, acc.: 85.16%] [G loss: 6.119019]\n",
      "epoch:1 step:1078 [D loss: 0.260362, acc.: 89.06%] [G loss: 4.458778]\n",
      "epoch:1 step:1079 [D loss: 0.046492, acc.: 99.22%] [G loss: 2.293354]\n",
      "epoch:1 step:1080 [D loss: 0.194221, acc.: 92.19%] [G loss: 5.374325]\n",
      "epoch:1 step:1081 [D loss: 0.120453, acc.: 94.53%] [G loss: 5.046110]\n",
      "epoch:1 step:1082 [D loss: 0.121697, acc.: 94.53%] [G loss: 4.027851]\n",
      "epoch:1 step:1083 [D loss: 0.133540, acc.: 96.09%] [G loss: 3.171503]\n",
      "epoch:1 step:1084 [D loss: 0.139692, acc.: 97.66%] [G loss: 4.651644]\n",
      "epoch:1 step:1085 [D loss: 0.048372, acc.: 98.44%] [G loss: 4.879103]\n",
      "epoch:1 step:1086 [D loss: 0.242427, acc.: 90.62%] [G loss: 4.151026]\n",
      "epoch:1 step:1087 [D loss: 0.406292, acc.: 81.25%] [G loss: 6.886751]\n",
      "epoch:1 step:1088 [D loss: 0.514431, acc.: 78.12%] [G loss: 2.424192]\n",
      "epoch:1 step:1089 [D loss: 0.356746, acc.: 82.81%] [G loss: 5.879510]\n",
      "epoch:1 step:1090 [D loss: 0.118631, acc.: 96.09%] [G loss: 5.200608]\n",
      "epoch:1 step:1091 [D loss: 0.117725, acc.: 94.53%] [G loss: 3.573911]\n",
      "epoch:1 step:1092 [D loss: 0.105511, acc.: 96.09%] [G loss: 3.921523]\n",
      "epoch:1 step:1093 [D loss: 0.060726, acc.: 99.22%] [G loss: 4.180027]\n",
      "epoch:1 step:1094 [D loss: 0.117450, acc.: 97.66%] [G loss: 3.834915]\n",
      "epoch:1 step:1095 [D loss: 0.100479, acc.: 98.44%] [G loss: 3.552440]\n",
      "epoch:1 step:1096 [D loss: 0.071871, acc.: 97.66%] [G loss: 3.497605]\n",
      "epoch:1 step:1097 [D loss: 0.363258, acc.: 88.28%] [G loss: 1.991194]\n",
      "epoch:1 step:1098 [D loss: 0.117516, acc.: 96.88%] [G loss: 3.494008]\n",
      "epoch:1 step:1099 [D loss: 0.030918, acc.: 100.00%] [G loss: 3.079337]\n",
      "epoch:1 step:1100 [D loss: 0.090089, acc.: 97.66%] [G loss: 0.951474]\n",
      "epoch:1 step:1101 [D loss: 0.110205, acc.: 96.88%] [G loss: 2.888242]\n",
      "epoch:1 step:1102 [D loss: 0.044554, acc.: 100.00%] [G loss: 3.781404]\n",
      "epoch:1 step:1103 [D loss: 0.579672, acc.: 72.66%] [G loss: 5.514179]\n",
      "epoch:1 step:1104 [D loss: 0.039617, acc.: 99.22%] [G loss: 6.929904]\n",
      "epoch:1 step:1105 [D loss: 0.325304, acc.: 85.94%] [G loss: 1.878842]\n",
      "epoch:1 step:1106 [D loss: 0.444554, acc.: 77.34%] [G loss: 5.993930]\n",
      "epoch:1 step:1107 [D loss: 0.116387, acc.: 94.53%] [G loss: 6.486232]\n",
      "epoch:1 step:1108 [D loss: 0.602355, acc.: 74.22%] [G loss: 0.844425]\n",
      "epoch:1 step:1109 [D loss: 0.307262, acc.: 84.38%] [G loss: 4.559389]\n",
      "epoch:1 step:1110 [D loss: 0.031182, acc.: 98.44%] [G loss: 4.900107]\n",
      "epoch:1 step:1111 [D loss: 0.417777, acc.: 78.91%] [G loss: 3.790312]\n",
      "epoch:1 step:1112 [D loss: 0.285923, acc.: 89.06%] [G loss: 4.672929]\n",
      "epoch:1 step:1113 [D loss: 0.038056, acc.: 99.22%] [G loss: 4.488591]\n",
      "epoch:1 step:1114 [D loss: 0.339084, acc.: 85.16%] [G loss: 7.194995]\n",
      "epoch:1 step:1115 [D loss: 0.516332, acc.: 75.78%] [G loss: 2.899364]\n",
      "epoch:1 step:1116 [D loss: 0.405584, acc.: 82.81%] [G loss: 7.154950]\n",
      "epoch:1 step:1117 [D loss: 0.197100, acc.: 90.62%] [G loss: 6.381151]\n",
      "epoch:1 step:1118 [D loss: 0.137335, acc.: 94.53%] [G loss: 4.277789]\n",
      "epoch:1 step:1119 [D loss: 0.453657, acc.: 80.47%] [G loss: 7.092957]\n",
      "epoch:1 step:1120 [D loss: 0.137124, acc.: 95.31%] [G loss: 7.374579]\n",
      "epoch:1 step:1121 [D loss: 0.352146, acc.: 83.59%] [G loss: 3.633570]\n",
      "epoch:1 step:1122 [D loss: 0.090064, acc.: 98.44%] [G loss: 3.255151]\n",
      "epoch:1 step:1123 [D loss: 0.053738, acc.: 99.22%] [G loss: 4.234817]\n",
      "epoch:1 step:1124 [D loss: 0.085666, acc.: 97.66%] [G loss: 4.335338]\n",
      "epoch:1 step:1125 [D loss: 0.191862, acc.: 95.31%] [G loss: 5.787797]\n",
      "epoch:1 step:1126 [D loss: 0.086518, acc.: 96.88%] [G loss: 4.664493]\n",
      "epoch:1 step:1127 [D loss: 0.382637, acc.: 82.81%] [G loss: 6.687500]\n",
      "epoch:1 step:1128 [D loss: 0.298762, acc.: 86.72%] [G loss: 4.732362]\n",
      "epoch:1 step:1129 [D loss: 0.118918, acc.: 95.31%] [G loss: 3.603720]\n",
      "epoch:1 step:1130 [D loss: 0.070339, acc.: 99.22%] [G loss: 4.119954]\n",
      "epoch:1 step:1131 [D loss: 0.076754, acc.: 99.22%] [G loss: 3.940062]\n",
      "epoch:1 step:1132 [D loss: 0.267569, acc.: 87.50%] [G loss: 5.823949]\n",
      "epoch:1 step:1133 [D loss: 0.229873, acc.: 89.06%] [G loss: 3.920428]\n",
      "epoch:1 step:1134 [D loss: 0.185832, acc.: 92.97%] [G loss: 5.551755]\n",
      "epoch:1 step:1135 [D loss: 0.186724, acc.: 93.75%] [G loss: 4.253917]\n",
      "epoch:1 step:1136 [D loss: 0.055232, acc.: 100.00%] [G loss: 2.355563]\n",
      "epoch:1 step:1137 [D loss: 0.542083, acc.: 68.75%] [G loss: 10.142500]\n",
      "epoch:1 step:1138 [D loss: 1.599150, acc.: 53.91%] [G loss: 6.459804]\n",
      "epoch:1 step:1139 [D loss: 0.214842, acc.: 90.62%] [G loss: 3.596568]\n",
      "epoch:1 step:1140 [D loss: 0.060497, acc.: 99.22%] [G loss: 2.449763]\n",
      "epoch:1 step:1141 [D loss: 0.035349, acc.: 100.00%] [G loss: 2.710403]\n",
      "epoch:1 step:1142 [D loss: 0.096309, acc.: 100.00%] [G loss: 3.364984]\n",
      "epoch:1 step:1143 [D loss: 0.062636, acc.: 100.00%] [G loss: 3.724780]\n",
      "epoch:1 step:1144 [D loss: 0.059708, acc.: 99.22%] [G loss: 2.568938]\n",
      "epoch:1 step:1145 [D loss: 0.142173, acc.: 96.88%] [G loss: 4.176875]\n",
      "epoch:1 step:1146 [D loss: 0.135661, acc.: 97.66%] [G loss: 3.903297]\n",
      "epoch:1 step:1147 [D loss: 0.356739, acc.: 85.94%] [G loss: 4.498864]\n",
      "epoch:1 step:1148 [D loss: 0.097860, acc.: 96.88%] [G loss: 4.632131]\n",
      "epoch:1 step:1149 [D loss: 0.109934, acc.: 98.44%] [G loss: 4.171267]\n",
      "epoch:1 step:1150 [D loss: 0.090858, acc.: 100.00%] [G loss: 5.075148]\n",
      "epoch:1 step:1151 [D loss: 0.051147, acc.: 98.44%] [G loss: 4.087836]\n",
      "epoch:1 step:1152 [D loss: 0.493084, acc.: 73.44%] [G loss: 9.030461]\n",
      "epoch:1 step:1153 [D loss: 1.517146, acc.: 52.34%] [G loss: 5.263571]\n",
      "epoch:1 step:1154 [D loss: 0.041493, acc.: 99.22%] [G loss: 3.908862]\n",
      "epoch:1 step:1155 [D loss: 0.040656, acc.: 99.22%] [G loss: 3.748081]\n",
      "epoch:1 step:1156 [D loss: 0.090443, acc.: 96.88%] [G loss: 4.083508]\n",
      "epoch:1 step:1157 [D loss: 0.087181, acc.: 98.44%] [G loss: 3.782917]\n",
      "epoch:1 step:1158 [D loss: 0.074469, acc.: 99.22%] [G loss: 3.720940]\n",
      "epoch:1 step:1159 [D loss: 0.085995, acc.: 100.00%] [G loss: 3.181312]\n",
      "epoch:1 step:1160 [D loss: 0.189484, acc.: 95.31%] [G loss: 4.241949]\n",
      "epoch:1 step:1161 [D loss: 0.139028, acc.: 96.88%] [G loss: 4.116875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1162 [D loss: 0.216544, acc.: 94.53%] [G loss: 3.545784]\n",
      "epoch:1 step:1163 [D loss: 0.309850, acc.: 88.28%] [G loss: 4.768344]\n",
      "epoch:1 step:1164 [D loss: 0.057521, acc.: 99.22%] [G loss: 4.494447]\n",
      "epoch:1 step:1165 [D loss: 0.070556, acc.: 100.00%] [G loss: 4.329776]\n",
      "epoch:1 step:1166 [D loss: 0.149420, acc.: 97.66%] [G loss: 3.926106]\n",
      "epoch:1 step:1167 [D loss: 0.164313, acc.: 92.97%] [G loss: 5.405762]\n",
      "epoch:1 step:1168 [D loss: 0.090853, acc.: 96.88%] [G loss: 5.237149]\n",
      "epoch:1 step:1169 [D loss: 0.241659, acc.: 89.84%] [G loss: 4.273952]\n",
      "epoch:1 step:1170 [D loss: 0.097311, acc.: 96.09%] [G loss: 4.888300]\n",
      "epoch:1 step:1171 [D loss: 0.188233, acc.: 95.31%] [G loss: 4.183578]\n",
      "epoch:1 step:1172 [D loss: 0.093911, acc.: 98.44%] [G loss: 5.136938]\n",
      "epoch:1 step:1173 [D loss: 0.045123, acc.: 100.00%] [G loss: 3.848265]\n",
      "epoch:1 step:1174 [D loss: 0.454532, acc.: 79.69%] [G loss: 7.195440]\n",
      "epoch:1 step:1175 [D loss: 0.493424, acc.: 77.34%] [G loss: 4.053667]\n",
      "epoch:1 step:1176 [D loss: 0.072601, acc.: 99.22%] [G loss: 2.864270]\n",
      "epoch:1 step:1177 [D loss: 0.117838, acc.: 95.31%] [G loss: 4.440386]\n",
      "epoch:1 step:1178 [D loss: 0.029107, acc.: 99.22%] [G loss: 3.953884]\n",
      "epoch:1 step:1179 [D loss: 0.356009, acc.: 84.38%] [G loss: 6.561456]\n",
      "epoch:1 step:1180 [D loss: 0.203496, acc.: 90.62%] [G loss: 5.260285]\n",
      "epoch:1 step:1181 [D loss: 0.198873, acc.: 94.53%] [G loss: 3.058232]\n",
      "epoch:1 step:1182 [D loss: 0.054897, acc.: 100.00%] [G loss: 3.833463]\n",
      "epoch:1 step:1183 [D loss: 0.182866, acc.: 92.19%] [G loss: 4.493682]\n",
      "epoch:1 step:1184 [D loss: 0.311313, acc.: 88.28%] [G loss: 3.628670]\n",
      "epoch:1 step:1185 [D loss: 0.099385, acc.: 97.66%] [G loss: 3.527245]\n",
      "epoch:1 step:1186 [D loss: 0.737198, acc.: 58.59%] [G loss: 8.438643]\n",
      "epoch:1 step:1187 [D loss: 1.011879, acc.: 62.50%] [G loss: 4.439131]\n",
      "epoch:1 step:1188 [D loss: 0.112413, acc.: 95.31%] [G loss: 3.847887]\n",
      "epoch:1 step:1189 [D loss: 0.051985, acc.: 100.00%] [G loss: 4.164565]\n",
      "epoch:1 step:1190 [D loss: 0.057797, acc.: 100.00%] [G loss: 3.892160]\n",
      "epoch:1 step:1191 [D loss: 0.117152, acc.: 96.09%] [G loss: 4.215943]\n",
      "epoch:1 step:1192 [D loss: 0.172112, acc.: 93.75%] [G loss: 5.344928]\n",
      "epoch:1 step:1193 [D loss: 0.159755, acc.: 96.09%] [G loss: 5.244764]\n",
      "epoch:1 step:1194 [D loss: 0.061022, acc.: 99.22%] [G loss: 3.813850]\n",
      "epoch:1 step:1195 [D loss: 0.381388, acc.: 81.25%] [G loss: 7.299796]\n",
      "epoch:1 step:1196 [D loss: 0.618531, acc.: 66.41%] [G loss: 4.286185]\n",
      "epoch:1 step:1197 [D loss: 0.131406, acc.: 97.66%] [G loss: 4.218516]\n",
      "epoch:1 step:1198 [D loss: 0.045844, acc.: 99.22%] [G loss: 3.309444]\n",
      "epoch:1 step:1199 [D loss: 0.087532, acc.: 99.22%] [G loss: 4.641322]\n",
      "epoch:1 step:1200 [D loss: 0.336138, acc.: 84.38%] [G loss: 6.981571]\n",
      "epoch:1 step:1201 [D loss: 0.657532, acc.: 70.31%] [G loss: 4.368065]\n",
      "epoch:1 step:1202 [D loss: 0.121240, acc.: 96.09%] [G loss: 4.575761]\n",
      "epoch:1 step:1203 [D loss: 0.038765, acc.: 100.00%] [G loss: 4.667986]\n",
      "epoch:1 step:1204 [D loss: 0.072164, acc.: 100.00%] [G loss: 3.411337]\n",
      "epoch:1 step:1205 [D loss: 0.606802, acc.: 71.88%] [G loss: 9.377288]\n",
      "epoch:1 step:1206 [D loss: 1.061278, acc.: 57.81%] [G loss: 5.747546]\n",
      "epoch:1 step:1207 [D loss: 0.060505, acc.: 98.44%] [G loss: 4.213964]\n",
      "epoch:1 step:1208 [D loss: 0.095951, acc.: 97.66%] [G loss: 3.588450]\n",
      "epoch:1 step:1209 [D loss: 0.050051, acc.: 100.00%] [G loss: 3.579110]\n",
      "epoch:1 step:1210 [D loss: 0.029060, acc.: 100.00%] [G loss: 3.085093]\n",
      "epoch:1 step:1211 [D loss: 0.329364, acc.: 83.59%] [G loss: 3.889695]\n",
      "epoch:1 step:1212 [D loss: 0.157372, acc.: 92.97%] [G loss: 3.425060]\n",
      "epoch:1 step:1213 [D loss: 0.147739, acc.: 94.53%] [G loss: 1.668498]\n",
      "epoch:1 step:1214 [D loss: 0.128931, acc.: 97.66%] [G loss: 2.500682]\n",
      "epoch:1 step:1215 [D loss: 0.031634, acc.: 99.22%] [G loss: 2.757572]\n",
      "epoch:1 step:1216 [D loss: 0.294287, acc.: 86.72%] [G loss: 4.577997]\n",
      "epoch:1 step:1217 [D loss: 0.184838, acc.: 92.19%] [G loss: 3.036868]\n",
      "epoch:1 step:1218 [D loss: 0.051622, acc.: 99.22%] [G loss: 2.467646]\n",
      "epoch:1 step:1219 [D loss: 0.029771, acc.: 100.00%] [G loss: 1.634814]\n",
      "epoch:1 step:1220 [D loss: 0.153727, acc.: 96.09%] [G loss: 2.514664]\n",
      "epoch:1 step:1221 [D loss: 0.069905, acc.: 98.44%] [G loss: 1.922224]\n",
      "epoch:1 step:1222 [D loss: 0.311895, acc.: 85.94%] [G loss: 3.255928]\n",
      "epoch:1 step:1223 [D loss: 0.025582, acc.: 100.00%] [G loss: 4.262608]\n",
      "epoch:1 step:1224 [D loss: 0.112415, acc.: 96.88%] [G loss: 3.217590]\n",
      "epoch:1 step:1225 [D loss: 0.148353, acc.: 95.31%] [G loss: 2.637723]\n",
      "epoch:1 step:1226 [D loss: 0.048455, acc.: 99.22%] [G loss: 2.627992]\n",
      "epoch:1 step:1227 [D loss: 0.223356, acc.: 93.75%] [G loss: 5.737737]\n",
      "epoch:1 step:1228 [D loss: 0.084168, acc.: 96.88%] [G loss: 6.455839]\n",
      "epoch:1 step:1229 [D loss: 0.111075, acc.: 96.88%] [G loss: 4.631285]\n",
      "epoch:1 step:1230 [D loss: 0.035562, acc.: 99.22%] [G loss: 3.679940]\n",
      "epoch:1 step:1231 [D loss: 0.032081, acc.: 100.00%] [G loss: 3.505150]\n",
      "epoch:1 step:1232 [D loss: 0.216259, acc.: 91.41%] [G loss: 6.310184]\n",
      "epoch:1 step:1233 [D loss: 0.029013, acc.: 99.22%] [G loss: 7.575586]\n",
      "epoch:1 step:1234 [D loss: 0.270609, acc.: 86.72%] [G loss: 4.310608]\n",
      "epoch:1 step:1235 [D loss: 0.021897, acc.: 100.00%] [G loss: 2.364201]\n",
      "epoch:1 step:1236 [D loss: 0.090976, acc.: 98.44%] [G loss: 2.589118]\n",
      "epoch:1 step:1237 [D loss: 0.021254, acc.: 100.00%] [G loss: 2.788816]\n",
      "epoch:1 step:1238 [D loss: 0.028729, acc.: 100.00%] [G loss: 2.194059]\n",
      "epoch:1 step:1239 [D loss: 0.049444, acc.: 100.00%] [G loss: 1.998482]\n",
      "epoch:1 step:1240 [D loss: 0.073318, acc.: 98.44%] [G loss: 1.180316]\n",
      "epoch:1 step:1241 [D loss: 0.032976, acc.: 100.00%] [G loss: 0.388112]\n",
      "epoch:1 step:1242 [D loss: 0.052118, acc.: 100.00%] [G loss: 0.743037]\n",
      "epoch:1 step:1243 [D loss: 0.023761, acc.: 100.00%] [G loss: 1.166326]\n",
      "epoch:1 step:1244 [D loss: 0.040066, acc.: 100.00%] [G loss: 1.560705]\n",
      "epoch:1 step:1245 [D loss: 0.051510, acc.: 98.44%] [G loss: 1.025809]\n",
      "epoch:1 step:1246 [D loss: 0.273087, acc.: 85.94%] [G loss: 8.338320]\n",
      "epoch:1 step:1247 [D loss: 0.694732, acc.: 71.09%] [G loss: 3.469355]\n",
      "epoch:1 step:1248 [D loss: 0.133587, acc.: 95.31%] [G loss: 4.194917]\n",
      "epoch:1 step:1249 [D loss: 0.003524, acc.: 100.00%] [G loss: 4.672422]\n",
      "epoch:1 step:1250 [D loss: 0.043150, acc.: 97.66%] [G loss: 2.935776]\n",
      "epoch:1 step:1251 [D loss: 0.018197, acc.: 99.22%] [G loss: 1.446046]\n",
      "epoch:1 step:1252 [D loss: 0.257718, acc.: 89.06%] [G loss: 5.898981]\n",
      "epoch:1 step:1253 [D loss: 0.139975, acc.: 94.53%] [G loss: 5.657626]\n",
      "epoch:1 step:1254 [D loss: 0.174831, acc.: 93.75%] [G loss: 1.263451]\n",
      "epoch:1 step:1255 [D loss: 0.265721, acc.: 88.28%] [G loss: 4.402568]\n",
      "epoch:1 step:1256 [D loss: 0.013041, acc.: 100.00%] [G loss: 5.810001]\n",
      "epoch:1 step:1257 [D loss: 0.539087, acc.: 75.78%] [G loss: 0.814811]\n",
      "epoch:1 step:1258 [D loss: 0.623466, acc.: 71.09%] [G loss: 6.745652]\n",
      "epoch:1 step:1259 [D loss: 0.220595, acc.: 89.84%] [G loss: 7.379619]\n",
      "epoch:1 step:1260 [D loss: 0.168989, acc.: 91.41%] [G loss: 6.059529]\n",
      "epoch:1 step:1261 [D loss: 0.019175, acc.: 100.00%] [G loss: 4.217852]\n",
      "epoch:1 step:1262 [D loss: 0.015826, acc.: 100.00%] [G loss: 3.591596]\n",
      "epoch:1 step:1263 [D loss: 0.086960, acc.: 99.22%] [G loss: 4.328802]\n",
      "epoch:1 step:1264 [D loss: 0.055121, acc.: 98.44%] [G loss: 4.366719]\n",
      "epoch:1 step:1265 [D loss: 0.032516, acc.: 100.00%] [G loss: 4.550718]\n",
      "epoch:1 step:1266 [D loss: 0.206190, acc.: 92.19%] [G loss: 4.576917]\n",
      "epoch:1 step:1267 [D loss: 0.123925, acc.: 96.88%] [G loss: 4.525338]\n",
      "epoch:1 step:1268 [D loss: 0.099682, acc.: 99.22%] [G loss: 3.497090]\n",
      "epoch:1 step:1269 [D loss: 0.259382, acc.: 87.50%] [G loss: 6.514039]\n",
      "epoch:1 step:1270 [D loss: 0.164786, acc.: 92.19%] [G loss: 5.787317]\n",
      "epoch:1 step:1271 [D loss: 0.041027, acc.: 100.00%] [G loss: 4.596322]\n",
      "epoch:1 step:1272 [D loss: 0.054927, acc.: 98.44%] [G loss: 4.285303]\n",
      "epoch:1 step:1273 [D loss: 0.024665, acc.: 100.00%] [G loss: 4.467847]\n",
      "epoch:1 step:1274 [D loss: 0.080231, acc.: 98.44%] [G loss: 5.221907]\n",
      "epoch:1 step:1275 [D loss: 0.019971, acc.: 100.00%] [G loss: 4.609332]\n",
      "epoch:1 step:1276 [D loss: 0.032636, acc.: 100.00%] [G loss: 5.238167]\n",
      "epoch:1 step:1277 [D loss: 0.106477, acc.: 99.22%] [G loss: 4.380795]\n",
      "epoch:1 step:1278 [D loss: 0.081717, acc.: 97.66%] [G loss: 3.810369]\n",
      "epoch:1 step:1279 [D loss: 0.068976, acc.: 99.22%] [G loss: 2.919117]\n",
      "epoch:1 step:1280 [D loss: 0.475459, acc.: 78.91%] [G loss: 8.775768]\n",
      "epoch:1 step:1281 [D loss: 0.357454, acc.: 77.34%] [G loss: 7.862157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1282 [D loss: 0.071151, acc.: 97.66%] [G loss: 6.172718]\n",
      "epoch:1 step:1283 [D loss: 0.010494, acc.: 100.00%] [G loss: 5.216728]\n",
      "epoch:1 step:1284 [D loss: 0.012619, acc.: 100.00%] [G loss: 3.930519]\n",
      "epoch:1 step:1285 [D loss: 0.055502, acc.: 98.44%] [G loss: 3.981806]\n",
      "epoch:1 step:1286 [D loss: 0.046079, acc.: 100.00%] [G loss: 5.193477]\n",
      "epoch:1 step:1287 [D loss: 0.056818, acc.: 99.22%] [G loss: 2.791297]\n",
      "epoch:1 step:1288 [D loss: 0.075006, acc.: 98.44%] [G loss: 3.401000]\n",
      "epoch:1 step:1289 [D loss: 0.044302, acc.: 99.22%] [G loss: 4.277659]\n",
      "epoch:1 step:1290 [D loss: 0.080354, acc.: 100.00%] [G loss: 4.044784]\n",
      "epoch:1 step:1291 [D loss: 0.095664, acc.: 97.66%] [G loss: 2.760657]\n",
      "epoch:1 step:1292 [D loss: 0.169840, acc.: 94.53%] [G loss: 2.879839]\n",
      "epoch:1 step:1293 [D loss: 0.065518, acc.: 99.22%] [G loss: 5.567483]\n",
      "epoch:1 step:1294 [D loss: 2.071405, acc.: 32.81%] [G loss: 7.620657]\n",
      "epoch:1 step:1295 [D loss: 0.915082, acc.: 64.06%] [G loss: 4.670581]\n",
      "epoch:1 step:1296 [D loss: 0.164577, acc.: 93.75%] [G loss: 3.241298]\n",
      "epoch:1 step:1297 [D loss: 0.044471, acc.: 99.22%] [G loss: 3.155873]\n",
      "epoch:1 step:1298 [D loss: 0.044128, acc.: 100.00%] [G loss: 3.240726]\n",
      "epoch:1 step:1299 [D loss: 0.061679, acc.: 99.22%] [G loss: 2.416131]\n",
      "epoch:1 step:1300 [D loss: 0.097614, acc.: 98.44%] [G loss: 3.290582]\n",
      "epoch:1 step:1301 [D loss: 0.023251, acc.: 100.00%] [G loss: 4.049103]\n",
      "epoch:1 step:1302 [D loss: 0.095777, acc.: 99.22%] [G loss: 3.464715]\n",
      "epoch:1 step:1303 [D loss: 0.161511, acc.: 95.31%] [G loss: 2.629123]\n",
      "epoch:1 step:1304 [D loss: 0.096031, acc.: 99.22%] [G loss: 2.831533]\n",
      "epoch:1 step:1305 [D loss: 0.089511, acc.: 99.22%] [G loss: 3.751454]\n",
      "epoch:1 step:1306 [D loss: 0.181485, acc.: 92.97%] [G loss: 3.276594]\n",
      "epoch:1 step:1307 [D loss: 0.635463, acc.: 69.53%] [G loss: 6.425795]\n",
      "epoch:1 step:1308 [D loss: 0.792093, acc.: 61.72%] [G loss: 4.397071]\n",
      "epoch:1 step:1309 [D loss: 0.118330, acc.: 96.09%] [G loss: 4.059084]\n",
      "epoch:1 step:1310 [D loss: 0.062299, acc.: 99.22%] [G loss: 4.540001]\n",
      "epoch:1 step:1311 [D loss: 0.087847, acc.: 99.22%] [G loss: 4.127055]\n",
      "epoch:1 step:1312 [D loss: 0.051646, acc.: 100.00%] [G loss: 4.135337]\n",
      "epoch:1 step:1313 [D loss: 0.097604, acc.: 98.44%] [G loss: 4.405714]\n",
      "epoch:1 step:1314 [D loss: 0.067901, acc.: 98.44%] [G loss: 5.245218]\n",
      "epoch:1 step:1315 [D loss: 0.100390, acc.: 98.44%] [G loss: 5.269544]\n",
      "epoch:1 step:1316 [D loss: 0.312874, acc.: 88.28%] [G loss: 5.449207]\n",
      "epoch:1 step:1317 [D loss: 0.080086, acc.: 97.66%] [G loss: 5.208079]\n",
      "epoch:1 step:1318 [D loss: 0.276783, acc.: 90.62%] [G loss: 5.005623]\n",
      "epoch:1 step:1319 [D loss: 0.085299, acc.: 99.22%] [G loss: 5.113297]\n",
      "epoch:1 step:1320 [D loss: 0.052606, acc.: 100.00%] [G loss: 4.963281]\n",
      "epoch:1 step:1321 [D loss: 0.164390, acc.: 96.88%] [G loss: 6.028452]\n",
      "epoch:1 step:1322 [D loss: 0.162719, acc.: 92.97%] [G loss: 3.934390]\n",
      "epoch:1 step:1323 [D loss: 0.101318, acc.: 97.66%] [G loss: 3.984823]\n",
      "epoch:1 step:1324 [D loss: 0.057333, acc.: 97.66%] [G loss: 5.778422]\n",
      "epoch:1 step:1325 [D loss: 0.047147, acc.: 99.22%] [G loss: 3.390872]\n",
      "epoch:1 step:1326 [D loss: 0.233246, acc.: 90.62%] [G loss: 9.032302]\n",
      "epoch:1 step:1327 [D loss: 0.714470, acc.: 70.31%] [G loss: 5.370804]\n",
      "epoch:1 step:1328 [D loss: 0.251781, acc.: 92.19%] [G loss: 5.285601]\n",
      "epoch:1 step:1329 [D loss: 0.020090, acc.: 100.00%] [G loss: 5.791910]\n",
      "epoch:1 step:1330 [D loss: 0.123506, acc.: 95.31%] [G loss: 4.780207]\n",
      "epoch:1 step:1331 [D loss: 0.042778, acc.: 99.22%] [G loss: 4.659625]\n",
      "epoch:1 step:1332 [D loss: 0.216543, acc.: 92.97%] [G loss: 6.901848]\n",
      "epoch:1 step:1333 [D loss: 0.511434, acc.: 75.78%] [G loss: 4.066264]\n",
      "epoch:1 step:1334 [D loss: 0.165775, acc.: 92.19%] [G loss: 6.646809]\n",
      "epoch:1 step:1335 [D loss: 0.065947, acc.: 98.44%] [G loss: 4.800501]\n",
      "epoch:1 step:1336 [D loss: 0.705410, acc.: 67.19%] [G loss: 9.497614]\n",
      "epoch:1 step:1337 [D loss: 0.681887, acc.: 67.97%] [G loss: 4.749535]\n",
      "epoch:1 step:1338 [D loss: 0.159491, acc.: 95.31%] [G loss: 4.941696]\n",
      "epoch:1 step:1339 [D loss: 0.028653, acc.: 99.22%] [G loss: 4.935669]\n",
      "epoch:1 step:1340 [D loss: 0.311184, acc.: 86.72%] [G loss: 5.243337]\n",
      "epoch:1 step:1341 [D loss: 0.118241, acc.: 96.88%] [G loss: 4.330146]\n",
      "epoch:1 step:1342 [D loss: 0.325731, acc.: 85.94%] [G loss: 6.495209]\n",
      "epoch:1 step:1343 [D loss: 0.473806, acc.: 85.94%] [G loss: 4.792153]\n",
      "epoch:1 step:1344 [D loss: 0.124745, acc.: 96.09%] [G loss: 4.104225]\n",
      "epoch:1 step:1345 [D loss: 0.146253, acc.: 96.88%] [G loss: 3.525640]\n",
      "epoch:1 step:1346 [D loss: 0.089481, acc.: 99.22%] [G loss: 4.553842]\n",
      "epoch:1 step:1347 [D loss: 0.251873, acc.: 89.84%] [G loss: 4.955155]\n",
      "epoch:1 step:1348 [D loss: 0.086382, acc.: 98.44%] [G loss: 4.688218]\n",
      "epoch:1 step:1349 [D loss: 0.243216, acc.: 92.97%] [G loss: 5.871769]\n",
      "epoch:1 step:1350 [D loss: 0.255859, acc.: 89.84%] [G loss: 3.405502]\n",
      "epoch:1 step:1351 [D loss: 0.147051, acc.: 95.31%] [G loss: 5.448387]\n",
      "epoch:1 step:1352 [D loss: 0.033576, acc.: 100.00%] [G loss: 5.219467]\n",
      "epoch:1 step:1353 [D loss: 0.081517, acc.: 96.88%] [G loss: 3.040318]\n",
      "epoch:1 step:1354 [D loss: 0.428916, acc.: 77.34%] [G loss: 8.349569]\n",
      "epoch:1 step:1355 [D loss: 0.771158, acc.: 64.84%] [G loss: 5.360819]\n",
      "epoch:1 step:1356 [D loss: 0.057473, acc.: 98.44%] [G loss: 3.863856]\n",
      "epoch:1 step:1357 [D loss: 0.031866, acc.: 100.00%] [G loss: 3.559175]\n",
      "epoch:1 step:1358 [D loss: 0.046690, acc.: 99.22%] [G loss: 2.476702]\n",
      "epoch:1 step:1359 [D loss: 0.130395, acc.: 95.31%] [G loss: 3.991532]\n",
      "epoch:1 step:1360 [D loss: 0.029430, acc.: 100.00%] [G loss: 3.013053]\n",
      "epoch:1 step:1361 [D loss: 0.926867, acc.: 50.78%] [G loss: 7.394932]\n",
      "epoch:1 step:1362 [D loss: 0.677028, acc.: 68.75%] [G loss: 5.303394]\n",
      "epoch:1 step:1363 [D loss: 0.020792, acc.: 100.00%] [G loss: 3.538653]\n",
      "epoch:1 step:1364 [D loss: 0.092329, acc.: 98.44%] [G loss: 2.635900]\n",
      "epoch:1 step:1365 [D loss: 0.081300, acc.: 98.44%] [G loss: 3.023163]\n",
      "epoch:1 step:1366 [D loss: 0.041811, acc.: 100.00%] [G loss: 2.064052]\n",
      "epoch:1 step:1367 [D loss: 0.132825, acc.: 95.31%] [G loss: 2.904717]\n",
      "epoch:1 step:1368 [D loss: 0.091749, acc.: 98.44%] [G loss: 3.223675]\n",
      "epoch:1 step:1369 [D loss: 0.204044, acc.: 95.31%] [G loss: 3.207096]\n",
      "epoch:1 step:1370 [D loss: 0.231824, acc.: 89.84%] [G loss: 3.794048]\n",
      "epoch:1 step:1371 [D loss: 0.180146, acc.: 94.53%] [G loss: 3.986852]\n",
      "epoch:1 step:1372 [D loss: 0.115839, acc.: 97.66%] [G loss: 3.145578]\n",
      "epoch:1 step:1373 [D loss: 0.059608, acc.: 99.22%] [G loss: 4.469567]\n",
      "epoch:1 step:1374 [D loss: 0.206167, acc.: 91.41%] [G loss: 3.828148]\n",
      "epoch:1 step:1375 [D loss: 0.051920, acc.: 99.22%] [G loss: 4.483271]\n",
      "epoch:1 step:1376 [D loss: 0.743307, acc.: 64.06%] [G loss: 7.799125]\n",
      "epoch:1 step:1377 [D loss: 0.582109, acc.: 69.53%] [G loss: 3.928193]\n",
      "epoch:1 step:1378 [D loss: 0.139806, acc.: 95.31%] [G loss: 5.134420]\n",
      "epoch:1 step:1379 [D loss: 0.025751, acc.: 100.00%] [G loss: 5.100777]\n",
      "epoch:1 step:1380 [D loss: 0.036817, acc.: 100.00%] [G loss: 4.098766]\n",
      "epoch:1 step:1381 [D loss: 0.048499, acc.: 100.00%] [G loss: 3.340505]\n",
      "epoch:1 step:1382 [D loss: 0.112927, acc.: 98.44%] [G loss: 3.331589]\n",
      "epoch:1 step:1383 [D loss: 0.183843, acc.: 97.66%] [G loss: 3.489286]\n",
      "epoch:1 step:1384 [D loss: 0.200820, acc.: 95.31%] [G loss: 4.444655]\n",
      "epoch:1 step:1385 [D loss: 0.111581, acc.: 96.88%] [G loss: 2.451409]\n",
      "epoch:1 step:1386 [D loss: 0.076343, acc.: 99.22%] [G loss: 1.790757]\n",
      "epoch:1 step:1387 [D loss: 0.040136, acc.: 100.00%] [G loss: 0.921105]\n",
      "epoch:1 step:1388 [D loss: 0.040028, acc.: 100.00%] [G loss: 0.476065]\n",
      "epoch:1 step:1389 [D loss: 0.060745, acc.: 100.00%] [G loss: 0.456678]\n",
      "epoch:1 step:1390 [D loss: 0.101375, acc.: 96.88%] [G loss: 1.191071]\n",
      "epoch:1 step:1391 [D loss: 0.125003, acc.: 96.88%] [G loss: 2.812190]\n",
      "epoch:1 step:1392 [D loss: 0.023977, acc.: 100.00%] [G loss: 1.750331]\n",
      "epoch:1 step:1393 [D loss: 0.314749, acc.: 82.03%] [G loss: 7.802838]\n",
      "epoch:1 step:1394 [D loss: 1.201595, acc.: 55.47%] [G loss: 3.084871]\n",
      "epoch:1 step:1395 [D loss: 0.045848, acc.: 98.44%] [G loss: 3.042535]\n",
      "epoch:1 step:1396 [D loss: 0.049811, acc.: 99.22%] [G loss: 2.495858]\n",
      "epoch:1 step:1397 [D loss: 0.058381, acc.: 99.22%] [G loss: 2.340157]\n",
      "epoch:1 step:1398 [D loss: 0.628016, acc.: 71.09%] [G loss: 6.157299]\n",
      "epoch:1 step:1399 [D loss: 0.418791, acc.: 79.69%] [G loss: 4.473146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1400 [D loss: 0.084060, acc.: 97.66%] [G loss: 3.141073]\n",
      "epoch:1 step:1401 [D loss: 0.238981, acc.: 91.41%] [G loss: 4.768527]\n",
      "epoch:1 step:1402 [D loss: 0.022744, acc.: 100.00%] [G loss: 4.412745]\n",
      "epoch:1 step:1403 [D loss: 0.199166, acc.: 91.41%] [G loss: 4.099009]\n",
      "epoch:1 step:1404 [D loss: 0.185785, acc.: 96.88%] [G loss: 4.517927]\n",
      "epoch:1 step:1405 [D loss: 0.052598, acc.: 100.00%] [G loss: 4.285618]\n",
      "epoch:1 step:1406 [D loss: 0.080087, acc.: 98.44%] [G loss: 3.954583]\n",
      "epoch:1 step:1407 [D loss: 0.168139, acc.: 94.53%] [G loss: 5.878530]\n",
      "epoch:1 step:1408 [D loss: 0.116299, acc.: 96.09%] [G loss: 5.595915]\n",
      "epoch:1 step:1409 [D loss: 0.046197, acc.: 100.00%] [G loss: 4.838001]\n",
      "epoch:1 step:1410 [D loss: 0.058254, acc.: 100.00%] [G loss: 4.848511]\n",
      "epoch:1 step:1411 [D loss: 0.035215, acc.: 100.00%] [G loss: 4.946867]\n",
      "epoch:1 step:1412 [D loss: 0.071455, acc.: 100.00%] [G loss: 5.150913]\n",
      "epoch:1 step:1413 [D loss: 0.060464, acc.: 100.00%] [G loss: 5.426352]\n",
      "epoch:1 step:1414 [D loss: 0.510403, acc.: 74.22%] [G loss: 8.020882]\n",
      "epoch:1 step:1415 [D loss: 0.101303, acc.: 96.09%] [G loss: 8.466324]\n",
      "epoch:1 step:1416 [D loss: 0.087694, acc.: 98.44%] [G loss: 6.929256]\n",
      "epoch:1 step:1417 [D loss: 0.060188, acc.: 98.44%] [G loss: 5.264365]\n",
      "epoch:1 step:1418 [D loss: 0.156545, acc.: 93.75%] [G loss: 5.555834]\n",
      "epoch:1 step:1419 [D loss: 0.033225, acc.: 100.00%] [G loss: 5.446756]\n",
      "epoch:1 step:1420 [D loss: 0.182433, acc.: 91.41%] [G loss: 6.302852]\n",
      "epoch:1 step:1421 [D loss: 0.142573, acc.: 96.09%] [G loss: 4.529090]\n",
      "epoch:1 step:1422 [D loss: 0.204608, acc.: 88.28%] [G loss: 7.310071]\n",
      "epoch:1 step:1423 [D loss: 0.084002, acc.: 97.66%] [G loss: 8.077121]\n",
      "epoch:1 step:1424 [D loss: 0.170295, acc.: 90.62%] [G loss: 5.207010]\n",
      "epoch:1 step:1425 [D loss: 0.049156, acc.: 100.00%] [G loss: 4.998445]\n",
      "epoch:1 step:1426 [D loss: 0.034878, acc.: 99.22%] [G loss: 5.264139]\n",
      "epoch:1 step:1427 [D loss: 0.026211, acc.: 100.00%] [G loss: 4.457803]\n",
      "epoch:1 step:1428 [D loss: 0.134511, acc.: 97.66%] [G loss: 4.463506]\n",
      "epoch:1 step:1429 [D loss: 0.035735, acc.: 99.22%] [G loss: 4.866160]\n",
      "epoch:1 step:1430 [D loss: 0.076945, acc.: 99.22%] [G loss: 3.770922]\n",
      "epoch:1 step:1431 [D loss: 0.108277, acc.: 97.66%] [G loss: 5.341513]\n",
      "epoch:1 step:1432 [D loss: 0.044049, acc.: 99.22%] [G loss: 4.908556]\n",
      "epoch:1 step:1433 [D loss: 0.075594, acc.: 99.22%] [G loss: 2.845869]\n",
      "epoch:1 step:1434 [D loss: 0.146785, acc.: 96.09%] [G loss: 6.669479]\n",
      "epoch:1 step:1435 [D loss: 0.206054, acc.: 89.06%] [G loss: 4.112874]\n",
      "epoch:1 step:1436 [D loss: 0.105557, acc.: 97.66%] [G loss: 2.797992]\n",
      "epoch:1 step:1437 [D loss: 0.027674, acc.: 99.22%] [G loss: 2.773483]\n",
      "epoch:1 step:1438 [D loss: 0.061904, acc.: 99.22%] [G loss: 0.660940]\n",
      "epoch:1 step:1439 [D loss: 0.227816, acc.: 92.19%] [G loss: 7.954338]\n",
      "epoch:1 step:1440 [D loss: 1.162281, acc.: 61.72%] [G loss: 1.457068]\n",
      "epoch:1 step:1441 [D loss: 0.390430, acc.: 77.34%] [G loss: 7.830472]\n",
      "epoch:1 step:1442 [D loss: 0.208760, acc.: 94.53%] [G loss: 8.054446]\n",
      "epoch:1 step:1443 [D loss: 0.161928, acc.: 93.75%] [G loss: 4.304928]\n",
      "epoch:1 step:1444 [D loss: 0.205250, acc.: 92.19%] [G loss: 5.426106]\n",
      "epoch:1 step:1445 [D loss: 0.092390, acc.: 96.88%] [G loss: 4.749309]\n",
      "epoch:1 step:1446 [D loss: 0.482635, acc.: 75.00%] [G loss: 7.264297]\n",
      "epoch:1 step:1447 [D loss: 0.456138, acc.: 81.25%] [G loss: 5.498872]\n",
      "epoch:1 step:1448 [D loss: 0.079615, acc.: 96.88%] [G loss: 4.078994]\n",
      "epoch:1 step:1449 [D loss: 0.066802, acc.: 99.22%] [G loss: 4.477055]\n",
      "epoch:1 step:1450 [D loss: 0.231740, acc.: 92.19%] [G loss: 5.516842]\n",
      "epoch:1 step:1451 [D loss: 0.202638, acc.: 91.41%] [G loss: 3.348812]\n",
      "epoch:1 step:1452 [D loss: 0.101865, acc.: 97.66%] [G loss: 4.696708]\n",
      "epoch:1 step:1453 [D loss: 0.093971, acc.: 97.66%] [G loss: 5.066771]\n",
      "epoch:1 step:1454 [D loss: 0.573516, acc.: 69.53%] [G loss: 6.841420]\n",
      "epoch:1 step:1455 [D loss: 0.227609, acc.: 91.41%] [G loss: 5.792436]\n",
      "epoch:1 step:1456 [D loss: 0.025106, acc.: 100.00%] [G loss: 5.071833]\n",
      "epoch:1 step:1457 [D loss: 0.056948, acc.: 99.22%] [G loss: 3.731766]\n",
      "epoch:1 step:1458 [D loss: 0.060865, acc.: 99.22%] [G loss: 3.890786]\n",
      "epoch:1 step:1459 [D loss: 0.044797, acc.: 100.00%] [G loss: 3.223009]\n",
      "epoch:1 step:1460 [D loss: 0.107617, acc.: 98.44%] [G loss: 2.128880]\n",
      "epoch:1 step:1461 [D loss: 0.510863, acc.: 75.78%] [G loss: 7.836003]\n",
      "epoch:1 step:1462 [D loss: 0.521023, acc.: 77.34%] [G loss: 5.220775]\n",
      "epoch:1 step:1463 [D loss: 0.061237, acc.: 99.22%] [G loss: 3.793078]\n",
      "epoch:1 step:1464 [D loss: 0.024492, acc.: 100.00%] [G loss: 2.926324]\n",
      "epoch:1 step:1465 [D loss: 0.201305, acc.: 90.62%] [G loss: 6.040760]\n",
      "epoch:1 step:1466 [D loss: 0.162879, acc.: 92.19%] [G loss: 5.626882]\n",
      "epoch:1 step:1467 [D loss: 0.073517, acc.: 97.66%] [G loss: 3.822602]\n",
      "epoch:1 step:1468 [D loss: 0.187019, acc.: 93.75%] [G loss: 6.594238]\n",
      "epoch:1 step:1469 [D loss: 0.133592, acc.: 92.97%] [G loss: 6.322023]\n",
      "epoch:1 step:1470 [D loss: 0.012713, acc.: 100.00%] [G loss: 4.958330]\n",
      "epoch:1 step:1471 [D loss: 0.084634, acc.: 97.66%] [G loss: 5.200539]\n",
      "epoch:1 step:1472 [D loss: 0.032697, acc.: 100.00%] [G loss: 5.217478]\n",
      "epoch:1 step:1473 [D loss: 0.095241, acc.: 97.66%] [G loss: 4.586406]\n",
      "epoch:1 step:1474 [D loss: 0.095446, acc.: 97.66%] [G loss: 6.109784]\n",
      "epoch:1 step:1475 [D loss: 0.068470, acc.: 98.44%] [G loss: 5.209867]\n",
      "epoch:1 step:1476 [D loss: 0.196621, acc.: 93.75%] [G loss: 7.762503]\n",
      "epoch:1 step:1477 [D loss: 0.662166, acc.: 70.31%] [G loss: 3.631546]\n",
      "epoch:1 step:1478 [D loss: 0.076442, acc.: 97.66%] [G loss: 5.600595]\n",
      "epoch:1 step:1479 [D loss: 0.001117, acc.: 100.00%] [G loss: 6.187107]\n",
      "epoch:1 step:1480 [D loss: 0.003806, acc.: 100.00%] [G loss: 2.817376]\n",
      "epoch:1 step:1481 [D loss: 0.015222, acc.: 100.00%] [G loss: 1.841075]\n",
      "epoch:1 step:1482 [D loss: 0.031196, acc.: 100.00%] [G loss: 0.814245]\n",
      "epoch:1 step:1483 [D loss: 0.029673, acc.: 100.00%] [G loss: 1.068341]\n",
      "epoch:1 step:1484 [D loss: 0.037125, acc.: 100.00%] [G loss: 0.664925]\n",
      "epoch:1 step:1485 [D loss: 0.037583, acc.: 100.00%] [G loss: 0.850425]\n",
      "epoch:1 step:1486 [D loss: 0.156113, acc.: 92.97%] [G loss: 0.432693]\n",
      "epoch:1 step:1487 [D loss: 0.070960, acc.: 100.00%] [G loss: 0.892381]\n",
      "epoch:1 step:1488 [D loss: 0.066133, acc.: 99.22%] [G loss: 2.377172]\n",
      "epoch:1 step:1489 [D loss: 0.034520, acc.: 99.22%] [G loss: 1.941972]\n",
      "epoch:1 step:1490 [D loss: 1.091650, acc.: 53.12%] [G loss: 12.802614]\n",
      "epoch:1 step:1491 [D loss: 3.834028, acc.: 50.78%] [G loss: 7.053651]\n",
      "epoch:1 step:1492 [D loss: 0.867741, acc.: 62.50%] [G loss: 2.908035]\n",
      "epoch:1 step:1493 [D loss: 0.112221, acc.: 98.44%] [G loss: 2.166414]\n",
      "epoch:1 step:1494 [D loss: 0.119348, acc.: 98.44%] [G loss: 2.664905]\n",
      "epoch:1 step:1495 [D loss: 0.086300, acc.: 99.22%] [G loss: 2.365692]\n",
      "epoch:1 step:1496 [D loss: 0.149858, acc.: 96.09%] [G loss: 2.776341]\n",
      "epoch:1 step:1497 [D loss: 0.179520, acc.: 96.88%] [G loss: 3.141958]\n",
      "epoch:1 step:1498 [D loss: 0.113185, acc.: 99.22%] [G loss: 3.041592]\n",
      "epoch:1 step:1499 [D loss: 0.427415, acc.: 83.59%] [G loss: 3.786530]\n",
      "epoch:1 step:1500 [D loss: 0.139408, acc.: 97.66%] [G loss: 4.244188]\n",
      "epoch:1 step:1501 [D loss: 0.301308, acc.: 91.41%] [G loss: 3.757628]\n",
      "epoch:1 step:1502 [D loss: 0.132915, acc.: 98.44%] [G loss: 3.538016]\n",
      "epoch:1 step:1503 [D loss: 0.138513, acc.: 97.66%] [G loss: 3.628501]\n",
      "epoch:1 step:1504 [D loss: 0.109816, acc.: 99.22%] [G loss: 3.724275]\n",
      "epoch:1 step:1505 [D loss: 0.175824, acc.: 96.09%] [G loss: 2.643608]\n",
      "epoch:1 step:1506 [D loss: 0.050773, acc.: 100.00%] [G loss: 2.249339]\n",
      "epoch:1 step:1507 [D loss: 0.239207, acc.: 91.41%] [G loss: 5.522617]\n",
      "epoch:1 step:1508 [D loss: 0.887570, acc.: 56.25%] [G loss: 5.505726]\n",
      "epoch:1 step:1509 [D loss: 0.129688, acc.: 94.53%] [G loss: 5.462314]\n",
      "epoch:1 step:1510 [D loss: 0.135068, acc.: 96.09%] [G loss: 4.006007]\n",
      "epoch:1 step:1511 [D loss: 0.158399, acc.: 94.53%] [G loss: 4.379833]\n",
      "epoch:1 step:1512 [D loss: 0.102442, acc.: 97.66%] [G loss: 3.135216]\n",
      "epoch:1 step:1513 [D loss: 0.227905, acc.: 89.06%] [G loss: 5.873972]\n",
      "epoch:1 step:1514 [D loss: 0.178470, acc.: 90.62%] [G loss: 4.429319]\n",
      "epoch:1 step:1515 [D loss: 0.914227, acc.: 57.81%] [G loss: 5.402245]\n",
      "epoch:1 step:1516 [D loss: 0.314861, acc.: 86.72%] [G loss: 5.326752]\n",
      "epoch:1 step:1517 [D loss: 0.069154, acc.: 98.44%] [G loss: 4.051811]\n",
      "epoch:1 step:1518 [D loss: 0.041502, acc.: 99.22%] [G loss: 3.912612]\n",
      "epoch:1 step:1519 [D loss: 0.077618, acc.: 100.00%] [G loss: 3.571799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1520 [D loss: 0.033150, acc.: 100.00%] [G loss: 4.013920]\n",
      "epoch:1 step:1521 [D loss: 0.059149, acc.: 100.00%] [G loss: 3.339943]\n",
      "epoch:1 step:1522 [D loss: 0.071226, acc.: 98.44%] [G loss: 4.010157]\n",
      "epoch:1 step:1523 [D loss: 0.045785, acc.: 100.00%] [G loss: 3.862318]\n",
      "epoch:1 step:1524 [D loss: 0.113275, acc.: 97.66%] [G loss: 4.355911]\n",
      "epoch:1 step:1525 [D loss: 0.079919, acc.: 100.00%] [G loss: 3.675225]\n",
      "epoch:1 step:1526 [D loss: 0.131306, acc.: 97.66%] [G loss: 4.712625]\n",
      "epoch:1 step:1527 [D loss: 0.101515, acc.: 95.31%] [G loss: 3.864789]\n",
      "epoch:1 step:1528 [D loss: 0.158354, acc.: 94.53%] [G loss: 4.646631]\n",
      "epoch:1 step:1529 [D loss: 0.036321, acc.: 99.22%] [G loss: 4.676093]\n",
      "epoch:1 step:1530 [D loss: 0.070619, acc.: 99.22%] [G loss: 4.435329]\n",
      "epoch:1 step:1531 [D loss: 0.051683, acc.: 100.00%] [G loss: 3.688091]\n",
      "epoch:1 step:1532 [D loss: 0.069068, acc.: 99.22%] [G loss: 3.833489]\n",
      "epoch:1 step:1533 [D loss: 0.085274, acc.: 98.44%] [G loss: 3.039504]\n",
      "epoch:1 step:1534 [D loss: 0.082570, acc.: 99.22%] [G loss: 3.872673]\n",
      "epoch:1 step:1535 [D loss: 0.145549, acc.: 92.97%] [G loss: 3.101261]\n",
      "epoch:1 step:1536 [D loss: 0.096165, acc.: 100.00%] [G loss: 4.017854]\n",
      "epoch:1 step:1537 [D loss: 0.055826, acc.: 99.22%] [G loss: 3.701728]\n",
      "epoch:1 step:1538 [D loss: 0.126160, acc.: 96.88%] [G loss: 4.331753]\n",
      "epoch:1 step:1539 [D loss: 0.248961, acc.: 89.84%] [G loss: 4.864846]\n",
      "epoch:1 step:1540 [D loss: 0.161564, acc.: 93.75%] [G loss: 2.634304]\n",
      "epoch:1 step:1541 [D loss: 0.529158, acc.: 71.09%] [G loss: 9.140114]\n",
      "epoch:1 step:1542 [D loss: 1.291723, acc.: 57.03%] [G loss: 2.488202]\n",
      "epoch:1 step:1543 [D loss: 0.048604, acc.: 100.00%] [G loss: 1.235417]\n",
      "epoch:1 step:1544 [D loss: 0.057712, acc.: 99.22%] [G loss: 1.388507]\n",
      "epoch:1 step:1545 [D loss: 0.073781, acc.: 98.44%] [G loss: 2.027102]\n",
      "epoch:1 step:1546 [D loss: 0.034595, acc.: 100.00%] [G loss: 2.122886]\n",
      "epoch:1 step:1547 [D loss: 0.165368, acc.: 92.97%] [G loss: 3.067199]\n",
      "epoch:1 step:1548 [D loss: 0.103243, acc.: 96.88%] [G loss: 3.577298]\n",
      "epoch:1 step:1549 [D loss: 0.346627, acc.: 83.59%] [G loss: 4.363863]\n",
      "epoch:1 step:1550 [D loss: 0.328950, acc.: 85.94%] [G loss: 3.836124]\n",
      "epoch:1 step:1551 [D loss: 0.177187, acc.: 95.31%] [G loss: 5.712444]\n",
      "epoch:1 step:1552 [D loss: 0.136749, acc.: 96.09%] [G loss: 3.917284]\n",
      "epoch:1 step:1553 [D loss: 0.157910, acc.: 94.53%] [G loss: 4.606355]\n",
      "epoch:1 step:1554 [D loss: 0.076310, acc.: 98.44%] [G loss: 4.661511]\n",
      "epoch:1 step:1555 [D loss: 0.193640, acc.: 90.62%] [G loss: 5.042162]\n",
      "epoch:1 step:1556 [D loss: 0.262041, acc.: 91.41%] [G loss: 4.457775]\n",
      "epoch:1 step:1557 [D loss: 0.106899, acc.: 97.66%] [G loss: 5.501267]\n",
      "epoch:1 step:1558 [D loss: 0.091440, acc.: 96.88%] [G loss: 3.775017]\n",
      "epoch:1 step:1559 [D loss: 0.140070, acc.: 94.53%] [G loss: 5.245382]\n",
      "epoch:1 step:1560 [D loss: 0.339432, acc.: 85.94%] [G loss: 3.159917]\n",
      "epoch:1 step:1561 [D loss: 0.342834, acc.: 84.38%] [G loss: 8.146817]\n",
      "epoch:1 step:1562 [D loss: 0.367334, acc.: 82.81%] [G loss: 5.841417]\n",
      "epoch:2 step:1563 [D loss: 0.247007, acc.: 88.28%] [G loss: 5.970124]\n",
      "epoch:2 step:1564 [D loss: 0.017003, acc.: 100.00%] [G loss: 6.713019]\n",
      "epoch:2 step:1565 [D loss: 0.077411, acc.: 98.44%] [G loss: 5.663240]\n",
      "epoch:2 step:1566 [D loss: 0.031624, acc.: 100.00%] [G loss: 4.678786]\n",
      "epoch:2 step:1567 [D loss: 0.087977, acc.: 96.88%] [G loss: 4.624766]\n",
      "epoch:2 step:1568 [D loss: 0.057784, acc.: 98.44%] [G loss: 4.734841]\n",
      "epoch:2 step:1569 [D loss: 0.406613, acc.: 76.56%] [G loss: 8.573715]\n",
      "epoch:2 step:1570 [D loss: 0.832682, acc.: 71.09%] [G loss: 4.372298]\n",
      "epoch:2 step:1571 [D loss: 0.167741, acc.: 94.53%] [G loss: 4.030288]\n",
      "epoch:2 step:1572 [D loss: 0.035400, acc.: 99.22%] [G loss: 4.965111]\n",
      "epoch:2 step:1573 [D loss: 0.097750, acc.: 96.88%] [G loss: 3.106455]\n",
      "epoch:2 step:1574 [D loss: 0.102320, acc.: 97.66%] [G loss: 2.419904]\n",
      "epoch:2 step:1575 [D loss: 0.127303, acc.: 96.88%] [G loss: 4.033945]\n",
      "epoch:2 step:1576 [D loss: 0.174858, acc.: 96.88%] [G loss: 3.174161]\n",
      "epoch:2 step:1577 [D loss: 0.202887, acc.: 91.41%] [G loss: 5.633786]\n",
      "epoch:2 step:1578 [D loss: 0.106786, acc.: 96.88%] [G loss: 4.469180]\n",
      "epoch:2 step:1579 [D loss: 0.083128, acc.: 97.66%] [G loss: 3.549944]\n",
      "epoch:2 step:1580 [D loss: 0.236084, acc.: 90.62%] [G loss: 6.778142]\n",
      "epoch:2 step:1581 [D loss: 0.376290, acc.: 85.94%] [G loss: 4.138732]\n",
      "epoch:2 step:1582 [D loss: 0.164962, acc.: 92.97%] [G loss: 6.751237]\n",
      "epoch:2 step:1583 [D loss: 0.092995, acc.: 97.66%] [G loss: 6.303949]\n",
      "epoch:2 step:1584 [D loss: 0.117767, acc.: 97.66%] [G loss: 4.820737]\n",
      "epoch:2 step:1585 [D loss: 0.412915, acc.: 85.16%] [G loss: 5.531347]\n",
      "epoch:2 step:1586 [D loss: 0.118753, acc.: 94.53%] [G loss: 5.259259]\n",
      "epoch:2 step:1587 [D loss: 0.088570, acc.: 96.88%] [G loss: 4.734879]\n",
      "epoch:2 step:1588 [D loss: 0.018744, acc.: 100.00%] [G loss: 4.658373]\n",
      "epoch:2 step:1589 [D loss: 0.054320, acc.: 100.00%] [G loss: 3.735196]\n",
      "epoch:2 step:1590 [D loss: 0.264850, acc.: 88.28%] [G loss: 5.816149]\n",
      "epoch:2 step:1591 [D loss: 0.317182, acc.: 84.38%] [G loss: 4.178443]\n",
      "epoch:2 step:1592 [D loss: 0.184942, acc.: 93.75%] [G loss: 5.088377]\n",
      "epoch:2 step:1593 [D loss: 0.053631, acc.: 97.66%] [G loss: 6.322154]\n",
      "epoch:2 step:1594 [D loss: 0.124814, acc.: 96.88%] [G loss: 4.819192]\n",
      "epoch:2 step:1595 [D loss: 0.167163, acc.: 93.75%] [G loss: 6.965013]\n",
      "epoch:2 step:1596 [D loss: 0.220123, acc.: 89.06%] [G loss: 4.728114]\n",
      "epoch:2 step:1597 [D loss: 0.234420, acc.: 91.41%] [G loss: 5.711480]\n",
      "epoch:2 step:1598 [D loss: 0.253078, acc.: 90.62%] [G loss: 3.202148]\n",
      "epoch:2 step:1599 [D loss: 0.255352, acc.: 87.50%] [G loss: 7.432478]\n",
      "epoch:2 step:1600 [D loss: 0.219022, acc.: 89.06%] [G loss: 5.115341]\n",
      "epoch:2 step:1601 [D loss: 0.038330, acc.: 99.22%] [G loss: 3.717175]\n",
      "epoch:2 step:1602 [D loss: 0.270993, acc.: 87.50%] [G loss: 6.981978]\n",
      "epoch:2 step:1603 [D loss: 0.262662, acc.: 86.72%] [G loss: 4.438041]\n",
      "epoch:2 step:1604 [D loss: 0.134357, acc.: 95.31%] [G loss: 5.469561]\n",
      "epoch:2 step:1605 [D loss: 0.024454, acc.: 99.22%] [G loss: 4.713008]\n",
      "epoch:2 step:1606 [D loss: 0.049270, acc.: 98.44%] [G loss: 3.902595]\n",
      "epoch:2 step:1607 [D loss: 0.108599, acc.: 96.88%] [G loss: 5.231334]\n",
      "epoch:2 step:1608 [D loss: 0.077486, acc.: 98.44%] [G loss: 3.880253]\n",
      "epoch:2 step:1609 [D loss: 0.115253, acc.: 96.88%] [G loss: 4.312074]\n",
      "epoch:2 step:1610 [D loss: 0.142831, acc.: 97.66%] [G loss: 6.037942]\n",
      "epoch:2 step:1611 [D loss: 0.735380, acc.: 61.72%] [G loss: 3.856759]\n",
      "epoch:2 step:1612 [D loss: 0.010242, acc.: 100.00%] [G loss: 4.858290]\n",
      "epoch:2 step:1613 [D loss: 0.007044, acc.: 100.00%] [G loss: 3.325638]\n",
      "epoch:2 step:1614 [D loss: 0.043835, acc.: 99.22%] [G loss: 1.733848]\n",
      "epoch:2 step:1615 [D loss: 0.110244, acc.: 96.09%] [G loss: 3.819253]\n",
      "epoch:2 step:1616 [D loss: 0.158013, acc.: 94.53%] [G loss: 1.491054]\n",
      "epoch:2 step:1617 [D loss: 0.203056, acc.: 89.84%] [G loss: 6.568321]\n",
      "epoch:2 step:1618 [D loss: 0.156162, acc.: 92.19%] [G loss: 5.216921]\n",
      "epoch:2 step:1619 [D loss: 0.077520, acc.: 98.44%] [G loss: 2.660895]\n",
      "epoch:2 step:1620 [D loss: 0.240028, acc.: 89.06%] [G loss: 6.862098]\n",
      "epoch:2 step:1621 [D loss: 0.063229, acc.: 97.66%] [G loss: 7.241220]\n",
      "epoch:2 step:1622 [D loss: 0.040312, acc.: 99.22%] [G loss: 5.849330]\n",
      "epoch:2 step:1623 [D loss: 0.062590, acc.: 99.22%] [G loss: 3.205924]\n",
      "epoch:2 step:1624 [D loss: 0.354385, acc.: 85.94%] [G loss: 7.244867]\n",
      "epoch:2 step:1625 [D loss: 0.084715, acc.: 96.09%] [G loss: 7.227127]\n",
      "epoch:2 step:1626 [D loss: 0.241579, acc.: 90.62%] [G loss: 3.170017]\n",
      "epoch:2 step:1627 [D loss: 0.176453, acc.: 94.53%] [G loss: 2.940351]\n",
      "epoch:2 step:1628 [D loss: 0.026648, acc.: 100.00%] [G loss: 5.638207]\n",
      "epoch:2 step:1629 [D loss: 0.418480, acc.: 78.12%] [G loss: 8.273993]\n",
      "epoch:2 step:1630 [D loss: 0.680680, acc.: 70.31%] [G loss: 2.231798]\n",
      "epoch:2 step:1631 [D loss: 0.183952, acc.: 92.97%] [G loss: 3.442501]\n",
      "epoch:2 step:1632 [D loss: 0.060519, acc.: 97.66%] [G loss: 5.266728]\n",
      "epoch:2 step:1633 [D loss: 0.070259, acc.: 97.66%] [G loss: 4.014009]\n",
      "epoch:2 step:1634 [D loss: 0.654948, acc.: 72.66%] [G loss: 9.511204]\n",
      "epoch:2 step:1635 [D loss: 0.862759, acc.: 67.19%] [G loss: 4.412035]\n",
      "epoch:2 step:1636 [D loss: 0.118727, acc.: 97.66%] [G loss: 1.816715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1637 [D loss: 0.557121, acc.: 77.34%] [G loss: 6.403090]\n",
      "epoch:2 step:1638 [D loss: 0.410498, acc.: 82.81%] [G loss: 5.312044]\n",
      "epoch:2 step:1639 [D loss: 0.462104, acc.: 77.34%] [G loss: 1.456034]\n",
      "epoch:2 step:1640 [D loss: 0.283659, acc.: 89.84%] [G loss: 3.676838]\n",
      "epoch:2 step:1641 [D loss: 0.083898, acc.: 98.44%] [G loss: 3.695854]\n",
      "epoch:2 step:1642 [D loss: 0.045086, acc.: 99.22%] [G loss: 2.632916]\n",
      "epoch:2 step:1643 [D loss: 0.144949, acc.: 95.31%] [G loss: 3.946012]\n",
      "epoch:2 step:1644 [D loss: 0.119034, acc.: 95.31%] [G loss: 3.160889]\n",
      "epoch:2 step:1645 [D loss: 0.104020, acc.: 96.88%] [G loss: 3.025795]\n",
      "epoch:2 step:1646 [D loss: 0.058261, acc.: 99.22%] [G loss: 3.097904]\n",
      "epoch:2 step:1647 [D loss: 0.204870, acc.: 94.53%] [G loss: 4.194794]\n",
      "epoch:2 step:1648 [D loss: 0.072259, acc.: 98.44%] [G loss: 4.161906]\n",
      "epoch:2 step:1649 [D loss: 0.122225, acc.: 96.09%] [G loss: 3.206972]\n",
      "epoch:2 step:1650 [D loss: 0.108610, acc.: 96.09%] [G loss: 4.373009]\n",
      "epoch:2 step:1651 [D loss: 0.026627, acc.: 100.00%] [G loss: 4.963445]\n",
      "epoch:2 step:1652 [D loss: 1.800764, acc.: 25.78%] [G loss: 8.746255]\n",
      "epoch:2 step:1653 [D loss: 0.227549, acc.: 89.06%] [G loss: 9.586401]\n",
      "epoch:2 step:1654 [D loss: 0.218180, acc.: 89.84%] [G loss: 6.703819]\n",
      "epoch:2 step:1655 [D loss: 0.026888, acc.: 99.22%] [G loss: 4.122111]\n",
      "epoch:2 step:1656 [D loss: 0.173122, acc.: 90.62%] [G loss: 4.797332]\n",
      "epoch:2 step:1657 [D loss: 0.070386, acc.: 97.66%] [G loss: 4.598727]\n",
      "epoch:2 step:1658 [D loss: 0.055312, acc.: 100.00%] [G loss: 4.160899]\n",
      "epoch:2 step:1659 [D loss: 0.239997, acc.: 93.75%] [G loss: 5.018785]\n",
      "epoch:2 step:1660 [D loss: 0.224967, acc.: 92.19%] [G loss: 3.522948]\n",
      "epoch:2 step:1661 [D loss: 0.236673, acc.: 89.06%] [G loss: 5.540900]\n",
      "epoch:2 step:1662 [D loss: 0.152374, acc.: 93.75%] [G loss: 4.414180]\n",
      "epoch:2 step:1663 [D loss: 0.056558, acc.: 99.22%] [G loss: 4.044032]\n",
      "epoch:2 step:1664 [D loss: 0.018673, acc.: 100.00%] [G loss: 3.233448]\n",
      "epoch:2 step:1665 [D loss: 0.107117, acc.: 99.22%] [G loss: 4.314751]\n",
      "epoch:2 step:1666 [D loss: 0.106689, acc.: 98.44%] [G loss: 2.868071]\n",
      "epoch:2 step:1667 [D loss: 0.133400, acc.: 97.66%] [G loss: 0.584670]\n",
      "epoch:2 step:1668 [D loss: 0.052349, acc.: 100.00%] [G loss: 1.682371]\n",
      "epoch:2 step:1669 [D loss: 0.043345, acc.: 98.44%] [G loss: 0.512812]\n",
      "epoch:2 step:1670 [D loss: 0.242690, acc.: 90.62%] [G loss: 3.224809]\n",
      "epoch:2 step:1671 [D loss: 0.176377, acc.: 92.97%] [G loss: 0.509757]\n",
      "epoch:2 step:1672 [D loss: 0.250525, acc.: 89.84%] [G loss: 2.730785]\n",
      "epoch:2 step:1673 [D loss: 0.126884, acc.: 96.88%] [G loss: 2.715392]\n",
      "epoch:2 step:1674 [D loss: 0.303260, acc.: 85.16%] [G loss: 1.267728]\n",
      "epoch:2 step:1675 [D loss: 0.157530, acc.: 94.53%] [G loss: 0.760606]\n",
      "epoch:2 step:1676 [D loss: 0.035625, acc.: 99.22%] [G loss: 0.452824]\n",
      "epoch:2 step:1677 [D loss: 0.140142, acc.: 94.53%] [G loss: 2.489620]\n",
      "epoch:2 step:1678 [D loss: 0.607624, acc.: 74.22%] [G loss: 2.468222]\n",
      "epoch:2 step:1679 [D loss: 0.432682, acc.: 83.59%] [G loss: 3.196881]\n",
      "epoch:2 step:1680 [D loss: 0.293279, acc.: 85.16%] [G loss: 3.761373]\n",
      "epoch:2 step:1681 [D loss: 0.298048, acc.: 87.50%] [G loss: 5.203835]\n",
      "epoch:2 step:1682 [D loss: 0.206247, acc.: 94.53%] [G loss: 4.344981]\n",
      "epoch:2 step:1683 [D loss: 0.046663, acc.: 99.22%] [G loss: 4.190272]\n",
      "epoch:2 step:1684 [D loss: 0.110685, acc.: 96.09%] [G loss: 5.067553]\n",
      "epoch:2 step:1685 [D loss: 0.061754, acc.: 99.22%] [G loss: 4.533883]\n",
      "epoch:2 step:1686 [D loss: 0.145043, acc.: 96.88%] [G loss: 5.320451]\n",
      "epoch:2 step:1687 [D loss: 0.149511, acc.: 94.53%] [G loss: 5.121789]\n",
      "epoch:2 step:1688 [D loss: 0.185966, acc.: 95.31%] [G loss: 5.261631]\n",
      "epoch:2 step:1689 [D loss: 0.088260, acc.: 95.31%] [G loss: 5.781914]\n",
      "epoch:2 step:1690 [D loss: 0.066967, acc.: 99.22%] [G loss: 4.594719]\n",
      "epoch:2 step:1691 [D loss: 0.066872, acc.: 99.22%] [G loss: 5.326196]\n",
      "epoch:2 step:1692 [D loss: 0.135710, acc.: 96.88%] [G loss: 4.534906]\n",
      "epoch:2 step:1693 [D loss: 0.071129, acc.: 99.22%] [G loss: 5.938635]\n",
      "epoch:2 step:1694 [D loss: 0.152039, acc.: 93.75%] [G loss: 6.890823]\n",
      "epoch:2 step:1695 [D loss: 0.021568, acc.: 100.00%] [G loss: 5.919525]\n",
      "epoch:2 step:1696 [D loss: 0.273560, acc.: 87.50%] [G loss: 7.802871]\n",
      "epoch:2 step:1697 [D loss: 0.170666, acc.: 92.97%] [G loss: 5.700168]\n",
      "epoch:2 step:1698 [D loss: 0.384239, acc.: 86.72%] [G loss: 8.579643]\n",
      "epoch:2 step:1699 [D loss: 0.435537, acc.: 82.81%] [G loss: 5.810565]\n",
      "epoch:2 step:1700 [D loss: 0.133181, acc.: 94.53%] [G loss: 5.551601]\n",
      "epoch:2 step:1701 [D loss: 0.071134, acc.: 99.22%] [G loss: 5.168515]\n",
      "epoch:2 step:1702 [D loss: 0.040075, acc.: 99.22%] [G loss: 2.612414]\n",
      "epoch:2 step:1703 [D loss: 0.046573, acc.: 100.00%] [G loss: 0.721225]\n",
      "epoch:2 step:1704 [D loss: 0.293005, acc.: 88.28%] [G loss: 6.800646]\n",
      "epoch:2 step:1705 [D loss: 0.820720, acc.: 68.75%] [G loss: 0.759490]\n",
      "epoch:2 step:1706 [D loss: 0.125280, acc.: 96.09%] [G loss: 1.766526]\n",
      "epoch:2 step:1707 [D loss: 0.010105, acc.: 100.00%] [G loss: 2.728574]\n",
      "epoch:2 step:1708 [D loss: 0.033975, acc.: 98.44%] [G loss: 2.477270]\n",
      "epoch:2 step:1709 [D loss: 0.051763, acc.: 98.44%] [G loss: 2.646862]\n",
      "epoch:2 step:1710 [D loss: 0.048679, acc.: 99.22%] [G loss: 2.127364]\n",
      "epoch:2 step:1711 [D loss: 0.400868, acc.: 77.34%] [G loss: 7.174514]\n",
      "epoch:2 step:1712 [D loss: 0.187658, acc.: 92.19%] [G loss: 7.253156]\n",
      "epoch:2 step:1713 [D loss: 0.179837, acc.: 92.97%] [G loss: 5.404852]\n",
      "epoch:2 step:1714 [D loss: 0.070619, acc.: 99.22%] [G loss: 5.337425]\n",
      "epoch:2 step:1715 [D loss: 0.143686, acc.: 97.66%] [G loss: 5.305925]\n",
      "epoch:2 step:1716 [D loss: 0.034515, acc.: 100.00%] [G loss: 6.038996]\n",
      "epoch:2 step:1717 [D loss: 0.037550, acc.: 99.22%] [G loss: 4.123549]\n",
      "epoch:2 step:1718 [D loss: 0.509306, acc.: 73.44%] [G loss: 9.088854]\n",
      "epoch:2 step:1719 [D loss: 1.575279, acc.: 52.34%] [G loss: 3.997902]\n",
      "epoch:2 step:1720 [D loss: 0.221691, acc.: 92.97%] [G loss: 4.839633]\n",
      "epoch:2 step:1721 [D loss: 0.040338, acc.: 98.44%] [G loss: 5.653988]\n",
      "epoch:2 step:1722 [D loss: 0.037605, acc.: 98.44%] [G loss: 4.878405]\n",
      "epoch:2 step:1723 [D loss: 0.054335, acc.: 99.22%] [G loss: 4.425265]\n",
      "epoch:2 step:1724 [D loss: 0.044145, acc.: 100.00%] [G loss: 3.482861]\n",
      "epoch:2 step:1725 [D loss: 0.134952, acc.: 95.31%] [G loss: 5.039830]\n",
      "epoch:2 step:1726 [D loss: 0.099401, acc.: 97.66%] [G loss: 3.902442]\n",
      "epoch:2 step:1727 [D loss: 0.243707, acc.: 90.62%] [G loss: 5.228337]\n",
      "epoch:2 step:1728 [D loss: 0.278405, acc.: 89.06%] [G loss: 5.063583]\n",
      "epoch:2 step:1729 [D loss: 0.534887, acc.: 76.56%] [G loss: 4.730840]\n",
      "epoch:2 step:1730 [D loss: 0.072984, acc.: 98.44%] [G loss: 5.446997]\n",
      "epoch:2 step:1731 [D loss: 0.082344, acc.: 97.66%] [G loss: 3.945307]\n",
      "epoch:2 step:1732 [D loss: 0.410038, acc.: 80.47%] [G loss: 6.683815]\n",
      "epoch:2 step:1733 [D loss: 0.791352, acc.: 64.84%] [G loss: 4.648162]\n",
      "epoch:2 step:1734 [D loss: 0.209619, acc.: 94.53%] [G loss: 4.095746]\n",
      "epoch:2 step:1735 [D loss: 0.239605, acc.: 92.19%] [G loss: 3.000711]\n",
      "epoch:2 step:1736 [D loss: 0.187128, acc.: 94.53%] [G loss: 4.467165]\n",
      "epoch:2 step:1737 [D loss: 0.291652, acc.: 85.94%] [G loss: 4.139210]\n",
      "epoch:2 step:1738 [D loss: 0.154914, acc.: 95.31%] [G loss: 3.730409]\n",
      "epoch:2 step:1739 [D loss: 0.108667, acc.: 99.22%] [G loss: 4.169397]\n",
      "epoch:2 step:1740 [D loss: 0.144530, acc.: 96.88%] [G loss: 2.595512]\n",
      "epoch:2 step:1741 [D loss: 0.282874, acc.: 85.94%] [G loss: 5.648846]\n",
      "epoch:2 step:1742 [D loss: 0.244318, acc.: 86.72%] [G loss: 3.568016]\n",
      "epoch:2 step:1743 [D loss: 0.134499, acc.: 93.75%] [G loss: 4.280636]\n",
      "epoch:2 step:1744 [D loss: 0.161996, acc.: 95.31%] [G loss: 3.053946]\n",
      "epoch:2 step:1745 [D loss: 0.289131, acc.: 84.38%] [G loss: 5.848915]\n",
      "epoch:2 step:1746 [D loss: 0.332671, acc.: 85.94%] [G loss: 3.499594]\n",
      "epoch:2 step:1747 [D loss: 0.233103, acc.: 92.19%] [G loss: 2.983404]\n",
      "epoch:2 step:1748 [D loss: 0.087091, acc.: 98.44%] [G loss: 3.869757]\n",
      "epoch:2 step:1749 [D loss: 0.066346, acc.: 99.22%] [G loss: 4.205579]\n",
      "epoch:2 step:1750 [D loss: 0.121631, acc.: 96.88%] [G loss: 2.429179]\n",
      "epoch:2 step:1751 [D loss: 0.155243, acc.: 94.53%] [G loss: 4.489903]\n",
      "epoch:2 step:1752 [D loss: 0.138244, acc.: 96.88%] [G loss: 5.148427]\n",
      "epoch:2 step:1753 [D loss: 0.157461, acc.: 96.09%] [G loss: 3.260648]\n",
      "epoch:2 step:1754 [D loss: 0.114818, acc.: 96.09%] [G loss: 3.565843]\n",
      "epoch:2 step:1755 [D loss: 0.060049, acc.: 100.00%] [G loss: 2.315711]\n",
      "epoch:2 step:1756 [D loss: 0.092161, acc.: 97.66%] [G loss: 3.215405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1757 [D loss: 0.802094, acc.: 56.25%] [G loss: 8.554940]\n",
      "epoch:2 step:1758 [D loss: 1.149451, acc.: 55.47%] [G loss: 4.750858]\n",
      "epoch:2 step:1759 [D loss: 0.165337, acc.: 93.75%] [G loss: 4.571348]\n",
      "epoch:2 step:1760 [D loss: 0.035664, acc.: 100.00%] [G loss: 4.951056]\n",
      "epoch:2 step:1761 [D loss: 0.089673, acc.: 98.44%] [G loss: 3.647962]\n",
      "epoch:2 step:1762 [D loss: 0.068284, acc.: 98.44%] [G loss: 3.612866]\n",
      "epoch:2 step:1763 [D loss: 0.084940, acc.: 97.66%] [G loss: 2.422075]\n",
      "epoch:2 step:1764 [D loss: 0.324058, acc.: 89.06%] [G loss: 4.865836]\n",
      "epoch:2 step:1765 [D loss: 0.429243, acc.: 75.00%] [G loss: 1.941253]\n",
      "epoch:2 step:1766 [D loss: 0.041535, acc.: 98.44%] [G loss: 1.295390]\n",
      "epoch:2 step:1767 [D loss: 0.076261, acc.: 98.44%] [G loss: 1.497890]\n",
      "epoch:2 step:1768 [D loss: 0.051897, acc.: 99.22%] [G loss: 1.380233]\n",
      "epoch:2 step:1769 [D loss: 0.023573, acc.: 100.00%] [G loss: 1.374420]\n",
      "epoch:2 step:1770 [D loss: 0.053555, acc.: 100.00%] [G loss: 0.875714]\n",
      "epoch:2 step:1771 [D loss: 0.051169, acc.: 100.00%] [G loss: 1.179689]\n",
      "epoch:2 step:1772 [D loss: 0.181769, acc.: 96.09%] [G loss: 1.887363]\n",
      "epoch:2 step:1773 [D loss: 0.104895, acc.: 96.09%] [G loss: 2.177534]\n",
      "epoch:2 step:1774 [D loss: 0.095763, acc.: 98.44%] [G loss: 2.413213]\n",
      "epoch:2 step:1775 [D loss: 0.087013, acc.: 98.44%] [G loss: 3.329960]\n",
      "epoch:2 step:1776 [D loss: 0.664765, acc.: 66.41%] [G loss: 3.827395]\n",
      "epoch:2 step:1777 [D loss: 0.189831, acc.: 92.97%] [G loss: 3.504037]\n",
      "epoch:2 step:1778 [D loss: 0.087797, acc.: 95.31%] [G loss: 2.435859]\n",
      "epoch:2 step:1779 [D loss: 0.020244, acc.: 100.00%] [G loss: 2.619487]\n",
      "epoch:2 step:1780 [D loss: 0.164241, acc.: 94.53%] [G loss: 4.881026]\n",
      "epoch:2 step:1781 [D loss: 0.158013, acc.: 92.19%] [G loss: 3.443068]\n",
      "epoch:2 step:1782 [D loss: 0.129132, acc.: 93.75%] [G loss: 5.386615]\n",
      "epoch:2 step:1783 [D loss: 0.040296, acc.: 100.00%] [G loss: 4.558162]\n",
      "epoch:2 step:1784 [D loss: 0.118276, acc.: 96.09%] [G loss: 3.504842]\n",
      "epoch:2 step:1785 [D loss: 0.130343, acc.: 96.09%] [G loss: 6.068595]\n",
      "epoch:2 step:1786 [D loss: 0.031781, acc.: 100.00%] [G loss: 6.196308]\n",
      "epoch:2 step:1787 [D loss: 0.200983, acc.: 92.97%] [G loss: 3.188464]\n",
      "epoch:2 step:1788 [D loss: 0.240073, acc.: 85.16%] [G loss: 6.287151]\n",
      "epoch:2 step:1789 [D loss: 0.038080, acc.: 98.44%] [G loss: 6.700459]\n",
      "epoch:2 step:1790 [D loss: 0.174430, acc.: 90.62%] [G loss: 2.844049]\n",
      "epoch:2 step:1791 [D loss: 0.029419, acc.: 100.00%] [G loss: 1.316868]\n",
      "epoch:2 step:1792 [D loss: 0.194244, acc.: 92.19%] [G loss: 3.210694]\n",
      "epoch:2 step:1793 [D loss: 0.043882, acc.: 99.22%] [G loss: 3.431949]\n",
      "epoch:2 step:1794 [D loss: 0.304869, acc.: 87.50%] [G loss: 0.617644]\n",
      "epoch:2 step:1795 [D loss: 0.104193, acc.: 97.66%] [G loss: 1.017270]\n",
      "epoch:2 step:1796 [D loss: 0.111956, acc.: 96.88%] [G loss: 0.325308]\n",
      "epoch:2 step:1797 [D loss: 0.064833, acc.: 97.66%] [G loss: 1.923219]\n",
      "epoch:2 step:1798 [D loss: 0.115282, acc.: 97.66%] [G loss: 0.893203]\n",
      "epoch:2 step:1799 [D loss: 0.040981, acc.: 99.22%] [G loss: 1.150681]\n",
      "epoch:2 step:1800 [D loss: 0.176680, acc.: 96.09%] [G loss: 3.831173]\n",
      "epoch:2 step:1801 [D loss: 0.960414, acc.: 54.69%] [G loss: 6.681623]\n",
      "epoch:2 step:1802 [D loss: 0.145961, acc.: 94.53%] [G loss: 6.759053]\n",
      "epoch:2 step:1803 [D loss: 0.263354, acc.: 86.72%] [G loss: 3.091721]\n",
      "epoch:2 step:1804 [D loss: 0.170527, acc.: 93.75%] [G loss: 3.366700]\n",
      "epoch:2 step:1805 [D loss: 0.182074, acc.: 92.97%] [G loss: 5.474044]\n",
      "epoch:2 step:1806 [D loss: 0.869162, acc.: 56.25%] [G loss: 4.088808]\n",
      "epoch:2 step:1807 [D loss: 0.051583, acc.: 98.44%] [G loss: 4.652916]\n",
      "epoch:2 step:1808 [D loss: 0.133563, acc.: 96.09%] [G loss: 3.936201]\n",
      "epoch:2 step:1809 [D loss: 0.193085, acc.: 91.41%] [G loss: 6.167433]\n",
      "epoch:2 step:1810 [D loss: 0.235689, acc.: 90.62%] [G loss: 3.429772]\n",
      "epoch:2 step:1811 [D loss: 0.155195, acc.: 92.97%] [G loss: 3.966036]\n",
      "epoch:2 step:1812 [D loss: 0.214566, acc.: 87.50%] [G loss: 7.087770]\n",
      "epoch:2 step:1813 [D loss: 0.528965, acc.: 75.00%] [G loss: 4.570912]\n",
      "epoch:2 step:1814 [D loss: 0.100976, acc.: 97.66%] [G loss: 5.952117]\n",
      "epoch:2 step:1815 [D loss: 0.081672, acc.: 97.66%] [G loss: 5.407945]\n",
      "epoch:2 step:1816 [D loss: 0.165977, acc.: 94.53%] [G loss: 4.485370]\n",
      "epoch:2 step:1817 [D loss: 0.137678, acc.: 96.88%] [G loss: 5.569121]\n",
      "epoch:2 step:1818 [D loss: 0.127985, acc.: 95.31%] [G loss: 5.813785]\n",
      "epoch:2 step:1819 [D loss: 0.160380, acc.: 94.53%] [G loss: 2.809113]\n",
      "epoch:2 step:1820 [D loss: 0.093363, acc.: 98.44%] [G loss: 4.257327]\n",
      "epoch:2 step:1821 [D loss: 0.072396, acc.: 98.44%] [G loss: 2.695412]\n",
      "epoch:2 step:1822 [D loss: 0.868472, acc.: 64.06%] [G loss: 11.882941]\n",
      "epoch:2 step:1823 [D loss: 2.596068, acc.: 50.00%] [G loss: 3.108027]\n",
      "epoch:2 step:1824 [D loss: 0.197553, acc.: 91.41%] [G loss: 3.172962]\n",
      "epoch:2 step:1825 [D loss: 0.083668, acc.: 97.66%] [G loss: 3.648515]\n",
      "epoch:2 step:1826 [D loss: 0.077486, acc.: 97.66%] [G loss: 2.574711]\n",
      "epoch:2 step:1827 [D loss: 0.057488, acc.: 99.22%] [G loss: 1.988538]\n",
      "epoch:2 step:1828 [D loss: 0.181174, acc.: 94.53%] [G loss: 2.430284]\n",
      "epoch:2 step:1829 [D loss: 0.102936, acc.: 97.66%] [G loss: 2.278563]\n",
      "epoch:2 step:1830 [D loss: 0.218431, acc.: 92.97%] [G loss: 4.004400]\n",
      "epoch:2 step:1831 [D loss: 0.749013, acc.: 64.84%] [G loss: 4.460934]\n",
      "epoch:2 step:1832 [D loss: 0.393669, acc.: 83.59%] [G loss: 2.737744]\n",
      "epoch:2 step:1833 [D loss: 0.365532, acc.: 78.12%] [G loss: 5.573387]\n",
      "epoch:2 step:1834 [D loss: 0.119181, acc.: 96.09%] [G loss: 5.940536]\n",
      "epoch:2 step:1835 [D loss: 0.243888, acc.: 89.06%] [G loss: 2.991076]\n",
      "epoch:2 step:1836 [D loss: 0.089957, acc.: 97.66%] [G loss: 2.792738]\n",
      "epoch:2 step:1837 [D loss: 0.064094, acc.: 99.22%] [G loss: 2.213276]\n",
      "epoch:2 step:1838 [D loss: 0.241387, acc.: 95.31%] [G loss: 2.743644]\n",
      "epoch:2 step:1839 [D loss: 0.044230, acc.: 100.00%] [G loss: 2.502696]\n",
      "epoch:2 step:1840 [D loss: 0.044536, acc.: 100.00%] [G loss: 1.002006]\n",
      "epoch:2 step:1841 [D loss: 0.255270, acc.: 89.06%] [G loss: 2.442954]\n",
      "epoch:2 step:1842 [D loss: 0.082207, acc.: 97.66%] [G loss: 2.761380]\n",
      "epoch:2 step:1843 [D loss: 0.119087, acc.: 96.88%] [G loss: 0.515008]\n",
      "epoch:2 step:1844 [D loss: 0.147479, acc.: 94.53%] [G loss: 2.367712]\n",
      "epoch:2 step:1845 [D loss: 0.059530, acc.: 98.44%] [G loss: 2.388084]\n",
      "epoch:2 step:1846 [D loss: 0.155879, acc.: 95.31%] [G loss: 2.703700]\n",
      "epoch:2 step:1847 [D loss: 0.415531, acc.: 82.81%] [G loss: 5.277949]\n",
      "epoch:2 step:1848 [D loss: 0.574321, acc.: 71.88%] [G loss: 1.621332]\n",
      "epoch:2 step:1849 [D loss: 0.470169, acc.: 78.91%] [G loss: 6.208941]\n",
      "epoch:2 step:1850 [D loss: 0.102890, acc.: 97.66%] [G loss: 6.588384]\n",
      "epoch:2 step:1851 [D loss: 0.262045, acc.: 91.41%] [G loss: 3.858793]\n",
      "epoch:2 step:1852 [D loss: 0.172487, acc.: 94.53%] [G loss: 2.982490]\n",
      "epoch:2 step:1853 [D loss: 0.194362, acc.: 92.97%] [G loss: 4.265322]\n",
      "epoch:2 step:1854 [D loss: 0.222575, acc.: 91.41%] [G loss: 3.200639]\n",
      "epoch:2 step:1855 [D loss: 0.588733, acc.: 67.19%] [G loss: 7.547805]\n",
      "epoch:2 step:1856 [D loss: 0.421680, acc.: 78.91%] [G loss: 6.353867]\n",
      "epoch:2 step:1857 [D loss: 0.162615, acc.: 91.41%] [G loss: 3.648031]\n",
      "epoch:2 step:1858 [D loss: 0.276227, acc.: 91.41%] [G loss: 5.225017]\n",
      "epoch:2 step:1859 [D loss: 0.041502, acc.: 98.44%] [G loss: 5.859230]\n",
      "epoch:2 step:1860 [D loss: 0.190275, acc.: 93.75%] [G loss: 4.415496]\n",
      "epoch:2 step:1861 [D loss: 0.285163, acc.: 88.28%] [G loss: 6.077663]\n",
      "epoch:2 step:1862 [D loss: 0.210396, acc.: 91.41%] [G loss: 5.448449]\n",
      "epoch:2 step:1863 [D loss: 0.129244, acc.: 96.09%] [G loss: 5.106741]\n",
      "epoch:2 step:1864 [D loss: 0.067439, acc.: 97.66%] [G loss: 5.200471]\n",
      "epoch:2 step:1865 [D loss: 0.152909, acc.: 94.53%] [G loss: 6.064452]\n",
      "epoch:2 step:1866 [D loss: 0.069584, acc.: 97.66%] [G loss: 5.617154]\n",
      "epoch:2 step:1867 [D loss: 0.371825, acc.: 84.38%] [G loss: 3.809102]\n",
      "epoch:2 step:1868 [D loss: 0.108689, acc.: 99.22%] [G loss: 4.881980]\n",
      "epoch:2 step:1869 [D loss: 0.029758, acc.: 100.00%] [G loss: 4.637156]\n",
      "epoch:2 step:1870 [D loss: 0.112693, acc.: 98.44%] [G loss: 4.593612]\n",
      "epoch:2 step:1871 [D loss: 0.083520, acc.: 96.88%] [G loss: 3.718224]\n",
      "epoch:2 step:1872 [D loss: 0.076844, acc.: 98.44%] [G loss: 4.038466]\n",
      "epoch:2 step:1873 [D loss: 0.163472, acc.: 92.97%] [G loss: 4.906711]\n",
      "epoch:2 step:1874 [D loss: 0.115498, acc.: 95.31%] [G loss: 4.095657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1875 [D loss: 0.586742, acc.: 68.75%] [G loss: 9.209173]\n",
      "epoch:2 step:1876 [D loss: 0.765953, acc.: 62.50%] [G loss: 5.783365]\n",
      "epoch:2 step:1877 [D loss: 0.031195, acc.: 100.00%] [G loss: 3.217096]\n",
      "epoch:2 step:1878 [D loss: 0.079876, acc.: 97.66%] [G loss: 3.496221]\n",
      "epoch:2 step:1879 [D loss: 0.032418, acc.: 100.00%] [G loss: 3.373752]\n",
      "epoch:2 step:1880 [D loss: 0.074395, acc.: 98.44%] [G loss: 3.143596]\n",
      "epoch:2 step:1881 [D loss: 0.109771, acc.: 96.09%] [G loss: 3.546901]\n",
      "epoch:2 step:1882 [D loss: 0.088326, acc.: 98.44%] [G loss: 3.001373]\n",
      "epoch:2 step:1883 [D loss: 0.573623, acc.: 71.88%] [G loss: 6.077662]\n",
      "epoch:2 step:1884 [D loss: 0.413092, acc.: 78.12%] [G loss: 3.260446]\n",
      "epoch:2 step:1885 [D loss: 0.063139, acc.: 98.44%] [G loss: 3.352023]\n",
      "epoch:2 step:1886 [D loss: 0.042550, acc.: 100.00%] [G loss: 2.859796]\n",
      "epoch:2 step:1887 [D loss: 0.178743, acc.: 93.75%] [G loss: 2.905408]\n",
      "epoch:2 step:1888 [D loss: 0.260675, acc.: 85.94%] [G loss: 5.880932]\n",
      "epoch:2 step:1889 [D loss: 0.473025, acc.: 78.91%] [G loss: 2.687631]\n",
      "epoch:2 step:1890 [D loss: 0.224017, acc.: 90.62%] [G loss: 5.267905]\n",
      "epoch:2 step:1891 [D loss: 0.072875, acc.: 98.44%] [G loss: 5.761879]\n",
      "epoch:2 step:1892 [D loss: 0.097785, acc.: 96.09%] [G loss: 4.077598]\n",
      "epoch:2 step:1893 [D loss: 0.295908, acc.: 83.59%] [G loss: 6.389147]\n",
      "epoch:2 step:1894 [D loss: 0.078191, acc.: 96.09%] [G loss: 6.244376]\n",
      "epoch:2 step:1895 [D loss: 0.091404, acc.: 96.09%] [G loss: 4.998317]\n",
      "epoch:2 step:1896 [D loss: 0.048822, acc.: 100.00%] [G loss: 4.412570]\n",
      "epoch:2 step:1897 [D loss: 0.142815, acc.: 95.31%] [G loss: 5.750842]\n",
      "epoch:2 step:1898 [D loss: 0.126652, acc.: 94.53%] [G loss: 3.693812]\n",
      "epoch:2 step:1899 [D loss: 0.124281, acc.: 97.66%] [G loss: 4.741663]\n",
      "epoch:2 step:1900 [D loss: 0.116712, acc.: 97.66%] [G loss: 4.043928]\n",
      "epoch:2 step:1901 [D loss: 0.235757, acc.: 87.50%] [G loss: 6.937950]\n",
      "epoch:2 step:1902 [D loss: 0.497913, acc.: 73.44%] [G loss: 4.072398]\n",
      "epoch:2 step:1903 [D loss: 0.140087, acc.: 96.09%] [G loss: 5.646574]\n",
      "epoch:2 step:1904 [D loss: 0.015394, acc.: 100.00%] [G loss: 5.858648]\n",
      "epoch:2 step:1905 [D loss: 0.101314, acc.: 96.09%] [G loss: 3.281684]\n",
      "epoch:2 step:1906 [D loss: 0.264846, acc.: 87.50%] [G loss: 6.707811]\n",
      "epoch:2 step:1907 [D loss: 0.269183, acc.: 86.72%] [G loss: 4.901413]\n",
      "epoch:2 step:1908 [D loss: 0.028502, acc.: 100.00%] [G loss: 3.017958]\n",
      "epoch:2 step:1909 [D loss: 0.168735, acc.: 93.75%] [G loss: 5.396222]\n",
      "epoch:2 step:1910 [D loss: 0.042360, acc.: 100.00%] [G loss: 5.833879]\n",
      "epoch:2 step:1911 [D loss: 0.057785, acc.: 98.44%] [G loss: 3.441986]\n",
      "epoch:2 step:1912 [D loss: 0.139063, acc.: 96.09%] [G loss: 5.318365]\n",
      "epoch:2 step:1913 [D loss: 0.121969, acc.: 96.09%] [G loss: 4.413109]\n",
      "epoch:2 step:1914 [D loss: 0.143759, acc.: 94.53%] [G loss: 2.196715]\n",
      "epoch:2 step:1915 [D loss: 0.429587, acc.: 80.47%] [G loss: 8.333036]\n",
      "epoch:2 step:1916 [D loss: 1.061354, acc.: 58.59%] [G loss: 4.131312]\n",
      "epoch:2 step:1917 [D loss: 0.246694, acc.: 88.28%] [G loss: 5.406845]\n",
      "epoch:2 step:1918 [D loss: 0.010146, acc.: 100.00%] [G loss: 6.226651]\n",
      "epoch:2 step:1919 [D loss: 0.168824, acc.: 94.53%] [G loss: 4.302123]\n",
      "epoch:2 step:1920 [D loss: 0.261094, acc.: 89.06%] [G loss: 5.919779]\n",
      "epoch:2 step:1921 [D loss: 0.083155, acc.: 96.09%] [G loss: 5.748907]\n",
      "epoch:2 step:1922 [D loss: 0.060846, acc.: 99.22%] [G loss: 4.661009]\n",
      "epoch:2 step:1923 [D loss: 0.111286, acc.: 96.88%] [G loss: 5.091859]\n",
      "epoch:2 step:1924 [D loss: 0.079575, acc.: 97.66%] [G loss: 5.027992]\n",
      "epoch:2 step:1925 [D loss: 0.061231, acc.: 99.22%] [G loss: 4.117098]\n",
      "epoch:2 step:1926 [D loss: 0.359827, acc.: 85.16%] [G loss: 6.238354]\n",
      "epoch:2 step:1927 [D loss: 0.161904, acc.: 93.75%] [G loss: 5.931937]\n",
      "epoch:2 step:1928 [D loss: 0.132140, acc.: 96.88%] [G loss: 3.754393]\n",
      "epoch:2 step:1929 [D loss: 0.190139, acc.: 94.53%] [G loss: 6.033982]\n",
      "epoch:2 step:1930 [D loss: 0.044679, acc.: 97.66%] [G loss: 6.182236]\n",
      "epoch:2 step:1931 [D loss: 0.147086, acc.: 95.31%] [G loss: 2.965862]\n",
      "epoch:2 step:1932 [D loss: 0.360894, acc.: 81.25%] [G loss: 7.982323]\n",
      "epoch:2 step:1933 [D loss: 0.553947, acc.: 72.66%] [G loss: 5.026760]\n",
      "epoch:2 step:1934 [D loss: 0.119032, acc.: 96.09%] [G loss: 5.214419]\n",
      "epoch:2 step:1935 [D loss: 0.051722, acc.: 99.22%] [G loss: 5.398914]\n",
      "epoch:2 step:1936 [D loss: 0.053661, acc.: 99.22%] [G loss: 4.290841]\n",
      "epoch:2 step:1937 [D loss: 0.121677, acc.: 95.31%] [G loss: 4.271842]\n",
      "epoch:2 step:1938 [D loss: 0.354348, acc.: 86.72%] [G loss: 5.965445]\n",
      "epoch:2 step:1939 [D loss: 0.277775, acc.: 89.06%] [G loss: 3.727981]\n",
      "epoch:2 step:1940 [D loss: 0.131400, acc.: 96.88%] [G loss: 3.556531]\n",
      "epoch:2 step:1941 [D loss: 0.088248, acc.: 100.00%] [G loss: 4.111329]\n",
      "epoch:2 step:1942 [D loss: 0.525758, acc.: 73.44%] [G loss: 6.501949]\n",
      "epoch:2 step:1943 [D loss: 0.663166, acc.: 67.19%] [G loss: 2.241088]\n",
      "epoch:2 step:1944 [D loss: 0.449386, acc.: 80.47%] [G loss: 5.650371]\n",
      "epoch:2 step:1945 [D loss: 0.120771, acc.: 94.53%] [G loss: 6.128748]\n",
      "epoch:2 step:1946 [D loss: 0.101118, acc.: 96.88%] [G loss: 5.023387]\n",
      "epoch:2 step:1947 [D loss: 0.051060, acc.: 99.22%] [G loss: 3.878390]\n",
      "epoch:2 step:1948 [D loss: 0.146999, acc.: 96.09%] [G loss: 4.747910]\n",
      "epoch:2 step:1949 [D loss: 0.214384, acc.: 91.41%] [G loss: 3.067972]\n",
      "epoch:2 step:1950 [D loss: 0.253940, acc.: 85.16%] [G loss: 6.354330]\n",
      "epoch:2 step:1951 [D loss: 0.192933, acc.: 90.62%] [G loss: 6.045592]\n",
      "epoch:2 step:1952 [D loss: 0.109387, acc.: 97.66%] [G loss: 4.090391]\n",
      "epoch:2 step:1953 [D loss: 0.094060, acc.: 99.22%] [G loss: 3.408255]\n",
      "epoch:2 step:1954 [D loss: 0.068117, acc.: 100.00%] [G loss: 4.214877]\n",
      "epoch:2 step:1955 [D loss: 0.093321, acc.: 98.44%] [G loss: 4.752157]\n",
      "epoch:2 step:1956 [D loss: 0.062248, acc.: 100.00%] [G loss: 4.127018]\n",
      "epoch:2 step:1957 [D loss: 0.094192, acc.: 97.66%] [G loss: 3.738120]\n",
      "epoch:2 step:1958 [D loss: 0.337537, acc.: 80.47%] [G loss: 5.340170]\n",
      "epoch:2 step:1959 [D loss: 0.106274, acc.: 96.88%] [G loss: 5.715424]\n",
      "epoch:2 step:1960 [D loss: 0.170080, acc.: 96.09%] [G loss: 4.330895]\n",
      "epoch:2 step:1961 [D loss: 0.063352, acc.: 98.44%] [G loss: 4.218014]\n",
      "epoch:2 step:1962 [D loss: 0.021729, acc.: 100.00%] [G loss: 4.695705]\n",
      "epoch:2 step:1963 [D loss: 0.332519, acc.: 87.50%] [G loss: 6.158241]\n",
      "epoch:2 step:1964 [D loss: 0.098952, acc.: 96.09%] [G loss: 4.875274]\n",
      "epoch:2 step:1965 [D loss: 0.129279, acc.: 98.44%] [G loss: 4.931982]\n",
      "epoch:2 step:1966 [D loss: 0.227704, acc.: 89.06%] [G loss: 3.853475]\n",
      "epoch:2 step:1967 [D loss: 0.114030, acc.: 98.44%] [G loss: 3.762688]\n",
      "epoch:2 step:1968 [D loss: 0.084830, acc.: 98.44%] [G loss: 5.034848]\n",
      "epoch:2 step:1969 [D loss: 0.041904, acc.: 100.00%] [G loss: 4.062372]\n",
      "epoch:2 step:1970 [D loss: 0.364576, acc.: 82.03%] [G loss: 8.640064]\n",
      "epoch:2 step:1971 [D loss: 0.529750, acc.: 74.22%] [G loss: 5.113095]\n",
      "epoch:2 step:1972 [D loss: 0.217560, acc.: 89.84%] [G loss: 6.259243]\n",
      "epoch:2 step:1973 [D loss: 0.086200, acc.: 98.44%] [G loss: 5.487718]\n",
      "epoch:2 step:1974 [D loss: 0.068310, acc.: 99.22%] [G loss: 4.314702]\n",
      "epoch:2 step:1975 [D loss: 0.181654, acc.: 93.75%] [G loss: 6.662434]\n",
      "epoch:2 step:1976 [D loss: 0.186008, acc.: 91.41%] [G loss: 5.170296]\n",
      "epoch:2 step:1977 [D loss: 0.252592, acc.: 91.41%] [G loss: 7.409727]\n",
      "epoch:2 step:1978 [D loss: 0.493704, acc.: 77.34%] [G loss: 4.495895]\n",
      "epoch:2 step:1979 [D loss: 0.062148, acc.: 100.00%] [G loss: 4.896695]\n",
      "epoch:2 step:1980 [D loss: 0.049179, acc.: 98.44%] [G loss: 3.677914]\n",
      "epoch:2 step:1981 [D loss: 0.086898, acc.: 98.44%] [G loss: 5.133858]\n",
      "epoch:2 step:1982 [D loss: 1.683747, acc.: 38.28%] [G loss: 9.230997]\n",
      "epoch:2 step:1983 [D loss: 2.041398, acc.: 50.00%] [G loss: 2.126681]\n",
      "epoch:2 step:1984 [D loss: 0.364584, acc.: 83.59%] [G loss: 3.857390]\n",
      "epoch:2 step:1985 [D loss: 0.136678, acc.: 95.31%] [G loss: 4.245073]\n",
      "epoch:2 step:1986 [D loss: 0.128696, acc.: 96.88%] [G loss: 2.640110]\n",
      "epoch:2 step:1987 [D loss: 0.146734, acc.: 96.09%] [G loss: 2.841710]\n",
      "epoch:2 step:1988 [D loss: 0.147904, acc.: 96.88%] [G loss: 3.044862]\n",
      "epoch:2 step:1989 [D loss: 0.220275, acc.: 92.97%] [G loss: 4.156876]\n",
      "epoch:2 step:1990 [D loss: 0.572335, acc.: 71.09%] [G loss: 4.133667]\n",
      "epoch:2 step:1991 [D loss: 0.197979, acc.: 91.41%] [G loss: 4.616394]\n",
      "epoch:2 step:1992 [D loss: 0.799078, acc.: 50.78%] [G loss: 5.279071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1993 [D loss: 0.393209, acc.: 81.25%] [G loss: 4.434326]\n",
      "epoch:2 step:1994 [D loss: 0.249684, acc.: 89.06%] [G loss: 3.889514]\n",
      "epoch:2 step:1995 [D loss: 0.319203, acc.: 86.72%] [G loss: 4.085914]\n",
      "epoch:2 step:1996 [D loss: 0.144106, acc.: 96.09%] [G loss: 3.140978]\n",
      "epoch:2 step:1997 [D loss: 0.114184, acc.: 97.66%] [G loss: 3.128230]\n",
      "epoch:2 step:1998 [D loss: 0.207688, acc.: 93.75%] [G loss: 4.701253]\n",
      "epoch:2 step:1999 [D loss: 0.376995, acc.: 85.94%] [G loss: 1.602602]\n",
      "epoch:2 step:2000 [D loss: 0.397132, acc.: 81.25%] [G loss: 5.659287]\n",
      "epoch:2 step:2001 [D loss: 0.138548, acc.: 92.19%] [G loss: 6.213114]\n",
      "epoch:2 step:2002 [D loss: 0.383733, acc.: 81.25%] [G loss: 2.115396]\n",
      "epoch:2 step:2003 [D loss: 0.321851, acc.: 84.38%] [G loss: 4.167239]\n",
      "epoch:2 step:2004 [D loss: 0.025023, acc.: 100.00%] [G loss: 5.263491]\n",
      "epoch:2 step:2005 [D loss: 0.193335, acc.: 91.41%] [G loss: 3.313945]\n",
      "epoch:2 step:2006 [D loss: 0.136534, acc.: 97.66%] [G loss: 2.738863]\n",
      "epoch:2 step:2007 [D loss: 0.090663, acc.: 98.44%] [G loss: 2.665473]\n",
      "epoch:2 step:2008 [D loss: 0.302666, acc.: 83.59%] [G loss: 4.809594]\n",
      "epoch:2 step:2009 [D loss: 0.440580, acc.: 78.91%] [G loss: 2.030850]\n",
      "epoch:2 step:2010 [D loss: 0.209463, acc.: 92.97%] [G loss: 3.998379]\n",
      "epoch:2 step:2011 [D loss: 0.080623, acc.: 98.44%] [G loss: 3.572691]\n",
      "epoch:2 step:2012 [D loss: 0.041191, acc.: 100.00%] [G loss: 3.283688]\n",
      "epoch:2 step:2013 [D loss: 0.098114, acc.: 97.66%] [G loss: 2.846022]\n",
      "epoch:2 step:2014 [D loss: 0.075899, acc.: 99.22%] [G loss: 3.518143]\n",
      "epoch:2 step:2015 [D loss: 0.071481, acc.: 96.88%] [G loss: 2.011664]\n",
      "epoch:2 step:2016 [D loss: 0.090697, acc.: 99.22%] [G loss: 1.437200]\n",
      "epoch:2 step:2017 [D loss: 0.068354, acc.: 99.22%] [G loss: 1.224925]\n",
      "epoch:2 step:2018 [D loss: 0.074481, acc.: 99.22%] [G loss: 1.738246]\n",
      "epoch:2 step:2019 [D loss: 0.078060, acc.: 99.22%] [G loss: 1.331798]\n",
      "epoch:2 step:2020 [D loss: 0.048424, acc.: 98.44%] [G loss: 1.351676]\n",
      "epoch:2 step:2021 [D loss: 0.083083, acc.: 97.66%] [G loss: 1.377825]\n",
      "epoch:2 step:2022 [D loss: 0.053455, acc.: 99.22%] [G loss: 1.380443]\n",
      "epoch:2 step:2023 [D loss: 0.027663, acc.: 100.00%] [G loss: 1.473526]\n",
      "epoch:2 step:2024 [D loss: 0.147633, acc.: 96.09%] [G loss: 2.795688]\n",
      "epoch:2 step:2025 [D loss: 0.208150, acc.: 89.84%] [G loss: 0.943168]\n",
      "epoch:2 step:2026 [D loss: 0.221321, acc.: 89.84%] [G loss: 4.464068]\n",
      "epoch:2 step:2027 [D loss: 0.030428, acc.: 98.44%] [G loss: 6.087808]\n",
      "epoch:2 step:2028 [D loss: 0.271571, acc.: 86.72%] [G loss: 2.953021]\n",
      "epoch:2 step:2029 [D loss: 0.191313, acc.: 90.62%] [G loss: 5.301738]\n",
      "epoch:2 step:2030 [D loss: 0.032526, acc.: 100.00%] [G loss: 4.720243]\n",
      "epoch:2 step:2031 [D loss: 0.219982, acc.: 91.41%] [G loss: 4.400886]\n",
      "epoch:2 step:2032 [D loss: 0.068511, acc.: 97.66%] [G loss: 4.667127]\n",
      "epoch:2 step:2033 [D loss: 0.761088, acc.: 66.41%] [G loss: 7.007709]\n",
      "epoch:2 step:2034 [D loss: 0.471199, acc.: 78.91%] [G loss: 5.093337]\n",
      "epoch:2 step:2035 [D loss: 0.046795, acc.: 99.22%] [G loss: 3.798253]\n",
      "epoch:2 step:2036 [D loss: 0.043541, acc.: 98.44%] [G loss: 3.655193]\n",
      "epoch:2 step:2037 [D loss: 0.058334, acc.: 99.22%] [G loss: 4.526407]\n",
      "epoch:2 step:2038 [D loss: 0.032668, acc.: 99.22%] [G loss: 3.719015]\n",
      "epoch:2 step:2039 [D loss: 0.057405, acc.: 99.22%] [G loss: 3.233299]\n",
      "epoch:2 step:2040 [D loss: 0.386982, acc.: 86.72%] [G loss: 5.420501]\n",
      "epoch:2 step:2041 [D loss: 0.192404, acc.: 89.84%] [G loss: 4.946382]\n",
      "epoch:2 step:2042 [D loss: 0.034273, acc.: 99.22%] [G loss: 2.651670]\n",
      "epoch:2 step:2043 [D loss: 0.084859, acc.: 96.88%] [G loss: 3.254994]\n",
      "epoch:2 step:2044 [D loss: 0.081731, acc.: 99.22%] [G loss: 2.669233]\n",
      "epoch:2 step:2045 [D loss: 0.125913, acc.: 97.66%] [G loss: 2.359125]\n",
      "epoch:2 step:2046 [D loss: 0.030198, acc.: 100.00%] [G loss: 2.616695]\n",
      "epoch:2 step:2047 [D loss: 0.257454, acc.: 91.41%] [G loss: 4.772934]\n",
      "epoch:2 step:2048 [D loss: 0.118722, acc.: 96.09%] [G loss: 4.199334]\n",
      "epoch:2 step:2049 [D loss: 0.143686, acc.: 95.31%] [G loss: 3.782563]\n",
      "epoch:2 step:2050 [D loss: 0.109883, acc.: 98.44%] [G loss: 2.847787]\n",
      "epoch:2 step:2051 [D loss: 0.074870, acc.: 96.88%] [G loss: 3.335211]\n",
      "epoch:2 step:2052 [D loss: 0.159680, acc.: 93.75%] [G loss: 4.790234]\n",
      "epoch:2 step:2053 [D loss: 0.109427, acc.: 96.09%] [G loss: 2.800328]\n",
      "epoch:2 step:2054 [D loss: 0.575078, acc.: 71.09%] [G loss: 8.262857]\n",
      "epoch:2 step:2055 [D loss: 0.626103, acc.: 70.31%] [G loss: 5.996139]\n",
      "epoch:2 step:2056 [D loss: 0.028703, acc.: 100.00%] [G loss: 4.778952]\n",
      "epoch:2 step:2057 [D loss: 0.052493, acc.: 98.44%] [G loss: 4.743214]\n",
      "epoch:2 step:2058 [D loss: 0.053263, acc.: 99.22%] [G loss: 5.401682]\n",
      "epoch:2 step:2059 [D loss: 0.047576, acc.: 99.22%] [G loss: 3.829627]\n",
      "epoch:2 step:2060 [D loss: 0.325257, acc.: 85.16%] [G loss: 5.619537]\n",
      "epoch:2 step:2061 [D loss: 0.279045, acc.: 88.28%] [G loss: 5.344105]\n",
      "epoch:2 step:2062 [D loss: 0.046108, acc.: 100.00%] [G loss: 4.608868]\n",
      "epoch:2 step:2063 [D loss: 0.241305, acc.: 90.62%] [G loss: 4.324596]\n",
      "epoch:2 step:2064 [D loss: 0.168251, acc.: 96.88%] [G loss: 4.549214]\n",
      "epoch:2 step:2065 [D loss: 0.145194, acc.: 95.31%] [G loss: 6.075922]\n",
      "epoch:2 step:2066 [D loss: 0.207410, acc.: 92.19%] [G loss: 2.123564]\n",
      "epoch:2 step:2067 [D loss: 0.585958, acc.: 72.66%] [G loss: 8.945393]\n",
      "epoch:2 step:2068 [D loss: 1.221687, acc.: 53.91%] [G loss: 1.695196]\n",
      "epoch:2 step:2069 [D loss: 0.406836, acc.: 82.81%] [G loss: 6.023107]\n",
      "epoch:2 step:2070 [D loss: 0.037816, acc.: 97.66%] [G loss: 7.048325]\n",
      "epoch:2 step:2071 [D loss: 0.121810, acc.: 94.53%] [G loss: 4.700483]\n",
      "epoch:2 step:2072 [D loss: 0.016033, acc.: 100.00%] [G loss: 1.769332]\n",
      "epoch:2 step:2073 [D loss: 0.270060, acc.: 86.72%] [G loss: 3.959214]\n",
      "epoch:2 step:2074 [D loss: 0.045786, acc.: 98.44%] [G loss: 5.472483]\n",
      "epoch:2 step:2075 [D loss: 0.454138, acc.: 79.69%] [G loss: 0.411493]\n",
      "epoch:2 step:2076 [D loss: 0.965096, acc.: 56.25%] [G loss: 8.227795]\n",
      "epoch:2 step:2077 [D loss: 1.186057, acc.: 57.03%] [G loss: 6.359532]\n",
      "epoch:2 step:2078 [D loss: 0.112483, acc.: 96.88%] [G loss: 4.005568]\n",
      "epoch:2 step:2079 [D loss: 0.029145, acc.: 100.00%] [G loss: 2.695395]\n",
      "epoch:2 step:2080 [D loss: 0.204561, acc.: 92.19%] [G loss: 4.457187]\n",
      "epoch:2 step:2081 [D loss: 0.051777, acc.: 99.22%] [G loss: 4.134724]\n",
      "epoch:2 step:2082 [D loss: 0.157611, acc.: 92.97%] [G loss: 3.480776]\n",
      "epoch:2 step:2083 [D loss: 0.131855, acc.: 96.09%] [G loss: 3.428234]\n",
      "epoch:2 step:2084 [D loss: 0.183874, acc.: 92.19%] [G loss: 4.017480]\n",
      "epoch:2 step:2085 [D loss: 0.035642, acc.: 100.00%] [G loss: 4.619778]\n",
      "epoch:2 step:2086 [D loss: 0.100271, acc.: 96.09%] [G loss: 3.906248]\n",
      "epoch:2 step:2087 [D loss: 0.085626, acc.: 98.44%] [G loss: 2.949813]\n",
      "epoch:2 step:2088 [D loss: 0.266061, acc.: 88.28%] [G loss: 4.311869]\n",
      "epoch:2 step:2089 [D loss: 0.136540, acc.: 94.53%] [G loss: 3.695582]\n",
      "epoch:2 step:2090 [D loss: 0.063377, acc.: 99.22%] [G loss: 2.005723]\n",
      "epoch:2 step:2091 [D loss: 0.048947, acc.: 99.22%] [G loss: 1.502017]\n",
      "epoch:2 step:2092 [D loss: 0.060730, acc.: 99.22%] [G loss: 0.798029]\n",
      "epoch:2 step:2093 [D loss: 0.340475, acc.: 81.25%] [G loss: 4.805567]\n",
      "epoch:2 step:2094 [D loss: 0.340293, acc.: 82.81%] [G loss: 2.280777]\n",
      "epoch:2 step:2095 [D loss: 0.102114, acc.: 97.66%] [G loss: 0.611581]\n",
      "epoch:2 step:2096 [D loss: 0.054896, acc.: 99.22%] [G loss: 1.926220]\n",
      "epoch:2 step:2097 [D loss: 0.028988, acc.: 99.22%] [G loss: 0.839636]\n",
      "epoch:2 step:2098 [D loss: 0.155390, acc.: 96.88%] [G loss: 3.523868]\n",
      "epoch:2 step:2099 [D loss: 0.092656, acc.: 98.44%] [G loss: 2.482015]\n",
      "epoch:2 step:2100 [D loss: 0.738656, acc.: 64.84%] [G loss: 3.466693]\n",
      "epoch:2 step:2101 [D loss: 0.124643, acc.: 94.53%] [G loss: 3.529102]\n",
      "epoch:2 step:2102 [D loss: 0.104352, acc.: 97.66%] [G loss: 2.077150]\n",
      "epoch:2 step:2103 [D loss: 0.084689, acc.: 96.09%] [G loss: 2.236634]\n",
      "epoch:2 step:2104 [D loss: 0.089830, acc.: 99.22%] [G loss: 0.923299]\n",
      "epoch:2 step:2105 [D loss: 0.099860, acc.: 97.66%] [G loss: 1.915474]\n",
      "epoch:2 step:2106 [D loss: 0.117246, acc.: 99.22%] [G loss: 1.672925]\n",
      "epoch:2 step:2107 [D loss: 0.069623, acc.: 98.44%] [G loss: 2.058661]\n",
      "epoch:2 step:2108 [D loss: 1.150896, acc.: 50.00%] [G loss: 7.761444]\n",
      "epoch:2 step:2109 [D loss: 0.903272, acc.: 60.16%] [G loss: 4.027252]\n",
      "epoch:2 step:2110 [D loss: 0.168189, acc.: 92.97%] [G loss: 4.451143]\n",
      "epoch:2 step:2111 [D loss: 0.145890, acc.: 95.31%] [G loss: 4.617088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2112 [D loss: 0.138956, acc.: 95.31%] [G loss: 4.896426]\n",
      "epoch:2 step:2113 [D loss: 0.093508, acc.: 97.66%] [G loss: 3.193108]\n",
      "epoch:2 step:2114 [D loss: 0.165130, acc.: 96.09%] [G loss: 3.906260]\n",
      "epoch:2 step:2115 [D loss: 0.230424, acc.: 92.97%] [G loss: 5.023654]\n",
      "epoch:2 step:2116 [D loss: 0.154632, acc.: 93.75%] [G loss: 4.575148]\n",
      "epoch:2 step:2117 [D loss: 0.076220, acc.: 100.00%] [G loss: 3.105699]\n",
      "epoch:2 step:2118 [D loss: 0.434547, acc.: 80.47%] [G loss: 5.367160]\n",
      "epoch:2 step:2119 [D loss: 0.275223, acc.: 86.72%] [G loss: 4.693340]\n",
      "epoch:2 step:2120 [D loss: 0.250573, acc.: 89.84%] [G loss: 2.953126]\n",
      "epoch:2 step:2121 [D loss: 0.046492, acc.: 100.00%] [G loss: 2.595886]\n",
      "epoch:2 step:2122 [D loss: 0.061377, acc.: 99.22%] [G loss: 2.962984]\n",
      "epoch:2 step:2123 [D loss: 0.057272, acc.: 99.22%] [G loss: 2.235060]\n",
      "epoch:2 step:2124 [D loss: 0.106844, acc.: 97.66%] [G loss: 1.044808]\n",
      "epoch:2 step:2125 [D loss: 0.061944, acc.: 100.00%] [G loss: 1.143288]\n",
      "epoch:2 step:2126 [D loss: 0.094499, acc.: 100.00%] [G loss: 0.721811]\n",
      "epoch:2 step:2127 [D loss: 0.286929, acc.: 86.72%] [G loss: 5.240112]\n",
      "epoch:2 step:2128 [D loss: 0.595973, acc.: 73.44%] [G loss: 0.834140]\n",
      "epoch:2 step:2129 [D loss: 0.316878, acc.: 83.59%] [G loss: 4.727497]\n",
      "epoch:2 step:2130 [D loss: 0.219230, acc.: 88.28%] [G loss: 4.466462]\n",
      "epoch:2 step:2131 [D loss: 0.031126, acc.: 99.22%] [G loss: 3.430438]\n",
      "epoch:2 step:2132 [D loss: 0.214511, acc.: 91.41%] [G loss: 3.521093]\n",
      "epoch:2 step:2133 [D loss: 0.161415, acc.: 92.97%] [G loss: 4.640201]\n",
      "epoch:2 step:2134 [D loss: 0.138110, acc.: 95.31%] [G loss: 4.210761]\n",
      "epoch:2 step:2135 [D loss: 0.459371, acc.: 74.22%] [G loss: 4.975391]\n",
      "epoch:2 step:2136 [D loss: 0.044217, acc.: 100.00%] [G loss: 5.158538]\n",
      "epoch:2 step:2137 [D loss: 0.415492, acc.: 78.91%] [G loss: 3.199358]\n",
      "epoch:2 step:2138 [D loss: 0.381418, acc.: 80.47%] [G loss: 6.896418]\n",
      "epoch:2 step:2139 [D loss: 0.289689, acc.: 85.94%] [G loss: 6.020564]\n",
      "epoch:2 step:2140 [D loss: 0.104118, acc.: 98.44%] [G loss: 3.931407]\n",
      "epoch:2 step:2141 [D loss: 0.129758, acc.: 95.31%] [G loss: 4.918911]\n",
      "epoch:2 step:2142 [D loss: 0.117575, acc.: 98.44%] [G loss: 4.797740]\n",
      "epoch:2 step:2143 [D loss: 0.126760, acc.: 98.44%] [G loss: 4.432401]\n",
      "epoch:2 step:2144 [D loss: 0.083879, acc.: 98.44%] [G loss: 3.160888]\n",
      "epoch:2 step:2145 [D loss: 0.062110, acc.: 98.44%] [G loss: 3.221644]\n",
      "epoch:2 step:2146 [D loss: 0.425354, acc.: 80.47%] [G loss: 5.161829]\n",
      "epoch:2 step:2147 [D loss: 0.215279, acc.: 89.84%] [G loss: 3.993699]\n",
      "epoch:2 step:2148 [D loss: 0.085868, acc.: 97.66%] [G loss: 1.702302]\n",
      "epoch:2 step:2149 [D loss: 0.083913, acc.: 99.22%] [G loss: 1.072295]\n",
      "epoch:2 step:2150 [D loss: 0.069663, acc.: 99.22%] [G loss: 1.256598]\n",
      "epoch:2 step:2151 [D loss: 0.123118, acc.: 97.66%] [G loss: 1.859156]\n",
      "epoch:2 step:2152 [D loss: 0.082197, acc.: 97.66%] [G loss: 0.613349]\n",
      "epoch:2 step:2153 [D loss: 0.113513, acc.: 96.88%] [G loss: 1.768952]\n",
      "epoch:2 step:2154 [D loss: 0.037257, acc.: 99.22%] [G loss: 1.110486]\n",
      "epoch:2 step:2155 [D loss: 0.107248, acc.: 98.44%] [G loss: 0.361457]\n",
      "epoch:2 step:2156 [D loss: 0.144812, acc.: 96.88%] [G loss: 1.748597]\n",
      "epoch:2 step:2157 [D loss: 0.030501, acc.: 100.00%] [G loss: 3.498616]\n",
      "epoch:2 step:2158 [D loss: 0.143037, acc.: 96.88%] [G loss: 0.564508]\n",
      "epoch:2 step:2159 [D loss: 0.172105, acc.: 90.62%] [G loss: 4.014863]\n",
      "epoch:2 step:2160 [D loss: 0.055818, acc.: 100.00%] [G loss: 4.374154]\n",
      "epoch:2 step:2161 [D loss: 0.018112, acc.: 100.00%] [G loss: 2.760383]\n",
      "epoch:2 step:2162 [D loss: 0.252256, acc.: 89.84%] [G loss: 3.199654]\n",
      "epoch:2 step:2163 [D loss: 0.055795, acc.: 98.44%] [G loss: 3.715041]\n",
      "epoch:2 step:2164 [D loss: 0.487307, acc.: 74.22%] [G loss: 5.971026]\n",
      "epoch:2 step:2165 [D loss: 0.382214, acc.: 82.03%] [G loss: 2.733838]\n",
      "epoch:2 step:2166 [D loss: 0.180150, acc.: 91.41%] [G loss: 5.696980]\n",
      "epoch:2 step:2167 [D loss: 0.033709, acc.: 99.22%] [G loss: 5.825303]\n",
      "epoch:2 step:2168 [D loss: 0.438373, acc.: 78.91%] [G loss: 6.287111]\n",
      "epoch:2 step:2169 [D loss: 0.188498, acc.: 92.97%] [G loss: 4.682722]\n",
      "epoch:2 step:2170 [D loss: 0.120692, acc.: 96.09%] [G loss: 6.074286]\n",
      "epoch:2 step:2171 [D loss: 0.180404, acc.: 92.97%] [G loss: 3.410870]\n",
      "epoch:2 step:2172 [D loss: 0.078776, acc.: 99.22%] [G loss: 3.096165]\n",
      "epoch:2 step:2173 [D loss: 0.256224, acc.: 87.50%] [G loss: 4.142733]\n",
      "epoch:2 step:2174 [D loss: 0.243658, acc.: 89.06%] [G loss: 2.074398]\n",
      "epoch:2 step:2175 [D loss: 0.220130, acc.: 92.97%] [G loss: 4.932479]\n",
      "epoch:2 step:2176 [D loss: 0.200258, acc.: 92.19%] [G loss: 2.186227]\n",
      "epoch:2 step:2177 [D loss: 0.029415, acc.: 98.44%] [G loss: 0.449092]\n",
      "epoch:2 step:2178 [D loss: 0.040616, acc.: 98.44%] [G loss: 0.428513]\n",
      "epoch:2 step:2179 [D loss: 0.053098, acc.: 99.22%] [G loss: 0.099566]\n",
      "epoch:2 step:2180 [D loss: 0.049274, acc.: 98.44%] [G loss: 0.068466]\n",
      "epoch:2 step:2181 [D loss: 0.066775, acc.: 99.22%] [G loss: 0.229454]\n",
      "epoch:2 step:2182 [D loss: 0.050926, acc.: 97.66%] [G loss: 0.495530]\n",
      "epoch:2 step:2183 [D loss: 0.020739, acc.: 100.00%] [G loss: 0.128495]\n",
      "epoch:2 step:2184 [D loss: 0.029173, acc.: 100.00%] [G loss: 0.380386]\n",
      "epoch:2 step:2185 [D loss: 0.929203, acc.: 66.41%] [G loss: 11.243217]\n",
      "epoch:2 step:2186 [D loss: 1.413721, acc.: 57.81%] [G loss: 3.168589]\n",
      "epoch:2 step:2187 [D loss: 0.574057, acc.: 79.69%] [G loss: 6.137208]\n",
      "epoch:2 step:2188 [D loss: 0.155834, acc.: 94.53%] [G loss: 5.452480]\n",
      "epoch:2 step:2189 [D loss: 0.059594, acc.: 98.44%] [G loss: 2.669212]\n",
      "epoch:2 step:2190 [D loss: 0.031915, acc.: 100.00%] [G loss: 0.905656]\n",
      "epoch:2 step:2191 [D loss: 0.515738, acc.: 75.00%] [G loss: 4.673344]\n",
      "epoch:2 step:2192 [D loss: 0.606527, acc.: 73.44%] [G loss: 2.683910]\n",
      "epoch:2 step:2193 [D loss: 0.052012, acc.: 97.66%] [G loss: 1.225556]\n",
      "epoch:2 step:2194 [D loss: 0.563001, acc.: 75.00%] [G loss: 6.340947]\n",
      "epoch:2 step:2195 [D loss: 0.283267, acc.: 86.72%] [G loss: 5.443165]\n",
      "epoch:2 step:2196 [D loss: 0.524601, acc.: 75.00%] [G loss: 4.775084]\n",
      "epoch:2 step:2197 [D loss: 0.125966, acc.: 96.09%] [G loss: 4.944639]\n",
      "epoch:2 step:2198 [D loss: 0.106595, acc.: 96.09%] [G loss: 5.349957]\n",
      "epoch:2 step:2199 [D loss: 0.136270, acc.: 96.09%] [G loss: 4.225070]\n",
      "epoch:2 step:2200 [D loss: 0.076477, acc.: 99.22%] [G loss: 3.616562]\n",
      "epoch:2 step:2201 [D loss: 0.263472, acc.: 89.06%] [G loss: 5.221046]\n",
      "epoch:2 step:2202 [D loss: 0.231231, acc.: 88.28%] [G loss: 4.430572]\n",
      "epoch:2 step:2203 [D loss: 0.138178, acc.: 96.09%] [G loss: 4.985499]\n",
      "epoch:2 step:2204 [D loss: 0.227559, acc.: 92.97%] [G loss: 3.915168]\n",
      "epoch:2 step:2205 [D loss: 0.090827, acc.: 98.44%] [G loss: 3.853566]\n",
      "epoch:2 step:2206 [D loss: 0.050405, acc.: 100.00%] [G loss: 4.111588]\n",
      "epoch:2 step:2207 [D loss: 0.056898, acc.: 100.00%] [G loss: 4.373116]\n",
      "epoch:2 step:2208 [D loss: 0.481640, acc.: 78.12%] [G loss: 6.694824]\n",
      "epoch:2 step:2209 [D loss: 0.177651, acc.: 91.41%] [G loss: 6.259706]\n",
      "epoch:2 step:2210 [D loss: 0.130471, acc.: 96.88%] [G loss: 2.820055]\n",
      "epoch:2 step:2211 [D loss: 0.117774, acc.: 96.88%] [G loss: 4.666653]\n",
      "epoch:2 step:2212 [D loss: 0.007478, acc.: 100.00%] [G loss: 5.153433]\n",
      "epoch:2 step:2213 [D loss: 0.025340, acc.: 100.00%] [G loss: 2.296309]\n",
      "epoch:2 step:2214 [D loss: 0.025632, acc.: 100.00%] [G loss: 0.977877]\n",
      "epoch:2 step:2215 [D loss: 0.258008, acc.: 92.19%] [G loss: 3.152685]\n",
      "epoch:2 step:2216 [D loss: 0.294338, acc.: 89.06%] [G loss: 1.522574]\n",
      "epoch:2 step:2217 [D loss: 0.105745, acc.: 96.88%] [G loss: 0.996836]\n",
      "epoch:2 step:2218 [D loss: 0.255275, acc.: 90.62%] [G loss: 0.132692]\n",
      "epoch:2 step:2219 [D loss: 0.235695, acc.: 92.19%] [G loss: 5.490379]\n",
      "epoch:2 step:2220 [D loss: 0.360291, acc.: 82.81%] [G loss: 0.976544]\n",
      "epoch:2 step:2221 [D loss: 0.355318, acc.: 80.47%] [G loss: 6.506450]\n",
      "epoch:2 step:2222 [D loss: 0.140164, acc.: 93.75%] [G loss: 6.636412]\n",
      "epoch:2 step:2223 [D loss: 0.133619, acc.: 94.53%] [G loss: 3.239944]\n",
      "epoch:2 step:2224 [D loss: 0.092861, acc.: 97.66%] [G loss: 2.401303]\n",
      "epoch:2 step:2225 [D loss: 0.019593, acc.: 100.00%] [G loss: 2.081175]\n",
      "epoch:2 step:2226 [D loss: 0.734682, acc.: 64.06%] [G loss: 9.197118]\n",
      "epoch:2 step:2227 [D loss: 2.044881, acc.: 50.00%] [G loss: 5.121475]\n",
      "epoch:2 step:2228 [D loss: 0.325626, acc.: 84.38%] [G loss: 2.939361]\n",
      "epoch:2 step:2229 [D loss: 0.079750, acc.: 98.44%] [G loss: 3.220798]\n",
      "epoch:2 step:2230 [D loss: 0.113326, acc.: 96.88%] [G loss: 3.939595]\n",
      "epoch:2 step:2231 [D loss: 0.059303, acc.: 98.44%] [G loss: 3.924240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2232 [D loss: 0.111533, acc.: 96.88%] [G loss: 3.645449]\n",
      "epoch:2 step:2233 [D loss: 0.083922, acc.: 98.44%] [G loss: 3.494436]\n",
      "epoch:2 step:2234 [D loss: 0.203105, acc.: 91.41%] [G loss: 4.138045]\n",
      "epoch:2 step:2235 [D loss: 0.096016, acc.: 96.09%] [G loss: 5.247832]\n",
      "epoch:2 step:2236 [D loss: 0.139814, acc.: 96.88%] [G loss: 3.334296]\n",
      "epoch:2 step:2237 [D loss: 0.129868, acc.: 95.31%] [G loss: 3.467506]\n",
      "epoch:2 step:2238 [D loss: 0.257347, acc.: 90.62%] [G loss: 3.632793]\n",
      "epoch:2 step:2239 [D loss: 0.148746, acc.: 95.31%] [G loss: 3.781806]\n",
      "epoch:2 step:2240 [D loss: 0.104702, acc.: 98.44%] [G loss: 3.939531]\n",
      "epoch:2 step:2241 [D loss: 0.176574, acc.: 94.53%] [G loss: 3.997286]\n",
      "epoch:2 step:2242 [D loss: 0.250379, acc.: 92.19%] [G loss: 4.003389]\n",
      "epoch:2 step:2243 [D loss: 0.174645, acc.: 92.97%] [G loss: 1.489453]\n",
      "epoch:2 step:2244 [D loss: 0.119155, acc.: 97.66%] [G loss: 2.688344]\n",
      "epoch:2 step:2245 [D loss: 0.093804, acc.: 98.44%] [G loss: 2.787494]\n",
      "epoch:2 step:2246 [D loss: 0.209011, acc.: 93.75%] [G loss: 4.844010]\n",
      "epoch:2 step:2247 [D loss: 1.313705, acc.: 46.09%] [G loss: 6.443447]\n",
      "epoch:2 step:2248 [D loss: 0.149670, acc.: 93.75%] [G loss: 6.552327]\n",
      "epoch:2 step:2249 [D loss: 0.285042, acc.: 88.28%] [G loss: 2.908484]\n",
      "epoch:2 step:2250 [D loss: 0.164693, acc.: 95.31%] [G loss: 3.147576]\n",
      "epoch:2 step:2251 [D loss: 0.086273, acc.: 97.66%] [G loss: 3.565103]\n",
      "epoch:2 step:2252 [D loss: 0.061425, acc.: 100.00%] [G loss: 3.683163]\n",
      "epoch:2 step:2253 [D loss: 0.429775, acc.: 75.78%] [G loss: 4.152618]\n",
      "epoch:2 step:2254 [D loss: 0.221336, acc.: 93.75%] [G loss: 2.773206]\n",
      "epoch:2 step:2255 [D loss: 0.484764, acc.: 77.34%] [G loss: 5.636512]\n",
      "epoch:2 step:2256 [D loss: 0.234040, acc.: 88.28%] [G loss: 4.373958]\n",
      "epoch:2 step:2257 [D loss: 0.246209, acc.: 91.41%] [G loss: 4.196036]\n",
      "epoch:2 step:2258 [D loss: 0.263342, acc.: 89.84%] [G loss: 3.999099]\n",
      "epoch:2 step:2259 [D loss: 0.176934, acc.: 94.53%] [G loss: 3.516739]\n",
      "epoch:2 step:2260 [D loss: 0.200178, acc.: 92.19%] [G loss: 1.804552]\n",
      "epoch:2 step:2261 [D loss: 0.469146, acc.: 78.12%] [G loss: 5.103987]\n",
      "epoch:2 step:2262 [D loss: 0.232001, acc.: 89.84%] [G loss: 4.683019]\n",
      "epoch:2 step:2263 [D loss: 0.103640, acc.: 96.88%] [G loss: 1.040105]\n",
      "epoch:2 step:2264 [D loss: 0.099899, acc.: 96.88%] [G loss: 1.915872]\n",
      "epoch:2 step:2265 [D loss: 0.043656, acc.: 99.22%] [G loss: 2.476167]\n",
      "epoch:2 step:2266 [D loss: 0.189346, acc.: 92.97%] [G loss: 1.023726]\n",
      "epoch:2 step:2267 [D loss: 0.100249, acc.: 97.66%] [G loss: 1.794245]\n",
      "epoch:2 step:2268 [D loss: 0.089826, acc.: 98.44%] [G loss: 0.852797]\n",
      "epoch:2 step:2269 [D loss: 0.431909, acc.: 78.12%] [G loss: 5.074176]\n",
      "epoch:2 step:2270 [D loss: 0.388021, acc.: 80.47%] [G loss: 2.394936]\n",
      "epoch:2 step:2271 [D loss: 0.079136, acc.: 97.66%] [G loss: 2.435003]\n",
      "epoch:2 step:2272 [D loss: 0.028869, acc.: 99.22%] [G loss: 0.940071]\n",
      "epoch:2 step:2273 [D loss: 0.539485, acc.: 78.91%] [G loss: 6.490846]\n",
      "epoch:2 step:2274 [D loss: 1.217438, acc.: 50.78%] [G loss: 2.021189]\n",
      "epoch:2 step:2275 [D loss: 0.318415, acc.: 85.94%] [G loss: 3.732721]\n",
      "epoch:2 step:2276 [D loss: 0.043838, acc.: 99.22%] [G loss: 4.569435]\n",
      "epoch:2 step:2277 [D loss: 0.111574, acc.: 96.88%] [G loss: 3.101760]\n",
      "epoch:2 step:2278 [D loss: 0.225162, acc.: 91.41%] [G loss: 3.389185]\n",
      "epoch:2 step:2279 [D loss: 0.098641, acc.: 99.22%] [G loss: 4.486128]\n",
      "epoch:2 step:2280 [D loss: 0.085283, acc.: 99.22%] [G loss: 3.306613]\n",
      "epoch:2 step:2281 [D loss: 0.210852, acc.: 91.41%] [G loss: 4.140602]\n",
      "epoch:2 step:2282 [D loss: 0.511256, acc.: 76.56%] [G loss: 5.531486]\n",
      "epoch:2 step:2283 [D loss: 0.054770, acc.: 98.44%] [G loss: 5.897088]\n",
      "epoch:2 step:2284 [D loss: 0.083787, acc.: 97.66%] [G loss: 3.453836]\n",
      "epoch:2 step:2285 [D loss: 0.156453, acc.: 96.88%] [G loss: 4.076617]\n",
      "epoch:2 step:2286 [D loss: 0.538214, acc.: 71.88%] [G loss: 6.652026]\n",
      "epoch:2 step:2287 [D loss: 0.248845, acc.: 89.84%] [G loss: 4.401131]\n",
      "epoch:2 step:2288 [D loss: 0.116087, acc.: 97.66%] [G loss: 2.494193]\n",
      "epoch:2 step:2289 [D loss: 0.163382, acc.: 94.53%] [G loss: 1.542315]\n",
      "epoch:2 step:2290 [D loss: 0.265912, acc.: 85.94%] [G loss: 3.933839]\n",
      "epoch:2 step:2291 [D loss: 0.305300, acc.: 85.94%] [G loss: 1.880081]\n",
      "epoch:2 step:2292 [D loss: 0.165531, acc.: 92.19%] [G loss: 1.903349]\n",
      "epoch:2 step:2293 [D loss: 0.082341, acc.: 97.66%] [G loss: 2.527249]\n",
      "epoch:2 step:2294 [D loss: 0.947304, acc.: 55.47%] [G loss: 7.510564]\n",
      "epoch:2 step:2295 [D loss: 0.327343, acc.: 84.38%] [G loss: 6.275992]\n",
      "epoch:2 step:2296 [D loss: 0.761101, acc.: 67.19%] [G loss: 1.747769]\n",
      "epoch:2 step:2297 [D loss: 0.350687, acc.: 78.91%] [G loss: 5.669187]\n",
      "epoch:2 step:2298 [D loss: 0.018146, acc.: 100.00%] [G loss: 6.621050]\n",
      "epoch:2 step:2299 [D loss: 0.202864, acc.: 91.41%] [G loss: 3.323499]\n",
      "epoch:2 step:2300 [D loss: 0.392288, acc.: 82.03%] [G loss: 6.020517]\n",
      "epoch:2 step:2301 [D loss: 0.156317, acc.: 93.75%] [G loss: 5.232181]\n",
      "epoch:2 step:2302 [D loss: 0.126999, acc.: 96.88%] [G loss: 2.232237]\n",
      "epoch:2 step:2303 [D loss: 0.467950, acc.: 78.12%] [G loss: 5.216791]\n",
      "epoch:2 step:2304 [D loss: 0.494824, acc.: 73.44%] [G loss: 3.629955]\n",
      "epoch:2 step:2305 [D loss: 0.168603, acc.: 95.31%] [G loss: 4.575017]\n",
      "epoch:2 step:2306 [D loss: 0.074255, acc.: 98.44%] [G loss: 4.083932]\n",
      "epoch:2 step:2307 [D loss: 0.179789, acc.: 94.53%] [G loss: 2.547327]\n",
      "epoch:2 step:2308 [D loss: 0.235775, acc.: 89.84%] [G loss: 4.653783]\n",
      "epoch:2 step:2309 [D loss: 0.105351, acc.: 96.09%] [G loss: 4.613171]\n",
      "epoch:2 step:2310 [D loss: 0.258094, acc.: 90.62%] [G loss: 4.065916]\n",
      "epoch:2 step:2311 [D loss: 0.157301, acc.: 94.53%] [G loss: 3.273051]\n",
      "epoch:2 step:2312 [D loss: 0.313221, acc.: 85.94%] [G loss: 4.709418]\n",
      "epoch:2 step:2313 [D loss: 0.357161, acc.: 80.47%] [G loss: 4.294914]\n",
      "epoch:2 step:2314 [D loss: 0.259483, acc.: 88.28%] [G loss: 4.654631]\n",
      "epoch:2 step:2315 [D loss: 0.128311, acc.: 96.88%] [G loss: 4.548158]\n",
      "epoch:2 step:2316 [D loss: 0.200140, acc.: 94.53%] [G loss: 3.863096]\n",
      "epoch:2 step:2317 [D loss: 0.206562, acc.: 91.41%] [G loss: 4.875566]\n",
      "epoch:2 step:2318 [D loss: 0.160743, acc.: 94.53%] [G loss: 5.178725]\n",
      "epoch:2 step:2319 [D loss: 0.099207, acc.: 96.09%] [G loss: 2.925566]\n",
      "epoch:2 step:2320 [D loss: 0.237218, acc.: 89.06%] [G loss: 4.051129]\n",
      "epoch:2 step:2321 [D loss: 0.181252, acc.: 93.75%] [G loss: 5.486814]\n",
      "epoch:2 step:2322 [D loss: 0.978981, acc.: 51.56%] [G loss: 6.433017]\n",
      "epoch:2 step:2323 [D loss: 0.071224, acc.: 98.44%] [G loss: 7.316288]\n",
      "epoch:2 step:2324 [D loss: 0.316438, acc.: 85.16%] [G loss: 4.067615]\n",
      "epoch:2 step:2325 [D loss: 0.260516, acc.: 90.62%] [G loss: 5.536641]\n",
      "epoch:2 step:2326 [D loss: 0.089696, acc.: 97.66%] [G loss: 4.307949]\n",
      "epoch:2 step:2327 [D loss: 0.143160, acc.: 97.66%] [G loss: 2.895912]\n",
      "epoch:2 step:2328 [D loss: 0.321185, acc.: 85.94%] [G loss: 6.078372]\n",
      "epoch:2 step:2329 [D loss: 0.474255, acc.: 79.69%] [G loss: 4.002205]\n",
      "epoch:2 step:2330 [D loss: 0.506108, acc.: 72.66%] [G loss: 5.104806]\n",
      "epoch:2 step:2331 [D loss: 0.136778, acc.: 95.31%] [G loss: 4.297906]\n",
      "epoch:2 step:2332 [D loss: 0.327608, acc.: 85.16%] [G loss: 3.922018]\n",
      "epoch:2 step:2333 [D loss: 0.561752, acc.: 71.09%] [G loss: 5.228670]\n",
      "epoch:2 step:2334 [D loss: 0.108457, acc.: 96.09%] [G loss: 4.600902]\n",
      "epoch:2 step:2335 [D loss: 0.249031, acc.: 92.19%] [G loss: 2.510408]\n",
      "epoch:2 step:2336 [D loss: 0.168756, acc.: 95.31%] [G loss: 4.412115]\n",
      "epoch:2 step:2337 [D loss: 0.117394, acc.: 97.66%] [G loss: 3.266627]\n",
      "epoch:2 step:2338 [D loss: 0.094363, acc.: 99.22%] [G loss: 3.174720]\n",
      "epoch:2 step:2339 [D loss: 0.200592, acc.: 96.09%] [G loss: 2.050190]\n",
      "epoch:2 step:2340 [D loss: 0.069296, acc.: 99.22%] [G loss: 2.727253]\n",
      "epoch:2 step:2341 [D loss: 0.541160, acc.: 68.75%] [G loss: 6.702931]\n",
      "epoch:2 step:2342 [D loss: 0.295741, acc.: 86.72%] [G loss: 4.152662]\n",
      "epoch:2 step:2343 [D loss: 0.090003, acc.: 98.44%] [G loss: 3.934291]\n",
      "epoch:3 step:2344 [D loss: 0.453365, acc.: 79.69%] [G loss: 4.268289]\n",
      "epoch:3 step:2345 [D loss: 0.138205, acc.: 93.75%] [G loss: 3.067230]\n",
      "epoch:3 step:2346 [D loss: 0.546346, acc.: 74.22%] [G loss: 7.157604]\n",
      "epoch:3 step:2347 [D loss: 0.349621, acc.: 84.38%] [G loss: 5.012735]\n",
      "epoch:3 step:2348 [D loss: 0.167409, acc.: 92.97%] [G loss: 5.464268]\n",
      "epoch:3 step:2349 [D loss: 0.027976, acc.: 100.00%] [G loss: 4.400804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2350 [D loss: 0.266819, acc.: 89.06%] [G loss: 5.184513]\n",
      "epoch:3 step:2351 [D loss: 0.045247, acc.: 100.00%] [G loss: 4.405354]\n",
      "epoch:3 step:2352 [D loss: 0.122132, acc.: 95.31%] [G loss: 3.969852]\n",
      "epoch:3 step:2353 [D loss: 0.493559, acc.: 75.78%] [G loss: 6.458244]\n",
      "epoch:3 step:2354 [D loss: 0.631651, acc.: 69.53%] [G loss: 1.136066]\n",
      "epoch:3 step:2355 [D loss: 0.479754, acc.: 77.34%] [G loss: 5.632720]\n",
      "epoch:3 step:2356 [D loss: 0.290248, acc.: 85.16%] [G loss: 4.676304]\n",
      "epoch:3 step:2357 [D loss: 0.177174, acc.: 94.53%] [G loss: 1.954208]\n",
      "epoch:3 step:2358 [D loss: 0.050342, acc.: 99.22%] [G loss: 2.164211]\n",
      "epoch:3 step:2359 [D loss: 0.225448, acc.: 92.19%] [G loss: 4.742481]\n",
      "epoch:3 step:2360 [D loss: 0.331983, acc.: 85.16%] [G loss: 1.406931]\n",
      "epoch:3 step:2361 [D loss: 0.866795, acc.: 63.28%] [G loss: 6.350900]\n",
      "epoch:3 step:2362 [D loss: 0.841550, acc.: 62.50%] [G loss: 4.333643]\n",
      "epoch:3 step:2363 [D loss: 0.236308, acc.: 91.41%] [G loss: 1.742796]\n",
      "epoch:3 step:2364 [D loss: 0.370419, acc.: 78.91%] [G loss: 3.821732]\n",
      "epoch:3 step:2365 [D loss: 0.058926, acc.: 97.66%] [G loss: 4.671654]\n",
      "epoch:3 step:2366 [D loss: 0.282847, acc.: 87.50%] [G loss: 2.903471]\n",
      "epoch:3 step:2367 [D loss: 0.196015, acc.: 92.19%] [G loss: 3.765868]\n",
      "epoch:3 step:2368 [D loss: 0.204068, acc.: 92.19%] [G loss: 2.772329]\n",
      "epoch:3 step:2369 [D loss: 0.188625, acc.: 96.88%] [G loss: 2.558891]\n",
      "epoch:3 step:2370 [D loss: 0.136016, acc.: 97.66%] [G loss: 2.888048]\n",
      "epoch:3 step:2371 [D loss: 0.243738, acc.: 92.19%] [G loss: 3.190956]\n",
      "epoch:3 step:2372 [D loss: 0.288383, acc.: 88.28%] [G loss: 5.001351]\n",
      "epoch:3 step:2373 [D loss: 0.681688, acc.: 64.06%] [G loss: 3.036594]\n",
      "epoch:3 step:2374 [D loss: 0.165456, acc.: 96.88%] [G loss: 4.051358]\n",
      "epoch:3 step:2375 [D loss: 0.170464, acc.: 94.53%] [G loss: 5.519570]\n",
      "epoch:3 step:2376 [D loss: 0.128898, acc.: 96.09%] [G loss: 4.640559]\n",
      "epoch:3 step:2377 [D loss: 0.229956, acc.: 92.97%] [G loss: 4.467752]\n",
      "epoch:3 step:2378 [D loss: 0.178629, acc.: 95.31%] [G loss: 3.201673]\n",
      "epoch:3 step:2379 [D loss: 0.139924, acc.: 96.88%] [G loss: 4.681871]\n",
      "epoch:3 step:2380 [D loss: 0.071238, acc.: 100.00%] [G loss: 2.902131]\n",
      "epoch:3 step:2381 [D loss: 0.319045, acc.: 84.38%] [G loss: 3.239605]\n",
      "epoch:3 step:2382 [D loss: 0.051801, acc.: 98.44%] [G loss: 2.677380]\n",
      "epoch:3 step:2383 [D loss: 0.203293, acc.: 92.19%] [G loss: 3.390199]\n",
      "epoch:3 step:2384 [D loss: 0.303314, acc.: 83.59%] [G loss: 1.056582]\n",
      "epoch:3 step:2385 [D loss: 0.034922, acc.: 100.00%] [G loss: 0.923476]\n",
      "epoch:3 step:2386 [D loss: 0.191625, acc.: 92.19%] [G loss: 2.392522]\n",
      "epoch:3 step:2387 [D loss: 0.174735, acc.: 93.75%] [G loss: 2.179940]\n",
      "epoch:3 step:2388 [D loss: 0.188339, acc.: 93.75%] [G loss: 3.093962]\n",
      "epoch:3 step:2389 [D loss: 0.080874, acc.: 98.44%] [G loss: 2.948602]\n",
      "epoch:3 step:2390 [D loss: 1.965201, acc.: 28.91%] [G loss: 9.063501]\n",
      "epoch:3 step:2391 [D loss: 1.367954, acc.: 56.25%] [G loss: 4.497349]\n",
      "epoch:3 step:2392 [D loss: 0.068050, acc.: 99.22%] [G loss: 3.150520]\n",
      "epoch:3 step:2393 [D loss: 0.210172, acc.: 92.97%] [G loss: 4.597762]\n",
      "epoch:3 step:2394 [D loss: 0.098222, acc.: 98.44%] [G loss: 4.786460]\n",
      "epoch:3 step:2395 [D loss: 0.264584, acc.: 89.84%] [G loss: 3.086527]\n",
      "epoch:3 step:2396 [D loss: 0.189555, acc.: 94.53%] [G loss: 3.358480]\n",
      "epoch:3 step:2397 [D loss: 0.327008, acc.: 82.03%] [G loss: 3.226724]\n",
      "epoch:3 step:2398 [D loss: 0.280648, acc.: 88.28%] [G loss: 0.961186]\n",
      "epoch:3 step:2399 [D loss: 0.393723, acc.: 81.25%] [G loss: 4.325296]\n",
      "epoch:3 step:2400 [D loss: 0.413618, acc.: 78.12%] [G loss: 2.710189]\n",
      "epoch:3 step:2401 [D loss: 0.099908, acc.: 98.44%] [G loss: 2.008875]\n",
      "epoch:3 step:2402 [D loss: 0.142178, acc.: 95.31%] [G loss: 3.280118]\n",
      "epoch:3 step:2403 [D loss: 0.407666, acc.: 78.91%] [G loss: 4.660912]\n",
      "epoch:3 step:2404 [D loss: 0.200412, acc.: 91.41%] [G loss: 4.559276]\n",
      "epoch:3 step:2405 [D loss: 0.059065, acc.: 99.22%] [G loss: 3.138132]\n",
      "epoch:3 step:2406 [D loss: 0.194667, acc.: 96.09%] [G loss: 3.328344]\n",
      "epoch:3 step:2407 [D loss: 0.424996, acc.: 78.12%] [G loss: 4.784342]\n",
      "epoch:3 step:2408 [D loss: 0.311466, acc.: 86.72%] [G loss: 4.626626]\n",
      "epoch:3 step:2409 [D loss: 0.012974, acc.: 100.00%] [G loss: 3.387161]\n",
      "epoch:3 step:2410 [D loss: 0.228813, acc.: 93.75%] [G loss: 2.646863]\n",
      "epoch:3 step:2411 [D loss: 0.056954, acc.: 99.22%] [G loss: 3.262059]\n",
      "epoch:3 step:2412 [D loss: 0.220996, acc.: 91.41%] [G loss: 2.004007]\n",
      "epoch:3 step:2413 [D loss: 0.109335, acc.: 96.09%] [G loss: 1.596957]\n",
      "epoch:3 step:2414 [D loss: 0.029162, acc.: 100.00%] [G loss: 0.819525]\n",
      "epoch:3 step:2415 [D loss: 0.252088, acc.: 92.97%] [G loss: 2.931967]\n",
      "epoch:3 step:2416 [D loss: 0.131018, acc.: 96.88%] [G loss: 1.701905]\n",
      "epoch:3 step:2417 [D loss: 0.039589, acc.: 100.00%] [G loss: 0.537389]\n",
      "epoch:3 step:2418 [D loss: 0.056465, acc.: 99.22%] [G loss: 0.174373]\n",
      "epoch:3 step:2419 [D loss: 0.037201, acc.: 99.22%] [G loss: 0.275898]\n",
      "epoch:3 step:2420 [D loss: 0.175307, acc.: 91.41%] [G loss: 0.437492]\n",
      "epoch:3 step:2421 [D loss: 0.018650, acc.: 100.00%] [G loss: 1.192828]\n",
      "epoch:3 step:2422 [D loss: 0.085252, acc.: 97.66%] [G loss: 0.145262]\n",
      "epoch:3 step:2423 [D loss: 0.027785, acc.: 100.00%] [G loss: 0.081070]\n",
      "epoch:3 step:2424 [D loss: 0.228210, acc.: 89.84%] [G loss: 2.667967]\n",
      "epoch:3 step:2425 [D loss: 0.583341, acc.: 71.09%] [G loss: 1.422392]\n",
      "epoch:3 step:2426 [D loss: 0.043878, acc.: 99.22%] [G loss: 0.662680]\n",
      "epoch:3 step:2427 [D loss: 0.368166, acc.: 79.69%] [G loss: 0.855429]\n",
      "epoch:3 step:2428 [D loss: 0.023211, acc.: 100.00%] [G loss: 3.326236]\n",
      "epoch:3 step:2429 [D loss: 0.214508, acc.: 92.97%] [G loss: 0.549471]\n",
      "epoch:3 step:2430 [D loss: 0.140381, acc.: 96.09%] [G loss: 2.107702]\n",
      "epoch:3 step:2431 [D loss: 0.185638, acc.: 94.53%] [G loss: 1.673420]\n",
      "epoch:3 step:2432 [D loss: 0.479968, acc.: 75.78%] [G loss: 5.211502]\n",
      "epoch:3 step:2433 [D loss: 0.229356, acc.: 89.84%] [G loss: 4.977416]\n",
      "epoch:3 step:2434 [D loss: 0.544978, acc.: 75.78%] [G loss: 4.539426]\n",
      "epoch:3 step:2435 [D loss: 0.101630, acc.: 98.44%] [G loss: 3.850724]\n",
      "epoch:3 step:2436 [D loss: 0.374862, acc.: 82.03%] [G loss: 6.816827]\n",
      "epoch:3 step:2437 [D loss: 0.315514, acc.: 85.16%] [G loss: 4.013324]\n",
      "epoch:3 step:2438 [D loss: 0.105075, acc.: 96.88%] [G loss: 3.385140]\n",
      "epoch:3 step:2439 [D loss: 0.042354, acc.: 100.00%] [G loss: 1.853197]\n",
      "epoch:3 step:2440 [D loss: 0.188648, acc.: 93.75%] [G loss: 3.014928]\n",
      "epoch:3 step:2441 [D loss: 0.119972, acc.: 95.31%] [G loss: 2.095767]\n",
      "epoch:3 step:2442 [D loss: 0.154165, acc.: 96.09%] [G loss: 0.279992]\n",
      "epoch:3 step:2443 [D loss: 0.045665, acc.: 100.00%] [G loss: 0.242296]\n",
      "epoch:3 step:2444 [D loss: 0.115523, acc.: 95.31%] [G loss: 0.642222]\n",
      "epoch:3 step:2445 [D loss: 0.030016, acc.: 99.22%] [G loss: 0.468608]\n",
      "epoch:3 step:2446 [D loss: 0.161272, acc.: 93.75%] [G loss: 0.272491]\n",
      "epoch:3 step:2447 [D loss: 0.010977, acc.: 100.00%] [G loss: 0.621161]\n",
      "epoch:3 step:2448 [D loss: 0.089898, acc.: 96.09%] [G loss: 4.193883]\n",
      "epoch:3 step:2449 [D loss: 0.076431, acc.: 96.88%] [G loss: 2.268856]\n",
      "epoch:3 step:2450 [D loss: 0.023450, acc.: 100.00%] [G loss: 0.358128]\n",
      "epoch:3 step:2451 [D loss: 0.434286, acc.: 77.34%] [G loss: 8.575050]\n",
      "epoch:3 step:2452 [D loss: 1.587965, acc.: 53.91%] [G loss: 1.484997]\n",
      "epoch:3 step:2453 [D loss: 0.393121, acc.: 79.69%] [G loss: 4.289780]\n",
      "epoch:3 step:2454 [D loss: 0.098802, acc.: 96.88%] [G loss: 4.903063]\n",
      "epoch:3 step:2455 [D loss: 0.103709, acc.: 96.88%] [G loss: 3.338964]\n",
      "epoch:3 step:2456 [D loss: 0.254403, acc.: 89.84%] [G loss: 5.037903]\n",
      "epoch:3 step:2457 [D loss: 0.233301, acc.: 92.19%] [G loss: 3.991806]\n",
      "epoch:3 step:2458 [D loss: 0.146701, acc.: 96.88%] [G loss: 4.620801]\n",
      "epoch:3 step:2459 [D loss: 0.191372, acc.: 93.75%] [G loss: 3.449182]\n",
      "epoch:3 step:2460 [D loss: 0.069513, acc.: 100.00%] [G loss: 4.052983]\n",
      "epoch:3 step:2461 [D loss: 0.111503, acc.: 98.44%] [G loss: 2.748541]\n",
      "epoch:3 step:2462 [D loss: 0.156874, acc.: 96.09%] [G loss: 2.649766]\n",
      "epoch:3 step:2463 [D loss: 0.068735, acc.: 99.22%] [G loss: 3.237648]\n",
      "epoch:3 step:2464 [D loss: 0.125019, acc.: 96.88%] [G loss: 1.553458]\n",
      "epoch:3 step:2465 [D loss: 0.102587, acc.: 98.44%] [G loss: 2.140997]\n",
      "epoch:3 step:2466 [D loss: 0.139506, acc.: 96.09%] [G loss: 1.333174]\n",
      "epoch:3 step:2467 [D loss: 0.108674, acc.: 97.66%] [G loss: 0.552118]\n",
      "epoch:3 step:2468 [D loss: 0.077933, acc.: 98.44%] [G loss: 0.145367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2469 [D loss: 0.125276, acc.: 98.44%] [G loss: 0.412941]\n",
      "epoch:3 step:2470 [D loss: 0.070702, acc.: 98.44%] [G loss: 0.392246]\n",
      "epoch:3 step:2471 [D loss: 0.115669, acc.: 96.88%] [G loss: 0.224283]\n",
      "epoch:3 step:2472 [D loss: 0.024818, acc.: 100.00%] [G loss: 0.254178]\n",
      "epoch:3 step:2473 [D loss: 0.667327, acc.: 65.62%] [G loss: 6.884971]\n",
      "epoch:3 step:2474 [D loss: 0.753833, acc.: 65.62%] [G loss: 2.645661]\n",
      "epoch:3 step:2475 [D loss: 0.137386, acc.: 94.53%] [G loss: 3.280830]\n",
      "epoch:3 step:2476 [D loss: 0.080250, acc.: 98.44%] [G loss: 3.347331]\n",
      "epoch:3 step:2477 [D loss: 0.133464, acc.: 95.31%] [G loss: 4.781940]\n",
      "epoch:3 step:2478 [D loss: 0.306078, acc.: 86.72%] [G loss: 4.200480]\n",
      "epoch:3 step:2479 [D loss: 0.403347, acc.: 81.25%] [G loss: 5.596116]\n",
      "epoch:3 step:2480 [D loss: 0.112309, acc.: 97.66%] [G loss: 5.358989]\n",
      "epoch:3 step:2481 [D loss: 0.689018, acc.: 65.62%] [G loss: 6.207224]\n",
      "epoch:3 step:2482 [D loss: 0.057320, acc.: 99.22%] [G loss: 6.792907]\n",
      "epoch:3 step:2483 [D loss: 0.277292, acc.: 85.94%] [G loss: 4.586065]\n",
      "epoch:3 step:2484 [D loss: 0.177015, acc.: 93.75%] [G loss: 4.894855]\n",
      "epoch:3 step:2485 [D loss: 0.092204, acc.: 97.66%] [G loss: 4.114007]\n",
      "epoch:3 step:2486 [D loss: 0.485686, acc.: 80.47%] [G loss: 4.321582]\n",
      "epoch:3 step:2487 [D loss: 0.207661, acc.: 90.62%] [G loss: 4.238426]\n",
      "epoch:3 step:2488 [D loss: 0.279125, acc.: 87.50%] [G loss: 5.448055]\n",
      "epoch:3 step:2489 [D loss: 0.171199, acc.: 96.09%] [G loss: 4.064937]\n",
      "epoch:3 step:2490 [D loss: 0.283676, acc.: 85.94%] [G loss: 5.690824]\n",
      "epoch:3 step:2491 [D loss: 0.397352, acc.: 81.25%] [G loss: 5.154878]\n",
      "epoch:3 step:2492 [D loss: 0.062973, acc.: 98.44%] [G loss: 4.375331]\n",
      "epoch:3 step:2493 [D loss: 0.087640, acc.: 97.66%] [G loss: 4.962818]\n",
      "epoch:3 step:2494 [D loss: 0.554356, acc.: 76.56%] [G loss: 8.128845]\n",
      "epoch:3 step:2495 [D loss: 1.119536, acc.: 58.59%] [G loss: 2.635313]\n",
      "epoch:3 step:2496 [D loss: 0.587669, acc.: 75.00%] [G loss: 6.174904]\n",
      "epoch:3 step:2497 [D loss: 0.125215, acc.: 94.53%] [G loss: 6.189521]\n",
      "epoch:3 step:2498 [D loss: 0.197896, acc.: 92.97%] [G loss: 4.053716]\n",
      "epoch:3 step:2499 [D loss: 0.063555, acc.: 99.22%] [G loss: 2.356739]\n",
      "epoch:3 step:2500 [D loss: 0.153684, acc.: 93.75%] [G loss: 3.067248]\n",
      "epoch:3 step:2501 [D loss: 0.527422, acc.: 71.88%] [G loss: 4.757025]\n",
      "epoch:3 step:2502 [D loss: 0.294670, acc.: 83.59%] [G loss: 2.288351]\n",
      "epoch:3 step:2503 [D loss: 0.258601, acc.: 89.84%] [G loss: 3.314659]\n",
      "epoch:3 step:2504 [D loss: 0.286911, acc.: 89.06%] [G loss: 4.391713]\n",
      "epoch:3 step:2505 [D loss: 0.066565, acc.: 97.66%] [G loss: 4.577915]\n",
      "epoch:3 step:2506 [D loss: 0.107675, acc.: 96.09%] [G loss: 2.212154]\n",
      "epoch:3 step:2507 [D loss: 0.178166, acc.: 96.09%] [G loss: 1.331051]\n",
      "epoch:3 step:2508 [D loss: 0.150932, acc.: 96.88%] [G loss: 2.819541]\n",
      "epoch:3 step:2509 [D loss: 0.174385, acc.: 93.75%] [G loss: 2.114441]\n",
      "epoch:3 step:2510 [D loss: 0.592505, acc.: 71.88%] [G loss: 7.109211]\n",
      "epoch:3 step:2511 [D loss: 0.378059, acc.: 80.47%] [G loss: 5.018603]\n",
      "epoch:3 step:2512 [D loss: 0.082740, acc.: 96.09%] [G loss: 2.789225]\n",
      "epoch:3 step:2513 [D loss: 0.098738, acc.: 98.44%] [G loss: 2.015918]\n",
      "epoch:3 step:2514 [D loss: 0.142683, acc.: 96.88%] [G loss: 3.471399]\n",
      "epoch:3 step:2515 [D loss: 0.032666, acc.: 100.00%] [G loss: 3.259518]\n",
      "epoch:3 step:2516 [D loss: 0.132829, acc.: 96.88%] [G loss: 2.617387]\n",
      "epoch:3 step:2517 [D loss: 0.803478, acc.: 64.06%] [G loss: 6.732540]\n",
      "epoch:3 step:2518 [D loss: 1.028576, acc.: 60.94%] [G loss: 4.436653]\n",
      "epoch:3 step:2519 [D loss: 0.181641, acc.: 96.09%] [G loss: 3.969548]\n",
      "epoch:3 step:2520 [D loss: 0.108096, acc.: 97.66%] [G loss: 5.196671]\n",
      "epoch:3 step:2521 [D loss: 0.134933, acc.: 93.75%] [G loss: 3.921627]\n",
      "epoch:3 step:2522 [D loss: 0.083106, acc.: 98.44%] [G loss: 2.098683]\n",
      "epoch:3 step:2523 [D loss: 0.319932, acc.: 82.03%] [G loss: 5.917046]\n",
      "epoch:3 step:2524 [D loss: 0.144964, acc.: 95.31%] [G loss: 5.947407]\n",
      "epoch:3 step:2525 [D loss: 0.175637, acc.: 90.62%] [G loss: 2.471091]\n",
      "epoch:3 step:2526 [D loss: 0.174129, acc.: 91.41%] [G loss: 3.203794]\n",
      "epoch:3 step:2527 [D loss: 0.015962, acc.: 100.00%] [G loss: 3.735044]\n",
      "epoch:3 step:2528 [D loss: 0.113994, acc.: 95.31%] [G loss: 0.827942]\n",
      "epoch:3 step:2529 [D loss: 0.182915, acc.: 96.88%] [G loss: 2.180902]\n",
      "epoch:3 step:2530 [D loss: 0.066155, acc.: 99.22%] [G loss: 1.198104]\n",
      "epoch:3 step:2531 [D loss: 0.203959, acc.: 92.19%] [G loss: 0.859776]\n",
      "epoch:3 step:2532 [D loss: 0.029608, acc.: 100.00%] [G loss: 0.428888]\n",
      "epoch:3 step:2533 [D loss: 0.049028, acc.: 99.22%] [G loss: 0.206407]\n",
      "epoch:3 step:2534 [D loss: 0.160609, acc.: 96.09%] [G loss: 2.159545]\n",
      "epoch:3 step:2535 [D loss: 0.117608, acc.: 95.31%] [G loss: 0.883623]\n",
      "epoch:3 step:2536 [D loss: 0.086892, acc.: 99.22%] [G loss: 0.336053]\n",
      "epoch:3 step:2537 [D loss: 0.114009, acc.: 99.22%] [G loss: 0.548231]\n",
      "epoch:3 step:2538 [D loss: 1.330096, acc.: 44.53%] [G loss: 7.967673]\n",
      "epoch:3 step:2539 [D loss: 2.099088, acc.: 50.78%] [G loss: 4.363077]\n",
      "epoch:3 step:2540 [D loss: 0.287822, acc.: 85.16%] [G loss: 3.415902]\n",
      "epoch:3 step:2541 [D loss: 0.102480, acc.: 95.31%] [G loss: 2.914521]\n",
      "epoch:3 step:2542 [D loss: 0.232403, acc.: 89.84%] [G loss: 3.109622]\n",
      "epoch:3 step:2543 [D loss: 0.444454, acc.: 78.91%] [G loss: 5.609474]\n",
      "epoch:3 step:2544 [D loss: 0.409907, acc.: 82.81%] [G loss: 4.693470]\n",
      "epoch:3 step:2545 [D loss: 0.848769, acc.: 57.81%] [G loss: 2.958147]\n",
      "epoch:3 step:2546 [D loss: 0.159478, acc.: 94.53%] [G loss: 4.437963]\n",
      "epoch:3 step:2547 [D loss: 0.174547, acc.: 93.75%] [G loss: 3.495286]\n",
      "epoch:3 step:2548 [D loss: 0.239993, acc.: 90.62%] [G loss: 4.708998]\n",
      "epoch:3 step:2549 [D loss: 0.232828, acc.: 95.31%] [G loss: 3.419981]\n",
      "epoch:3 step:2550 [D loss: 0.426561, acc.: 80.47%] [G loss: 3.266496]\n",
      "epoch:3 step:2551 [D loss: 0.190168, acc.: 94.53%] [G loss: 3.768510]\n",
      "epoch:3 step:2552 [D loss: 0.244673, acc.: 89.84%] [G loss: 3.212004]\n",
      "epoch:3 step:2553 [D loss: 0.126797, acc.: 96.88%] [G loss: 2.617055]\n",
      "epoch:3 step:2554 [D loss: 0.161923, acc.: 94.53%] [G loss: 3.260371]\n",
      "epoch:3 step:2555 [D loss: 0.242401, acc.: 89.06%] [G loss: 3.163093]\n",
      "epoch:3 step:2556 [D loss: 0.486657, acc.: 76.56%] [G loss: 5.289432]\n",
      "epoch:3 step:2557 [D loss: 0.363348, acc.: 78.12%] [G loss: 3.933604]\n",
      "epoch:3 step:2558 [D loss: 0.230365, acc.: 89.84%] [G loss: 3.610360]\n",
      "epoch:3 step:2559 [D loss: 0.092012, acc.: 96.88%] [G loss: 3.515076]\n",
      "epoch:3 step:2560 [D loss: 0.131267, acc.: 96.88%] [G loss: 2.559308]\n",
      "epoch:3 step:2561 [D loss: 0.259554, acc.: 87.50%] [G loss: 4.386938]\n",
      "epoch:3 step:2562 [D loss: 0.166585, acc.: 95.31%] [G loss: 3.886900]\n",
      "epoch:3 step:2563 [D loss: 0.282654, acc.: 89.06%] [G loss: 2.527145]\n",
      "epoch:3 step:2564 [D loss: 0.121981, acc.: 96.88%] [G loss: 4.313432]\n",
      "epoch:3 step:2565 [D loss: 0.182113, acc.: 96.88%] [G loss: 2.262693]\n",
      "epoch:3 step:2566 [D loss: 0.438091, acc.: 81.25%] [G loss: 4.194933]\n",
      "epoch:3 step:2567 [D loss: 0.136585, acc.: 94.53%] [G loss: 4.160610]\n",
      "epoch:3 step:2568 [D loss: 0.428061, acc.: 82.03%] [G loss: 4.472827]\n",
      "epoch:3 step:2569 [D loss: 0.169527, acc.: 92.19%] [G loss: 4.074639]\n",
      "epoch:3 step:2570 [D loss: 0.044907, acc.: 100.00%] [G loss: 3.357997]\n",
      "epoch:3 step:2571 [D loss: 0.103687, acc.: 96.09%] [G loss: 3.579371]\n",
      "epoch:3 step:2572 [D loss: 0.100407, acc.: 97.66%] [G loss: 3.032585]\n",
      "epoch:3 step:2573 [D loss: 0.359991, acc.: 81.25%] [G loss: 6.019287]\n",
      "epoch:3 step:2574 [D loss: 0.307241, acc.: 85.16%] [G loss: 4.401876]\n",
      "epoch:3 step:2575 [D loss: 0.122711, acc.: 96.88%] [G loss: 4.280855]\n",
      "epoch:3 step:2576 [D loss: 0.065439, acc.: 99.22%] [G loss: 5.221208]\n",
      "epoch:3 step:2577 [D loss: 0.369047, acc.: 86.72%] [G loss: 5.237155]\n",
      "epoch:3 step:2578 [D loss: 0.086406, acc.: 97.66%] [G loss: 5.088993]\n",
      "epoch:3 step:2579 [D loss: 0.173434, acc.: 96.09%] [G loss: 3.514398]\n",
      "epoch:3 step:2580 [D loss: 0.131553, acc.: 97.66%] [G loss: 3.517753]\n",
      "epoch:3 step:2581 [D loss: 0.068700, acc.: 99.22%] [G loss: 4.574223]\n",
      "epoch:3 step:2582 [D loss: 0.134728, acc.: 96.88%] [G loss: 3.430777]\n",
      "epoch:3 step:2583 [D loss: 0.818077, acc.: 56.25%] [G loss: 7.877733]\n",
      "epoch:3 step:2584 [D loss: 0.992924, acc.: 57.81%] [G loss: 2.705252]\n",
      "epoch:3 step:2585 [D loss: 0.169061, acc.: 94.53%] [G loss: 2.715182]\n",
      "epoch:3 step:2586 [D loss: 0.020631, acc.: 100.00%] [G loss: 2.489272]\n",
      "epoch:3 step:2587 [D loss: 0.090425, acc.: 97.66%] [G loss: 3.331939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2588 [D loss: 0.137793, acc.: 96.88%] [G loss: 2.247650]\n",
      "epoch:3 step:2589 [D loss: 0.084792, acc.: 100.00%] [G loss: 1.273420]\n",
      "epoch:3 step:2590 [D loss: 0.414433, acc.: 80.47%] [G loss: 3.709829]\n",
      "epoch:3 step:2591 [D loss: 0.441979, acc.: 75.78%] [G loss: 1.480603]\n",
      "epoch:3 step:2592 [D loss: 0.407343, acc.: 78.91%] [G loss: 4.295135]\n",
      "epoch:3 step:2593 [D loss: 0.859512, acc.: 64.06%] [G loss: 1.695478]\n",
      "epoch:3 step:2594 [D loss: 0.337811, acc.: 82.03%] [G loss: 3.395301]\n",
      "epoch:3 step:2595 [D loss: 0.116477, acc.: 98.44%] [G loss: 4.333664]\n",
      "epoch:3 step:2596 [D loss: 0.192022, acc.: 90.62%] [G loss: 1.868146]\n",
      "epoch:3 step:2597 [D loss: 0.443081, acc.: 80.47%] [G loss: 3.534306]\n",
      "epoch:3 step:2598 [D loss: 1.197268, acc.: 46.09%] [G loss: 4.651973]\n",
      "epoch:3 step:2599 [D loss: 0.163173, acc.: 95.31%] [G loss: 5.181765]\n",
      "epoch:3 step:2600 [D loss: 0.042251, acc.: 99.22%] [G loss: 4.394814]\n",
      "epoch:3 step:2601 [D loss: 0.162373, acc.: 95.31%] [G loss: 4.510062]\n",
      "epoch:3 step:2602 [D loss: 0.033529, acc.: 100.00%] [G loss: 3.790011]\n",
      "epoch:3 step:2603 [D loss: 0.116015, acc.: 98.44%] [G loss: 2.756772]\n",
      "epoch:3 step:2604 [D loss: 0.178089, acc.: 94.53%] [G loss: 2.487981]\n",
      "epoch:3 step:2605 [D loss: 0.096369, acc.: 99.22%] [G loss: 2.028153]\n",
      "epoch:3 step:2606 [D loss: 0.072943, acc.: 99.22%] [G loss: 2.285118]\n",
      "epoch:3 step:2607 [D loss: 0.115812, acc.: 98.44%] [G loss: 2.683830]\n",
      "epoch:3 step:2608 [D loss: 0.420727, acc.: 81.25%] [G loss: 5.206951]\n",
      "epoch:3 step:2609 [D loss: 0.515586, acc.: 73.44%] [G loss: 2.385450]\n",
      "epoch:3 step:2610 [D loss: 0.314695, acc.: 85.16%] [G loss: 4.658216]\n",
      "epoch:3 step:2611 [D loss: 0.023569, acc.: 100.00%] [G loss: 5.578623]\n",
      "epoch:3 step:2612 [D loss: 0.239773, acc.: 92.97%] [G loss: 1.459843]\n",
      "epoch:3 step:2613 [D loss: 0.520093, acc.: 74.22%] [G loss: 5.496371]\n",
      "epoch:3 step:2614 [D loss: 0.341775, acc.: 82.81%] [G loss: 3.941357]\n",
      "epoch:3 step:2615 [D loss: 0.073290, acc.: 98.44%] [G loss: 1.605255]\n",
      "epoch:3 step:2616 [D loss: 0.123298, acc.: 96.09%] [G loss: 1.490267]\n",
      "epoch:3 step:2617 [D loss: 0.126173, acc.: 95.31%] [G loss: 2.338130]\n",
      "epoch:3 step:2618 [D loss: 1.236027, acc.: 46.88%] [G loss: 6.296149]\n",
      "epoch:3 step:2619 [D loss: 0.876940, acc.: 61.72%] [G loss: 1.365547]\n",
      "epoch:3 step:2620 [D loss: 0.181455, acc.: 94.53%] [G loss: 1.125907]\n",
      "epoch:3 step:2621 [D loss: 0.125759, acc.: 96.88%] [G loss: 3.344597]\n",
      "epoch:3 step:2622 [D loss: 0.103121, acc.: 96.09%] [G loss: 2.415875]\n",
      "epoch:3 step:2623 [D loss: 0.037502, acc.: 100.00%] [G loss: 1.444246]\n",
      "epoch:3 step:2624 [D loss: 0.088220, acc.: 98.44%] [G loss: 1.995010]\n",
      "epoch:3 step:2625 [D loss: 0.314615, acc.: 84.38%] [G loss: 2.797532]\n",
      "epoch:3 step:2626 [D loss: 0.079582, acc.: 97.66%] [G loss: 3.717425]\n",
      "epoch:3 step:2627 [D loss: 0.410711, acc.: 79.69%] [G loss: 1.791379]\n",
      "epoch:3 step:2628 [D loss: 0.271416, acc.: 86.72%] [G loss: 4.847082]\n",
      "epoch:3 step:2629 [D loss: 0.351101, acc.: 86.72%] [G loss: 2.700675]\n",
      "epoch:3 step:2630 [D loss: 0.245080, acc.: 89.06%] [G loss: 4.337376]\n",
      "epoch:3 step:2631 [D loss: 0.095040, acc.: 98.44%] [G loss: 4.492234]\n",
      "epoch:3 step:2632 [D loss: 0.206012, acc.: 94.53%] [G loss: 2.962723]\n",
      "epoch:3 step:2633 [D loss: 0.165399, acc.: 94.53%] [G loss: 3.021743]\n",
      "epoch:3 step:2634 [D loss: 0.051661, acc.: 100.00%] [G loss: 4.356367]\n",
      "epoch:3 step:2635 [D loss: 0.176858, acc.: 96.09%] [G loss: 3.066463]\n",
      "epoch:3 step:2636 [D loss: 0.109707, acc.: 97.66%] [G loss: 3.008324]\n",
      "epoch:3 step:2637 [D loss: 0.114726, acc.: 98.44%] [G loss: 2.823949]\n",
      "epoch:3 step:2638 [D loss: 0.212079, acc.: 92.19%] [G loss: 2.359650]\n",
      "epoch:3 step:2639 [D loss: 0.110139, acc.: 96.88%] [G loss: 1.567504]\n",
      "epoch:3 step:2640 [D loss: 0.134522, acc.: 96.88%] [G loss: 2.027601]\n",
      "epoch:3 step:2641 [D loss: 0.328889, acc.: 89.06%] [G loss: 4.679288]\n",
      "epoch:3 step:2642 [D loss: 0.319562, acc.: 82.81%] [G loss: 2.180937]\n",
      "epoch:3 step:2643 [D loss: 0.012529, acc.: 100.00%] [G loss: 1.157990]\n",
      "epoch:3 step:2644 [D loss: 0.361935, acc.: 77.34%] [G loss: 6.603842]\n",
      "epoch:3 step:2645 [D loss: 0.601722, acc.: 70.31%] [G loss: 3.911240]\n",
      "epoch:3 step:2646 [D loss: 0.033597, acc.: 100.00%] [G loss: 2.167285]\n",
      "epoch:3 step:2647 [D loss: 0.066535, acc.: 98.44%] [G loss: 2.352713]\n",
      "epoch:3 step:2648 [D loss: 0.113733, acc.: 96.09%] [G loss: 2.574245]\n",
      "epoch:3 step:2649 [D loss: 0.044374, acc.: 100.00%] [G loss: 3.086231]\n",
      "epoch:3 step:2650 [D loss: 0.994214, acc.: 50.78%] [G loss: 6.698857]\n",
      "epoch:3 step:2651 [D loss: 0.621275, acc.: 68.75%] [G loss: 4.400546]\n",
      "epoch:3 step:2652 [D loss: 0.194350, acc.: 92.19%] [G loss: 2.669107]\n",
      "epoch:3 step:2653 [D loss: 0.117354, acc.: 95.31%] [G loss: 3.261207]\n",
      "epoch:3 step:2654 [D loss: 0.088228, acc.: 97.66%] [G loss: 3.606993]\n",
      "epoch:3 step:2655 [D loss: 0.533182, acc.: 71.09%] [G loss: 5.700473]\n",
      "epoch:3 step:2656 [D loss: 0.227798, acc.: 89.06%] [G loss: 4.389778]\n",
      "epoch:3 step:2657 [D loss: 0.339169, acc.: 86.72%] [G loss: 3.020531]\n",
      "epoch:3 step:2658 [D loss: 0.200661, acc.: 95.31%] [G loss: 4.494647]\n",
      "epoch:3 step:2659 [D loss: 0.153799, acc.: 96.88%] [G loss: 3.849899]\n",
      "epoch:3 step:2660 [D loss: 0.301520, acc.: 87.50%] [G loss: 4.328996]\n",
      "epoch:3 step:2661 [D loss: 0.065780, acc.: 100.00%] [G loss: 4.215117]\n",
      "epoch:3 step:2662 [D loss: 0.368675, acc.: 84.38%] [G loss: 4.852592]\n",
      "epoch:3 step:2663 [D loss: 0.126877, acc.: 97.66%] [G loss: 3.821240]\n",
      "epoch:3 step:2664 [D loss: 0.137714, acc.: 96.88%] [G loss: 2.685174]\n",
      "epoch:3 step:2665 [D loss: 0.232164, acc.: 89.84%] [G loss: 5.430891]\n",
      "epoch:3 step:2666 [D loss: 0.154467, acc.: 94.53%] [G loss: 4.049755]\n",
      "epoch:3 step:2667 [D loss: 0.130968, acc.: 96.88%] [G loss: 3.694909]\n",
      "epoch:3 step:2668 [D loss: 0.156778, acc.: 96.88%] [G loss: 2.741859]\n",
      "epoch:3 step:2669 [D loss: 0.308585, acc.: 85.94%] [G loss: 6.041388]\n",
      "epoch:3 step:2670 [D loss: 0.501689, acc.: 71.09%] [G loss: 2.022262]\n",
      "epoch:3 step:2671 [D loss: 0.418717, acc.: 78.12%] [G loss: 5.456158]\n",
      "epoch:3 step:2672 [D loss: 0.271809, acc.: 88.28%] [G loss: 4.470643]\n",
      "epoch:3 step:2673 [D loss: 0.092408, acc.: 98.44%] [G loss: 3.489723]\n",
      "epoch:3 step:2674 [D loss: 0.126906, acc.: 96.88%] [G loss: 3.834192]\n",
      "epoch:3 step:2675 [D loss: 0.082520, acc.: 99.22%] [G loss: 2.879761]\n",
      "epoch:3 step:2676 [D loss: 0.066279, acc.: 99.22%] [G loss: 2.723832]\n",
      "epoch:3 step:2677 [D loss: 0.972911, acc.: 53.12%] [G loss: 5.690712]\n",
      "epoch:3 step:2678 [D loss: 0.472761, acc.: 75.00%] [G loss: 3.886756]\n",
      "epoch:3 step:2679 [D loss: 0.169559, acc.: 94.53%] [G loss: 2.313366]\n",
      "epoch:3 step:2680 [D loss: 0.081532, acc.: 96.88%] [G loss: 1.652587]\n",
      "epoch:3 step:2681 [D loss: 0.034452, acc.: 100.00%] [G loss: 1.289974]\n",
      "epoch:3 step:2682 [D loss: 0.395650, acc.: 79.69%] [G loss: 4.398538]\n",
      "epoch:3 step:2683 [D loss: 0.924219, acc.: 60.94%] [G loss: 1.949264]\n",
      "epoch:3 step:2684 [D loss: 0.176935, acc.: 95.31%] [G loss: 2.692534]\n",
      "epoch:3 step:2685 [D loss: 0.156601, acc.: 94.53%] [G loss: 3.336007]\n",
      "epoch:3 step:2686 [D loss: 0.147851, acc.: 97.66%] [G loss: 3.155314]\n",
      "epoch:3 step:2687 [D loss: 0.192506, acc.: 91.41%] [G loss: 3.549609]\n",
      "epoch:3 step:2688 [D loss: 0.187092, acc.: 92.97%] [G loss: 3.386319]\n",
      "epoch:3 step:2689 [D loss: 0.123474, acc.: 96.09%] [G loss: 4.518429]\n",
      "epoch:3 step:2690 [D loss: 0.069178, acc.: 98.44%] [G loss: 3.910977]\n",
      "epoch:3 step:2691 [D loss: 0.164399, acc.: 96.09%] [G loss: 4.844892]\n",
      "epoch:3 step:2692 [D loss: 0.051806, acc.: 100.00%] [G loss: 4.092235]\n",
      "epoch:3 step:2693 [D loss: 0.363810, acc.: 82.81%] [G loss: 4.502523]\n",
      "epoch:3 step:2694 [D loss: 0.116387, acc.: 96.09%] [G loss: 3.803972]\n",
      "epoch:3 step:2695 [D loss: 0.191479, acc.: 94.53%] [G loss: 2.594116]\n",
      "epoch:3 step:2696 [D loss: 0.094639, acc.: 99.22%] [G loss: 2.286551]\n",
      "epoch:3 step:2697 [D loss: 0.254216, acc.: 90.62%] [G loss: 3.298811]\n",
      "epoch:3 step:2698 [D loss: 0.934770, acc.: 57.03%] [G loss: 4.398549]\n",
      "epoch:3 step:2699 [D loss: 0.547537, acc.: 74.22%] [G loss: 3.119941]\n",
      "epoch:3 step:2700 [D loss: 0.247096, acc.: 91.41%] [G loss: 2.003750]\n",
      "epoch:3 step:2701 [D loss: 0.147779, acc.: 95.31%] [G loss: 4.214113]\n",
      "epoch:3 step:2702 [D loss: 0.292064, acc.: 92.19%] [G loss: 2.343146]\n",
      "epoch:3 step:2703 [D loss: 0.180856, acc.: 92.97%] [G loss: 4.677855]\n",
      "epoch:3 step:2704 [D loss: 0.177238, acc.: 93.75%] [G loss: 3.902540]\n",
      "epoch:3 step:2705 [D loss: 0.589149, acc.: 68.75%] [G loss: 3.878557]\n",
      "epoch:3 step:2706 [D loss: 0.112962, acc.: 95.31%] [G loss: 4.238957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2707 [D loss: 0.221286, acc.: 90.62%] [G loss: 3.357009]\n",
      "epoch:3 step:2708 [D loss: 0.273587, acc.: 86.72%] [G loss: 5.096114]\n",
      "epoch:3 step:2709 [D loss: 0.278902, acc.: 86.72%] [G loss: 3.658413]\n",
      "epoch:3 step:2710 [D loss: 0.308329, acc.: 85.94%] [G loss: 5.537693]\n",
      "epoch:3 step:2711 [D loss: 0.052711, acc.: 100.00%] [G loss: 5.751013]\n",
      "epoch:3 step:2712 [D loss: 0.534428, acc.: 72.66%] [G loss: 4.058011]\n",
      "epoch:3 step:2713 [D loss: 0.055678, acc.: 99.22%] [G loss: 4.475990]\n",
      "epoch:3 step:2714 [D loss: 0.224089, acc.: 91.41%] [G loss: 3.406317]\n",
      "epoch:3 step:2715 [D loss: 0.280685, acc.: 88.28%] [G loss: 6.069846]\n",
      "epoch:3 step:2716 [D loss: 0.748764, acc.: 59.38%] [G loss: 4.432592]\n",
      "epoch:3 step:2717 [D loss: 0.225086, acc.: 92.97%] [G loss: 3.685953]\n",
      "epoch:3 step:2718 [D loss: 0.452155, acc.: 75.78%] [G loss: 6.241584]\n",
      "epoch:3 step:2719 [D loss: 0.070779, acc.: 99.22%] [G loss: 6.489410]\n",
      "epoch:3 step:2720 [D loss: 0.233872, acc.: 90.62%] [G loss: 3.661259]\n",
      "epoch:3 step:2721 [D loss: 0.563514, acc.: 75.00%] [G loss: 6.672403]\n",
      "epoch:3 step:2722 [D loss: 0.346726, acc.: 82.81%] [G loss: 4.846390]\n",
      "epoch:3 step:2723 [D loss: 0.274292, acc.: 92.19%] [G loss: 5.035915]\n",
      "epoch:3 step:2724 [D loss: 0.058890, acc.: 99.22%] [G loss: 4.372713]\n",
      "epoch:3 step:2725 [D loss: 0.076180, acc.: 99.22%] [G loss: 2.589971]\n",
      "epoch:3 step:2726 [D loss: 0.217417, acc.: 92.97%] [G loss: 4.411085]\n",
      "epoch:3 step:2727 [D loss: 0.264058, acc.: 89.84%] [G loss: 2.499597]\n",
      "epoch:3 step:2728 [D loss: 0.078402, acc.: 99.22%] [G loss: 2.618962]\n",
      "epoch:3 step:2729 [D loss: 0.516394, acc.: 69.53%] [G loss: 3.673885]\n",
      "epoch:3 step:2730 [D loss: 0.074754, acc.: 97.66%] [G loss: 4.523053]\n",
      "epoch:3 step:2731 [D loss: 1.387123, acc.: 35.16%] [G loss: 6.101284]\n",
      "epoch:3 step:2732 [D loss: 0.190692, acc.: 91.41%] [G loss: 5.076921]\n",
      "epoch:3 step:2733 [D loss: 0.169766, acc.: 94.53%] [G loss: 3.483763]\n",
      "epoch:3 step:2734 [D loss: 0.524638, acc.: 75.00%] [G loss: 7.520900]\n",
      "epoch:3 step:2735 [D loss: 0.559674, acc.: 71.88%] [G loss: 4.688272]\n",
      "epoch:3 step:2736 [D loss: 0.287727, acc.: 86.72%] [G loss: 4.633005]\n",
      "epoch:3 step:2737 [D loss: 0.211809, acc.: 91.41%] [G loss: 4.118557]\n",
      "epoch:3 step:2738 [D loss: 0.772934, acc.: 62.50%] [G loss: 4.588871]\n",
      "epoch:3 step:2739 [D loss: 0.125489, acc.: 95.31%] [G loss: 4.164981]\n",
      "epoch:3 step:2740 [D loss: 0.207531, acc.: 95.31%] [G loss: 3.137261]\n",
      "epoch:3 step:2741 [D loss: 0.309341, acc.: 85.16%] [G loss: 5.444590]\n",
      "epoch:3 step:2742 [D loss: 0.170537, acc.: 93.75%] [G loss: 3.707424]\n",
      "epoch:3 step:2743 [D loss: 0.180024, acc.: 95.31%] [G loss: 3.771931]\n",
      "epoch:3 step:2744 [D loss: 0.612366, acc.: 67.19%] [G loss: 4.482758]\n",
      "epoch:3 step:2745 [D loss: 0.198796, acc.: 93.75%] [G loss: 3.472569]\n",
      "epoch:3 step:2746 [D loss: 0.177472, acc.: 95.31%] [G loss: 1.671020]\n",
      "epoch:3 step:2747 [D loss: 0.136096, acc.: 94.53%] [G loss: 2.457085]\n",
      "epoch:3 step:2748 [D loss: 0.482879, acc.: 75.78%] [G loss: 4.621151]\n",
      "epoch:3 step:2749 [D loss: 0.126555, acc.: 96.09%] [G loss: 3.803782]\n",
      "epoch:3 step:2750 [D loss: 0.280766, acc.: 87.50%] [G loss: 2.132279]\n",
      "epoch:3 step:2751 [D loss: 0.284068, acc.: 88.28%] [G loss: 3.151223]\n",
      "epoch:3 step:2752 [D loss: 0.230110, acc.: 91.41%] [G loss: 2.466991]\n",
      "epoch:3 step:2753 [D loss: 0.144663, acc.: 94.53%] [G loss: 3.703925]\n",
      "epoch:3 step:2754 [D loss: 0.653319, acc.: 71.88%] [G loss: 4.477717]\n",
      "epoch:3 step:2755 [D loss: 0.265930, acc.: 87.50%] [G loss: 3.784233]\n",
      "epoch:3 step:2756 [D loss: 0.250654, acc.: 90.62%] [G loss: 3.906490]\n",
      "epoch:3 step:2757 [D loss: 0.168489, acc.: 95.31%] [G loss: 3.791060]\n",
      "epoch:3 step:2758 [D loss: 0.176114, acc.: 94.53%] [G loss: 3.817693]\n",
      "epoch:3 step:2759 [D loss: 0.172642, acc.: 92.97%] [G loss: 2.310358]\n",
      "epoch:3 step:2760 [D loss: 0.345054, acc.: 82.81%] [G loss: 5.739534]\n",
      "epoch:3 step:2761 [D loss: 0.344487, acc.: 82.81%] [G loss: 3.452826]\n",
      "epoch:3 step:2762 [D loss: 0.137359, acc.: 96.09%] [G loss: 3.927562]\n",
      "epoch:3 step:2763 [D loss: 0.146000, acc.: 93.75%] [G loss: 3.731306]\n",
      "epoch:3 step:2764 [D loss: 0.089206, acc.: 97.66%] [G loss: 3.254876]\n",
      "epoch:3 step:2765 [D loss: 0.098924, acc.: 97.66%] [G loss: 2.895508]\n",
      "epoch:3 step:2766 [D loss: 0.072089, acc.: 98.44%] [G loss: 1.505960]\n",
      "epoch:3 step:2767 [D loss: 0.118248, acc.: 97.66%] [G loss: 1.696396]\n",
      "epoch:3 step:2768 [D loss: 0.222380, acc.: 92.19%] [G loss: 3.238204]\n",
      "epoch:3 step:2769 [D loss: 0.384777, acc.: 84.38%] [G loss: 1.133478]\n",
      "epoch:3 step:2770 [D loss: 0.367540, acc.: 80.47%] [G loss: 6.452409]\n",
      "epoch:3 step:2771 [D loss: 1.382916, acc.: 56.25%] [G loss: 0.973915]\n",
      "epoch:3 step:2772 [D loss: 0.793413, acc.: 66.41%] [G loss: 6.227822]\n",
      "epoch:3 step:2773 [D loss: 0.183556, acc.: 89.06%] [G loss: 5.527118]\n",
      "epoch:3 step:2774 [D loss: 0.296725, acc.: 86.72%] [G loss: 2.316720]\n",
      "epoch:3 step:2775 [D loss: 0.270990, acc.: 86.72%] [G loss: 4.761871]\n",
      "epoch:3 step:2776 [D loss: 0.023636, acc.: 100.00%] [G loss: 5.110985]\n",
      "epoch:3 step:2777 [D loss: 0.100076, acc.: 97.66%] [G loss: 3.614185]\n",
      "epoch:3 step:2778 [D loss: 0.133415, acc.: 96.88%] [G loss: 4.496588]\n",
      "epoch:3 step:2779 [D loss: 0.158490, acc.: 95.31%] [G loss: 3.467988]\n",
      "epoch:3 step:2780 [D loss: 0.176849, acc.: 96.09%] [G loss: 3.510364]\n",
      "epoch:3 step:2781 [D loss: 0.379759, acc.: 81.25%] [G loss: 4.752469]\n",
      "epoch:3 step:2782 [D loss: 0.515651, acc.: 75.78%] [G loss: 2.963388]\n",
      "epoch:3 step:2783 [D loss: 0.155845, acc.: 96.88%] [G loss: 2.605518]\n",
      "epoch:3 step:2784 [D loss: 0.111396, acc.: 99.22%] [G loss: 2.885033]\n",
      "epoch:3 step:2785 [D loss: 0.130561, acc.: 96.88%] [G loss: 2.796200]\n",
      "epoch:3 step:2786 [D loss: 0.337166, acc.: 89.06%] [G loss: 4.115186]\n",
      "epoch:3 step:2787 [D loss: 0.611490, acc.: 69.53%] [G loss: 3.156491]\n",
      "epoch:3 step:2788 [D loss: 0.245649, acc.: 92.97%] [G loss: 4.094777]\n",
      "epoch:3 step:2789 [D loss: 0.239807, acc.: 92.19%] [G loss: 3.077753]\n",
      "epoch:3 step:2790 [D loss: 0.129022, acc.: 97.66%] [G loss: 3.339949]\n",
      "epoch:3 step:2791 [D loss: 0.289984, acc.: 90.62%] [G loss: 2.715251]\n",
      "epoch:3 step:2792 [D loss: 0.288799, acc.: 91.41%] [G loss: 3.761349]\n",
      "epoch:3 step:2793 [D loss: 0.105235, acc.: 97.66%] [G loss: 2.002831]\n",
      "epoch:3 step:2794 [D loss: 0.247248, acc.: 88.28%] [G loss: 4.041777]\n",
      "epoch:3 step:2795 [D loss: 0.201087, acc.: 93.75%] [G loss: 3.770815]\n",
      "epoch:3 step:2796 [D loss: 0.109776, acc.: 97.66%] [G loss: 2.317289]\n",
      "epoch:3 step:2797 [D loss: 0.408822, acc.: 82.81%] [G loss: 5.458012]\n",
      "epoch:3 step:2798 [D loss: 0.768851, acc.: 61.72%] [G loss: 1.481286]\n",
      "epoch:3 step:2799 [D loss: 0.344547, acc.: 79.69%] [G loss: 5.083950]\n",
      "epoch:3 step:2800 [D loss: 0.245050, acc.: 88.28%] [G loss: 3.903398]\n",
      "epoch:3 step:2801 [D loss: 0.157945, acc.: 92.97%] [G loss: 5.169610]\n",
      "epoch:3 step:2802 [D loss: 0.086175, acc.: 100.00%] [G loss: 3.146487]\n",
      "epoch:3 step:2803 [D loss: 0.266345, acc.: 88.28%] [G loss: 3.804934]\n",
      "epoch:3 step:2804 [D loss: 0.414709, acc.: 80.47%] [G loss: 4.583701]\n",
      "epoch:3 step:2805 [D loss: 0.133643, acc.: 96.88%] [G loss: 4.226532]\n",
      "epoch:3 step:2806 [D loss: 0.800037, acc.: 60.94%] [G loss: 7.109367]\n",
      "epoch:3 step:2807 [D loss: 0.385434, acc.: 78.12%] [G loss: 5.726813]\n",
      "epoch:3 step:2808 [D loss: 0.104905, acc.: 97.66%] [G loss: 3.699188]\n",
      "epoch:3 step:2809 [D loss: 0.280762, acc.: 87.50%] [G loss: 5.296468]\n",
      "epoch:3 step:2810 [D loss: 0.099135, acc.: 96.88%] [G loss: 4.890404]\n",
      "epoch:3 step:2811 [D loss: 0.372393, acc.: 85.16%] [G loss: 3.377456]\n",
      "epoch:3 step:2812 [D loss: 0.375183, acc.: 80.47%] [G loss: 5.469412]\n",
      "epoch:3 step:2813 [D loss: 0.459235, acc.: 78.12%] [G loss: 3.619436]\n",
      "epoch:3 step:2814 [D loss: 0.328472, acc.: 87.50%] [G loss: 5.083739]\n",
      "epoch:3 step:2815 [D loss: 0.374511, acc.: 82.03%] [G loss: 3.779565]\n",
      "epoch:3 step:2816 [D loss: 0.183618, acc.: 94.53%] [G loss: 2.806147]\n",
      "epoch:3 step:2817 [D loss: 0.253224, acc.: 89.84%] [G loss: 4.617794]\n",
      "epoch:3 step:2818 [D loss: 0.589530, acc.: 71.88%] [G loss: 5.429699]\n",
      "epoch:3 step:2819 [D loss: 0.574889, acc.: 74.22%] [G loss: 3.462213]\n",
      "epoch:3 step:2820 [D loss: 0.096133, acc.: 98.44%] [G loss: 3.134374]\n",
      "epoch:3 step:2821 [D loss: 0.159593, acc.: 93.75%] [G loss: 4.219286]\n",
      "epoch:3 step:2822 [D loss: 0.240355, acc.: 90.62%] [G loss: 3.387956]\n",
      "epoch:3 step:2823 [D loss: 0.150610, acc.: 93.75%] [G loss: 5.027250]\n",
      "epoch:3 step:2824 [D loss: 0.400612, acc.: 83.59%] [G loss: 4.096548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2825 [D loss: 0.250808, acc.: 90.62%] [G loss: 4.346714]\n",
      "epoch:3 step:2826 [D loss: 0.088315, acc.: 99.22%] [G loss: 3.507380]\n",
      "epoch:3 step:2827 [D loss: 0.146186, acc.: 95.31%] [G loss: 4.725607]\n",
      "epoch:3 step:2828 [D loss: 0.293344, acc.: 89.06%] [G loss: 4.285894]\n",
      "epoch:3 step:2829 [D loss: 0.231556, acc.: 91.41%] [G loss: 4.802368]\n",
      "epoch:3 step:2830 [D loss: 0.103962, acc.: 98.44%] [G loss: 3.765726]\n",
      "epoch:3 step:2831 [D loss: 0.466820, acc.: 78.91%] [G loss: 3.845204]\n",
      "epoch:3 step:2832 [D loss: 0.372200, acc.: 84.38%] [G loss: 4.089588]\n",
      "epoch:3 step:2833 [D loss: 0.218538, acc.: 91.41%] [G loss: 3.937002]\n",
      "epoch:3 step:2834 [D loss: 0.226453, acc.: 91.41%] [G loss: 5.054404]\n",
      "epoch:3 step:2835 [D loss: 0.232870, acc.: 92.97%] [G loss: 4.000834]\n",
      "epoch:3 step:2836 [D loss: 0.035756, acc.: 100.00%] [G loss: 3.769835]\n",
      "epoch:3 step:2837 [D loss: 0.352963, acc.: 83.59%] [G loss: 3.644311]\n",
      "epoch:3 step:2838 [D loss: 0.136320, acc.: 96.09%] [G loss: 5.011539]\n",
      "epoch:3 step:2839 [D loss: 0.606434, acc.: 60.94%] [G loss: 5.781243]\n",
      "epoch:3 step:2840 [D loss: 0.608033, acc.: 73.44%] [G loss: 3.700090]\n",
      "epoch:3 step:2841 [D loss: 0.302090, acc.: 87.50%] [G loss: 5.375889]\n",
      "epoch:3 step:2842 [D loss: 0.131121, acc.: 94.53%] [G loss: 5.379208]\n",
      "epoch:3 step:2843 [D loss: 0.204384, acc.: 94.53%] [G loss: 2.368647]\n",
      "epoch:3 step:2844 [D loss: 0.379111, acc.: 83.59%] [G loss: 5.309334]\n",
      "epoch:3 step:2845 [D loss: 0.064423, acc.: 98.44%] [G loss: 6.468805]\n",
      "epoch:3 step:2846 [D loss: 0.246785, acc.: 88.28%] [G loss: 2.607129]\n",
      "epoch:3 step:2847 [D loss: 0.319800, acc.: 83.59%] [G loss: 5.839451]\n",
      "epoch:3 step:2848 [D loss: 0.166237, acc.: 94.53%] [G loss: 4.711242]\n",
      "epoch:3 step:2849 [D loss: 0.374371, acc.: 82.81%] [G loss: 3.838334]\n",
      "epoch:3 step:2850 [D loss: 0.112468, acc.: 96.88%] [G loss: 3.520303]\n",
      "epoch:3 step:2851 [D loss: 1.011406, acc.: 58.59%] [G loss: 8.241666]\n",
      "epoch:3 step:2852 [D loss: 0.846866, acc.: 63.28%] [G loss: 5.235505]\n",
      "epoch:3 step:2853 [D loss: 0.257399, acc.: 91.41%] [G loss: 3.712568]\n",
      "epoch:3 step:2854 [D loss: 0.256006, acc.: 91.41%] [G loss: 4.071467]\n",
      "epoch:3 step:2855 [D loss: 0.040255, acc.: 100.00%] [G loss: 4.419009]\n",
      "epoch:3 step:2856 [D loss: 0.101578, acc.: 96.88%] [G loss: 3.301153]\n",
      "epoch:3 step:2857 [D loss: 0.251967, acc.: 90.62%] [G loss: 4.367431]\n",
      "epoch:3 step:2858 [D loss: 0.147019, acc.: 97.66%] [G loss: 3.234570]\n",
      "epoch:3 step:2859 [D loss: 0.163367, acc.: 96.88%] [G loss: 2.320134]\n",
      "epoch:3 step:2860 [D loss: 0.090387, acc.: 98.44%] [G loss: 1.985542]\n",
      "epoch:3 step:2861 [D loss: 0.143011, acc.: 96.88%] [G loss: 2.637165]\n",
      "epoch:3 step:2862 [D loss: 0.324943, acc.: 85.16%] [G loss: 2.163608]\n",
      "epoch:3 step:2863 [D loss: 0.099592, acc.: 99.22%] [G loss: 1.958125]\n",
      "epoch:3 step:2864 [D loss: 0.358493, acc.: 88.28%] [G loss: 3.512006]\n",
      "epoch:3 step:2865 [D loss: 0.721451, acc.: 58.59%] [G loss: 4.172116]\n",
      "epoch:3 step:2866 [D loss: 0.110849, acc.: 97.66%] [G loss: 4.032299]\n",
      "epoch:3 step:2867 [D loss: 0.489888, acc.: 80.47%] [G loss: 4.157968]\n",
      "epoch:3 step:2868 [D loss: 0.101229, acc.: 98.44%] [G loss: 3.997458]\n",
      "epoch:3 step:2869 [D loss: 0.994755, acc.: 56.25%] [G loss: 4.015627]\n",
      "epoch:3 step:2870 [D loss: 0.104184, acc.: 97.66%] [G loss: 3.836402]\n",
      "epoch:3 step:2871 [D loss: 0.221202, acc.: 91.41%] [G loss: 3.414180]\n",
      "epoch:3 step:2872 [D loss: 0.186289, acc.: 92.97%] [G loss: 3.679113]\n",
      "epoch:3 step:2873 [D loss: 0.067747, acc.: 100.00%] [G loss: 3.284814]\n",
      "epoch:3 step:2874 [D loss: 0.198072, acc.: 94.53%] [G loss: 3.756612]\n",
      "epoch:3 step:2875 [D loss: 0.295151, acc.: 89.06%] [G loss: 4.799690]\n",
      "epoch:3 step:2876 [D loss: 0.727333, acc.: 65.62%] [G loss: 3.170377]\n",
      "epoch:3 step:2877 [D loss: 0.181450, acc.: 95.31%] [G loss: 3.863718]\n",
      "epoch:3 step:2878 [D loss: 0.665780, acc.: 65.62%] [G loss: 4.371787]\n",
      "epoch:3 step:2879 [D loss: 0.189252, acc.: 92.97%] [G loss: 3.605306]\n",
      "epoch:3 step:2880 [D loss: 0.106503, acc.: 96.88%] [G loss: 1.535925]\n",
      "epoch:3 step:2881 [D loss: 0.433852, acc.: 81.25%] [G loss: 4.963186]\n",
      "epoch:3 step:2882 [D loss: 0.796141, acc.: 63.28%] [G loss: 3.517948]\n",
      "epoch:3 step:2883 [D loss: 0.056170, acc.: 98.44%] [G loss: 2.804413]\n",
      "epoch:3 step:2884 [D loss: 0.232015, acc.: 89.06%] [G loss: 4.404272]\n",
      "epoch:3 step:2885 [D loss: 0.141459, acc.: 93.75%] [G loss: 3.954277]\n",
      "epoch:3 step:2886 [D loss: 0.197003, acc.: 92.19%] [G loss: 3.459542]\n",
      "epoch:3 step:2887 [D loss: 0.225993, acc.: 90.62%] [G loss: 4.055896]\n",
      "epoch:3 step:2888 [D loss: 0.170087, acc.: 95.31%] [G loss: 3.137306]\n",
      "epoch:3 step:2889 [D loss: 0.657775, acc.: 68.75%] [G loss: 4.613115]\n",
      "epoch:3 step:2890 [D loss: 0.328006, acc.: 83.59%] [G loss: 2.663587]\n",
      "epoch:3 step:2891 [D loss: 0.276435, acc.: 88.28%] [G loss: 3.790902]\n",
      "epoch:3 step:2892 [D loss: 0.168952, acc.: 96.88%] [G loss: 3.628959]\n",
      "epoch:3 step:2893 [D loss: 0.705864, acc.: 67.19%] [G loss: 6.212778]\n",
      "epoch:3 step:2894 [D loss: 0.162470, acc.: 94.53%] [G loss: 5.165895]\n",
      "epoch:3 step:2895 [D loss: 1.527595, acc.: 38.28%] [G loss: 5.943550]\n",
      "epoch:3 step:2896 [D loss: 0.286134, acc.: 85.16%] [G loss: 5.122162]\n",
      "epoch:3 step:2897 [D loss: 0.316855, acc.: 85.94%] [G loss: 3.541149]\n",
      "epoch:3 step:2898 [D loss: 0.196751, acc.: 92.97%] [G loss: 4.572799]\n",
      "epoch:3 step:2899 [D loss: 0.126173, acc.: 96.09%] [G loss: 4.131149]\n",
      "epoch:3 step:2900 [D loss: 0.164408, acc.: 95.31%] [G loss: 3.811154]\n",
      "epoch:3 step:2901 [D loss: 0.217220, acc.: 92.97%] [G loss: 4.696459]\n",
      "epoch:3 step:2902 [D loss: 0.204324, acc.: 95.31%] [G loss: 3.256215]\n",
      "epoch:3 step:2903 [D loss: 0.158596, acc.: 96.88%] [G loss: 3.565079]\n",
      "epoch:3 step:2904 [D loss: 0.133482, acc.: 95.31%] [G loss: 2.746798]\n",
      "epoch:3 step:2905 [D loss: 0.316953, acc.: 84.38%] [G loss: 3.021441]\n",
      "epoch:3 step:2906 [D loss: 0.154966, acc.: 92.97%] [G loss: 2.566676]\n",
      "epoch:3 step:2907 [D loss: 0.170589, acc.: 95.31%] [G loss: 3.104695]\n",
      "epoch:3 step:2908 [D loss: 0.361335, acc.: 86.72%] [G loss: 5.106529]\n",
      "epoch:3 step:2909 [D loss: 0.622786, acc.: 66.41%] [G loss: 3.810061]\n",
      "epoch:3 step:2910 [D loss: 0.225040, acc.: 90.62%] [G loss: 1.878481]\n",
      "epoch:3 step:2911 [D loss: 0.141086, acc.: 95.31%] [G loss: 3.776803]\n",
      "epoch:3 step:2912 [D loss: 0.114770, acc.: 96.88%] [G loss: 3.363908]\n",
      "epoch:3 step:2913 [D loss: 0.215582, acc.: 96.09%] [G loss: 4.672511]\n",
      "epoch:3 step:2914 [D loss: 0.213467, acc.: 92.97%] [G loss: 3.640091]\n",
      "epoch:3 step:2915 [D loss: 0.362774, acc.: 82.81%] [G loss: 4.897438]\n",
      "epoch:3 step:2916 [D loss: 0.375217, acc.: 85.16%] [G loss: 3.757816]\n",
      "epoch:3 step:2917 [D loss: 0.065786, acc.: 99.22%] [G loss: 2.958295]\n",
      "epoch:3 step:2918 [D loss: 0.225936, acc.: 90.62%] [G loss: 5.475071]\n",
      "epoch:3 step:2919 [D loss: 0.317446, acc.: 85.16%] [G loss: 2.903879]\n",
      "epoch:3 step:2920 [D loss: 0.365025, acc.: 79.69%] [G loss: 6.738073]\n",
      "epoch:3 step:2921 [D loss: 0.358012, acc.: 85.16%] [G loss: 4.420872]\n",
      "epoch:3 step:2922 [D loss: 0.394152, acc.: 80.47%] [G loss: 5.657335]\n",
      "epoch:3 step:2923 [D loss: 0.164217, acc.: 92.97%] [G loss: 4.667948]\n",
      "epoch:3 step:2924 [D loss: 0.244342, acc.: 90.62%] [G loss: 3.350389]\n",
      "epoch:3 step:2925 [D loss: 0.206355, acc.: 89.06%] [G loss: 3.758319]\n",
      "epoch:3 step:2926 [D loss: 0.074553, acc.: 98.44%] [G loss: 3.900974]\n",
      "epoch:3 step:2927 [D loss: 0.325736, acc.: 84.38%] [G loss: 3.618604]\n",
      "epoch:3 step:2928 [D loss: 0.158725, acc.: 94.53%] [G loss: 2.914185]\n",
      "epoch:3 step:2929 [D loss: 0.210854, acc.: 92.19%] [G loss: 3.781005]\n",
      "epoch:3 step:2930 [D loss: 0.266139, acc.: 91.41%] [G loss: 3.628661]\n",
      "epoch:3 step:2931 [D loss: 1.235075, acc.: 47.66%] [G loss: 5.272649]\n",
      "epoch:3 step:2932 [D loss: 0.459488, acc.: 77.34%] [G loss: 4.802808]\n",
      "epoch:3 step:2933 [D loss: 0.429557, acc.: 75.00%] [G loss: 3.296839]\n",
      "epoch:3 step:2934 [D loss: 0.281185, acc.: 88.28%] [G loss: 3.998896]\n",
      "epoch:3 step:2935 [D loss: 0.205846, acc.: 94.53%] [G loss: 4.445912]\n",
      "epoch:3 step:2936 [D loss: 0.185595, acc.: 95.31%] [G loss: 3.950036]\n",
      "epoch:3 step:2937 [D loss: 0.377393, acc.: 84.38%] [G loss: 4.077691]\n",
      "epoch:3 step:2938 [D loss: 0.213731, acc.: 92.19%] [G loss: 4.216990]\n",
      "epoch:3 step:2939 [D loss: 0.270949, acc.: 87.50%] [G loss: 4.726024]\n",
      "epoch:3 step:2940 [D loss: 0.120614, acc.: 96.88%] [G loss: 5.340192]\n",
      "epoch:3 step:2941 [D loss: 0.441668, acc.: 82.03%] [G loss: 5.471756]\n",
      "epoch:3 step:2942 [D loss: 0.041289, acc.: 100.00%] [G loss: 5.916120]\n",
      "epoch:3 step:2943 [D loss: 0.105280, acc.: 97.66%] [G loss: 3.736603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2944 [D loss: 0.270967, acc.: 87.50%] [G loss: 4.908328]\n",
      "epoch:3 step:2945 [D loss: 0.268134, acc.: 87.50%] [G loss: 4.169847]\n",
      "epoch:3 step:2946 [D loss: 0.198136, acc.: 92.19%] [G loss: 4.393368]\n",
      "epoch:3 step:2947 [D loss: 0.214309, acc.: 93.75%] [G loss: 4.821340]\n",
      "epoch:3 step:2948 [D loss: 0.143314, acc.: 95.31%] [G loss: 4.218642]\n",
      "epoch:3 step:2949 [D loss: 0.099509, acc.: 99.22%] [G loss: 3.871747]\n",
      "epoch:3 step:2950 [D loss: 0.184052, acc.: 93.75%] [G loss: 4.272547]\n",
      "epoch:3 step:2951 [D loss: 0.136136, acc.: 96.09%] [G loss: 4.434486]\n",
      "epoch:3 step:2952 [D loss: 0.790050, acc.: 57.81%] [G loss: 8.378664]\n",
      "epoch:3 step:2953 [D loss: 0.403172, acc.: 76.56%] [G loss: 5.867749]\n",
      "epoch:3 step:2954 [D loss: 0.157300, acc.: 92.19%] [G loss: 4.595577]\n",
      "epoch:3 step:2955 [D loss: 0.037798, acc.: 100.00%] [G loss: 3.463289]\n",
      "epoch:3 step:2956 [D loss: 0.205540, acc.: 91.41%] [G loss: 4.876911]\n",
      "epoch:3 step:2957 [D loss: 0.096755, acc.: 96.88%] [G loss: 4.858588]\n",
      "epoch:3 step:2958 [D loss: 0.214779, acc.: 92.19%] [G loss: 4.481456]\n",
      "epoch:3 step:2959 [D loss: 0.224387, acc.: 89.84%] [G loss: 2.989181]\n",
      "epoch:3 step:2960 [D loss: 1.226717, acc.: 53.12%] [G loss: 8.717969]\n",
      "epoch:3 step:2961 [D loss: 1.763704, acc.: 52.34%] [G loss: 4.033903]\n",
      "epoch:3 step:2962 [D loss: 0.540156, acc.: 73.44%] [G loss: 3.244122]\n",
      "epoch:3 step:2963 [D loss: 0.135531, acc.: 96.88%] [G loss: 3.464622]\n",
      "epoch:3 step:2964 [D loss: 0.097077, acc.: 98.44%] [G loss: 3.705627]\n",
      "epoch:3 step:2965 [D loss: 0.182260, acc.: 97.66%] [G loss: 3.255895]\n",
      "epoch:3 step:2966 [D loss: 0.470108, acc.: 77.34%] [G loss: 3.825620]\n",
      "epoch:3 step:2967 [D loss: 0.224234, acc.: 92.19%] [G loss: 4.138554]\n",
      "epoch:3 step:2968 [D loss: 0.152421, acc.: 96.09%] [G loss: 3.516030]\n",
      "epoch:3 step:2969 [D loss: 0.182844, acc.: 94.53%] [G loss: 3.956522]\n",
      "epoch:3 step:2970 [D loss: 0.207056, acc.: 95.31%] [G loss: 2.303825]\n",
      "epoch:3 step:2971 [D loss: 0.248871, acc.: 88.28%] [G loss: 3.765352]\n",
      "epoch:3 step:2972 [D loss: 0.187955, acc.: 95.31%] [G loss: 4.048058]\n",
      "epoch:3 step:2973 [D loss: 0.222831, acc.: 92.19%] [G loss: 3.420362]\n",
      "epoch:3 step:2974 [D loss: 0.305782, acc.: 89.06%] [G loss: 3.133530]\n",
      "epoch:3 step:2975 [D loss: 0.071463, acc.: 99.22%] [G loss: 2.841930]\n",
      "epoch:3 step:2976 [D loss: 0.090585, acc.: 98.44%] [G loss: 2.805802]\n",
      "epoch:3 step:2977 [D loss: 0.323478, acc.: 86.72%] [G loss: 4.214574]\n",
      "epoch:3 step:2978 [D loss: 0.307474, acc.: 87.50%] [G loss: 3.458140]\n",
      "epoch:3 step:2979 [D loss: 0.163259, acc.: 96.09%] [G loss: 4.103868]\n",
      "epoch:3 step:2980 [D loss: 0.165404, acc.: 96.88%] [G loss: 3.228863]\n",
      "epoch:3 step:2981 [D loss: 0.203659, acc.: 92.19%] [G loss: 3.039083]\n",
      "epoch:3 step:2982 [D loss: 0.204285, acc.: 93.75%] [G loss: 3.120337]\n",
      "epoch:3 step:2983 [D loss: 0.102340, acc.: 97.66%] [G loss: 2.357525]\n",
      "epoch:3 step:2984 [D loss: 0.146516, acc.: 96.09%] [G loss: 2.364278]\n",
      "epoch:3 step:2985 [D loss: 0.100124, acc.: 96.09%] [G loss: 2.724075]\n",
      "epoch:3 step:2986 [D loss: 0.579160, acc.: 71.88%] [G loss: 4.658477]\n",
      "epoch:3 step:2987 [D loss: 0.296532, acc.: 85.16%] [G loss: 3.209031]\n",
      "epoch:3 step:2988 [D loss: 0.148386, acc.: 96.09%] [G loss: 1.777742]\n",
      "epoch:3 step:2989 [D loss: 0.103022, acc.: 97.66%] [G loss: 1.636108]\n",
      "epoch:3 step:2990 [D loss: 0.243426, acc.: 91.41%] [G loss: 3.916528]\n",
      "epoch:3 step:2991 [D loss: 0.236632, acc.: 91.41%] [G loss: 3.878510]\n",
      "epoch:3 step:2992 [D loss: 0.174598, acc.: 93.75%] [G loss: 2.502483]\n",
      "epoch:3 step:2993 [D loss: 0.318107, acc.: 85.94%] [G loss: 4.016250]\n",
      "epoch:3 step:2994 [D loss: 0.308893, acc.: 87.50%] [G loss: 3.380655]\n",
      "epoch:3 step:2995 [D loss: 0.513948, acc.: 72.66%] [G loss: 5.318286]\n",
      "epoch:3 step:2996 [D loss: 0.253726, acc.: 89.06%] [G loss: 5.253100]\n",
      "epoch:3 step:2997 [D loss: 0.130313, acc.: 93.75%] [G loss: 3.951161]\n",
      "epoch:3 step:2998 [D loss: 0.161261, acc.: 96.09%] [G loss: 4.625968]\n",
      "epoch:3 step:2999 [D loss: 0.205901, acc.: 92.97%] [G loss: 4.627645]\n",
      "epoch:3 step:3000 [D loss: 0.192290, acc.: 96.09%] [G loss: 4.249513]\n",
      "epoch:3 step:3001 [D loss: 0.145328, acc.: 94.53%] [G loss: 4.555857]\n",
      "epoch:3 step:3002 [D loss: 0.125831, acc.: 97.66%] [G loss: 5.134817]\n",
      "epoch:3 step:3003 [D loss: 0.235293, acc.: 92.19%] [G loss: 4.826553]\n",
      "epoch:3 step:3004 [D loss: 0.227009, acc.: 87.50%] [G loss: 6.295547]\n",
      "epoch:3 step:3005 [D loss: 0.269242, acc.: 85.94%] [G loss: 4.149053]\n",
      "epoch:3 step:3006 [D loss: 0.215456, acc.: 90.62%] [G loss: 5.770838]\n",
      "epoch:3 step:3007 [D loss: 0.120304, acc.: 96.09%] [G loss: 4.762974]\n",
      "epoch:3 step:3008 [D loss: 0.891801, acc.: 53.91%] [G loss: 7.806505]\n",
      "epoch:3 step:3009 [D loss: 0.583178, acc.: 71.09%] [G loss: 3.942542]\n",
      "epoch:3 step:3010 [D loss: 0.250147, acc.: 89.84%] [G loss: 4.913705]\n",
      "epoch:3 step:3011 [D loss: 0.288135, acc.: 86.72%] [G loss: 2.927194]\n",
      "epoch:3 step:3012 [D loss: 0.577145, acc.: 71.88%] [G loss: 6.102908]\n",
      "epoch:3 step:3013 [D loss: 0.570321, acc.: 71.09%] [G loss: 3.104287]\n",
      "epoch:3 step:3014 [D loss: 0.278244, acc.: 88.28%] [G loss: 5.497141]\n",
      "epoch:3 step:3015 [D loss: 0.113893, acc.: 95.31%] [G loss: 5.195765]\n",
      "epoch:3 step:3016 [D loss: 0.103029, acc.: 97.66%] [G loss: 2.986692]\n",
      "epoch:3 step:3017 [D loss: 0.727188, acc.: 67.97%] [G loss: 7.520068]\n",
      "epoch:3 step:3018 [D loss: 0.918542, acc.: 64.06%] [G loss: 1.904303]\n",
      "epoch:3 step:3019 [D loss: 0.685816, acc.: 68.75%] [G loss: 6.861309]\n",
      "epoch:3 step:3020 [D loss: 0.772726, acc.: 64.06%] [G loss: 4.878098]\n",
      "epoch:3 step:3021 [D loss: 0.084016, acc.: 99.22%] [G loss: 3.648741]\n",
      "epoch:3 step:3022 [D loss: 0.094677, acc.: 99.22%] [G loss: 3.244529]\n",
      "epoch:3 step:3023 [D loss: 0.188474, acc.: 95.31%] [G loss: 3.314342]\n",
      "epoch:3 step:3024 [D loss: 0.151514, acc.: 96.09%] [G loss: 3.534949]\n",
      "epoch:3 step:3025 [D loss: 0.648863, acc.: 70.31%] [G loss: 4.446135]\n",
      "epoch:3 step:3026 [D loss: 0.219262, acc.: 90.62%] [G loss: 4.659182]\n",
      "epoch:3 step:3027 [D loss: 0.442122, acc.: 75.78%] [G loss: 3.596147]\n",
      "epoch:3 step:3028 [D loss: 0.308380, acc.: 89.06%] [G loss: 3.506598]\n",
      "epoch:3 step:3029 [D loss: 0.279310, acc.: 86.72%] [G loss: 3.199074]\n",
      "epoch:3 step:3030 [D loss: 0.361082, acc.: 86.72%] [G loss: 4.587646]\n",
      "epoch:3 step:3031 [D loss: 0.354226, acc.: 83.59%] [G loss: 2.863023]\n",
      "epoch:3 step:3032 [D loss: 0.238239, acc.: 92.97%] [G loss: 2.036645]\n",
      "epoch:3 step:3033 [D loss: 0.178818, acc.: 92.19%] [G loss: 5.001923]\n",
      "epoch:3 step:3034 [D loss: 0.204877, acc.: 91.41%] [G loss: 2.068172]\n",
      "epoch:3 step:3035 [D loss: 0.459995, acc.: 78.12%] [G loss: 2.798211]\n",
      "epoch:3 step:3036 [D loss: 0.082940, acc.: 98.44%] [G loss: 3.294269]\n",
      "epoch:3 step:3037 [D loss: 0.342378, acc.: 88.28%] [G loss: 1.813544]\n",
      "epoch:3 step:3038 [D loss: 0.082024, acc.: 97.66%] [G loss: 2.582246]\n",
      "epoch:3 step:3039 [D loss: 0.273963, acc.: 86.72%] [G loss: 3.894964]\n",
      "epoch:3 step:3040 [D loss: 0.145586, acc.: 96.88%] [G loss: 3.042613]\n",
      "epoch:3 step:3041 [D loss: 0.557106, acc.: 70.31%] [G loss: 5.569360]\n",
      "epoch:3 step:3042 [D loss: 0.596261, acc.: 67.19%] [G loss: 3.415354]\n",
      "epoch:3 step:3043 [D loss: 0.169825, acc.: 92.19%] [G loss: 5.050364]\n",
      "epoch:3 step:3044 [D loss: 0.121410, acc.: 96.09%] [G loss: 4.508579]\n",
      "epoch:3 step:3045 [D loss: 0.127185, acc.: 96.09%] [G loss: 3.414749]\n",
      "epoch:3 step:3046 [D loss: 0.200628, acc.: 93.75%] [G loss: 5.444645]\n",
      "epoch:3 step:3047 [D loss: 0.216336, acc.: 91.41%] [G loss: 3.768682]\n",
      "epoch:3 step:3048 [D loss: 0.124309, acc.: 96.88%] [G loss: 3.004101]\n",
      "epoch:3 step:3049 [D loss: 0.212221, acc.: 93.75%] [G loss: 3.185998]\n",
      "epoch:3 step:3050 [D loss: 0.132785, acc.: 96.88%] [G loss: 3.778576]\n",
      "epoch:3 step:3051 [D loss: 0.097403, acc.: 97.66%] [G loss: 3.088989]\n",
      "epoch:3 step:3052 [D loss: 0.226820, acc.: 91.41%] [G loss: 3.662129]\n",
      "epoch:3 step:3053 [D loss: 0.209282, acc.: 90.62%] [G loss: 3.416657]\n",
      "epoch:3 step:3054 [D loss: 0.236648, acc.: 92.97%] [G loss: 5.336684]\n",
      "epoch:3 step:3055 [D loss: 1.100018, acc.: 46.09%] [G loss: 4.616854]\n",
      "epoch:3 step:3056 [D loss: 0.356121, acc.: 88.28%] [G loss: 3.925616]\n",
      "epoch:3 step:3057 [D loss: 0.158181, acc.: 96.88%] [G loss: 3.435549]\n",
      "epoch:3 step:3058 [D loss: 0.211926, acc.: 92.97%] [G loss: 4.277301]\n",
      "epoch:3 step:3059 [D loss: 0.053537, acc.: 98.44%] [G loss: 4.157102]\n",
      "epoch:3 step:3060 [D loss: 0.136873, acc.: 93.75%] [G loss: 4.038441]\n",
      "epoch:3 step:3061 [D loss: 0.218075, acc.: 90.62%] [G loss: 1.884690]\n",
      "epoch:3 step:3062 [D loss: 0.200925, acc.: 92.97%] [G loss: 4.186485]\n",
      "epoch:3 step:3063 [D loss: 0.122404, acc.: 99.22%] [G loss: 4.363794]\n",
      "epoch:3 step:3064 [D loss: 1.167949, acc.: 39.06%] [G loss: 7.005891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3065 [D loss: 0.383511, acc.: 82.03%] [G loss: 4.758574]\n",
      "epoch:3 step:3066 [D loss: 0.277117, acc.: 86.72%] [G loss: 5.353767]\n",
      "epoch:3 step:3067 [D loss: 0.306569, acc.: 88.28%] [G loss: 2.949364]\n",
      "epoch:3 step:3068 [D loss: 0.315582, acc.: 85.94%] [G loss: 2.699187]\n",
      "epoch:3 step:3069 [D loss: 0.219945, acc.: 93.75%] [G loss: 4.517673]\n",
      "epoch:3 step:3070 [D loss: 0.469073, acc.: 75.00%] [G loss: 1.294912]\n",
      "epoch:3 step:3071 [D loss: 0.190103, acc.: 92.19%] [G loss: 1.212710]\n",
      "epoch:3 step:3072 [D loss: 0.036591, acc.: 100.00%] [G loss: 1.425874]\n",
      "epoch:3 step:3073 [D loss: 0.089474, acc.: 96.88%] [G loss: 0.535203]\n",
      "epoch:3 step:3074 [D loss: 0.281719, acc.: 85.94%] [G loss: 2.553976]\n",
      "epoch:3 step:3075 [D loss: 0.351045, acc.: 85.94%] [G loss: 0.915231]\n",
      "epoch:3 step:3076 [D loss: 0.108658, acc.: 96.88%] [G loss: 0.891516]\n",
      "epoch:3 step:3077 [D loss: 0.081815, acc.: 96.88%] [G loss: 0.108301]\n",
      "epoch:3 step:3078 [D loss: 0.020083, acc.: 100.00%] [G loss: 0.309004]\n",
      "epoch:3 step:3079 [D loss: 0.119722, acc.: 94.53%] [G loss: 1.922329]\n",
      "epoch:3 step:3080 [D loss: 0.156620, acc.: 96.09%] [G loss: 1.101998]\n",
      "epoch:3 step:3081 [D loss: 0.150401, acc.: 96.09%] [G loss: 0.269500]\n",
      "epoch:3 step:3082 [D loss: 0.126430, acc.: 95.31%] [G loss: 0.808411]\n",
      "epoch:3 step:3083 [D loss: 0.059957, acc.: 99.22%] [G loss: 1.953367]\n",
      "epoch:3 step:3084 [D loss: 0.335426, acc.: 84.38%] [G loss: 5.011821]\n",
      "epoch:3 step:3085 [D loss: 1.016314, acc.: 54.69%] [G loss: 3.894211]\n",
      "epoch:3 step:3086 [D loss: 0.131574, acc.: 97.66%] [G loss: 4.530725]\n",
      "epoch:3 step:3087 [D loss: 0.057342, acc.: 98.44%] [G loss: 4.289181]\n",
      "epoch:3 step:3088 [D loss: 0.315334, acc.: 84.38%] [G loss: 5.879591]\n",
      "epoch:3 step:3089 [D loss: 0.434308, acc.: 78.91%] [G loss: 4.598449]\n",
      "epoch:3 step:3090 [D loss: 0.145768, acc.: 96.88%] [G loss: 3.948102]\n",
      "epoch:3 step:3091 [D loss: 0.073198, acc.: 98.44%] [G loss: 4.355492]\n",
      "epoch:3 step:3092 [D loss: 0.087721, acc.: 99.22%] [G loss: 4.136095]\n",
      "epoch:3 step:3093 [D loss: 0.127892, acc.: 98.44%] [G loss: 3.738917]\n",
      "epoch:3 step:3094 [D loss: 0.682872, acc.: 65.62%] [G loss: 7.076113]\n",
      "epoch:3 step:3095 [D loss: 0.751131, acc.: 64.84%] [G loss: 2.284095]\n",
      "epoch:3 step:3096 [D loss: 0.271133, acc.: 87.50%] [G loss: 4.093994]\n",
      "epoch:3 step:3097 [D loss: 0.115728, acc.: 98.44%] [G loss: 4.292484]\n",
      "epoch:3 step:3098 [D loss: 0.119960, acc.: 96.88%] [G loss: 3.754110]\n",
      "epoch:3 step:3099 [D loss: 0.343479, acc.: 85.16%] [G loss: 4.835609]\n",
      "epoch:3 step:3100 [D loss: 0.333102, acc.: 85.94%] [G loss: 3.491342]\n",
      "epoch:3 step:3101 [D loss: 0.143798, acc.: 94.53%] [G loss: 2.472268]\n",
      "epoch:3 step:3102 [D loss: 0.223235, acc.: 93.75%] [G loss: 3.593587]\n",
      "epoch:3 step:3103 [D loss: 0.415096, acc.: 83.59%] [G loss: 2.935562]\n",
      "epoch:3 step:3104 [D loss: 0.245089, acc.: 91.41%] [G loss: 4.310849]\n",
      "epoch:3 step:3105 [D loss: 0.284013, acc.: 89.84%] [G loss: 2.785241]\n",
      "epoch:3 step:3106 [D loss: 0.120273, acc.: 96.88%] [G loss: 2.331561]\n",
      "epoch:3 step:3107 [D loss: 0.379398, acc.: 82.03%] [G loss: 3.732589]\n",
      "epoch:3 step:3108 [D loss: 0.211446, acc.: 92.97%] [G loss: 2.886705]\n",
      "epoch:3 step:3109 [D loss: 0.811984, acc.: 57.81%] [G loss: 5.166894]\n",
      "epoch:3 step:3110 [D loss: 0.495531, acc.: 73.44%] [G loss: 2.660269]\n",
      "epoch:3 step:3111 [D loss: 0.147491, acc.: 93.75%] [G loss: 3.379071]\n",
      "epoch:3 step:3112 [D loss: 0.189467, acc.: 93.75%] [G loss: 5.063955]\n",
      "epoch:3 step:3113 [D loss: 0.092783, acc.: 97.66%] [G loss: 4.072064]\n",
      "epoch:3 step:3114 [D loss: 0.120047, acc.: 99.22%] [G loss: 2.587976]\n",
      "epoch:3 step:3115 [D loss: 0.454063, acc.: 78.91%] [G loss: 4.703478]\n",
      "epoch:3 step:3116 [D loss: 0.338176, acc.: 82.81%] [G loss: 3.602023]\n",
      "epoch:3 step:3117 [D loss: 0.181319, acc.: 93.75%] [G loss: 3.199830]\n",
      "epoch:3 step:3118 [D loss: 0.107593, acc.: 96.09%] [G loss: 3.427060]\n",
      "epoch:3 step:3119 [D loss: 0.185317, acc.: 92.19%] [G loss: 3.045625]\n",
      "epoch:3 step:3120 [D loss: 0.504020, acc.: 77.34%] [G loss: 5.071400]\n",
      "epoch:3 step:3121 [D loss: 0.060330, acc.: 97.66%] [G loss: 5.741064]\n",
      "epoch:3 step:3122 [D loss: 0.933437, acc.: 52.34%] [G loss: 3.439134]\n",
      "epoch:3 step:3123 [D loss: 0.296447, acc.: 89.06%] [G loss: 5.194156]\n",
      "epoch:3 step:3124 [D loss: 0.152330, acc.: 96.88%] [G loss: 5.032168]\n",
      "epoch:4 step:3125 [D loss: 0.207875, acc.: 91.41%] [G loss: 3.291634]\n",
      "epoch:4 step:3126 [D loss: 0.331384, acc.: 78.91%] [G loss: 5.120512]\n",
      "epoch:4 step:3127 [D loss: 0.234916, acc.: 89.84%] [G loss: 4.012581]\n",
      "epoch:4 step:3128 [D loss: 0.089687, acc.: 98.44%] [G loss: 3.937545]\n",
      "epoch:4 step:3129 [D loss: 0.169702, acc.: 96.09%] [G loss: 4.146751]\n",
      "epoch:4 step:3130 [D loss: 0.227115, acc.: 92.19%] [G loss: 2.535853]\n",
      "epoch:4 step:3131 [D loss: 0.186355, acc.: 91.41%] [G loss: 2.870569]\n",
      "epoch:4 step:3132 [D loss: 0.200668, acc.: 91.41%] [G loss: 4.231505]\n",
      "epoch:4 step:3133 [D loss: 0.125328, acc.: 96.88%] [G loss: 3.434989]\n",
      "epoch:4 step:3134 [D loss: 0.674749, acc.: 67.97%] [G loss: 6.474975]\n",
      "epoch:4 step:3135 [D loss: 0.177721, acc.: 92.19%] [G loss: 5.447392]\n",
      "epoch:4 step:3136 [D loss: 0.403137, acc.: 82.03%] [G loss: 3.279364]\n",
      "epoch:4 step:3137 [D loss: 0.138376, acc.: 95.31%] [G loss: 3.275120]\n",
      "epoch:4 step:3138 [D loss: 0.348851, acc.: 84.38%] [G loss: 2.755365]\n",
      "epoch:4 step:3139 [D loss: 0.118084, acc.: 97.66%] [G loss: 3.777714]\n",
      "epoch:4 step:3140 [D loss: 0.275673, acc.: 91.41%] [G loss: 3.328582]\n",
      "epoch:4 step:3141 [D loss: 0.247913, acc.: 91.41%] [G loss: 3.897237]\n",
      "epoch:4 step:3142 [D loss: 0.323135, acc.: 87.50%] [G loss: 4.546358]\n",
      "epoch:4 step:3143 [D loss: 0.474579, acc.: 75.78%] [G loss: 4.236780]\n",
      "epoch:4 step:3144 [D loss: 0.039381, acc.: 100.00%] [G loss: 5.005662]\n",
      "epoch:4 step:3145 [D loss: 0.156913, acc.: 93.75%] [G loss: 3.721262]\n",
      "epoch:4 step:3146 [D loss: 0.156707, acc.: 94.53%] [G loss: 5.138726]\n",
      "epoch:4 step:3147 [D loss: 0.170848, acc.: 92.97%] [G loss: 4.516596]\n",
      "epoch:4 step:3148 [D loss: 0.127870, acc.: 97.66%] [G loss: 3.541159]\n",
      "epoch:4 step:3149 [D loss: 1.246302, acc.: 46.09%] [G loss: 8.593346]\n",
      "epoch:4 step:3150 [D loss: 1.085298, acc.: 60.16%] [G loss: 3.935301]\n",
      "epoch:4 step:3151 [D loss: 0.133847, acc.: 96.09%] [G loss: 4.114886]\n",
      "epoch:4 step:3152 [D loss: 0.169835, acc.: 92.97%] [G loss: 5.087371]\n",
      "epoch:4 step:3153 [D loss: 0.176924, acc.: 92.97%] [G loss: 4.676166]\n",
      "epoch:4 step:3154 [D loss: 0.188569, acc.: 92.19%] [G loss: 3.053592]\n",
      "epoch:4 step:3155 [D loss: 0.362479, acc.: 82.03%] [G loss: 4.441771]\n",
      "epoch:4 step:3156 [D loss: 0.285544, acc.: 86.72%] [G loss: 4.294754]\n",
      "epoch:4 step:3157 [D loss: 0.398805, acc.: 85.16%] [G loss: 3.262173]\n",
      "epoch:4 step:3158 [D loss: 0.280994, acc.: 85.16%] [G loss: 5.371185]\n",
      "epoch:4 step:3159 [D loss: 0.404171, acc.: 84.38%] [G loss: 3.488174]\n",
      "epoch:4 step:3160 [D loss: 0.316283, acc.: 87.50%] [G loss: 3.434860]\n",
      "epoch:4 step:3161 [D loss: 0.073556, acc.: 99.22%] [G loss: 4.794831]\n",
      "epoch:4 step:3162 [D loss: 0.254115, acc.: 90.62%] [G loss: 3.663223]\n",
      "epoch:4 step:3163 [D loss: 0.102781, acc.: 99.22%] [G loss: 4.037855]\n",
      "epoch:4 step:3164 [D loss: 0.187252, acc.: 93.75%] [G loss: 4.280888]\n",
      "epoch:4 step:3165 [D loss: 0.731747, acc.: 60.16%] [G loss: 4.248425]\n",
      "epoch:4 step:3166 [D loss: 0.200254, acc.: 92.19%] [G loss: 3.144304]\n",
      "epoch:4 step:3167 [D loss: 0.227114, acc.: 91.41%] [G loss: 3.264018]\n",
      "epoch:4 step:3168 [D loss: 0.160584, acc.: 96.09%] [G loss: 3.270544]\n",
      "epoch:4 step:3169 [D loss: 0.309236, acc.: 89.84%] [G loss: 3.377898]\n",
      "epoch:4 step:3170 [D loss: 0.206384, acc.: 93.75%] [G loss: 2.858836]\n",
      "epoch:4 step:3171 [D loss: 0.329282, acc.: 85.94%] [G loss: 3.951930]\n",
      "epoch:4 step:3172 [D loss: 0.124226, acc.: 96.88%] [G loss: 4.455860]\n",
      "epoch:4 step:3173 [D loss: 0.906502, acc.: 47.66%] [G loss: 4.201478]\n",
      "epoch:4 step:3174 [D loss: 0.352347, acc.: 84.38%] [G loss: 3.016805]\n",
      "epoch:4 step:3175 [D loss: 0.412349, acc.: 78.91%] [G loss: 5.157544]\n",
      "epoch:4 step:3176 [D loss: 0.277152, acc.: 86.72%] [G loss: 4.489575]\n",
      "epoch:4 step:3177 [D loss: 0.246931, acc.: 92.97%] [G loss: 3.875223]\n",
      "epoch:4 step:3178 [D loss: 0.196531, acc.: 93.75%] [G loss: 4.764478]\n",
      "epoch:4 step:3179 [D loss: 0.140268, acc.: 96.09%] [G loss: 4.245891]\n",
      "epoch:4 step:3180 [D loss: 0.497237, acc.: 78.12%] [G loss: 5.229841]\n",
      "epoch:4 step:3181 [D loss: 0.154020, acc.: 93.75%] [G loss: 4.480625]\n",
      "epoch:4 step:3182 [D loss: 0.045731, acc.: 99.22%] [G loss: 3.743680]\n",
      "epoch:4 step:3183 [D loss: 0.134879, acc.: 96.09%] [G loss: 3.784178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3184 [D loss: 0.197832, acc.: 92.97%] [G loss: 3.233954]\n",
      "epoch:4 step:3185 [D loss: 0.277750, acc.: 86.72%] [G loss: 4.214773]\n",
      "epoch:4 step:3186 [D loss: 0.573911, acc.: 73.44%] [G loss: 5.445453]\n",
      "epoch:4 step:3187 [D loss: 0.325870, acc.: 82.81%] [G loss: 3.921197]\n",
      "epoch:4 step:3188 [D loss: 0.146514, acc.: 95.31%] [G loss: 3.303040]\n",
      "epoch:4 step:3189 [D loss: 0.244038, acc.: 94.53%] [G loss: 3.259671]\n",
      "epoch:4 step:3190 [D loss: 0.135627, acc.: 97.66%] [G loss: 4.365980]\n",
      "epoch:4 step:3191 [D loss: 0.149147, acc.: 95.31%] [G loss: 1.357599]\n",
      "epoch:4 step:3192 [D loss: 0.191138, acc.: 92.97%] [G loss: 3.252288]\n",
      "epoch:4 step:3193 [D loss: 0.179265, acc.: 91.41%] [G loss: 1.548903]\n",
      "epoch:4 step:3194 [D loss: 0.551068, acc.: 77.34%] [G loss: 7.141770]\n",
      "epoch:4 step:3195 [D loss: 1.674843, acc.: 50.00%] [G loss: 1.780103]\n",
      "epoch:4 step:3196 [D loss: 0.561227, acc.: 80.47%] [G loss: 4.085921]\n",
      "epoch:4 step:3197 [D loss: 0.123683, acc.: 97.66%] [G loss: 4.480761]\n",
      "epoch:4 step:3198 [D loss: 1.053463, acc.: 49.22%] [G loss: 4.931885]\n",
      "epoch:4 step:3199 [D loss: 0.221721, acc.: 91.41%] [G loss: 4.362617]\n",
      "epoch:4 step:3200 [D loss: 0.237604, acc.: 89.84%] [G loss: 2.489860]\n",
      "epoch:4 step:3201 [D loss: 0.529261, acc.: 76.56%] [G loss: 4.247112]\n",
      "epoch:4 step:3202 [D loss: 0.287676, acc.: 88.28%] [G loss: 4.270694]\n",
      "epoch:4 step:3203 [D loss: 0.385997, acc.: 82.03%] [G loss: 3.688844]\n",
      "epoch:4 step:3204 [D loss: 0.211349, acc.: 92.97%] [G loss: 3.209597]\n",
      "epoch:4 step:3205 [D loss: 0.286782, acc.: 88.28%] [G loss: 3.978806]\n",
      "epoch:4 step:3206 [D loss: 0.135297, acc.: 97.66%] [G loss: 4.184468]\n",
      "epoch:4 step:3207 [D loss: 0.220188, acc.: 89.06%] [G loss: 3.261626]\n",
      "epoch:4 step:3208 [D loss: 0.212137, acc.: 95.31%] [G loss: 2.670143]\n",
      "epoch:4 step:3209 [D loss: 0.186682, acc.: 96.09%] [G loss: 4.497174]\n",
      "epoch:4 step:3210 [D loss: 0.125622, acc.: 98.44%] [G loss: 4.260263]\n",
      "epoch:4 step:3211 [D loss: 0.134296, acc.: 96.88%] [G loss: 3.185930]\n",
      "epoch:4 step:3212 [D loss: 0.459989, acc.: 76.56%] [G loss: 4.418233]\n",
      "epoch:4 step:3213 [D loss: 0.418770, acc.: 84.38%] [G loss: 4.249245]\n",
      "epoch:4 step:3214 [D loss: 0.346077, acc.: 80.47%] [G loss: 3.650769]\n",
      "epoch:4 step:3215 [D loss: 0.324347, acc.: 87.50%] [G loss: 4.392275]\n",
      "epoch:4 step:3216 [D loss: 0.319093, acc.: 85.16%] [G loss: 3.845670]\n",
      "epoch:4 step:3217 [D loss: 0.704201, acc.: 61.72%] [G loss: 5.738859]\n",
      "epoch:4 step:3218 [D loss: 0.549653, acc.: 75.78%] [G loss: 3.649000]\n",
      "epoch:4 step:3219 [D loss: 0.246345, acc.: 91.41%] [G loss: 2.832196]\n",
      "epoch:4 step:3220 [D loss: 0.229010, acc.: 92.19%] [G loss: 3.791919]\n",
      "epoch:4 step:3221 [D loss: 0.110744, acc.: 97.66%] [G loss: 3.336261]\n",
      "epoch:4 step:3222 [D loss: 0.163744, acc.: 96.88%] [G loss: 4.035259]\n",
      "epoch:4 step:3223 [D loss: 0.245097, acc.: 90.62%] [G loss: 3.670849]\n",
      "epoch:4 step:3224 [D loss: 0.262541, acc.: 89.84%] [G loss: 3.397069]\n",
      "epoch:4 step:3225 [D loss: 0.183234, acc.: 96.88%] [G loss: 2.170700]\n",
      "epoch:4 step:3226 [D loss: 0.454388, acc.: 77.34%] [G loss: 4.953062]\n",
      "epoch:4 step:3227 [D loss: 0.563364, acc.: 71.88%] [G loss: 1.491777]\n",
      "epoch:4 step:3228 [D loss: 0.197873, acc.: 92.19%] [G loss: 3.422999]\n",
      "epoch:4 step:3229 [D loss: 0.190310, acc.: 96.88%] [G loss: 4.615382]\n",
      "epoch:4 step:3230 [D loss: 0.299293, acc.: 85.94%] [G loss: 2.517088]\n",
      "epoch:4 step:3231 [D loss: 0.233695, acc.: 88.28%] [G loss: 3.246417]\n",
      "epoch:4 step:3232 [D loss: 0.303478, acc.: 86.72%] [G loss: 4.044502]\n",
      "epoch:4 step:3233 [D loss: 0.172132, acc.: 94.53%] [G loss: 3.689037]\n",
      "epoch:4 step:3234 [D loss: 0.173818, acc.: 93.75%] [G loss: 4.614345]\n",
      "epoch:4 step:3235 [D loss: 0.415759, acc.: 79.69%] [G loss: 2.547804]\n",
      "epoch:4 step:3236 [D loss: 0.196788, acc.: 90.62%] [G loss: 5.488187]\n",
      "epoch:4 step:3237 [D loss: 0.183084, acc.: 94.53%] [G loss: 4.145724]\n",
      "epoch:4 step:3238 [D loss: 0.204522, acc.: 93.75%] [G loss: 4.736477]\n",
      "epoch:4 step:3239 [D loss: 0.371344, acc.: 87.50%] [G loss: 2.860143]\n",
      "epoch:4 step:3240 [D loss: 0.079050, acc.: 97.66%] [G loss: 2.392186]\n",
      "epoch:4 step:3241 [D loss: 0.614888, acc.: 68.75%] [G loss: 7.172418]\n",
      "epoch:4 step:3242 [D loss: 0.630306, acc.: 67.19%] [G loss: 4.041834]\n",
      "epoch:4 step:3243 [D loss: 0.163456, acc.: 93.75%] [G loss: 3.860889]\n",
      "epoch:4 step:3244 [D loss: 0.076422, acc.: 99.22%] [G loss: 3.208120]\n",
      "epoch:4 step:3245 [D loss: 0.057610, acc.: 99.22%] [G loss: 4.181684]\n",
      "epoch:4 step:3246 [D loss: 0.167168, acc.: 92.97%] [G loss: 3.225569]\n",
      "epoch:4 step:3247 [D loss: 0.170866, acc.: 96.88%] [G loss: 3.396496]\n",
      "epoch:4 step:3248 [D loss: 0.246558, acc.: 90.62%] [G loss: 3.702348]\n",
      "epoch:4 step:3249 [D loss: 0.185665, acc.: 95.31%] [G loss: 3.149899]\n",
      "epoch:4 step:3250 [D loss: 0.648196, acc.: 71.88%] [G loss: 4.798007]\n",
      "epoch:4 step:3251 [D loss: 0.148208, acc.: 95.31%] [G loss: 4.660174]\n",
      "epoch:4 step:3252 [D loss: 0.169916, acc.: 96.09%] [G loss: 4.005316]\n",
      "epoch:4 step:3253 [D loss: 0.165185, acc.: 94.53%] [G loss: 2.763572]\n",
      "epoch:4 step:3254 [D loss: 0.355156, acc.: 85.16%] [G loss: 4.470585]\n",
      "epoch:4 step:3255 [D loss: 0.051515, acc.: 100.00%] [G loss: 4.365134]\n",
      "epoch:4 step:3256 [D loss: 0.166182, acc.: 94.53%] [G loss: 2.792492]\n",
      "epoch:4 step:3257 [D loss: 0.135754, acc.: 96.88%] [G loss: 1.936125]\n",
      "epoch:4 step:3258 [D loss: 0.430109, acc.: 75.78%] [G loss: 5.943972]\n",
      "epoch:4 step:3259 [D loss: 0.654703, acc.: 62.50%] [G loss: 2.384511]\n",
      "epoch:4 step:3260 [D loss: 0.097462, acc.: 99.22%] [G loss: 2.553749]\n",
      "epoch:4 step:3261 [D loss: 0.115312, acc.: 99.22%] [G loss: 3.914144]\n",
      "epoch:4 step:3262 [D loss: 0.653875, acc.: 69.53%] [G loss: 2.771080]\n",
      "epoch:4 step:3263 [D loss: 0.055946, acc.: 100.00%] [G loss: 3.820174]\n",
      "epoch:4 step:3264 [D loss: 0.316897, acc.: 89.84%] [G loss: 2.324982]\n",
      "epoch:4 step:3265 [D loss: 0.232613, acc.: 91.41%] [G loss: 4.025934]\n",
      "epoch:4 step:3266 [D loss: 0.234767, acc.: 88.28%] [G loss: 3.073184]\n",
      "epoch:4 step:3267 [D loss: 0.300087, acc.: 85.94%] [G loss: 3.990730]\n",
      "epoch:4 step:3268 [D loss: 0.280667, acc.: 90.62%] [G loss: 3.169003]\n",
      "epoch:4 step:3269 [D loss: 0.101517, acc.: 99.22%] [G loss: 4.175775]\n",
      "epoch:4 step:3270 [D loss: 0.149631, acc.: 96.88%] [G loss: 3.955706]\n",
      "epoch:4 step:3271 [D loss: 0.285194, acc.: 89.84%] [G loss: 3.102470]\n",
      "epoch:4 step:3272 [D loss: 0.551474, acc.: 73.44%] [G loss: 7.688426]\n",
      "epoch:4 step:3273 [D loss: 1.270849, acc.: 56.25%] [G loss: 1.795564]\n",
      "epoch:4 step:3274 [D loss: 0.559958, acc.: 71.88%] [G loss: 4.446174]\n",
      "epoch:4 step:3275 [D loss: 0.217281, acc.: 90.62%] [G loss: 5.045511]\n",
      "epoch:4 step:3276 [D loss: 0.248826, acc.: 89.06%] [G loss: 3.330363]\n",
      "epoch:4 step:3277 [D loss: 0.458721, acc.: 79.69%] [G loss: 3.253519]\n",
      "epoch:4 step:3278 [D loss: 0.086515, acc.: 98.44%] [G loss: 4.527403]\n",
      "epoch:4 step:3279 [D loss: 0.624550, acc.: 66.41%] [G loss: 3.847646]\n",
      "epoch:4 step:3280 [D loss: 0.277404, acc.: 89.06%] [G loss: 4.176870]\n",
      "epoch:4 step:3281 [D loss: 0.205738, acc.: 94.53%] [G loss: 3.431322]\n",
      "epoch:4 step:3282 [D loss: 0.474441, acc.: 73.44%] [G loss: 2.302260]\n",
      "epoch:4 step:3283 [D loss: 0.364908, acc.: 82.81%] [G loss: 5.261821]\n",
      "epoch:4 step:3284 [D loss: 0.592483, acc.: 72.66%] [G loss: 2.558832]\n",
      "epoch:4 step:3285 [D loss: 0.145224, acc.: 99.22%] [G loss: 2.232594]\n",
      "epoch:4 step:3286 [D loss: 0.131762, acc.: 97.66%] [G loss: 2.886924]\n",
      "epoch:4 step:3287 [D loss: 0.125870, acc.: 97.66%] [G loss: 3.301450]\n",
      "epoch:4 step:3288 [D loss: 0.287518, acc.: 89.06%] [G loss: 3.352840]\n",
      "epoch:4 step:3289 [D loss: 0.344634, acc.: 86.72%] [G loss: 1.613625]\n",
      "epoch:4 step:3290 [D loss: 0.883225, acc.: 57.81%] [G loss: 6.078364]\n",
      "epoch:4 step:3291 [D loss: 0.809866, acc.: 62.50%] [G loss: 2.745462]\n",
      "epoch:4 step:3292 [D loss: 0.053933, acc.: 100.00%] [G loss: 1.472611]\n",
      "epoch:4 step:3293 [D loss: 0.223948, acc.: 90.62%] [G loss: 3.789517]\n",
      "epoch:4 step:3294 [D loss: 0.231428, acc.: 92.97%] [G loss: 1.983424]\n",
      "epoch:4 step:3295 [D loss: 0.909451, acc.: 47.66%] [G loss: 2.397888]\n",
      "epoch:4 step:3296 [D loss: 0.101451, acc.: 99.22%] [G loss: 4.279694]\n",
      "epoch:4 step:3297 [D loss: 0.429430, acc.: 81.25%] [G loss: 1.810184]\n",
      "epoch:4 step:3298 [D loss: 0.188267, acc.: 92.19%] [G loss: 3.336343]\n",
      "epoch:4 step:3299 [D loss: 0.162387, acc.: 93.75%] [G loss: 2.449093]\n",
      "epoch:4 step:3300 [D loss: 0.343966, acc.: 84.38%] [G loss: 1.073838]\n",
      "epoch:4 step:3301 [D loss: 0.428023, acc.: 79.69%] [G loss: 5.242107]\n",
      "epoch:4 step:3302 [D loss: 0.679086, acc.: 63.28%] [G loss: 2.436224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3303 [D loss: 0.085182, acc.: 99.22%] [G loss: 2.159744]\n",
      "epoch:4 step:3304 [D loss: 0.135318, acc.: 96.88%] [G loss: 2.707264]\n",
      "epoch:4 step:3305 [D loss: 0.095748, acc.: 98.44%] [G loss: 3.179467]\n",
      "epoch:4 step:3306 [D loss: 0.154184, acc.: 94.53%] [G loss: 2.915555]\n",
      "epoch:4 step:3307 [D loss: 0.221677, acc.: 93.75%] [G loss: 2.026466]\n",
      "epoch:4 step:3308 [D loss: 0.151642, acc.: 96.88%] [G loss: 1.973877]\n",
      "epoch:4 step:3309 [D loss: 0.450363, acc.: 81.25%] [G loss: 4.959982]\n",
      "epoch:4 step:3310 [D loss: 1.066113, acc.: 54.69%] [G loss: 0.730797]\n",
      "epoch:4 step:3311 [D loss: 0.561703, acc.: 75.00%] [G loss: 4.425796]\n",
      "epoch:4 step:3312 [D loss: 0.455467, acc.: 73.44%] [G loss: 2.680503]\n",
      "epoch:4 step:3313 [D loss: 0.098666, acc.: 97.66%] [G loss: 2.275481]\n",
      "epoch:4 step:3314 [D loss: 0.145975, acc.: 96.88%] [G loss: 3.112678]\n",
      "epoch:4 step:3315 [D loss: 0.060708, acc.: 100.00%] [G loss: 3.542627]\n",
      "epoch:4 step:3316 [D loss: 0.099518, acc.: 100.00%] [G loss: 2.820675]\n",
      "epoch:4 step:3317 [D loss: 0.392010, acc.: 84.38%] [G loss: 4.530735]\n",
      "epoch:4 step:3318 [D loss: 0.310262, acc.: 82.03%] [G loss: 3.102671]\n",
      "epoch:4 step:3319 [D loss: 0.204546, acc.: 93.75%] [G loss: 3.398284]\n",
      "epoch:4 step:3320 [D loss: 0.239643, acc.: 89.06%] [G loss: 3.092525]\n",
      "epoch:4 step:3321 [D loss: 0.150856, acc.: 95.31%] [G loss: 3.601874]\n",
      "epoch:4 step:3322 [D loss: 0.128554, acc.: 96.88%] [G loss: 3.554322]\n",
      "epoch:4 step:3323 [D loss: 0.295860, acc.: 92.97%] [G loss: 4.022743]\n",
      "epoch:4 step:3324 [D loss: 0.190993, acc.: 93.75%] [G loss: 3.232700]\n",
      "epoch:4 step:3325 [D loss: 0.179548, acc.: 93.75%] [G loss: 2.578015]\n",
      "epoch:4 step:3326 [D loss: 0.293239, acc.: 89.84%] [G loss: 4.112034]\n",
      "epoch:4 step:3327 [D loss: 0.139814, acc.: 96.09%] [G loss: 2.730740]\n",
      "epoch:4 step:3328 [D loss: 0.188092, acc.: 93.75%] [G loss: 3.431973]\n",
      "epoch:4 step:3329 [D loss: 0.293071, acc.: 89.84%] [G loss: 1.630119]\n",
      "epoch:4 step:3330 [D loss: 0.105479, acc.: 98.44%] [G loss: 2.904039]\n",
      "epoch:4 step:3331 [D loss: 0.171857, acc.: 94.53%] [G loss: 2.783859]\n",
      "epoch:4 step:3332 [D loss: 0.327729, acc.: 87.50%] [G loss: 3.205187]\n",
      "epoch:4 step:3333 [D loss: 0.412398, acc.: 80.47%] [G loss: 2.345226]\n",
      "epoch:4 step:3334 [D loss: 0.192330, acc.: 94.53%] [G loss: 2.861108]\n",
      "epoch:4 step:3335 [D loss: 0.042372, acc.: 99.22%] [G loss: 4.138546]\n",
      "epoch:4 step:3336 [D loss: 0.083433, acc.: 98.44%] [G loss: 1.293635]\n",
      "epoch:4 step:3337 [D loss: 0.086239, acc.: 97.66%] [G loss: 0.560952]\n",
      "epoch:4 step:3338 [D loss: 0.362230, acc.: 83.59%] [G loss: 2.693952]\n",
      "epoch:4 step:3339 [D loss: 0.160342, acc.: 94.53%] [G loss: 1.701091]\n",
      "epoch:4 step:3340 [D loss: 0.488156, acc.: 75.78%] [G loss: 3.803270]\n",
      "epoch:4 step:3341 [D loss: 0.126191, acc.: 95.31%] [G loss: 3.939652]\n",
      "epoch:4 step:3342 [D loss: 0.774301, acc.: 59.38%] [G loss: 1.322033]\n",
      "epoch:4 step:3343 [D loss: 0.047603, acc.: 99.22%] [G loss: 3.253805]\n",
      "epoch:4 step:3344 [D loss: 0.211022, acc.: 93.75%] [G loss: 2.296602]\n",
      "epoch:4 step:3345 [D loss: 0.066772, acc.: 97.66%] [G loss: 2.079020]\n",
      "epoch:4 step:3346 [D loss: 0.817743, acc.: 69.53%] [G loss: 7.055288]\n",
      "epoch:4 step:3347 [D loss: 1.080471, acc.: 57.81%] [G loss: 3.800668]\n",
      "epoch:4 step:3348 [D loss: 0.131181, acc.: 96.88%] [G loss: 2.529504]\n",
      "epoch:4 step:3349 [D loss: 0.223861, acc.: 92.97%] [G loss: 4.321103]\n",
      "epoch:4 step:3350 [D loss: 0.256611, acc.: 87.50%] [G loss: 2.668277]\n",
      "epoch:4 step:3351 [D loss: 0.450227, acc.: 78.91%] [G loss: 4.358080]\n",
      "epoch:4 step:3352 [D loss: 0.152414, acc.: 96.88%] [G loss: 3.786116]\n",
      "epoch:4 step:3353 [D loss: 0.239043, acc.: 92.19%] [G loss: 1.366923]\n",
      "epoch:4 step:3354 [D loss: 0.127445, acc.: 96.09%] [G loss: 2.188741]\n",
      "epoch:4 step:3355 [D loss: 0.029515, acc.: 99.22%] [G loss: 2.284404]\n",
      "epoch:4 step:3356 [D loss: 0.499454, acc.: 74.22%] [G loss: 2.198806]\n",
      "epoch:4 step:3357 [D loss: 0.087906, acc.: 97.66%] [G loss: 2.474799]\n",
      "epoch:4 step:3358 [D loss: 0.152049, acc.: 98.44%] [G loss: 1.241987]\n",
      "epoch:4 step:3359 [D loss: 0.220466, acc.: 92.97%] [G loss: 2.908484]\n",
      "epoch:4 step:3360 [D loss: 0.248186, acc.: 91.41%] [G loss: 2.759870]\n",
      "epoch:4 step:3361 [D loss: 0.164898, acc.: 96.09%] [G loss: 1.375858]\n",
      "epoch:4 step:3362 [D loss: 0.111735, acc.: 96.88%] [G loss: 1.411936]\n",
      "epoch:4 step:3363 [D loss: 0.075579, acc.: 99.22%] [G loss: 1.635175]\n",
      "epoch:4 step:3364 [D loss: 0.084362, acc.: 100.00%] [G loss: 2.064067]\n",
      "epoch:4 step:3365 [D loss: 0.342466, acc.: 85.16%] [G loss: 3.262877]\n",
      "epoch:4 step:3366 [D loss: 0.540487, acc.: 75.00%] [G loss: 1.530536]\n",
      "epoch:4 step:3367 [D loss: 0.061567, acc.: 99.22%] [G loss: 1.192351]\n",
      "epoch:4 step:3368 [D loss: 0.068110, acc.: 99.22%] [G loss: 2.046904]\n",
      "epoch:4 step:3369 [D loss: 0.108959, acc.: 98.44%] [G loss: 0.842576]\n",
      "epoch:4 step:3370 [D loss: 0.231959, acc.: 89.84%] [G loss: 2.920177]\n",
      "epoch:4 step:3371 [D loss: 0.359403, acc.: 82.03%] [G loss: 0.677880]\n",
      "epoch:4 step:3372 [D loss: 0.591610, acc.: 71.09%] [G loss: 5.732607]\n",
      "epoch:4 step:3373 [D loss: 0.375494, acc.: 79.69%] [G loss: 3.193453]\n",
      "epoch:4 step:3374 [D loss: 0.227509, acc.: 92.19%] [G loss: 3.053383]\n",
      "epoch:4 step:3375 [D loss: 0.153558, acc.: 95.31%] [G loss: 3.008477]\n",
      "epoch:4 step:3376 [D loss: 0.246842, acc.: 88.28%] [G loss: 5.798254]\n",
      "epoch:4 step:3377 [D loss: 0.289745, acc.: 85.94%] [G loss: 4.062770]\n",
      "epoch:4 step:3378 [D loss: 0.076515, acc.: 100.00%] [G loss: 3.180476]\n",
      "epoch:4 step:3379 [D loss: 0.043412, acc.: 99.22%] [G loss: 3.345439]\n",
      "epoch:4 step:3380 [D loss: 0.115843, acc.: 97.66%] [G loss: 3.386466]\n",
      "epoch:4 step:3381 [D loss: 0.065536, acc.: 100.00%] [G loss: 3.915183]\n",
      "epoch:4 step:3382 [D loss: 0.073499, acc.: 98.44%] [G loss: 3.041304]\n",
      "epoch:4 step:3383 [D loss: 0.058574, acc.: 99.22%] [G loss: 3.189816]\n",
      "epoch:4 step:3384 [D loss: 0.704130, acc.: 64.84%] [G loss: 5.331393]\n",
      "epoch:4 step:3385 [D loss: 0.870178, acc.: 60.94%] [G loss: 2.791639]\n",
      "epoch:4 step:3386 [D loss: 0.049799, acc.: 100.00%] [G loss: 2.328447]\n",
      "epoch:4 step:3387 [D loss: 0.085296, acc.: 99.22%] [G loss: 2.808362]\n",
      "epoch:4 step:3388 [D loss: 0.070603, acc.: 98.44%] [G loss: 3.000304]\n",
      "epoch:4 step:3389 [D loss: 0.042831, acc.: 100.00%] [G loss: 2.027322]\n",
      "epoch:4 step:3390 [D loss: 0.120476, acc.: 97.66%] [G loss: 2.090387]\n",
      "epoch:4 step:3391 [D loss: 0.348261, acc.: 85.94%] [G loss: 3.093309]\n",
      "epoch:4 step:3392 [D loss: 0.125547, acc.: 95.31%] [G loss: 3.121946]\n",
      "epoch:4 step:3393 [D loss: 0.158985, acc.: 96.09%] [G loss: 1.891009]\n",
      "epoch:4 step:3394 [D loss: 0.142309, acc.: 96.09%] [G loss: 1.810034]\n",
      "epoch:4 step:3395 [D loss: 0.062317, acc.: 100.00%] [G loss: 0.592884]\n",
      "epoch:4 step:3396 [D loss: 0.072126, acc.: 99.22%] [G loss: 0.700275]\n",
      "epoch:4 step:3397 [D loss: 0.156208, acc.: 94.53%] [G loss: 0.720851]\n",
      "epoch:4 step:3398 [D loss: 0.067920, acc.: 99.22%] [G loss: 0.680257]\n",
      "epoch:4 step:3399 [D loss: 2.127807, acc.: 23.44%] [G loss: 6.290818]\n",
      "epoch:4 step:3400 [D loss: 0.475291, acc.: 81.25%] [G loss: 5.704849]\n",
      "epoch:4 step:3401 [D loss: 1.071941, acc.: 57.03%] [G loss: 1.013234]\n",
      "epoch:4 step:3402 [D loss: 0.315638, acc.: 84.38%] [G loss: 2.343857]\n",
      "epoch:4 step:3403 [D loss: 0.296328, acc.: 84.38%] [G loss: 5.405416]\n",
      "epoch:4 step:3404 [D loss: 0.173415, acc.: 92.19%] [G loss: 4.796100]\n",
      "epoch:4 step:3405 [D loss: 0.241401, acc.: 89.84%] [G loss: 2.589165]\n",
      "epoch:4 step:3406 [D loss: 0.326013, acc.: 86.72%] [G loss: 4.165071]\n",
      "epoch:4 step:3407 [D loss: 0.119330, acc.: 96.09%] [G loss: 4.118717]\n",
      "epoch:4 step:3408 [D loss: 0.322912, acc.: 85.16%] [G loss: 4.100140]\n",
      "epoch:4 step:3409 [D loss: 0.210772, acc.: 95.31%] [G loss: 4.139687]\n",
      "epoch:4 step:3410 [D loss: 0.777743, acc.: 61.72%] [G loss: 4.151755]\n",
      "epoch:4 step:3411 [D loss: 0.309015, acc.: 89.06%] [G loss: 3.872787]\n",
      "epoch:4 step:3412 [D loss: 0.230827, acc.: 90.62%] [G loss: 3.459500]\n",
      "epoch:4 step:3413 [D loss: 0.219263, acc.: 92.19%] [G loss: 3.142455]\n",
      "epoch:4 step:3414 [D loss: 0.170770, acc.: 96.88%] [G loss: 3.288078]\n",
      "epoch:4 step:3415 [D loss: 0.408213, acc.: 82.03%] [G loss: 2.942667]\n",
      "epoch:4 step:3416 [D loss: 0.289467, acc.: 86.72%] [G loss: 4.417692]\n",
      "epoch:4 step:3417 [D loss: 0.371755, acc.: 80.47%] [G loss: 3.070431]\n",
      "epoch:4 step:3418 [D loss: 0.194687, acc.: 96.09%] [G loss: 2.513266]\n",
      "epoch:4 step:3419 [D loss: 0.380239, acc.: 83.59%] [G loss: 5.250892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3420 [D loss: 0.486612, acc.: 76.56%] [G loss: 3.298231]\n",
      "epoch:4 step:3421 [D loss: 0.362956, acc.: 84.38%] [G loss: 2.942718]\n",
      "epoch:4 step:3422 [D loss: 0.124383, acc.: 96.09%] [G loss: 2.760605]\n",
      "epoch:4 step:3423 [D loss: 0.205966, acc.: 94.53%] [G loss: 3.289062]\n",
      "epoch:4 step:3424 [D loss: 0.185428, acc.: 96.09%] [G loss: 4.327501]\n",
      "epoch:4 step:3425 [D loss: 0.272998, acc.: 91.41%] [G loss: 2.463184]\n",
      "epoch:4 step:3426 [D loss: 0.258804, acc.: 89.84%] [G loss: 3.126827]\n",
      "epoch:4 step:3427 [D loss: 0.123876, acc.: 96.09%] [G loss: 2.119897]\n",
      "epoch:4 step:3428 [D loss: 0.148670, acc.: 96.88%] [G loss: 1.950101]\n",
      "epoch:4 step:3429 [D loss: 0.507823, acc.: 76.56%] [G loss: 4.609425]\n",
      "epoch:4 step:3430 [D loss: 0.379971, acc.: 82.81%] [G loss: 2.657624]\n",
      "epoch:4 step:3431 [D loss: 0.543724, acc.: 71.88%] [G loss: 4.118869]\n",
      "epoch:4 step:3432 [D loss: 0.400253, acc.: 81.25%] [G loss: 2.894616]\n",
      "epoch:4 step:3433 [D loss: 0.180560, acc.: 95.31%] [G loss: 2.402496]\n",
      "epoch:4 step:3434 [D loss: 0.210133, acc.: 94.53%] [G loss: 3.014394]\n",
      "epoch:4 step:3435 [D loss: 0.119203, acc.: 96.09%] [G loss: 3.280157]\n",
      "epoch:4 step:3436 [D loss: 0.335728, acc.: 85.16%] [G loss: 3.957947]\n",
      "epoch:4 step:3437 [D loss: 0.499753, acc.: 74.22%] [G loss: 4.347376]\n",
      "epoch:4 step:3438 [D loss: 0.367660, acc.: 82.81%] [G loss: 4.395288]\n",
      "epoch:4 step:3439 [D loss: 0.224380, acc.: 89.84%] [G loss: 2.994166]\n",
      "epoch:4 step:3440 [D loss: 0.339837, acc.: 85.16%] [G loss: 5.028437]\n",
      "epoch:4 step:3441 [D loss: 0.476140, acc.: 76.56%] [G loss: 2.987032]\n",
      "epoch:4 step:3442 [D loss: 0.114567, acc.: 98.44%] [G loss: 3.092438]\n",
      "epoch:4 step:3443 [D loss: 0.145334, acc.: 96.88%] [G loss: 3.585494]\n",
      "epoch:4 step:3444 [D loss: 0.372464, acc.: 85.16%] [G loss: 3.366438]\n",
      "epoch:4 step:3445 [D loss: 0.139950, acc.: 97.66%] [G loss: 3.041720]\n",
      "epoch:4 step:3446 [D loss: 0.075753, acc.: 99.22%] [G loss: 1.526725]\n",
      "epoch:4 step:3447 [D loss: 0.251716, acc.: 89.06%] [G loss: 3.643602]\n",
      "epoch:4 step:3448 [D loss: 0.134406, acc.: 93.75%] [G loss: 2.404462]\n",
      "epoch:4 step:3449 [D loss: 0.208082, acc.: 92.19%] [G loss: 1.675699]\n",
      "epoch:4 step:3450 [D loss: 0.114218, acc.: 96.88%] [G loss: 1.217378]\n",
      "epoch:4 step:3451 [D loss: 0.039672, acc.: 100.00%] [G loss: 1.895865]\n",
      "epoch:4 step:3452 [D loss: 0.437287, acc.: 82.03%] [G loss: 5.229239]\n",
      "epoch:4 step:3453 [D loss: 0.387573, acc.: 82.81%] [G loss: 1.468161]\n",
      "epoch:4 step:3454 [D loss: 0.044678, acc.: 99.22%] [G loss: 1.290338]\n",
      "epoch:4 step:3455 [D loss: 0.569229, acc.: 68.75%] [G loss: 4.846137]\n",
      "epoch:4 step:3456 [D loss: 0.356144, acc.: 77.34%] [G loss: 3.939099]\n",
      "epoch:4 step:3457 [D loss: 0.125993, acc.: 96.88%] [G loss: 1.772016]\n",
      "epoch:4 step:3458 [D loss: 0.278136, acc.: 89.06%] [G loss: 3.473241]\n",
      "epoch:4 step:3459 [D loss: 0.206347, acc.: 91.41%] [G loss: 2.877429]\n",
      "epoch:4 step:3460 [D loss: 0.812890, acc.: 59.38%] [G loss: 6.381139]\n",
      "epoch:4 step:3461 [D loss: 0.398545, acc.: 75.00%] [G loss: 4.487121]\n",
      "epoch:4 step:3462 [D loss: 0.156711, acc.: 93.75%] [G loss: 2.375686]\n",
      "epoch:4 step:3463 [D loss: 0.351745, acc.: 82.81%] [G loss: 4.011144]\n",
      "epoch:4 step:3464 [D loss: 0.160740, acc.: 92.97%] [G loss: 4.021242]\n",
      "epoch:4 step:3465 [D loss: 0.121909, acc.: 96.88%] [G loss: 3.107355]\n",
      "epoch:4 step:3466 [D loss: 0.133075, acc.: 94.53%] [G loss: 3.285567]\n",
      "epoch:4 step:3467 [D loss: 0.526091, acc.: 74.22%] [G loss: 6.000538]\n",
      "epoch:4 step:3468 [D loss: 0.391817, acc.: 79.69%] [G loss: 4.925563]\n",
      "epoch:4 step:3469 [D loss: 0.126178, acc.: 95.31%] [G loss: 2.854040]\n",
      "epoch:4 step:3470 [D loss: 0.082148, acc.: 98.44%] [G loss: 3.492200]\n",
      "epoch:4 step:3471 [D loss: 0.038960, acc.: 100.00%] [G loss: 3.941897]\n",
      "epoch:4 step:3472 [D loss: 0.095447, acc.: 97.66%] [G loss: 1.364737]\n",
      "epoch:4 step:3473 [D loss: 0.140880, acc.: 94.53%] [G loss: 2.504406]\n",
      "epoch:4 step:3474 [D loss: 0.369400, acc.: 82.03%] [G loss: 1.429917]\n",
      "epoch:4 step:3475 [D loss: 0.361328, acc.: 84.38%] [G loss: 4.715384]\n",
      "epoch:4 step:3476 [D loss: 0.427128, acc.: 79.69%] [G loss: 1.598647]\n",
      "epoch:4 step:3477 [D loss: 0.123904, acc.: 96.09%] [G loss: 1.462329]\n",
      "epoch:4 step:3478 [D loss: 0.109677, acc.: 96.88%] [G loss: 2.179379]\n",
      "epoch:4 step:3479 [D loss: 0.083116, acc.: 98.44%] [G loss: 2.812797]\n",
      "epoch:4 step:3480 [D loss: 0.244104, acc.: 90.62%] [G loss: 1.622280]\n",
      "epoch:4 step:3481 [D loss: 0.182860, acc.: 94.53%] [G loss: 1.396985]\n",
      "epoch:4 step:3482 [D loss: 0.142627, acc.: 96.88%] [G loss: 2.271680]\n",
      "epoch:4 step:3483 [D loss: 0.484386, acc.: 75.78%] [G loss: 3.444023]\n",
      "epoch:4 step:3484 [D loss: 0.237151, acc.: 91.41%] [G loss: 2.202571]\n",
      "epoch:4 step:3485 [D loss: 0.103623, acc.: 96.09%] [G loss: 3.611657]\n",
      "epoch:4 step:3486 [D loss: 0.284002, acc.: 89.06%] [G loss: 5.125142]\n",
      "epoch:4 step:3487 [D loss: 0.157641, acc.: 96.09%] [G loss: 4.028015]\n",
      "epoch:4 step:3488 [D loss: 0.258542, acc.: 90.62%] [G loss: 3.763795]\n",
      "epoch:4 step:3489 [D loss: 0.065232, acc.: 97.66%] [G loss: 4.627647]\n",
      "epoch:4 step:3490 [D loss: 0.108615, acc.: 97.66%] [G loss: 1.648203]\n",
      "epoch:4 step:3491 [D loss: 0.098544, acc.: 97.66%] [G loss: 1.338235]\n",
      "epoch:4 step:3492 [D loss: 0.321035, acc.: 84.38%] [G loss: 4.892488]\n",
      "epoch:4 step:3493 [D loss: 0.288510, acc.: 84.38%] [G loss: 2.774636]\n",
      "epoch:4 step:3494 [D loss: 0.054838, acc.: 100.00%] [G loss: 2.177586]\n",
      "epoch:4 step:3495 [D loss: 0.061804, acc.: 99.22%] [G loss: 1.949523]\n",
      "epoch:4 step:3496 [D loss: 0.027839, acc.: 100.00%] [G loss: 2.276609]\n",
      "epoch:4 step:3497 [D loss: 0.070041, acc.: 98.44%] [G loss: 1.705120]\n",
      "epoch:4 step:3498 [D loss: 0.364202, acc.: 82.03%] [G loss: 6.479961]\n",
      "epoch:4 step:3499 [D loss: 0.915854, acc.: 61.72%] [G loss: 1.394123]\n",
      "epoch:4 step:3500 [D loss: 0.676417, acc.: 69.53%] [G loss: 7.166786]\n",
      "epoch:4 step:3501 [D loss: 0.198167, acc.: 89.84%] [G loss: 6.311454]\n",
      "epoch:4 step:3502 [D loss: 0.256310, acc.: 88.28%] [G loss: 3.707075]\n",
      "epoch:4 step:3503 [D loss: 0.476220, acc.: 76.56%] [G loss: 7.307993]\n",
      "epoch:4 step:3504 [D loss: 0.568533, acc.: 72.66%] [G loss: 4.673239]\n",
      "epoch:4 step:3505 [D loss: 0.077497, acc.: 99.22%] [G loss: 3.236132]\n",
      "epoch:4 step:3506 [D loss: 0.118583, acc.: 96.88%] [G loss: 2.870503]\n",
      "epoch:4 step:3507 [D loss: 0.065436, acc.: 99.22%] [G loss: 3.474500]\n",
      "epoch:4 step:3508 [D loss: 0.090571, acc.: 98.44%] [G loss: 1.730554]\n",
      "epoch:4 step:3509 [D loss: 0.428412, acc.: 78.91%] [G loss: 4.361063]\n",
      "epoch:4 step:3510 [D loss: 0.772233, acc.: 63.28%] [G loss: 1.403232]\n",
      "epoch:4 step:3511 [D loss: 0.792524, acc.: 61.72%] [G loss: 5.950400]\n",
      "epoch:4 step:3512 [D loss: 0.740700, acc.: 65.62%] [G loss: 3.669264]\n",
      "epoch:4 step:3513 [D loss: 0.208600, acc.: 92.97%] [G loss: 2.781379]\n",
      "epoch:4 step:3514 [D loss: 0.154193, acc.: 95.31%] [G loss: 2.106122]\n",
      "epoch:4 step:3515 [D loss: 0.379695, acc.: 84.38%] [G loss: 3.799685]\n",
      "epoch:4 step:3516 [D loss: 0.235672, acc.: 94.53%] [G loss: 3.504796]\n",
      "epoch:4 step:3517 [D loss: 0.192246, acc.: 96.09%] [G loss: 3.177520]\n",
      "epoch:4 step:3518 [D loss: 0.349359, acc.: 86.72%] [G loss: 4.596787]\n",
      "epoch:4 step:3519 [D loss: 0.595646, acc.: 74.22%] [G loss: 2.418800]\n",
      "epoch:4 step:3520 [D loss: 0.126632, acc.: 97.66%] [G loss: 2.269088]\n",
      "epoch:4 step:3521 [D loss: 0.219964, acc.: 91.41%] [G loss: 4.054440]\n",
      "epoch:4 step:3522 [D loss: 0.170616, acc.: 93.75%] [G loss: 4.150031]\n",
      "epoch:4 step:3523 [D loss: 0.095276, acc.: 99.22%] [G loss: 3.575619]\n",
      "epoch:4 step:3524 [D loss: 0.129737, acc.: 99.22%] [G loss: 3.096798]\n",
      "epoch:4 step:3525 [D loss: 0.242243, acc.: 89.84%] [G loss: 3.357318]\n",
      "epoch:4 step:3526 [D loss: 0.338105, acc.: 87.50%] [G loss: 3.156949]\n",
      "epoch:4 step:3527 [D loss: 0.267060, acc.: 89.84%] [G loss: 4.712174]\n",
      "epoch:4 step:3528 [D loss: 0.400974, acc.: 80.47%] [G loss: 3.822771]\n",
      "epoch:4 step:3529 [D loss: 0.454501, acc.: 78.91%] [G loss: 4.436324]\n",
      "epoch:4 step:3530 [D loss: 0.094276, acc.: 98.44%] [G loss: 3.834954]\n",
      "epoch:4 step:3531 [D loss: 0.157307, acc.: 96.88%] [G loss: 3.455501]\n",
      "epoch:4 step:3532 [D loss: 0.293088, acc.: 88.28%] [G loss: 4.956451]\n",
      "epoch:4 step:3533 [D loss: 0.435156, acc.: 78.91%] [G loss: 2.840052]\n",
      "epoch:4 step:3534 [D loss: 0.377972, acc.: 82.03%] [G loss: 5.442748]\n",
      "epoch:4 step:3535 [D loss: 0.285641, acc.: 85.94%] [G loss: 4.476532]\n",
      "epoch:4 step:3536 [D loss: 0.081916, acc.: 96.88%] [G loss: 4.425247]\n",
      "epoch:4 step:3537 [D loss: 0.052984, acc.: 100.00%] [G loss: 2.900682]\n",
      "epoch:4 step:3538 [D loss: 0.118021, acc.: 96.88%] [G loss: 2.965315]\n",
      "epoch:4 step:3539 [D loss: 0.064342, acc.: 100.00%] [G loss: 2.732956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3540 [D loss: 0.472678, acc.: 82.03%] [G loss: 2.380282]\n",
      "epoch:4 step:3541 [D loss: 0.231862, acc.: 92.19%] [G loss: 2.965014]\n",
      "epoch:4 step:3542 [D loss: 0.175273, acc.: 96.88%] [G loss: 2.301289]\n",
      "epoch:4 step:3543 [D loss: 0.093909, acc.: 100.00%] [G loss: 3.052318]\n",
      "epoch:4 step:3544 [D loss: 0.149601, acc.: 95.31%] [G loss: 1.597909]\n",
      "epoch:4 step:3545 [D loss: 0.281864, acc.: 89.06%] [G loss: 2.175076]\n",
      "epoch:4 step:3546 [D loss: 0.125671, acc.: 96.88%] [G loss: 2.124462]\n",
      "epoch:4 step:3547 [D loss: 0.098072, acc.: 98.44%] [G loss: 1.467566]\n",
      "epoch:4 step:3548 [D loss: 0.718362, acc.: 67.19%] [G loss: 8.646974]\n",
      "epoch:4 step:3549 [D loss: 1.533946, acc.: 50.78%] [G loss: 3.011897]\n",
      "epoch:4 step:3550 [D loss: 0.281803, acc.: 86.72%] [G loss: 3.468626]\n",
      "epoch:4 step:3551 [D loss: 0.025735, acc.: 100.00%] [G loss: 5.051431]\n",
      "epoch:4 step:3552 [D loss: 0.142244, acc.: 94.53%] [G loss: 2.812491]\n",
      "epoch:4 step:3553 [D loss: 0.122034, acc.: 96.88%] [G loss: 2.183744]\n",
      "epoch:4 step:3554 [D loss: 0.189540, acc.: 93.75%] [G loss: 3.944546]\n",
      "epoch:4 step:3555 [D loss: 0.094564, acc.: 96.09%] [G loss: 4.398978]\n",
      "epoch:4 step:3556 [D loss: 0.252764, acc.: 90.62%] [G loss: 2.290217]\n",
      "epoch:4 step:3557 [D loss: 0.132151, acc.: 99.22%] [G loss: 2.553516]\n",
      "epoch:4 step:3558 [D loss: 0.476280, acc.: 72.66%] [G loss: 3.678749]\n",
      "epoch:4 step:3559 [D loss: 0.091413, acc.: 97.66%] [G loss: 5.316023]\n",
      "epoch:4 step:3560 [D loss: 0.215439, acc.: 94.53%] [G loss: 3.686823]\n",
      "epoch:4 step:3561 [D loss: 0.106268, acc.: 98.44%] [G loss: 2.325913]\n",
      "epoch:4 step:3562 [D loss: 0.328989, acc.: 83.59%] [G loss: 5.199121]\n",
      "epoch:4 step:3563 [D loss: 0.162005, acc.: 92.97%] [G loss: 3.966660]\n",
      "epoch:4 step:3564 [D loss: 0.083361, acc.: 99.22%] [G loss: 3.308747]\n",
      "epoch:4 step:3565 [D loss: 0.112426, acc.: 96.88%] [G loss: 1.156276]\n",
      "epoch:4 step:3566 [D loss: 0.043107, acc.: 100.00%] [G loss: 1.283201]\n",
      "epoch:4 step:3567 [D loss: 0.341315, acc.: 82.81%] [G loss: 4.265094]\n",
      "epoch:4 step:3568 [D loss: 0.305330, acc.: 82.81%] [G loss: 3.587576]\n",
      "epoch:4 step:3569 [D loss: 0.072559, acc.: 98.44%] [G loss: 2.130416]\n",
      "epoch:4 step:3570 [D loss: 0.145810, acc.: 95.31%] [G loss: 2.429854]\n",
      "epoch:4 step:3571 [D loss: 0.074228, acc.: 99.22%] [G loss: 1.992112]\n",
      "epoch:4 step:3572 [D loss: 0.410825, acc.: 78.91%] [G loss: 4.692478]\n",
      "epoch:4 step:3573 [D loss: 0.067892, acc.: 100.00%] [G loss: 3.788396]\n",
      "epoch:4 step:3574 [D loss: 0.594616, acc.: 68.75%] [G loss: 6.039895]\n",
      "epoch:4 step:3575 [D loss: 0.239074, acc.: 84.38%] [G loss: 3.617423]\n",
      "epoch:4 step:3576 [D loss: 0.136950, acc.: 95.31%] [G loss: 3.738233]\n",
      "epoch:4 step:3577 [D loss: 0.140259, acc.: 96.09%] [G loss: 3.009272]\n",
      "epoch:4 step:3578 [D loss: 0.281553, acc.: 86.72%] [G loss: 6.444118]\n",
      "epoch:4 step:3579 [D loss: 0.327661, acc.: 79.69%] [G loss: 3.752387]\n",
      "epoch:4 step:3580 [D loss: 0.274422, acc.: 89.06%] [G loss: 5.935228]\n",
      "epoch:4 step:3581 [D loss: 0.036195, acc.: 99.22%] [G loss: 6.753203]\n",
      "epoch:4 step:3582 [D loss: 0.206935, acc.: 92.97%] [G loss: 4.323685]\n",
      "epoch:4 step:3583 [D loss: 0.238926, acc.: 89.06%] [G loss: 5.507659]\n",
      "epoch:4 step:3584 [D loss: 0.472411, acc.: 77.34%] [G loss: 2.377095]\n",
      "epoch:4 step:3585 [D loss: 0.099537, acc.: 98.44%] [G loss: 3.925842]\n",
      "epoch:4 step:3586 [D loss: 0.172984, acc.: 93.75%] [G loss: 4.620004]\n",
      "epoch:4 step:3587 [D loss: 0.123485, acc.: 95.31%] [G loss: 2.811061]\n",
      "epoch:4 step:3588 [D loss: 0.188083, acc.: 94.53%] [G loss: 3.357471]\n",
      "epoch:4 step:3589 [D loss: 0.441263, acc.: 81.25%] [G loss: 3.781834]\n",
      "epoch:4 step:3590 [D loss: 0.103219, acc.: 96.09%] [G loss: 3.827952]\n",
      "epoch:4 step:3591 [D loss: 0.066064, acc.: 97.66%] [G loss: 1.837100]\n",
      "epoch:4 step:3592 [D loss: 0.640014, acc.: 67.97%] [G loss: 6.432035]\n",
      "epoch:4 step:3593 [D loss: 0.355283, acc.: 81.25%] [G loss: 3.048958]\n",
      "epoch:4 step:3594 [D loss: 0.090369, acc.: 98.44%] [G loss: 1.582459]\n",
      "epoch:4 step:3595 [D loss: 0.347089, acc.: 82.81%] [G loss: 5.087010]\n",
      "epoch:4 step:3596 [D loss: 0.196259, acc.: 91.41%] [G loss: 3.870743]\n",
      "epoch:4 step:3597 [D loss: 0.075729, acc.: 98.44%] [G loss: 1.623510]\n",
      "epoch:4 step:3598 [D loss: 0.222342, acc.: 91.41%] [G loss: 1.794606]\n",
      "epoch:4 step:3599 [D loss: 0.140982, acc.: 96.09%] [G loss: 3.187220]\n",
      "epoch:4 step:3600 [D loss: 0.439528, acc.: 77.34%] [G loss: 4.836701]\n",
      "epoch:4 step:3601 [D loss: 0.395266, acc.: 80.47%] [G loss: 4.491642]\n",
      "epoch:4 step:3602 [D loss: 0.093557, acc.: 97.66%] [G loss: 4.536663]\n",
      "epoch:4 step:3603 [D loss: 0.112438, acc.: 96.88%] [G loss: 4.615954]\n",
      "epoch:4 step:3604 [D loss: 0.300556, acc.: 86.72%] [G loss: 6.498537]\n",
      "epoch:4 step:3605 [D loss: 0.482490, acc.: 79.69%] [G loss: 3.511828]\n",
      "epoch:4 step:3606 [D loss: 0.652908, acc.: 65.62%] [G loss: 7.995432]\n",
      "epoch:4 step:3607 [D loss: 0.343258, acc.: 85.16%] [G loss: 5.972335]\n",
      "epoch:4 step:3608 [D loss: 0.125554, acc.: 96.09%] [G loss: 4.284161]\n",
      "epoch:4 step:3609 [D loss: 0.077790, acc.: 98.44%] [G loss: 5.071707]\n",
      "epoch:4 step:3610 [D loss: 0.100630, acc.: 97.66%] [G loss: 1.822983]\n",
      "epoch:4 step:3611 [D loss: 0.195770, acc.: 92.19%] [G loss: 3.886718]\n",
      "epoch:4 step:3612 [D loss: 0.278767, acc.: 87.50%] [G loss: 2.787797]\n",
      "epoch:4 step:3613 [D loss: 0.238546, acc.: 89.06%] [G loss: 1.883597]\n",
      "epoch:4 step:3614 [D loss: 0.155008, acc.: 96.09%] [G loss: 2.193321]\n",
      "epoch:4 step:3615 [D loss: 0.462183, acc.: 78.12%] [G loss: 4.692755]\n",
      "epoch:4 step:3616 [D loss: 0.112427, acc.: 95.31%] [G loss: 4.506220]\n",
      "epoch:4 step:3617 [D loss: 0.059782, acc.: 100.00%] [G loss: 3.299020]\n",
      "epoch:4 step:3618 [D loss: 0.085229, acc.: 97.66%] [G loss: 4.161791]\n",
      "epoch:4 step:3619 [D loss: 0.209014, acc.: 92.19%] [G loss: 6.078315]\n",
      "epoch:4 step:3620 [D loss: 1.053609, acc.: 53.12%] [G loss: 4.318731]\n",
      "epoch:4 step:3621 [D loss: 0.087975, acc.: 97.66%] [G loss: 4.899881]\n",
      "epoch:4 step:3622 [D loss: 0.162672, acc.: 92.97%] [G loss: 4.216306]\n",
      "epoch:4 step:3623 [D loss: 0.321679, acc.: 85.94%] [G loss: 5.826674]\n",
      "epoch:4 step:3624 [D loss: 0.224297, acc.: 88.28%] [G loss: 4.172460]\n",
      "epoch:4 step:3625 [D loss: 0.424011, acc.: 78.91%] [G loss: 6.884578]\n",
      "epoch:4 step:3626 [D loss: 0.441472, acc.: 74.22%] [G loss: 4.387537]\n",
      "epoch:4 step:3627 [D loss: 0.218429, acc.: 90.62%] [G loss: 4.655149]\n",
      "epoch:4 step:3628 [D loss: 0.120850, acc.: 95.31%] [G loss: 4.056150]\n",
      "epoch:4 step:3629 [D loss: 0.176676, acc.: 92.97%] [G loss: 5.521459]\n",
      "epoch:4 step:3630 [D loss: 0.272671, acc.: 89.06%] [G loss: 2.842902]\n",
      "epoch:4 step:3631 [D loss: 0.240465, acc.: 89.06%] [G loss: 4.379967]\n",
      "epoch:4 step:3632 [D loss: 0.132466, acc.: 98.44%] [G loss: 3.804097]\n",
      "epoch:4 step:3633 [D loss: 0.121264, acc.: 98.44%] [G loss: 2.779868]\n",
      "epoch:4 step:3634 [D loss: 0.108139, acc.: 96.88%] [G loss: 2.181755]\n",
      "epoch:4 step:3635 [D loss: 0.407780, acc.: 82.81%] [G loss: 4.042144]\n",
      "epoch:4 step:3636 [D loss: 0.149718, acc.: 96.09%] [G loss: 4.577859]\n",
      "epoch:4 step:3637 [D loss: 0.407833, acc.: 85.16%] [G loss: 4.422018]\n",
      "epoch:4 step:3638 [D loss: 0.407128, acc.: 79.69%] [G loss: 3.513373]\n",
      "epoch:4 step:3639 [D loss: 0.111725, acc.: 97.66%] [G loss: 3.669706]\n",
      "epoch:4 step:3640 [D loss: 0.190964, acc.: 95.31%] [G loss: 3.545442]\n",
      "epoch:4 step:3641 [D loss: 0.210219, acc.: 93.75%] [G loss: 4.916099]\n",
      "epoch:4 step:3642 [D loss: 0.061896, acc.: 99.22%] [G loss: 4.494126]\n",
      "epoch:4 step:3643 [D loss: 0.470337, acc.: 77.34%] [G loss: 5.906174]\n",
      "epoch:4 step:3644 [D loss: 0.179853, acc.: 94.53%] [G loss: 4.878106]\n",
      "epoch:4 step:3645 [D loss: 0.389621, acc.: 82.03%] [G loss: 2.694720]\n",
      "epoch:4 step:3646 [D loss: 0.144116, acc.: 96.09%] [G loss: 4.439213]\n",
      "epoch:4 step:3647 [D loss: 0.102017, acc.: 96.09%] [G loss: 4.980759]\n",
      "epoch:4 step:3648 [D loss: 1.558258, acc.: 25.00%] [G loss: 6.740935]\n",
      "epoch:4 step:3649 [D loss: 0.388272, acc.: 82.03%] [G loss: 3.963737]\n",
      "epoch:4 step:3650 [D loss: 0.178791, acc.: 94.53%] [G loss: 2.735937]\n",
      "epoch:4 step:3651 [D loss: 0.169759, acc.: 92.97%] [G loss: 4.881518]\n",
      "epoch:4 step:3652 [D loss: 0.082197, acc.: 98.44%] [G loss: 4.283577]\n",
      "epoch:4 step:3653 [D loss: 0.256896, acc.: 89.84%] [G loss: 3.965249]\n",
      "epoch:4 step:3654 [D loss: 0.056857, acc.: 97.66%] [G loss: 4.298218]\n",
      "epoch:4 step:3655 [D loss: 0.289778, acc.: 84.38%] [G loss: 4.883603]\n",
      "epoch:4 step:3656 [D loss: 0.592732, acc.: 72.66%] [G loss: 3.848289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3657 [D loss: 0.448228, acc.: 78.91%] [G loss: 6.316875]\n",
      "epoch:4 step:3658 [D loss: 0.203762, acc.: 94.53%] [G loss: 4.538657]\n",
      "epoch:4 step:3659 [D loss: 0.038708, acc.: 99.22%] [G loss: 3.547024]\n",
      "epoch:4 step:3660 [D loss: 0.058007, acc.: 100.00%] [G loss: 1.847580]\n",
      "epoch:4 step:3661 [D loss: 0.219072, acc.: 88.28%] [G loss: 2.839130]\n",
      "epoch:4 step:3662 [D loss: 0.079630, acc.: 97.66%] [G loss: 2.054114]\n",
      "epoch:4 step:3663 [D loss: 0.133154, acc.: 96.09%] [G loss: 1.427656]\n",
      "epoch:4 step:3664 [D loss: 0.022308, acc.: 100.00%] [G loss: 0.442328]\n",
      "epoch:4 step:3665 [D loss: 0.013597, acc.: 99.22%] [G loss: 0.145425]\n",
      "epoch:4 step:3666 [D loss: 0.026530, acc.: 100.00%] [G loss: 0.058878]\n",
      "epoch:4 step:3667 [D loss: 0.168583, acc.: 93.75%] [G loss: 2.034331]\n",
      "epoch:4 step:3668 [D loss: 0.076761, acc.: 97.66%] [G loss: 0.957273]\n",
      "epoch:4 step:3669 [D loss: 0.056308, acc.: 98.44%] [G loss: 0.433470]\n",
      "epoch:4 step:3670 [D loss: 0.065387, acc.: 99.22%] [G loss: 0.132174]\n",
      "epoch:4 step:3671 [D loss: 0.111442, acc.: 98.44%] [G loss: 0.758921]\n",
      "epoch:4 step:3672 [D loss: 0.187741, acc.: 93.75%] [G loss: 0.628808]\n",
      "epoch:4 step:3673 [D loss: 0.048623, acc.: 99.22%] [G loss: 0.693174]\n",
      "epoch:4 step:3674 [D loss: 0.062227, acc.: 99.22%] [G loss: 0.530938]\n",
      "epoch:4 step:3675 [D loss: 0.032336, acc.: 100.00%] [G loss: 0.524992]\n",
      "epoch:4 step:3676 [D loss: 3.231503, acc.: 35.16%] [G loss: 8.387901]\n",
      "epoch:4 step:3677 [D loss: 1.635926, acc.: 53.91%] [G loss: 4.952902]\n",
      "epoch:4 step:3678 [D loss: 0.382336, acc.: 81.25%] [G loss: 2.255828]\n",
      "epoch:4 step:3679 [D loss: 0.209614, acc.: 91.41%] [G loss: 4.260055]\n",
      "epoch:4 step:3680 [D loss: 0.049337, acc.: 99.22%] [G loss: 4.717097]\n",
      "epoch:4 step:3681 [D loss: 0.108005, acc.: 97.66%] [G loss: 2.200758]\n",
      "epoch:4 step:3682 [D loss: 0.098910, acc.: 97.66%] [G loss: 2.215534]\n",
      "epoch:4 step:3683 [D loss: 0.165000, acc.: 95.31%] [G loss: 3.488156]\n",
      "epoch:4 step:3684 [D loss: 0.047195, acc.: 99.22%] [G loss: 2.552653]\n",
      "epoch:4 step:3685 [D loss: 0.118178, acc.: 96.09%] [G loss: 1.434597]\n",
      "epoch:4 step:3686 [D loss: 0.211404, acc.: 93.75%] [G loss: 0.823168]\n",
      "epoch:4 step:3687 [D loss: 0.093389, acc.: 99.22%] [G loss: 1.214916]\n",
      "epoch:4 step:3688 [D loss: 0.176723, acc.: 92.97%] [G loss: 0.488584]\n",
      "epoch:4 step:3689 [D loss: 0.101925, acc.: 99.22%] [G loss: 0.540280]\n",
      "epoch:4 step:3690 [D loss: 0.206231, acc.: 92.19%] [G loss: 0.655507]\n",
      "epoch:4 step:3691 [D loss: 0.034392, acc.: 100.00%] [G loss: 0.641859]\n",
      "epoch:4 step:3692 [D loss: 0.412623, acc.: 76.56%] [G loss: 6.384485]\n",
      "epoch:4 step:3693 [D loss: 0.293609, acc.: 85.16%] [G loss: 5.648075]\n",
      "epoch:4 step:3694 [D loss: 0.387129, acc.: 79.69%] [G loss: 0.289516]\n",
      "epoch:4 step:3695 [D loss: 0.732574, acc.: 62.50%] [G loss: 5.091058]\n",
      "epoch:4 step:3696 [D loss: 0.104249, acc.: 96.88%] [G loss: 6.453221]\n",
      "epoch:4 step:3697 [D loss: 1.060703, acc.: 52.34%] [G loss: 1.882354]\n",
      "epoch:4 step:3698 [D loss: 0.386697, acc.: 75.78%] [G loss: 3.775770]\n",
      "epoch:4 step:3699 [D loss: 0.135492, acc.: 97.66%] [G loss: 3.872675]\n",
      "epoch:4 step:3700 [D loss: 0.274847, acc.: 88.28%] [G loss: 3.002829]\n",
      "epoch:4 step:3701 [D loss: 0.271119, acc.: 89.84%] [G loss: 3.179978]\n",
      "epoch:4 step:3702 [D loss: 0.271860, acc.: 89.84%] [G loss: 3.111461]\n",
      "epoch:4 step:3703 [D loss: 0.207631, acc.: 92.97%] [G loss: 2.761562]\n",
      "epoch:4 step:3704 [D loss: 0.336848, acc.: 88.28%] [G loss: 1.406584]\n",
      "epoch:4 step:3705 [D loss: 0.345343, acc.: 82.81%] [G loss: 2.067281]\n",
      "epoch:4 step:3706 [D loss: 0.104259, acc.: 97.66%] [G loss: 2.263141]\n",
      "epoch:4 step:3707 [D loss: 0.257183, acc.: 92.19%] [G loss: 2.278230]\n",
      "epoch:4 step:3708 [D loss: 0.131548, acc.: 96.09%] [G loss: 1.115166]\n",
      "epoch:4 step:3709 [D loss: 0.549576, acc.: 70.31%] [G loss: 4.218618]\n",
      "epoch:4 step:3710 [D loss: 0.182342, acc.: 92.19%] [G loss: 3.252355]\n",
      "epoch:4 step:3711 [D loss: 0.113062, acc.: 96.88%] [G loss: 2.395993]\n",
      "epoch:4 step:3712 [D loss: 0.360248, acc.: 81.25%] [G loss: 3.889024]\n",
      "epoch:4 step:3713 [D loss: 0.252104, acc.: 90.62%] [G loss: 3.444368]\n",
      "epoch:4 step:3714 [D loss: 0.263253, acc.: 91.41%] [G loss: 3.729097]\n",
      "epoch:4 step:3715 [D loss: 0.172978, acc.: 96.09%] [G loss: 3.754432]\n",
      "epoch:4 step:3716 [D loss: 0.149445, acc.: 99.22%] [G loss: 3.441612]\n",
      "epoch:4 step:3717 [D loss: 0.213832, acc.: 92.19%] [G loss: 4.683119]\n",
      "epoch:4 step:3718 [D loss: 0.396446, acc.: 82.03%] [G loss: 2.795388]\n",
      "epoch:4 step:3719 [D loss: 0.041639, acc.: 100.00%] [G loss: 2.869476]\n",
      "epoch:4 step:3720 [D loss: 0.042363, acc.: 99.22%] [G loss: 2.419246]\n",
      "epoch:4 step:3721 [D loss: 0.140973, acc.: 96.88%] [G loss: 2.520369]\n",
      "epoch:4 step:3722 [D loss: 0.094540, acc.: 96.88%] [G loss: 2.108781]\n",
      "epoch:4 step:3723 [D loss: 0.309297, acc.: 88.28%] [G loss: 4.008765]\n",
      "epoch:4 step:3724 [D loss: 0.216803, acc.: 89.84%] [G loss: 2.862809]\n",
      "epoch:4 step:3725 [D loss: 0.233919, acc.: 89.06%] [G loss: 1.305846]\n",
      "epoch:4 step:3726 [D loss: 0.585998, acc.: 71.88%] [G loss: 5.620716]\n",
      "epoch:4 step:3727 [D loss: 0.147570, acc.: 93.75%] [G loss: 5.645916]\n",
      "epoch:4 step:3728 [D loss: 1.059556, acc.: 55.47%] [G loss: 0.863312]\n",
      "epoch:4 step:3729 [D loss: 0.734576, acc.: 74.22%] [G loss: 5.739180]\n",
      "epoch:4 step:3730 [D loss: 0.165918, acc.: 92.97%] [G loss: 6.561541]\n",
      "epoch:4 step:3731 [D loss: 0.296883, acc.: 89.06%] [G loss: 3.677559]\n",
      "epoch:4 step:3732 [D loss: 0.164572, acc.: 93.75%] [G loss: 3.782965]\n",
      "epoch:4 step:3733 [D loss: 0.108030, acc.: 99.22%] [G loss: 2.761402]\n",
      "epoch:4 step:3734 [D loss: 0.192810, acc.: 93.75%] [G loss: 3.676325]\n",
      "epoch:4 step:3735 [D loss: 0.068791, acc.: 99.22%] [G loss: 3.714580]\n",
      "epoch:4 step:3736 [D loss: 0.109302, acc.: 96.88%] [G loss: 1.732742]\n",
      "epoch:4 step:3737 [D loss: 0.185880, acc.: 96.88%] [G loss: 2.027708]\n",
      "epoch:4 step:3738 [D loss: 0.208144, acc.: 93.75%] [G loss: 3.361905]\n",
      "epoch:4 step:3739 [D loss: 0.169642, acc.: 94.53%] [G loss: 1.645639]\n",
      "epoch:4 step:3740 [D loss: 0.430818, acc.: 80.47%] [G loss: 5.504236]\n",
      "epoch:4 step:3741 [D loss: 0.369733, acc.: 81.25%] [G loss: 3.028988]\n",
      "epoch:4 step:3742 [D loss: 0.315137, acc.: 85.94%] [G loss: 5.280571]\n",
      "epoch:4 step:3743 [D loss: 0.363939, acc.: 76.56%] [G loss: 2.867500]\n",
      "epoch:4 step:3744 [D loss: 0.308595, acc.: 84.38%] [G loss: 5.086484]\n",
      "epoch:4 step:3745 [D loss: 0.042468, acc.: 100.00%] [G loss: 5.508614]\n",
      "epoch:4 step:3746 [D loss: 0.154052, acc.: 96.09%] [G loss: 3.388431]\n",
      "epoch:4 step:3747 [D loss: 0.350833, acc.: 83.59%] [G loss: 4.143307]\n",
      "epoch:4 step:3748 [D loss: 0.093891, acc.: 97.66%] [G loss: 4.078290]\n",
      "epoch:4 step:3749 [D loss: 0.144064, acc.: 94.53%] [G loss: 1.757219]\n",
      "epoch:4 step:3750 [D loss: 0.240244, acc.: 92.19%] [G loss: 2.486157]\n",
      "epoch:4 step:3751 [D loss: 0.327294, acc.: 85.94%] [G loss: 3.697317]\n",
      "epoch:4 step:3752 [D loss: 0.281436, acc.: 86.72%] [G loss: 3.037116]\n",
      "epoch:4 step:3753 [D loss: 0.163507, acc.: 96.09%] [G loss: 2.031744]\n",
      "epoch:4 step:3754 [D loss: 0.057099, acc.: 99.22%] [G loss: 3.134873]\n",
      "epoch:4 step:3755 [D loss: 0.505244, acc.: 72.66%] [G loss: 5.009523]\n",
      "epoch:4 step:3756 [D loss: 0.359374, acc.: 82.81%] [G loss: 4.031006]\n",
      "epoch:4 step:3757 [D loss: 0.114584, acc.: 97.66%] [G loss: 1.478492]\n",
      "epoch:4 step:3758 [D loss: 0.357466, acc.: 82.81%] [G loss: 4.908415]\n",
      "epoch:4 step:3759 [D loss: 0.149104, acc.: 92.97%] [G loss: 2.855406]\n",
      "epoch:4 step:3760 [D loss: 0.186702, acc.: 94.53%] [G loss: 3.176332]\n",
      "epoch:4 step:3761 [D loss: 0.556343, acc.: 74.22%] [G loss: 2.289515]\n",
      "epoch:4 step:3762 [D loss: 0.184428, acc.: 93.75%] [G loss: 4.081243]\n",
      "epoch:4 step:3763 [D loss: 0.378778, acc.: 82.81%] [G loss: 2.097363]\n",
      "epoch:4 step:3764 [D loss: 0.345785, acc.: 82.03%] [G loss: 4.885280]\n",
      "epoch:4 step:3765 [D loss: 0.170101, acc.: 93.75%] [G loss: 4.672110]\n",
      "epoch:4 step:3766 [D loss: 0.156906, acc.: 95.31%] [G loss: 4.317435]\n",
      "epoch:4 step:3767 [D loss: 0.065631, acc.: 98.44%] [G loss: 2.929498]\n",
      "epoch:4 step:3768 [D loss: 0.126538, acc.: 96.09%] [G loss: 2.400862]\n",
      "epoch:4 step:3769 [D loss: 0.183187, acc.: 93.75%] [G loss: 3.070501]\n",
      "epoch:4 step:3770 [D loss: 2.675598, acc.: 17.19%] [G loss: 5.968145]\n",
      "epoch:4 step:3771 [D loss: 0.544011, acc.: 71.88%] [G loss: 4.679188]\n",
      "epoch:4 step:3772 [D loss: 0.248404, acc.: 89.06%] [G loss: 2.997749]\n",
      "epoch:4 step:3773 [D loss: 0.108658, acc.: 98.44%] [G loss: 1.840937]\n",
      "epoch:4 step:3774 [D loss: 0.297751, acc.: 88.28%] [G loss: 3.399771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3775 [D loss: 0.119252, acc.: 96.88%] [G loss: 4.319449]\n",
      "epoch:4 step:3776 [D loss: 0.370324, acc.: 82.81%] [G loss: 4.075459]\n",
      "epoch:4 step:3777 [D loss: 0.226771, acc.: 89.84%] [G loss: 3.928945]\n",
      "epoch:4 step:3778 [D loss: 0.142702, acc.: 95.31%] [G loss: 2.845294]\n",
      "epoch:4 step:3779 [D loss: 0.177673, acc.: 94.53%] [G loss: 3.320773]\n",
      "epoch:4 step:3780 [D loss: 0.276538, acc.: 88.28%] [G loss: 3.624269]\n",
      "epoch:4 step:3781 [D loss: 0.165376, acc.: 98.44%] [G loss: 3.203741]\n",
      "epoch:4 step:3782 [D loss: 0.298514, acc.: 87.50%] [G loss: 3.432080]\n",
      "epoch:4 step:3783 [D loss: 0.189607, acc.: 91.41%] [G loss: 4.455186]\n",
      "epoch:4 step:3784 [D loss: 0.319545, acc.: 85.16%] [G loss: 2.450784]\n",
      "epoch:4 step:3785 [D loss: 0.132137, acc.: 99.22%] [G loss: 1.903081]\n",
      "epoch:4 step:3786 [D loss: 0.066387, acc.: 100.00%] [G loss: 1.943545]\n",
      "epoch:4 step:3787 [D loss: 0.257779, acc.: 90.62%] [G loss: 4.483229]\n",
      "epoch:4 step:3788 [D loss: 0.212670, acc.: 89.06%] [G loss: 3.042997]\n",
      "epoch:4 step:3789 [D loss: 0.085941, acc.: 98.44%] [G loss: 1.585082]\n",
      "epoch:4 step:3790 [D loss: 0.154628, acc.: 95.31%] [G loss: 2.319179]\n",
      "epoch:4 step:3791 [D loss: 0.364466, acc.: 85.16%] [G loss: 4.607411]\n",
      "epoch:4 step:3792 [D loss: 0.235535, acc.: 89.06%] [G loss: 1.336840]\n",
      "epoch:4 step:3793 [D loss: 0.389212, acc.: 84.38%] [G loss: 4.255498]\n",
      "epoch:4 step:3794 [D loss: 0.077906, acc.: 97.66%] [G loss: 4.674624]\n",
      "epoch:4 step:3795 [D loss: 0.252116, acc.: 89.06%] [G loss: 1.031944]\n",
      "epoch:4 step:3796 [D loss: 1.704122, acc.: 42.97%] [G loss: 7.129574]\n",
      "epoch:4 step:3797 [D loss: 1.044172, acc.: 57.81%] [G loss: 3.900624]\n",
      "epoch:4 step:3798 [D loss: 0.197443, acc.: 94.53%] [G loss: 3.609890]\n",
      "epoch:4 step:3799 [D loss: 0.063805, acc.: 99.22%] [G loss: 3.736004]\n",
      "epoch:4 step:3800 [D loss: 0.154912, acc.: 93.75%] [G loss: 3.035131]\n",
      "epoch:4 step:3801 [D loss: 0.130150, acc.: 97.66%] [G loss: 3.667518]\n",
      "epoch:4 step:3802 [D loss: 0.151772, acc.: 94.53%] [G loss: 3.929805]\n",
      "epoch:4 step:3803 [D loss: 0.253285, acc.: 92.19%] [G loss: 3.737040]\n",
      "epoch:4 step:3804 [D loss: 0.096160, acc.: 98.44%] [G loss: 3.268263]\n",
      "epoch:4 step:3805 [D loss: 0.278066, acc.: 86.72%] [G loss: 3.732075]\n",
      "epoch:4 step:3806 [D loss: 0.276229, acc.: 90.62%] [G loss: 2.656561]\n",
      "epoch:4 step:3807 [D loss: 0.204655, acc.: 92.19%] [G loss: 3.846190]\n",
      "epoch:4 step:3808 [D loss: 0.128833, acc.: 96.88%] [G loss: 4.064984]\n",
      "epoch:4 step:3809 [D loss: 0.421299, acc.: 83.59%] [G loss: 2.617764]\n",
      "epoch:4 step:3810 [D loss: 0.106128, acc.: 97.66%] [G loss: 1.854974]\n",
      "epoch:4 step:3811 [D loss: 0.150637, acc.: 96.09%] [G loss: 2.630919]\n",
      "epoch:4 step:3812 [D loss: 0.223271, acc.: 92.19%] [G loss: 2.245883]\n",
      "epoch:4 step:3813 [D loss: 0.085659, acc.: 100.00%] [G loss: 1.947501]\n",
      "epoch:4 step:3814 [D loss: 0.046991, acc.: 98.44%] [G loss: 1.420709]\n",
      "epoch:4 step:3815 [D loss: 0.361177, acc.: 78.91%] [G loss: 5.046682]\n",
      "epoch:4 step:3816 [D loss: 1.198728, acc.: 40.62%] [G loss: 2.499359]\n",
      "epoch:4 step:3817 [D loss: 0.198337, acc.: 94.53%] [G loss: 4.032731]\n",
      "epoch:4 step:3818 [D loss: 0.206189, acc.: 90.62%] [G loss: 3.714030]\n",
      "epoch:4 step:3819 [D loss: 0.154227, acc.: 95.31%] [G loss: 3.433445]\n",
      "epoch:4 step:3820 [D loss: 0.285412, acc.: 86.72%] [G loss: 4.101445]\n",
      "epoch:4 step:3821 [D loss: 0.197914, acc.: 93.75%] [G loss: 3.703490]\n",
      "epoch:4 step:3822 [D loss: 0.063682, acc.: 98.44%] [G loss: 3.109342]\n",
      "epoch:4 step:3823 [D loss: 0.358020, acc.: 84.38%] [G loss: 3.642675]\n",
      "epoch:4 step:3824 [D loss: 0.198540, acc.: 91.41%] [G loss: 3.981672]\n",
      "epoch:4 step:3825 [D loss: 0.093132, acc.: 98.44%] [G loss: 3.885225]\n",
      "epoch:4 step:3826 [D loss: 0.157799, acc.: 96.88%] [G loss: 2.960066]\n",
      "epoch:4 step:3827 [D loss: 0.115455, acc.: 96.88%] [G loss: 2.196660]\n",
      "epoch:4 step:3828 [D loss: 0.380898, acc.: 82.81%] [G loss: 1.304219]\n",
      "epoch:4 step:3829 [D loss: 0.172909, acc.: 94.53%] [G loss: 1.003041]\n",
      "epoch:4 step:3830 [D loss: 0.339347, acc.: 82.81%] [G loss: 5.146020]\n",
      "epoch:4 step:3831 [D loss: 0.823468, acc.: 57.03%] [G loss: 4.191991]\n",
      "epoch:4 step:3832 [D loss: 0.052738, acc.: 100.00%] [G loss: 3.304561]\n",
      "epoch:4 step:3833 [D loss: 0.270233, acc.: 89.06%] [G loss: 4.053489]\n",
      "epoch:4 step:3834 [D loss: 0.207678, acc.: 90.62%] [G loss: 4.176896]\n",
      "epoch:4 step:3835 [D loss: 0.377849, acc.: 84.38%] [G loss: 3.764783]\n",
      "epoch:4 step:3836 [D loss: 0.115354, acc.: 99.22%] [G loss: 4.071046]\n",
      "epoch:4 step:3837 [D loss: 0.382176, acc.: 82.03%] [G loss: 3.755981]\n",
      "epoch:4 step:3838 [D loss: 0.294209, acc.: 89.06%] [G loss: 6.289268]\n",
      "epoch:4 step:3839 [D loss: 0.372736, acc.: 78.91%] [G loss: 3.528338]\n",
      "epoch:4 step:3840 [D loss: 0.162574, acc.: 95.31%] [G loss: 4.082673]\n",
      "epoch:4 step:3841 [D loss: 0.144658, acc.: 95.31%] [G loss: 3.987531]\n",
      "epoch:4 step:3842 [D loss: 0.258893, acc.: 87.50%] [G loss: 4.227695]\n",
      "epoch:4 step:3843 [D loss: 0.118861, acc.: 96.88%] [G loss: 4.637717]\n",
      "epoch:4 step:3844 [D loss: 0.103458, acc.: 99.22%] [G loss: 2.567730]\n",
      "epoch:4 step:3845 [D loss: 0.336131, acc.: 83.59%] [G loss: 5.068697]\n",
      "epoch:4 step:3846 [D loss: 0.179908, acc.: 94.53%] [G loss: 2.830640]\n",
      "epoch:4 step:3847 [D loss: 0.084187, acc.: 99.22%] [G loss: 1.211412]\n",
      "epoch:4 step:3848 [D loss: 0.079739, acc.: 98.44%] [G loss: 2.021076]\n",
      "epoch:4 step:3849 [D loss: 0.194854, acc.: 92.97%] [G loss: 4.071630]\n",
      "epoch:4 step:3850 [D loss: 0.151069, acc.: 93.75%] [G loss: 1.762511]\n",
      "epoch:4 step:3851 [D loss: 0.288669, acc.: 86.72%] [G loss: 5.122210]\n",
      "epoch:4 step:3852 [D loss: 0.221186, acc.: 91.41%] [G loss: 2.573439]\n",
      "epoch:4 step:3853 [D loss: 0.097896, acc.: 96.09%] [G loss: 3.415420]\n",
      "epoch:4 step:3854 [D loss: 0.043476, acc.: 99.22%] [G loss: 2.772607]\n",
      "epoch:4 step:3855 [D loss: 0.239443, acc.: 93.75%] [G loss: 4.442440]\n",
      "epoch:4 step:3856 [D loss: 1.045370, acc.: 50.78%] [G loss: 7.759416]\n",
      "epoch:4 step:3857 [D loss: 0.587861, acc.: 71.09%] [G loss: 3.173350]\n",
      "epoch:4 step:3858 [D loss: 0.353638, acc.: 82.81%] [G loss: 6.688589]\n",
      "epoch:4 step:3859 [D loss: 0.048162, acc.: 99.22%] [G loss: 6.858860]\n",
      "epoch:4 step:3860 [D loss: 0.077548, acc.: 96.88%] [G loss: 4.211828]\n",
      "epoch:4 step:3861 [D loss: 0.082465, acc.: 100.00%] [G loss: 2.074497]\n",
      "epoch:4 step:3862 [D loss: 0.167426, acc.: 91.41%] [G loss: 1.808298]\n",
      "epoch:4 step:3863 [D loss: 0.094716, acc.: 96.09%] [G loss: 2.865629]\n",
      "epoch:4 step:3864 [D loss: 0.216880, acc.: 90.62%] [G loss: 3.361187]\n",
      "epoch:4 step:3865 [D loss: 0.157581, acc.: 94.53%] [G loss: 2.942900]\n",
      "epoch:4 step:3866 [D loss: 0.327736, acc.: 85.94%] [G loss: 1.242951]\n",
      "epoch:4 step:3867 [D loss: 0.214441, acc.: 91.41%] [G loss: 3.441245]\n",
      "epoch:4 step:3868 [D loss: 0.160460, acc.: 93.75%] [G loss: 3.235208]\n",
      "epoch:4 step:3869 [D loss: 0.176682, acc.: 94.53%] [G loss: 2.103171]\n",
      "epoch:4 step:3870 [D loss: 0.114858, acc.: 96.88%] [G loss: 1.293322]\n",
      "epoch:4 step:3871 [D loss: 0.132910, acc.: 96.09%] [G loss: 1.732929]\n",
      "epoch:4 step:3872 [D loss: 0.573550, acc.: 76.56%] [G loss: 4.955384]\n",
      "epoch:4 step:3873 [D loss: 0.649851, acc.: 75.00%] [G loss: 1.856323]\n",
      "epoch:4 step:3874 [D loss: 0.860810, acc.: 64.06%] [G loss: 7.709503]\n",
      "epoch:4 step:3875 [D loss: 0.654142, acc.: 71.09%] [G loss: 3.114740]\n",
      "epoch:4 step:3876 [D loss: 0.102657, acc.: 96.88%] [G loss: 1.722586]\n",
      "epoch:4 step:3877 [D loss: 0.214661, acc.: 92.97%] [G loss: 3.446725]\n",
      "epoch:4 step:3878 [D loss: 0.519925, acc.: 76.56%] [G loss: 2.900748]\n",
      "epoch:4 step:3879 [D loss: 0.071944, acc.: 98.44%] [G loss: 3.278707]\n",
      "epoch:4 step:3880 [D loss: 0.475053, acc.: 75.00%] [G loss: 3.312572]\n",
      "epoch:4 step:3881 [D loss: 0.120109, acc.: 96.88%] [G loss: 2.145204]\n",
      "epoch:4 step:3882 [D loss: 0.293742, acc.: 86.72%] [G loss: 3.786301]\n",
      "epoch:4 step:3883 [D loss: 0.695116, acc.: 65.62%] [G loss: 4.194450]\n",
      "epoch:4 step:3884 [D loss: 0.264957, acc.: 87.50%] [G loss: 5.170933]\n",
      "epoch:4 step:3885 [D loss: 0.151144, acc.: 95.31%] [G loss: 5.158737]\n",
      "epoch:4 step:3886 [D loss: 1.308340, acc.: 41.41%] [G loss: 4.811962]\n",
      "epoch:4 step:3887 [D loss: 0.013870, acc.: 100.00%] [G loss: 5.105000]\n",
      "epoch:4 step:3888 [D loss: 0.095796, acc.: 98.44%] [G loss: 4.128860]\n",
      "epoch:4 step:3889 [D loss: 0.054530, acc.: 99.22%] [G loss: 2.360419]\n",
      "epoch:4 step:3890 [D loss: 0.264347, acc.: 85.16%] [G loss: 4.811464]\n",
      "epoch:4 step:3891 [D loss: 0.128099, acc.: 93.75%] [G loss: 2.256774]\n",
      "epoch:4 step:3892 [D loss: 0.073866, acc.: 99.22%] [G loss: 1.133906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3893 [D loss: 0.092584, acc.: 98.44%] [G loss: 1.378005]\n",
      "epoch:4 step:3894 [D loss: 0.064918, acc.: 99.22%] [G loss: 0.616036]\n",
      "epoch:4 step:3895 [D loss: 0.059093, acc.: 99.22%] [G loss: 0.964128]\n",
      "epoch:4 step:3896 [D loss: 0.069405, acc.: 97.66%] [G loss: 0.227910]\n",
      "epoch:4 step:3897 [D loss: 0.873134, acc.: 60.16%] [G loss: 8.027588]\n",
      "epoch:4 step:3898 [D loss: 2.090868, acc.: 50.78%] [G loss: 2.311150]\n",
      "epoch:4 step:3899 [D loss: 0.189630, acc.: 94.53%] [G loss: 0.998114]\n",
      "epoch:4 step:3900 [D loss: 0.209667, acc.: 90.62%] [G loss: 2.275253]\n",
      "epoch:4 step:3901 [D loss: 0.164148, acc.: 92.19%] [G loss: 1.681013]\n",
      "epoch:4 step:3902 [D loss: 0.113653, acc.: 96.88%] [G loss: 1.990184]\n",
      "epoch:4 step:3903 [D loss: 0.410220, acc.: 77.34%] [G loss: 1.413524]\n",
      "epoch:4 step:3904 [D loss: 0.460406, acc.: 79.69%] [G loss: 1.019197]\n",
      "epoch:4 step:3905 [D loss: 0.215037, acc.: 89.84%] [G loss: 2.426757]\n",
      "epoch:5 step:3906 [D loss: 0.084685, acc.: 98.44%] [G loss: 2.857333]\n",
      "epoch:5 step:3907 [D loss: 0.137478, acc.: 96.88%] [G loss: 3.181824]\n",
      "epoch:5 step:3908 [D loss: 0.265422, acc.: 90.62%] [G loss: 3.114272]\n",
      "epoch:5 step:3909 [D loss: 0.137775, acc.: 96.88%] [G loss: 4.149898]\n",
      "epoch:5 step:3910 [D loss: 0.414748, acc.: 79.69%] [G loss: 4.486181]\n",
      "epoch:5 step:3911 [D loss: 0.071204, acc.: 99.22%] [G loss: 4.432483]\n",
      "epoch:5 step:3912 [D loss: 0.185247, acc.: 95.31%] [G loss: 3.812736]\n",
      "epoch:5 step:3913 [D loss: 0.284319, acc.: 88.28%] [G loss: 3.995095]\n",
      "epoch:5 step:3914 [D loss: 0.235682, acc.: 89.06%] [G loss: 3.667506]\n",
      "epoch:5 step:3915 [D loss: 0.333336, acc.: 85.16%] [G loss: 4.364931]\n",
      "epoch:5 step:3916 [D loss: 0.049385, acc.: 99.22%] [G loss: 3.248020]\n",
      "epoch:5 step:3917 [D loss: 0.194611, acc.: 93.75%] [G loss: 2.460477]\n",
      "epoch:5 step:3918 [D loss: 0.059402, acc.: 100.00%] [G loss: 1.085248]\n",
      "epoch:5 step:3919 [D loss: 0.132840, acc.: 96.88%] [G loss: 1.244724]\n",
      "epoch:5 step:3920 [D loss: 0.095146, acc.: 100.00%] [G loss: 1.561286]\n",
      "epoch:5 step:3921 [D loss: 0.036089, acc.: 100.00%] [G loss: 1.042604]\n",
      "epoch:5 step:3922 [D loss: 0.139707, acc.: 95.31%] [G loss: 2.680411]\n",
      "epoch:5 step:3923 [D loss: 0.091232, acc.: 97.66%] [G loss: 2.951833]\n",
      "epoch:5 step:3924 [D loss: 0.405831, acc.: 82.81%] [G loss: 2.477836]\n",
      "epoch:5 step:3925 [D loss: 0.060258, acc.: 97.66%] [G loss: 2.071381]\n",
      "epoch:5 step:3926 [D loss: 0.518194, acc.: 79.69%] [G loss: 6.004917]\n",
      "epoch:5 step:3927 [D loss: 0.474941, acc.: 73.44%] [G loss: 2.485031]\n",
      "epoch:5 step:3928 [D loss: 0.276698, acc.: 88.28%] [G loss: 4.837916]\n",
      "epoch:5 step:3929 [D loss: 0.098767, acc.: 97.66%] [G loss: 4.614240]\n",
      "epoch:5 step:3930 [D loss: 0.326785, acc.: 81.25%] [G loss: 2.230498]\n",
      "epoch:5 step:3931 [D loss: 0.260494, acc.: 86.72%] [G loss: 5.212602]\n",
      "epoch:5 step:3932 [D loss: 0.110393, acc.: 96.88%] [G loss: 4.855113]\n",
      "epoch:5 step:3933 [D loss: 0.850055, acc.: 60.94%] [G loss: 5.216830]\n",
      "epoch:5 step:3934 [D loss: 0.126814, acc.: 94.53%] [G loss: 4.530920]\n",
      "epoch:5 step:3935 [D loss: 0.461579, acc.: 80.47%] [G loss: 2.650270]\n",
      "epoch:5 step:3936 [D loss: 0.108644, acc.: 96.09%] [G loss: 3.167242]\n",
      "epoch:5 step:3937 [D loss: 0.124626, acc.: 98.44%] [G loss: 4.172285]\n",
      "epoch:5 step:3938 [D loss: 0.229795, acc.: 91.41%] [G loss: 3.611422]\n",
      "epoch:5 step:3939 [D loss: 0.083455, acc.: 99.22%] [G loss: 3.870338]\n",
      "epoch:5 step:3940 [D loss: 0.372799, acc.: 86.72%] [G loss: 4.217803]\n",
      "epoch:5 step:3941 [D loss: 0.638431, acc.: 71.88%] [G loss: 3.459446]\n",
      "epoch:5 step:3942 [D loss: 0.103140, acc.: 97.66%] [G loss: 3.739498]\n",
      "epoch:5 step:3943 [D loss: 0.138458, acc.: 98.44%] [G loss: 3.207616]\n",
      "epoch:5 step:3944 [D loss: 0.181468, acc.: 93.75%] [G loss: 3.783576]\n",
      "epoch:5 step:3945 [D loss: 0.247853, acc.: 89.84%] [G loss: 3.028572]\n",
      "epoch:5 step:3946 [D loss: 0.296147, acc.: 87.50%] [G loss: 3.888794]\n",
      "epoch:5 step:3947 [D loss: 0.091210, acc.: 98.44%] [G loss: 3.954094]\n",
      "epoch:5 step:3948 [D loss: 0.334630, acc.: 86.72%] [G loss: 4.210869]\n",
      "epoch:5 step:3949 [D loss: 0.339494, acc.: 85.16%] [G loss: 3.866727]\n",
      "epoch:5 step:3950 [D loss: 0.111160, acc.: 96.88%] [G loss: 4.159291]\n",
      "epoch:5 step:3951 [D loss: 0.252874, acc.: 89.84%] [G loss: 4.302530]\n",
      "epoch:5 step:3952 [D loss: 0.162152, acc.: 95.31%] [G loss: 5.251628]\n",
      "epoch:5 step:3953 [D loss: 0.193693, acc.: 96.09%] [G loss: 3.647345]\n",
      "epoch:5 step:3954 [D loss: 0.259899, acc.: 91.41%] [G loss: 3.649400]\n",
      "epoch:5 step:3955 [D loss: 0.258515, acc.: 88.28%] [G loss: 3.845662]\n",
      "epoch:5 step:3956 [D loss: 0.267796, acc.: 89.06%] [G loss: 4.915936]\n",
      "epoch:5 step:3957 [D loss: 0.162520, acc.: 93.75%] [G loss: 5.201557]\n",
      "epoch:5 step:3958 [D loss: 0.189213, acc.: 94.53%] [G loss: 3.977936]\n",
      "epoch:5 step:3959 [D loss: 0.427795, acc.: 79.69%] [G loss: 6.945623]\n",
      "epoch:5 step:3960 [D loss: 0.458898, acc.: 75.00%] [G loss: 3.361142]\n",
      "epoch:5 step:3961 [D loss: 0.535553, acc.: 75.00%] [G loss: 7.312357]\n",
      "epoch:5 step:3962 [D loss: 0.443246, acc.: 80.47%] [G loss: 4.896790]\n",
      "epoch:5 step:3963 [D loss: 0.133531, acc.: 93.75%] [G loss: 5.966836]\n",
      "epoch:5 step:3964 [D loss: 0.073253, acc.: 98.44%] [G loss: 4.770103]\n",
      "epoch:5 step:3965 [D loss: 0.182599, acc.: 93.75%] [G loss: 2.820349]\n",
      "epoch:5 step:3966 [D loss: 0.127340, acc.: 96.09%] [G loss: 3.600716]\n",
      "epoch:5 step:3967 [D loss: 0.291126, acc.: 84.38%] [G loss: 3.595256]\n",
      "epoch:5 step:3968 [D loss: 0.120308, acc.: 96.09%] [G loss: 3.379740]\n",
      "epoch:5 step:3969 [D loss: 0.274588, acc.: 90.62%] [G loss: 3.815930]\n",
      "epoch:5 step:3970 [D loss: 0.512806, acc.: 75.00%] [G loss: 4.322253]\n",
      "epoch:5 step:3971 [D loss: 0.084482, acc.: 99.22%] [G loss: 3.703487]\n",
      "epoch:5 step:3972 [D loss: 0.090328, acc.: 99.22%] [G loss: 4.441272]\n",
      "epoch:5 step:3973 [D loss: 0.144014, acc.: 95.31%] [G loss: 3.002197]\n",
      "epoch:5 step:3974 [D loss: 0.185046, acc.: 92.97%] [G loss: 4.247034]\n",
      "epoch:5 step:3975 [D loss: 0.351074, acc.: 82.03%] [G loss: 2.571761]\n",
      "epoch:5 step:3976 [D loss: 0.346580, acc.: 79.69%] [G loss: 7.625136]\n",
      "epoch:5 step:3977 [D loss: 0.633835, acc.: 70.31%] [G loss: 2.819796]\n",
      "epoch:5 step:3978 [D loss: 0.318974, acc.: 85.94%] [G loss: 6.312168]\n",
      "epoch:5 step:3979 [D loss: 0.273530, acc.: 89.06%] [G loss: 3.484703]\n",
      "epoch:5 step:3980 [D loss: 0.125251, acc.: 96.88%] [G loss: 3.386164]\n",
      "epoch:5 step:3981 [D loss: 0.264920, acc.: 87.50%] [G loss: 5.760417]\n",
      "epoch:5 step:3982 [D loss: 0.329579, acc.: 84.38%] [G loss: 4.283443]\n",
      "epoch:5 step:3983 [D loss: 0.234561, acc.: 88.28%] [G loss: 5.919197]\n",
      "epoch:5 step:3984 [D loss: 0.123556, acc.: 96.09%] [G loss: 4.943928]\n",
      "epoch:5 step:3985 [D loss: 0.077246, acc.: 98.44%] [G loss: 5.252251]\n",
      "epoch:5 step:3986 [D loss: 0.282141, acc.: 86.72%] [G loss: 6.846525]\n",
      "epoch:5 step:3987 [D loss: 0.224667, acc.: 90.62%] [G loss: 3.539314]\n",
      "epoch:5 step:3988 [D loss: 0.113403, acc.: 98.44%] [G loss: 2.785232]\n",
      "epoch:5 step:3989 [D loss: 0.117112, acc.: 97.66%] [G loss: 4.322927]\n",
      "epoch:5 step:3990 [D loss: 0.355122, acc.: 82.03%] [G loss: 5.087849]\n",
      "epoch:5 step:3991 [D loss: 0.922878, acc.: 54.69%] [G loss: 5.122069]\n",
      "epoch:5 step:3992 [D loss: 0.039494, acc.: 100.00%] [G loss: 4.710747]\n",
      "epoch:5 step:3993 [D loss: 0.252342, acc.: 88.28%] [G loss: 3.118764]\n",
      "epoch:5 step:3994 [D loss: 0.065199, acc.: 99.22%] [G loss: 1.990060]\n",
      "epoch:5 step:3995 [D loss: 0.300172, acc.: 83.59%] [G loss: 5.212823]\n",
      "epoch:5 step:3996 [D loss: 0.505133, acc.: 73.44%] [G loss: 3.450504]\n",
      "epoch:5 step:3997 [D loss: 0.026050, acc.: 100.00%] [G loss: 3.595787]\n",
      "epoch:5 step:3998 [D loss: 0.077555, acc.: 97.66%] [G loss: 2.939057]\n",
      "epoch:5 step:3999 [D loss: 0.171282, acc.: 96.09%] [G loss: 4.505587]\n",
      "epoch:5 step:4000 [D loss: 0.046411, acc.: 99.22%] [G loss: 3.518290]\n",
      "epoch:5 step:4001 [D loss: 0.464314, acc.: 79.69%] [G loss: 6.719548]\n",
      "epoch:5 step:4002 [D loss: 0.182830, acc.: 92.19%] [G loss: 4.529073]\n",
      "epoch:5 step:4003 [D loss: 0.097062, acc.: 99.22%] [G loss: 3.153491]\n",
      "epoch:5 step:4004 [D loss: 0.258525, acc.: 89.06%] [G loss: 5.737477]\n",
      "epoch:5 step:4005 [D loss: 0.357360, acc.: 83.59%] [G loss: 3.134367]\n",
      "epoch:5 step:4006 [D loss: 0.282891, acc.: 88.28%] [G loss: 5.364801]\n",
      "epoch:5 step:4007 [D loss: 0.234708, acc.: 89.06%] [G loss: 4.163281]\n",
      "epoch:5 step:4008 [D loss: 0.060880, acc.: 96.88%] [G loss: 1.736130]\n",
      "epoch:5 step:4009 [D loss: 0.104540, acc.: 97.66%] [G loss: 2.965670]\n",
      "epoch:5 step:4010 [D loss: 0.040649, acc.: 100.00%] [G loss: 3.151231]\n",
      "epoch:5 step:4011 [D loss: 0.085277, acc.: 99.22%] [G loss: 2.681809]\n",
      "epoch:5 step:4012 [D loss: 0.217429, acc.: 92.97%] [G loss: 1.323702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4013 [D loss: 0.215228, acc.: 92.19%] [G loss: 4.557234]\n",
      "epoch:5 step:4014 [D loss: 0.225247, acc.: 89.06%] [G loss: 0.507057]\n",
      "epoch:5 step:4015 [D loss: 0.019667, acc.: 100.00%] [G loss: 0.743254]\n",
      "epoch:5 step:4016 [D loss: 1.093238, acc.: 54.69%] [G loss: 10.171663]\n",
      "epoch:5 step:4017 [D loss: 1.876664, acc.: 52.34%] [G loss: 6.136477]\n",
      "epoch:5 step:4018 [D loss: 0.029996, acc.: 100.00%] [G loss: 2.412594]\n",
      "epoch:5 step:4019 [D loss: 0.232936, acc.: 91.41%] [G loss: 2.867802]\n",
      "epoch:5 step:4020 [D loss: 0.206118, acc.: 94.53%] [G loss: 3.714542]\n",
      "epoch:5 step:4021 [D loss: 0.178823, acc.: 94.53%] [G loss: 2.922656]\n",
      "epoch:5 step:4022 [D loss: 0.337527, acc.: 79.69%] [G loss: 4.725308]\n",
      "epoch:5 step:4023 [D loss: 0.173011, acc.: 94.53%] [G loss: 4.395988]\n",
      "epoch:5 step:4024 [D loss: 0.179072, acc.: 96.09%] [G loss: 3.283573]\n",
      "epoch:5 step:4025 [D loss: 0.483147, acc.: 76.56%] [G loss: 4.118505]\n",
      "epoch:5 step:4026 [D loss: 0.098305, acc.: 96.09%] [G loss: 4.507590]\n",
      "epoch:5 step:4027 [D loss: 0.347411, acc.: 86.72%] [G loss: 4.419002]\n",
      "epoch:5 step:4028 [D loss: 0.237367, acc.: 94.53%] [G loss: 2.436965]\n",
      "epoch:5 step:4029 [D loss: 0.239235, acc.: 91.41%] [G loss: 4.101742]\n",
      "epoch:5 step:4030 [D loss: 0.072214, acc.: 96.88%] [G loss: 5.098470]\n",
      "epoch:5 step:4031 [D loss: 0.586016, acc.: 67.19%] [G loss: 3.361983]\n",
      "epoch:5 step:4032 [D loss: 0.066971, acc.: 99.22%] [G loss: 4.069961]\n",
      "epoch:5 step:4033 [D loss: 0.580143, acc.: 68.75%] [G loss: 4.754442]\n",
      "epoch:5 step:4034 [D loss: 0.196371, acc.: 92.19%] [G loss: 3.682997]\n",
      "epoch:5 step:4035 [D loss: 0.394377, acc.: 83.59%] [G loss: 4.111372]\n",
      "epoch:5 step:4036 [D loss: 0.101875, acc.: 99.22%] [G loss: 4.455243]\n",
      "epoch:5 step:4037 [D loss: 0.255876, acc.: 91.41%] [G loss: 2.180947]\n",
      "epoch:5 step:4038 [D loss: 0.742746, acc.: 61.72%] [G loss: 7.766486]\n",
      "epoch:5 step:4039 [D loss: 1.414788, acc.: 51.56%] [G loss: 4.181067]\n",
      "epoch:5 step:4040 [D loss: 0.343974, acc.: 84.38%] [G loss: 2.522703]\n",
      "epoch:5 step:4041 [D loss: 0.064056, acc.: 99.22%] [G loss: 3.759156]\n",
      "epoch:5 step:4042 [D loss: 0.063744, acc.: 100.00%] [G loss: 2.472086]\n",
      "epoch:5 step:4043 [D loss: 0.293247, acc.: 88.28%] [G loss: 4.041075]\n",
      "epoch:5 step:4044 [D loss: 0.104380, acc.: 96.88%] [G loss: 4.420354]\n",
      "epoch:5 step:4045 [D loss: 0.315453, acc.: 86.72%] [G loss: 3.384424]\n",
      "epoch:5 step:4046 [D loss: 0.138255, acc.: 96.88%] [G loss: 2.855921]\n",
      "epoch:5 step:4047 [D loss: 0.227579, acc.: 91.41%] [G loss: 4.696965]\n",
      "epoch:5 step:4048 [D loss: 0.141310, acc.: 93.75%] [G loss: 3.821752]\n",
      "epoch:5 step:4049 [D loss: 0.256323, acc.: 86.72%] [G loss: 3.340073]\n",
      "epoch:5 step:4050 [D loss: 0.118829, acc.: 97.66%] [G loss: 3.913123]\n",
      "epoch:5 step:4051 [D loss: 0.179193, acc.: 92.19%] [G loss: 3.307828]\n",
      "epoch:5 step:4052 [D loss: 0.506663, acc.: 73.44%] [G loss: 5.008963]\n",
      "epoch:5 step:4053 [D loss: 0.247977, acc.: 87.50%] [G loss: 3.979870]\n",
      "epoch:5 step:4054 [D loss: 0.163821, acc.: 96.09%] [G loss: 3.053107]\n",
      "epoch:5 step:4055 [D loss: 0.068640, acc.: 99.22%] [G loss: 2.702450]\n",
      "epoch:5 step:4056 [D loss: 0.258536, acc.: 90.62%] [G loss: 4.126417]\n",
      "epoch:5 step:4057 [D loss: 0.299478, acc.: 88.28%] [G loss: 3.915531]\n",
      "epoch:5 step:4058 [D loss: 0.294899, acc.: 86.72%] [G loss: 3.210714]\n",
      "epoch:5 step:4059 [D loss: 0.045461, acc.: 100.00%] [G loss: 3.455782]\n",
      "epoch:5 step:4060 [D loss: 0.116118, acc.: 96.88%] [G loss: 3.405495]\n",
      "epoch:5 step:4061 [D loss: 0.189421, acc.: 93.75%] [G loss: 3.261230]\n",
      "epoch:5 step:4062 [D loss: 0.125373, acc.: 98.44%] [G loss: 3.690578]\n",
      "epoch:5 step:4063 [D loss: 0.296520, acc.: 90.62%] [G loss: 3.277835]\n",
      "epoch:5 step:4064 [D loss: 0.113719, acc.: 98.44%] [G loss: 3.935988]\n",
      "epoch:5 step:4065 [D loss: 0.365878, acc.: 85.16%] [G loss: 5.201646]\n",
      "epoch:5 step:4066 [D loss: 0.317575, acc.: 85.16%] [G loss: 3.282938]\n",
      "epoch:5 step:4067 [D loss: 0.127682, acc.: 96.88%] [G loss: 4.105710]\n",
      "epoch:5 step:4068 [D loss: 0.195055, acc.: 93.75%] [G loss: 4.397407]\n",
      "epoch:5 step:4069 [D loss: 0.084607, acc.: 99.22%] [G loss: 4.019924]\n",
      "epoch:5 step:4070 [D loss: 0.135471, acc.: 96.09%] [G loss: 4.558831]\n",
      "epoch:5 step:4071 [D loss: 0.107916, acc.: 99.22%] [G loss: 3.461804]\n",
      "epoch:5 step:4072 [D loss: 0.159599, acc.: 94.53%] [G loss: 5.298406]\n",
      "epoch:5 step:4073 [D loss: 0.152692, acc.: 96.09%] [G loss: 4.094125]\n",
      "epoch:5 step:4074 [D loss: 0.169243, acc.: 96.09%] [G loss: 3.771993]\n",
      "epoch:5 step:4075 [D loss: 0.124967, acc.: 96.88%] [G loss: 3.442918]\n",
      "epoch:5 step:4076 [D loss: 0.284982, acc.: 90.62%] [G loss: 6.372317]\n",
      "epoch:5 step:4077 [D loss: 0.677903, acc.: 65.62%] [G loss: 1.008252]\n",
      "epoch:5 step:4078 [D loss: 0.214751, acc.: 90.62%] [G loss: 4.625936]\n",
      "epoch:5 step:4079 [D loss: 0.187354, acc.: 92.19%] [G loss: 2.384119]\n",
      "epoch:5 step:4080 [D loss: 0.085883, acc.: 98.44%] [G loss: 4.671136]\n",
      "epoch:5 step:4081 [D loss: 0.707913, acc.: 64.06%] [G loss: 5.218447]\n",
      "epoch:5 step:4082 [D loss: 0.450554, acc.: 76.56%] [G loss: 2.257210]\n",
      "epoch:5 step:4083 [D loss: 0.039400, acc.: 100.00%] [G loss: 2.311466]\n",
      "epoch:5 step:4084 [D loss: 0.077805, acc.: 99.22%] [G loss: 2.431604]\n",
      "epoch:5 step:4085 [D loss: 0.278947, acc.: 87.50%] [G loss: 4.765028]\n",
      "epoch:5 step:4086 [D loss: 0.593925, acc.: 68.75%] [G loss: 0.777970]\n",
      "epoch:5 step:4087 [D loss: 0.673053, acc.: 66.41%] [G loss: 6.696303]\n",
      "epoch:5 step:4088 [D loss: 0.754019, acc.: 67.97%] [G loss: 3.226035]\n",
      "epoch:5 step:4089 [D loss: 0.417689, acc.: 81.25%] [G loss: 4.891278]\n",
      "epoch:5 step:4090 [D loss: 0.251094, acc.: 89.06%] [G loss: 3.603794]\n",
      "epoch:5 step:4091 [D loss: 0.113779, acc.: 96.88%] [G loss: 3.008843]\n",
      "epoch:5 step:4092 [D loss: 0.501170, acc.: 73.44%] [G loss: 6.609021]\n",
      "epoch:5 step:4093 [D loss: 0.810140, acc.: 64.06%] [G loss: 2.690727]\n",
      "epoch:5 step:4094 [D loss: 0.096116, acc.: 98.44%] [G loss: 2.439355]\n",
      "epoch:5 step:4095 [D loss: 0.063115, acc.: 100.00%] [G loss: 2.424144]\n",
      "epoch:5 step:4096 [D loss: 0.052992, acc.: 100.00%] [G loss: 1.696953]\n",
      "epoch:5 step:4097 [D loss: 0.152377, acc.: 97.66%] [G loss: 2.053657]\n",
      "epoch:5 step:4098 [D loss: 0.142657, acc.: 96.88%] [G loss: 0.420099]\n",
      "epoch:5 step:4099 [D loss: 0.125352, acc.: 97.66%] [G loss: 1.181801]\n",
      "epoch:5 step:4100 [D loss: 0.132515, acc.: 96.88%] [G loss: 0.821761]\n",
      "epoch:5 step:4101 [D loss: 0.144533, acc.: 93.75%] [G loss: 2.217452]\n",
      "epoch:5 step:4102 [D loss: 0.396002, acc.: 77.34%] [G loss: 0.108453]\n",
      "epoch:5 step:4103 [D loss: 0.956885, acc.: 59.38%] [G loss: 4.971949]\n",
      "epoch:5 step:4104 [D loss: 0.877409, acc.: 61.72%] [G loss: 1.664299]\n",
      "epoch:5 step:4105 [D loss: 0.189211, acc.: 93.75%] [G loss: 1.578912]\n",
      "epoch:5 step:4106 [D loss: 0.124262, acc.: 96.09%] [G loss: 3.444786]\n",
      "epoch:5 step:4107 [D loss: 0.152069, acc.: 95.31%] [G loss: 3.041077]\n",
      "epoch:5 step:4108 [D loss: 0.130419, acc.: 96.09%] [G loss: 3.493587]\n",
      "epoch:5 step:4109 [D loss: 0.139397, acc.: 96.88%] [G loss: 2.651973]\n",
      "epoch:5 step:4110 [D loss: 0.328219, acc.: 85.16%] [G loss: 3.156740]\n",
      "epoch:5 step:4111 [D loss: 0.094828, acc.: 99.22%] [G loss: 2.953173]\n",
      "epoch:5 step:4112 [D loss: 0.240748, acc.: 91.41%] [G loss: 3.724834]\n",
      "epoch:5 step:4113 [D loss: 1.108904, acc.: 46.88%] [G loss: 4.892900]\n",
      "epoch:5 step:4114 [D loss: 0.680796, acc.: 66.41%] [G loss: 3.027379]\n",
      "epoch:5 step:4115 [D loss: 0.139262, acc.: 96.09%] [G loss: 3.314077]\n",
      "epoch:5 step:4116 [D loss: 0.063712, acc.: 100.00%] [G loss: 1.301223]\n",
      "epoch:5 step:4117 [D loss: 0.040735, acc.: 100.00%] [G loss: 1.991673]\n",
      "epoch:5 step:4118 [D loss: 0.268788, acc.: 89.84%] [G loss: 3.477921]\n",
      "epoch:5 step:4119 [D loss: 0.549004, acc.: 70.31%] [G loss: 1.273662]\n",
      "epoch:5 step:4120 [D loss: 0.204118, acc.: 91.41%] [G loss: 2.778263]\n",
      "epoch:5 step:4121 [D loss: 0.051473, acc.: 100.00%] [G loss: 3.207865]\n",
      "epoch:5 step:4122 [D loss: 0.248989, acc.: 91.41%] [G loss: 1.628213]\n",
      "epoch:5 step:4123 [D loss: 0.082762, acc.: 100.00%] [G loss: 2.153566]\n",
      "epoch:5 step:4124 [D loss: 0.061875, acc.: 99.22%] [G loss: 1.608209]\n",
      "epoch:5 step:4125 [D loss: 0.267483, acc.: 91.41%] [G loss: 3.836138]\n",
      "epoch:5 step:4126 [D loss: 0.100575, acc.: 97.66%] [G loss: 2.994108]\n",
      "epoch:5 step:4127 [D loss: 0.108933, acc.: 98.44%] [G loss: 2.219069]\n",
      "epoch:5 step:4128 [D loss: 0.310222, acc.: 86.72%] [G loss: 4.330876]\n",
      "epoch:5 step:4129 [D loss: 0.796121, acc.: 50.00%] [G loss: 3.895473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4130 [D loss: 0.096272, acc.: 97.66%] [G loss: 4.593677]\n",
      "epoch:5 step:4131 [D loss: 0.093852, acc.: 98.44%] [G loss: 3.845424]\n",
      "epoch:5 step:4132 [D loss: 0.229054, acc.: 93.75%] [G loss: 3.886312]\n",
      "epoch:5 step:4133 [D loss: 0.044340, acc.: 100.00%] [G loss: 3.291182]\n",
      "epoch:5 step:4134 [D loss: 0.120324, acc.: 99.22%] [G loss: 3.910143]\n",
      "epoch:5 step:4135 [D loss: 0.602350, acc.: 71.88%] [G loss: 5.243449]\n",
      "epoch:5 step:4136 [D loss: 0.054212, acc.: 98.44%] [G loss: 5.253793]\n",
      "epoch:5 step:4137 [D loss: 0.424418, acc.: 81.25%] [G loss: 1.875438]\n",
      "epoch:5 step:4138 [D loss: 0.311013, acc.: 86.72%] [G loss: 3.510730]\n",
      "epoch:5 step:4139 [D loss: 0.065714, acc.: 99.22%] [G loss: 3.421710]\n",
      "epoch:5 step:4140 [D loss: 0.130008, acc.: 96.09%] [G loss: 3.430719]\n",
      "epoch:5 step:4141 [D loss: 0.144719, acc.: 97.66%] [G loss: 3.759090]\n",
      "epoch:5 step:4142 [D loss: 0.070691, acc.: 100.00%] [G loss: 3.867703]\n",
      "epoch:5 step:4143 [D loss: 0.147853, acc.: 94.53%] [G loss: 2.926061]\n",
      "epoch:5 step:4144 [D loss: 0.286132, acc.: 87.50%] [G loss: 4.655187]\n",
      "epoch:5 step:4145 [D loss: 0.392054, acc.: 85.16%] [G loss: 2.818728]\n",
      "epoch:5 step:4146 [D loss: 0.106144, acc.: 97.66%] [G loss: 2.458444]\n",
      "epoch:5 step:4147 [D loss: 0.113948, acc.: 97.66%] [G loss: 3.029764]\n",
      "epoch:5 step:4148 [D loss: 0.081693, acc.: 97.66%] [G loss: 3.046350]\n",
      "epoch:5 step:4149 [D loss: 0.316586, acc.: 84.38%] [G loss: 3.144846]\n",
      "epoch:5 step:4150 [D loss: 0.113356, acc.: 97.66%] [G loss: 2.794357]\n",
      "epoch:5 step:4151 [D loss: 0.424560, acc.: 81.25%] [G loss: 4.560887]\n",
      "epoch:5 step:4152 [D loss: 0.204669, acc.: 91.41%] [G loss: 3.337034]\n",
      "epoch:5 step:4153 [D loss: 0.276694, acc.: 89.84%] [G loss: 6.267280]\n",
      "epoch:5 step:4154 [D loss: 0.227510, acc.: 91.41%] [G loss: 4.789593]\n",
      "epoch:5 step:4155 [D loss: 0.124702, acc.: 96.09%] [G loss: 4.106416]\n",
      "epoch:5 step:4156 [D loss: 0.125598, acc.: 96.88%] [G loss: 4.121640]\n",
      "epoch:5 step:4157 [D loss: 0.097621, acc.: 98.44%] [G loss: 2.543751]\n",
      "epoch:5 step:4158 [D loss: 0.243850, acc.: 91.41%] [G loss: 6.308749]\n",
      "epoch:5 step:4159 [D loss: 0.317921, acc.: 82.03%] [G loss: 1.546896]\n",
      "epoch:5 step:4160 [D loss: 0.155419, acc.: 95.31%] [G loss: 3.276524]\n",
      "epoch:5 step:4161 [D loss: 0.043442, acc.: 99.22%] [G loss: 3.230593]\n",
      "epoch:5 step:4162 [D loss: 0.178972, acc.: 94.53%] [G loss: 2.814503]\n",
      "epoch:5 step:4163 [D loss: 0.090386, acc.: 98.44%] [G loss: 2.463634]\n",
      "epoch:5 step:4164 [D loss: 0.197819, acc.: 94.53%] [G loss: 1.143718]\n",
      "epoch:5 step:4165 [D loss: 0.204216, acc.: 92.19%] [G loss: 4.132188]\n",
      "epoch:5 step:4166 [D loss: 0.589114, acc.: 72.66%] [G loss: 0.447964]\n",
      "epoch:5 step:4167 [D loss: 0.243279, acc.: 87.50%] [G loss: 5.103107]\n",
      "epoch:5 step:4168 [D loss: 0.117192, acc.: 95.31%] [G loss: 5.471471]\n",
      "epoch:5 step:4169 [D loss: 0.152706, acc.: 95.31%] [G loss: 1.792552]\n",
      "epoch:5 step:4170 [D loss: 0.350729, acc.: 85.16%] [G loss: 3.720048]\n",
      "epoch:5 step:4171 [D loss: 0.231422, acc.: 89.06%] [G loss: 1.321131]\n",
      "epoch:5 step:4172 [D loss: 3.356950, acc.: 13.28%] [G loss: 4.477634]\n",
      "epoch:5 step:4173 [D loss: 0.109732, acc.: 96.09%] [G loss: 6.794398]\n",
      "epoch:5 step:4174 [D loss: 0.803712, acc.: 61.72%] [G loss: 2.657586]\n",
      "epoch:5 step:4175 [D loss: 0.191379, acc.: 95.31%] [G loss: 2.252317]\n",
      "epoch:5 step:4176 [D loss: 0.082679, acc.: 99.22%] [G loss: 3.780257]\n",
      "epoch:5 step:4177 [D loss: 0.117941, acc.: 99.22%] [G loss: 3.783530]\n",
      "epoch:5 step:4178 [D loss: 0.123276, acc.: 96.09%] [G loss: 3.423740]\n",
      "epoch:5 step:4179 [D loss: 0.177105, acc.: 96.09%] [G loss: 2.742870]\n",
      "epoch:5 step:4180 [D loss: 0.157396, acc.: 97.66%] [G loss: 3.407910]\n",
      "epoch:5 step:4181 [D loss: 0.223062, acc.: 92.19%] [G loss: 3.843596]\n",
      "epoch:5 step:4182 [D loss: 0.427199, acc.: 79.69%] [G loss: 3.738059]\n",
      "epoch:5 step:4183 [D loss: 0.062719, acc.: 99.22%] [G loss: 4.246603]\n",
      "epoch:5 step:4184 [D loss: 0.071305, acc.: 99.22%] [G loss: 1.985506]\n",
      "epoch:5 step:4185 [D loss: 0.410569, acc.: 81.25%] [G loss: 4.628672]\n",
      "epoch:5 step:4186 [D loss: 0.227172, acc.: 91.41%] [G loss: 2.436820]\n",
      "epoch:5 step:4187 [D loss: 0.145022, acc.: 93.75%] [G loss: 0.697214]\n",
      "epoch:5 step:4188 [D loss: 0.027549, acc.: 100.00%] [G loss: 0.185295]\n",
      "epoch:5 step:4189 [D loss: 0.063344, acc.: 100.00%] [G loss: 0.500592]\n",
      "epoch:5 step:4190 [D loss: 0.053736, acc.: 100.00%] [G loss: 1.075136]\n",
      "epoch:5 step:4191 [D loss: 0.065708, acc.: 100.00%] [G loss: 0.365039]\n",
      "epoch:5 step:4192 [D loss: 0.095132, acc.: 99.22%] [G loss: 0.349233]\n",
      "epoch:5 step:4193 [D loss: 0.265576, acc.: 89.06%] [G loss: 3.278948]\n",
      "epoch:5 step:4194 [D loss: 0.610167, acc.: 69.53%] [G loss: 0.233642]\n",
      "epoch:5 step:4195 [D loss: 0.110822, acc.: 95.31%] [G loss: 0.900597]\n",
      "epoch:5 step:4196 [D loss: 0.061612, acc.: 97.66%] [G loss: 2.069172]\n",
      "epoch:5 step:4197 [D loss: 0.103813, acc.: 96.09%] [G loss: 2.123415]\n",
      "epoch:5 step:4198 [D loss: 0.030108, acc.: 100.00%] [G loss: 2.248619]\n",
      "epoch:5 step:4199 [D loss: 0.097830, acc.: 98.44%] [G loss: 1.935470]\n",
      "epoch:5 step:4200 [D loss: 2.096817, acc.: 18.75%] [G loss: 6.957129]\n",
      "epoch:5 step:4201 [D loss: 0.602260, acc.: 67.97%] [G loss: 5.528080]\n",
      "epoch:5 step:4202 [D loss: 0.182208, acc.: 91.41%] [G loss: 2.553735]\n",
      "epoch:5 step:4203 [D loss: 0.157601, acc.: 96.09%] [G loss: 3.561013]\n",
      "epoch:5 step:4204 [D loss: 0.123654, acc.: 96.88%] [G loss: 3.680875]\n",
      "epoch:5 step:4205 [D loss: 0.158220, acc.: 97.66%] [G loss: 3.204703]\n",
      "epoch:5 step:4206 [D loss: 0.087410, acc.: 99.22%] [G loss: 3.761196]\n",
      "epoch:5 step:4207 [D loss: 0.112032, acc.: 96.88%] [G loss: 2.462806]\n",
      "epoch:5 step:4208 [D loss: 0.183612, acc.: 94.53%] [G loss: 3.405729]\n",
      "epoch:5 step:4209 [D loss: 0.063059, acc.: 100.00%] [G loss: 2.504072]\n",
      "epoch:5 step:4210 [D loss: 0.116522, acc.: 98.44%] [G loss: 1.456687]\n",
      "epoch:5 step:4211 [D loss: 0.136450, acc.: 99.22%] [G loss: 1.564606]\n",
      "epoch:5 step:4212 [D loss: 0.285430, acc.: 89.84%] [G loss: 0.987449]\n",
      "epoch:5 step:4213 [D loss: 0.066251, acc.: 100.00%] [G loss: 0.801383]\n",
      "epoch:5 step:4214 [D loss: 0.345232, acc.: 82.81%] [G loss: 1.474420]\n",
      "epoch:5 step:4215 [D loss: 0.184137, acc.: 96.09%] [G loss: 0.792532]\n",
      "epoch:5 step:4216 [D loss: 0.038817, acc.: 100.00%] [G loss: 1.028466]\n",
      "epoch:5 step:4217 [D loss: 0.194001, acc.: 94.53%] [G loss: 1.853701]\n",
      "epoch:5 step:4218 [D loss: 0.117638, acc.: 97.66%] [G loss: 1.010426]\n",
      "epoch:5 step:4219 [D loss: 1.128029, acc.: 49.22%] [G loss: 7.330847]\n",
      "epoch:5 step:4220 [D loss: 1.037452, acc.: 53.91%] [G loss: 5.630160]\n",
      "epoch:5 step:4221 [D loss: 0.312109, acc.: 84.38%] [G loss: 1.595051]\n",
      "epoch:5 step:4222 [D loss: 0.261177, acc.: 87.50%] [G loss: 2.380057]\n",
      "epoch:5 step:4223 [D loss: 0.034035, acc.: 99.22%] [G loss: 2.452214]\n",
      "epoch:5 step:4224 [D loss: 0.345609, acc.: 82.81%] [G loss: 0.968721]\n",
      "epoch:5 step:4225 [D loss: 0.135449, acc.: 96.09%] [G loss: 0.669618]\n",
      "epoch:5 step:4226 [D loss: 0.494946, acc.: 73.44%] [G loss: 4.271301]\n",
      "epoch:5 step:4227 [D loss: 0.516208, acc.: 72.66%] [G loss: 3.439250]\n",
      "epoch:5 step:4228 [D loss: 0.295361, acc.: 91.41%] [G loss: 3.709745]\n",
      "epoch:5 step:4229 [D loss: 0.069325, acc.: 98.44%] [G loss: 3.884524]\n",
      "epoch:5 step:4230 [D loss: 0.162902, acc.: 94.53%] [G loss: 2.711941]\n",
      "epoch:5 step:4231 [D loss: 0.141055, acc.: 96.88%] [G loss: 4.340849]\n",
      "epoch:5 step:4232 [D loss: 0.321909, acc.: 85.16%] [G loss: 4.398702]\n",
      "epoch:5 step:4233 [D loss: 0.306083, acc.: 85.94%] [G loss: 2.280236]\n",
      "epoch:5 step:4234 [D loss: 0.056467, acc.: 100.00%] [G loss: 2.829933]\n",
      "epoch:5 step:4235 [D loss: 0.101659, acc.: 96.09%] [G loss: 3.640556]\n",
      "epoch:5 step:4236 [D loss: 0.295438, acc.: 88.28%] [G loss: 4.846762]\n",
      "epoch:5 step:4237 [D loss: 0.098068, acc.: 97.66%] [G loss: 3.555054]\n",
      "epoch:5 step:4238 [D loss: 0.295971, acc.: 89.06%] [G loss: 1.583154]\n",
      "epoch:5 step:4239 [D loss: 0.076159, acc.: 99.22%] [G loss: 1.566607]\n",
      "epoch:5 step:4240 [D loss: 0.093647, acc.: 99.22%] [G loss: 1.727326]\n",
      "epoch:5 step:4241 [D loss: 0.053124, acc.: 100.00%] [G loss: 1.568848]\n",
      "epoch:5 step:4242 [D loss: 0.073945, acc.: 99.22%] [G loss: 0.652358]\n",
      "epoch:5 step:4243 [D loss: 0.041312, acc.: 100.00%] [G loss: 0.453016]\n",
      "epoch:5 step:4244 [D loss: 0.054044, acc.: 100.00%] [G loss: 0.295717]\n",
      "epoch:5 step:4245 [D loss: 0.221219, acc.: 90.62%] [G loss: 1.270262]\n",
      "epoch:5 step:4246 [D loss: 0.263967, acc.: 86.72%] [G loss: 1.648805]\n",
      "epoch:5 step:4247 [D loss: 0.074587, acc.: 98.44%] [G loss: 0.785278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4248 [D loss: 0.083332, acc.: 98.44%] [G loss: 0.831223]\n",
      "epoch:5 step:4249 [D loss: 0.053641, acc.: 99.22%] [G loss: 0.846892]\n",
      "epoch:5 step:4250 [D loss: 0.203444, acc.: 92.97%] [G loss: 4.834438]\n",
      "epoch:5 step:4251 [D loss: 0.847735, acc.: 51.56%] [G loss: 5.962529]\n",
      "epoch:5 step:4252 [D loss: 0.200535, acc.: 91.41%] [G loss: 4.052993]\n",
      "epoch:5 step:4253 [D loss: 0.105357, acc.: 97.66%] [G loss: 3.359193]\n",
      "epoch:5 step:4254 [D loss: 0.253135, acc.: 89.06%] [G loss: 4.665172]\n",
      "epoch:5 step:4255 [D loss: 1.244560, acc.: 42.19%] [G loss: 4.916220]\n",
      "epoch:5 step:4256 [D loss: 0.031674, acc.: 100.00%] [G loss: 6.218083]\n",
      "epoch:5 step:4257 [D loss: 0.334674, acc.: 83.59%] [G loss: 3.452887]\n",
      "epoch:5 step:4258 [D loss: 0.291082, acc.: 88.28%] [G loss: 3.982697]\n",
      "epoch:5 step:4259 [D loss: 0.034595, acc.: 100.00%] [G loss: 3.496085]\n",
      "epoch:5 step:4260 [D loss: 0.094789, acc.: 99.22%] [G loss: 3.217950]\n",
      "epoch:5 step:4261 [D loss: 0.206357, acc.: 92.97%] [G loss: 3.388479]\n",
      "epoch:5 step:4262 [D loss: 0.090764, acc.: 99.22%] [G loss: 3.809329]\n",
      "epoch:5 step:4263 [D loss: 0.274376, acc.: 91.41%] [G loss: 3.196188]\n",
      "epoch:5 step:4264 [D loss: 0.099302, acc.: 99.22%] [G loss: 3.644869]\n",
      "epoch:5 step:4265 [D loss: 0.340275, acc.: 86.72%] [G loss: 3.616571]\n",
      "epoch:5 step:4266 [D loss: 0.368175, acc.: 84.38%] [G loss: 4.155802]\n",
      "epoch:5 step:4267 [D loss: 0.080426, acc.: 99.22%] [G loss: 3.520554]\n",
      "epoch:5 step:4268 [D loss: 0.087268, acc.: 98.44%] [G loss: 3.574426]\n",
      "epoch:5 step:4269 [D loss: 0.092027, acc.: 98.44%] [G loss: 3.819606]\n",
      "epoch:5 step:4270 [D loss: 0.085420, acc.: 100.00%] [G loss: 2.877637]\n",
      "epoch:5 step:4271 [D loss: 0.741089, acc.: 63.28%] [G loss: 7.025249]\n",
      "epoch:5 step:4272 [D loss: 1.426939, acc.: 50.00%] [G loss: 2.964256]\n",
      "epoch:5 step:4273 [D loss: 0.177063, acc.: 92.19%] [G loss: 4.482058]\n",
      "epoch:5 step:4274 [D loss: 0.040714, acc.: 99.22%] [G loss: 4.581210]\n",
      "epoch:5 step:4275 [D loss: 0.782743, acc.: 62.50%] [G loss: 4.693003]\n",
      "epoch:5 step:4276 [D loss: 0.044211, acc.: 100.00%] [G loss: 5.361132]\n",
      "epoch:5 step:4277 [D loss: 0.334942, acc.: 87.50%] [G loss: 2.799529]\n",
      "epoch:5 step:4278 [D loss: 0.424813, acc.: 77.34%] [G loss: 4.609986]\n",
      "epoch:5 step:4279 [D loss: 0.203007, acc.: 91.41%] [G loss: 4.613640]\n",
      "epoch:5 step:4280 [D loss: 0.223015, acc.: 92.97%] [G loss: 3.434090]\n",
      "epoch:5 step:4281 [D loss: 0.136269, acc.: 96.09%] [G loss: 4.674783]\n",
      "epoch:5 step:4282 [D loss: 0.832763, acc.: 59.38%] [G loss: 3.394581]\n",
      "epoch:5 step:4283 [D loss: 0.164603, acc.: 94.53%] [G loss: 2.946755]\n",
      "epoch:5 step:4284 [D loss: 0.285613, acc.: 86.72%] [G loss: 4.721149]\n",
      "epoch:5 step:4285 [D loss: 0.094251, acc.: 97.66%] [G loss: 4.264499]\n",
      "epoch:5 step:4286 [D loss: 0.291533, acc.: 89.84%] [G loss: 4.129940]\n",
      "epoch:5 step:4287 [D loss: 0.203639, acc.: 92.97%] [G loss: 3.826132]\n",
      "epoch:5 step:4288 [D loss: 0.107195, acc.: 96.88%] [G loss: 3.055675]\n",
      "epoch:5 step:4289 [D loss: 0.266545, acc.: 91.41%] [G loss: 3.339105]\n",
      "epoch:5 step:4290 [D loss: 0.450431, acc.: 79.69%] [G loss: 4.236744]\n",
      "epoch:5 step:4291 [D loss: 0.452184, acc.: 76.56%] [G loss: 4.162960]\n",
      "epoch:5 step:4292 [D loss: 0.162687, acc.: 93.75%] [G loss: 3.646688]\n",
      "epoch:5 step:4293 [D loss: 0.224665, acc.: 93.75%] [G loss: 3.800026]\n",
      "epoch:5 step:4294 [D loss: 0.207273, acc.: 94.53%] [G loss: 3.480479]\n",
      "epoch:5 step:4295 [D loss: 0.215682, acc.: 91.41%] [G loss: 4.723929]\n",
      "epoch:5 step:4296 [D loss: 0.191951, acc.: 95.31%] [G loss: 4.361791]\n",
      "epoch:5 step:4297 [D loss: 0.140741, acc.: 96.88%] [G loss: 4.649986]\n",
      "epoch:5 step:4298 [D loss: 0.240985, acc.: 90.62%] [G loss: 5.129427]\n",
      "epoch:5 step:4299 [D loss: 0.226793, acc.: 90.62%] [G loss: 3.581677]\n",
      "epoch:5 step:4300 [D loss: 0.165431, acc.: 97.66%] [G loss: 4.203750]\n",
      "epoch:5 step:4301 [D loss: 0.202239, acc.: 93.75%] [G loss: 3.763308]\n",
      "epoch:5 step:4302 [D loss: 0.128241, acc.: 97.66%] [G loss: 5.339515]\n",
      "epoch:5 step:4303 [D loss: 0.167768, acc.: 96.09%] [G loss: 4.691433]\n",
      "epoch:5 step:4304 [D loss: 0.099813, acc.: 98.44%] [G loss: 3.779433]\n",
      "epoch:5 step:4305 [D loss: 0.080726, acc.: 97.66%] [G loss: 2.309300]\n",
      "epoch:5 step:4306 [D loss: 0.170507, acc.: 96.88%] [G loss: 1.178268]\n",
      "epoch:5 step:4307 [D loss: 0.273982, acc.: 87.50%] [G loss: 3.339580]\n",
      "epoch:5 step:4308 [D loss: 0.200689, acc.: 89.06%] [G loss: 1.848355]\n",
      "epoch:5 step:4309 [D loss: 0.033418, acc.: 100.00%] [G loss: 0.208768]\n",
      "epoch:5 step:4310 [D loss: 0.219712, acc.: 91.41%] [G loss: 1.915299]\n",
      "epoch:5 step:4311 [D loss: 0.138285, acc.: 95.31%] [G loss: 1.521841]\n",
      "epoch:5 step:4312 [D loss: 0.246404, acc.: 86.72%] [G loss: 6.898285]\n",
      "epoch:5 step:4313 [D loss: 0.596277, acc.: 68.75%] [G loss: 0.658985]\n",
      "epoch:5 step:4314 [D loss: 0.445328, acc.: 79.69%] [G loss: 8.043507]\n",
      "epoch:5 step:4315 [D loss: 0.506460, acc.: 75.00%] [G loss: 4.030019]\n",
      "epoch:5 step:4316 [D loss: 0.360166, acc.: 86.72%] [G loss: 4.637693]\n",
      "epoch:5 step:4317 [D loss: 0.047755, acc.: 99.22%] [G loss: 5.107431]\n",
      "epoch:5 step:4318 [D loss: 0.126406, acc.: 96.88%] [G loss: 4.094733]\n",
      "epoch:5 step:4319 [D loss: 0.833121, acc.: 61.72%] [G loss: 8.505899]\n",
      "epoch:5 step:4320 [D loss: 0.851397, acc.: 59.38%] [G loss: 4.373652]\n",
      "epoch:5 step:4321 [D loss: 0.137223, acc.: 94.53%] [G loss: 3.551339]\n",
      "epoch:5 step:4322 [D loss: 0.158163, acc.: 94.53%] [G loss: 3.506674]\n",
      "epoch:5 step:4323 [D loss: 0.080875, acc.: 99.22%] [G loss: 4.239808]\n",
      "epoch:5 step:4324 [D loss: 0.153076, acc.: 92.97%] [G loss: 3.531174]\n",
      "epoch:5 step:4325 [D loss: 0.431607, acc.: 80.47%] [G loss: 5.023774]\n",
      "epoch:5 step:4326 [D loss: 0.357429, acc.: 82.81%] [G loss: 1.955450]\n",
      "epoch:5 step:4327 [D loss: 0.178142, acc.: 93.75%] [G loss: 3.399892]\n",
      "epoch:5 step:4328 [D loss: 0.051959, acc.: 99.22%] [G loss: 3.755546]\n",
      "epoch:5 step:4329 [D loss: 0.487455, acc.: 78.91%] [G loss: 5.164928]\n",
      "epoch:5 step:4330 [D loss: 0.222635, acc.: 89.06%] [G loss: 3.285695]\n",
      "epoch:5 step:4331 [D loss: 0.115679, acc.: 96.88%] [G loss: 3.084234]\n",
      "epoch:5 step:4332 [D loss: 0.112259, acc.: 96.09%] [G loss: 3.552700]\n",
      "epoch:5 step:4333 [D loss: 0.272613, acc.: 90.62%] [G loss: 2.103192]\n",
      "epoch:5 step:4334 [D loss: 0.195385, acc.: 93.75%] [G loss: 3.977848]\n",
      "epoch:5 step:4335 [D loss: 0.127355, acc.: 96.09%] [G loss: 4.184903]\n",
      "epoch:5 step:4336 [D loss: 0.438118, acc.: 77.34%] [G loss: 3.656540]\n",
      "epoch:5 step:4337 [D loss: 0.089130, acc.: 99.22%] [G loss: 4.513337]\n",
      "epoch:5 step:4338 [D loss: 0.433714, acc.: 75.00%] [G loss: 4.022429]\n",
      "epoch:5 step:4339 [D loss: 0.695318, acc.: 66.41%] [G loss: 4.039355]\n",
      "epoch:5 step:4340 [D loss: 0.079101, acc.: 96.09%] [G loss: 5.222965]\n",
      "epoch:5 step:4341 [D loss: 0.098790, acc.: 99.22%] [G loss: 3.930118]\n",
      "epoch:5 step:4342 [D loss: 0.186231, acc.: 94.53%] [G loss: 3.275068]\n",
      "epoch:5 step:4343 [D loss: 0.092716, acc.: 98.44%] [G loss: 4.292127]\n",
      "epoch:5 step:4344 [D loss: 0.181926, acc.: 93.75%] [G loss: 5.730772]\n",
      "epoch:5 step:4345 [D loss: 0.399124, acc.: 81.25%] [G loss: 3.205225]\n",
      "epoch:5 step:4346 [D loss: 0.086496, acc.: 97.66%] [G loss: 3.252097]\n",
      "epoch:5 step:4347 [D loss: 0.292770, acc.: 85.16%] [G loss: 6.426502]\n",
      "epoch:5 step:4348 [D loss: 0.295764, acc.: 85.94%] [G loss: 3.899973]\n",
      "epoch:5 step:4349 [D loss: 0.343930, acc.: 88.28%] [G loss: 4.076815]\n",
      "epoch:5 step:4350 [D loss: 0.125263, acc.: 97.66%] [G loss: 5.045731]\n",
      "epoch:5 step:4351 [D loss: 0.046177, acc.: 100.00%] [G loss: 3.796936]\n",
      "epoch:5 step:4352 [D loss: 0.177627, acc.: 95.31%] [G loss: 4.660259]\n",
      "epoch:5 step:4353 [D loss: 0.127152, acc.: 95.31%] [G loss: 2.787922]\n",
      "epoch:5 step:4354 [D loss: 0.387564, acc.: 82.81%] [G loss: 4.666889]\n",
      "epoch:5 step:4355 [D loss: 0.134058, acc.: 95.31%] [G loss: 4.561995]\n",
      "epoch:5 step:4356 [D loss: 0.230439, acc.: 87.50%] [G loss: 2.557329]\n",
      "epoch:5 step:4357 [D loss: 0.274047, acc.: 85.94%] [G loss: 5.483782]\n",
      "epoch:5 step:4358 [D loss: 0.244138, acc.: 87.50%] [G loss: 3.401133]\n",
      "epoch:5 step:4359 [D loss: 0.105154, acc.: 96.88%] [G loss: 2.421658]\n",
      "epoch:5 step:4360 [D loss: 0.285168, acc.: 87.50%] [G loss: 6.066585]\n",
      "epoch:5 step:4361 [D loss: 0.154183, acc.: 96.09%] [G loss: 4.349208]\n",
      "epoch:5 step:4362 [D loss: 0.139979, acc.: 95.31%] [G loss: 1.715979]\n",
      "epoch:5 step:4363 [D loss: 0.137012, acc.: 96.09%] [G loss: 3.074413]\n",
      "epoch:5 step:4364 [D loss: 0.035180, acc.: 99.22%] [G loss: 2.634522]\n",
      "epoch:5 step:4365 [D loss: 0.966992, acc.: 46.09%] [G loss: 4.252419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4366 [D loss: 0.087673, acc.: 97.66%] [G loss: 5.321513]\n",
      "epoch:5 step:4367 [D loss: 0.390396, acc.: 82.03%] [G loss: 2.246314]\n",
      "epoch:5 step:4368 [D loss: 0.112075, acc.: 96.88%] [G loss: 2.638268]\n",
      "epoch:5 step:4369 [D loss: 0.102809, acc.: 97.66%] [G loss: 3.359720]\n",
      "epoch:5 step:4370 [D loss: 0.097601, acc.: 97.66%] [G loss: 3.869141]\n",
      "epoch:5 step:4371 [D loss: 0.236127, acc.: 91.41%] [G loss: 3.126297]\n",
      "epoch:5 step:4372 [D loss: 0.199407, acc.: 94.53%] [G loss: 3.230347]\n",
      "epoch:5 step:4373 [D loss: 0.207125, acc.: 92.19%] [G loss: 3.674165]\n",
      "epoch:5 step:4374 [D loss: 0.264807, acc.: 92.97%] [G loss: 5.493823]\n",
      "epoch:5 step:4375 [D loss: 0.221399, acc.: 93.75%] [G loss: 4.060297]\n",
      "epoch:5 step:4376 [D loss: 0.719979, acc.: 62.50%] [G loss: 8.028767]\n",
      "epoch:5 step:4377 [D loss: 0.573301, acc.: 73.44%] [G loss: 4.994967]\n",
      "epoch:5 step:4378 [D loss: 0.388902, acc.: 84.38%] [G loss: 6.779961]\n",
      "epoch:5 step:4379 [D loss: 1.014616, acc.: 50.78%] [G loss: 4.261504]\n",
      "epoch:5 step:4380 [D loss: 0.025909, acc.: 100.00%] [G loss: 5.272524]\n",
      "epoch:5 step:4381 [D loss: 0.044025, acc.: 99.22%] [G loss: 3.492167]\n",
      "epoch:5 step:4382 [D loss: 0.056160, acc.: 99.22%] [G loss: 3.486961]\n",
      "epoch:5 step:4383 [D loss: 0.143372, acc.: 94.53%] [G loss: 3.461304]\n",
      "epoch:5 step:4384 [D loss: 0.118836, acc.: 96.88%] [G loss: 3.360594]\n",
      "epoch:5 step:4385 [D loss: 0.093865, acc.: 98.44%] [G loss: 3.631689]\n",
      "epoch:5 step:4386 [D loss: 0.543227, acc.: 71.88%] [G loss: 5.405813]\n",
      "epoch:5 step:4387 [D loss: 0.235238, acc.: 89.06%] [G loss: 4.424056]\n",
      "epoch:5 step:4388 [D loss: 0.031462, acc.: 100.00%] [G loss: 3.649428]\n",
      "epoch:5 step:4389 [D loss: 0.152741, acc.: 92.97%] [G loss: 4.652435]\n",
      "epoch:5 step:4390 [D loss: 0.234140, acc.: 91.41%] [G loss: 5.199391]\n",
      "epoch:5 step:4391 [D loss: 0.139142, acc.: 95.31%] [G loss: 3.584178]\n",
      "epoch:5 step:4392 [D loss: 0.165747, acc.: 92.97%] [G loss: 4.807448]\n",
      "epoch:5 step:4393 [D loss: 0.081403, acc.: 98.44%] [G loss: 3.292179]\n",
      "epoch:5 step:4394 [D loss: 0.211425, acc.: 92.19%] [G loss: 4.781020]\n",
      "epoch:5 step:4395 [D loss: 0.038905, acc.: 99.22%] [G loss: 4.774989]\n",
      "epoch:5 step:4396 [D loss: 0.334266, acc.: 83.59%] [G loss: 4.961766]\n",
      "epoch:5 step:4397 [D loss: 0.234215, acc.: 92.19%] [G loss: 3.619470]\n",
      "epoch:5 step:4398 [D loss: 0.043368, acc.: 100.00%] [G loss: 1.781557]\n",
      "epoch:5 step:4399 [D loss: 0.057506, acc.: 98.44%] [G loss: 0.578054]\n",
      "epoch:5 step:4400 [D loss: 0.244071, acc.: 90.62%] [G loss: 3.753917]\n",
      "epoch:5 step:4401 [D loss: 0.174879, acc.: 92.97%] [G loss: 3.619204]\n",
      "epoch:5 step:4402 [D loss: 0.088714, acc.: 98.44%] [G loss: 1.086347]\n",
      "epoch:5 step:4403 [D loss: 0.113187, acc.: 97.66%] [G loss: 0.253735]\n",
      "epoch:5 step:4404 [D loss: 0.056408, acc.: 99.22%] [G loss: 0.419123]\n",
      "epoch:5 step:4405 [D loss: 0.090891, acc.: 96.88%] [G loss: 1.131317]\n",
      "epoch:5 step:4406 [D loss: 0.491855, acc.: 75.78%] [G loss: 6.698001]\n",
      "epoch:5 step:4407 [D loss: 0.765885, acc.: 64.06%] [G loss: 2.169059]\n",
      "epoch:5 step:4408 [D loss: 0.246403, acc.: 91.41%] [G loss: 4.889726]\n",
      "epoch:5 step:4409 [D loss: 0.024957, acc.: 100.00%] [G loss: 6.030417]\n",
      "epoch:5 step:4410 [D loss: 0.150491, acc.: 94.53%] [G loss: 4.125972]\n",
      "epoch:5 step:4411 [D loss: 0.250800, acc.: 87.50%] [G loss: 5.638067]\n",
      "epoch:5 step:4412 [D loss: 0.219172, acc.: 87.50%] [G loss: 3.521338]\n",
      "epoch:5 step:4413 [D loss: 0.101273, acc.: 97.66%] [G loss: 3.410243]\n",
      "epoch:5 step:4414 [D loss: 0.122214, acc.: 95.31%] [G loss: 0.629283]\n",
      "epoch:5 step:4415 [D loss: 0.224960, acc.: 89.06%] [G loss: 3.685033]\n",
      "epoch:5 step:4416 [D loss: 0.166296, acc.: 92.97%] [G loss: 2.166207]\n",
      "epoch:5 step:4417 [D loss: 0.075295, acc.: 97.66%] [G loss: 1.831349]\n",
      "epoch:5 step:4418 [D loss: 0.035075, acc.: 100.00%] [G loss: 1.356764]\n",
      "epoch:5 step:4419 [D loss: 0.058246, acc.: 99.22%] [G loss: 0.697236]\n",
      "epoch:5 step:4420 [D loss: 0.062924, acc.: 99.22%] [G loss: 0.266154]\n",
      "epoch:5 step:4421 [D loss: 0.117250, acc.: 95.31%] [G loss: 0.344927]\n",
      "epoch:5 step:4422 [D loss: 0.223890, acc.: 91.41%] [G loss: 1.436587]\n",
      "epoch:5 step:4423 [D loss: 0.046042, acc.: 99.22%] [G loss: 2.737850]\n",
      "epoch:5 step:4424 [D loss: 0.050243, acc.: 99.22%] [G loss: 0.563993]\n",
      "epoch:5 step:4425 [D loss: 0.311469, acc.: 82.81%] [G loss: 5.419833]\n",
      "epoch:5 step:4426 [D loss: 0.385644, acc.: 79.69%] [G loss: 1.770581]\n",
      "epoch:5 step:4427 [D loss: 0.094925, acc.: 96.09%] [G loss: 1.453392]\n",
      "epoch:5 step:4428 [D loss: 0.197365, acc.: 90.62%] [G loss: 5.469591]\n",
      "epoch:5 step:4429 [D loss: 2.484141, acc.: 13.28%] [G loss: 8.149427]\n",
      "epoch:5 step:4430 [D loss: 0.086702, acc.: 97.66%] [G loss: 7.521141]\n",
      "epoch:5 step:4431 [D loss: 0.748234, acc.: 57.81%] [G loss: 3.313321]\n",
      "epoch:5 step:4432 [D loss: 0.303765, acc.: 84.38%] [G loss: 5.249237]\n",
      "epoch:5 step:4433 [D loss: 0.039079, acc.: 100.00%] [G loss: 6.240692]\n",
      "epoch:5 step:4434 [D loss: 0.247845, acc.: 89.84%] [G loss: 3.122719]\n",
      "epoch:5 step:4435 [D loss: 0.188759, acc.: 91.41%] [G loss: 3.332420]\n",
      "epoch:5 step:4436 [D loss: 0.249642, acc.: 90.62%] [G loss: 3.863647]\n",
      "epoch:5 step:4437 [D loss: 0.926404, acc.: 50.78%] [G loss: 3.456370]\n",
      "epoch:5 step:4438 [D loss: 0.312696, acc.: 86.72%] [G loss: 1.811413]\n",
      "epoch:5 step:4439 [D loss: 0.124943, acc.: 96.09%] [G loss: 2.259485]\n",
      "epoch:5 step:4440 [D loss: 0.075478, acc.: 99.22%] [G loss: 2.633388]\n",
      "epoch:5 step:4441 [D loss: 0.195092, acc.: 94.53%] [G loss: 3.268502]\n",
      "epoch:5 step:4442 [D loss: 0.275850, acc.: 92.97%] [G loss: 0.964254]\n",
      "epoch:5 step:4443 [D loss: 0.258697, acc.: 89.06%] [G loss: 2.919599]\n",
      "epoch:5 step:4444 [D loss: 0.307896, acc.: 85.16%] [G loss: 2.430263]\n",
      "epoch:5 step:4445 [D loss: 0.060202, acc.: 99.22%] [G loss: 0.726506]\n",
      "epoch:5 step:4446 [D loss: 0.102532, acc.: 96.88%] [G loss: 2.395495]\n",
      "epoch:5 step:4447 [D loss: 0.258477, acc.: 88.28%] [G loss: 2.798771]\n",
      "epoch:5 step:4448 [D loss: 0.125420, acc.: 96.88%] [G loss: 2.090684]\n",
      "epoch:5 step:4449 [D loss: 0.388639, acc.: 82.81%] [G loss: 2.795623]\n",
      "epoch:5 step:4450 [D loss: 0.060766, acc.: 99.22%] [G loss: 4.595534]\n",
      "epoch:5 step:4451 [D loss: 0.458299, acc.: 82.81%] [G loss: 1.481248]\n",
      "epoch:5 step:4452 [D loss: 0.226174, acc.: 87.50%] [G loss: 3.522989]\n",
      "epoch:5 step:4453 [D loss: 0.318894, acc.: 87.50%] [G loss: 2.721801]\n",
      "epoch:5 step:4454 [D loss: 0.104719, acc.: 97.66%] [G loss: 2.942821]\n",
      "epoch:5 step:4455 [D loss: 0.050876, acc.: 100.00%] [G loss: 3.134382]\n",
      "epoch:5 step:4456 [D loss: 0.090207, acc.: 97.66%] [G loss: 2.864824]\n",
      "epoch:5 step:4457 [D loss: 1.332958, acc.: 41.41%] [G loss: 7.948729]\n",
      "epoch:5 step:4458 [D loss: 0.431859, acc.: 73.44%] [G loss: 6.410343]\n",
      "epoch:5 step:4459 [D loss: 0.205349, acc.: 91.41%] [G loss: 4.970870]\n",
      "epoch:5 step:4460 [D loss: 0.058565, acc.: 100.00%] [G loss: 2.125954]\n",
      "epoch:5 step:4461 [D loss: 0.124224, acc.: 95.31%] [G loss: 3.848332]\n",
      "epoch:5 step:4462 [D loss: 0.056498, acc.: 100.00%] [G loss: 3.837140]\n",
      "epoch:5 step:4463 [D loss: 0.208863, acc.: 92.19%] [G loss: 4.912316]\n",
      "epoch:5 step:4464 [D loss: 0.120911, acc.: 95.31%] [G loss: 1.878605]\n",
      "epoch:5 step:4465 [D loss: 0.111351, acc.: 97.66%] [G loss: 1.562647]\n",
      "epoch:5 step:4466 [D loss: 0.129552, acc.: 96.88%] [G loss: 1.085075]\n",
      "epoch:5 step:4467 [D loss: 0.324774, acc.: 85.94%] [G loss: 4.447259]\n",
      "epoch:5 step:4468 [D loss: 0.089500, acc.: 97.66%] [G loss: 2.487327]\n",
      "epoch:5 step:4469 [D loss: 0.176460, acc.: 95.31%] [G loss: 1.424343]\n",
      "epoch:5 step:4470 [D loss: 0.099218, acc.: 97.66%] [G loss: 0.384466]\n",
      "epoch:5 step:4471 [D loss: 0.351662, acc.: 82.81%] [G loss: 2.021839]\n",
      "epoch:5 step:4472 [D loss: 0.327472, acc.: 83.59%] [G loss: 0.727446]\n",
      "epoch:5 step:4473 [D loss: 0.049567, acc.: 99.22%] [G loss: 1.011020]\n",
      "epoch:5 step:4474 [D loss: 0.157513, acc.: 93.75%] [G loss: 2.659415]\n",
      "epoch:5 step:4475 [D loss: 0.060098, acc.: 98.44%] [G loss: 1.545897]\n",
      "epoch:5 step:4476 [D loss: 0.203570, acc.: 92.19%] [G loss: 0.942796]\n",
      "epoch:5 step:4477 [D loss: 0.052246, acc.: 100.00%] [G loss: 2.416278]\n",
      "epoch:5 step:4478 [D loss: 0.169196, acc.: 95.31%] [G loss: 2.828085]\n",
      "epoch:5 step:4479 [D loss: 0.070421, acc.: 99.22%] [G loss: 3.748992]\n",
      "epoch:5 step:4480 [D loss: 0.137706, acc.: 96.88%] [G loss: 1.113062]\n",
      "epoch:5 step:4481 [D loss: 0.115685, acc.: 97.66%] [G loss: 2.297520]\n",
      "epoch:5 step:4482 [D loss: 0.079225, acc.: 99.22%] [G loss: 2.992402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4483 [D loss: 0.318788, acc.: 88.28%] [G loss: 5.475376]\n",
      "epoch:5 step:4484 [D loss: 0.068469, acc.: 98.44%] [G loss: 5.184557]\n",
      "epoch:5 step:4485 [D loss: 1.406986, acc.: 42.97%] [G loss: 8.770140]\n",
      "epoch:5 step:4486 [D loss: 0.618212, acc.: 69.53%] [G loss: 3.803153]\n",
      "epoch:5 step:4487 [D loss: 0.118409, acc.: 96.09%] [G loss: 4.497059]\n",
      "epoch:5 step:4488 [D loss: 0.055478, acc.: 98.44%] [G loss: 4.421254]\n",
      "epoch:5 step:4489 [D loss: 0.245311, acc.: 91.41%] [G loss: 4.715634]\n",
      "epoch:5 step:4490 [D loss: 0.107097, acc.: 97.66%] [G loss: 4.949964]\n",
      "epoch:5 step:4491 [D loss: 0.150814, acc.: 95.31%] [G loss: 4.092532]\n",
      "epoch:5 step:4492 [D loss: 0.260308, acc.: 92.19%] [G loss: 4.976441]\n",
      "epoch:5 step:4493 [D loss: 0.404664, acc.: 82.03%] [G loss: 5.951026]\n",
      "epoch:5 step:4494 [D loss: 0.490120, acc.: 79.69%] [G loss: 2.784538]\n",
      "epoch:5 step:4495 [D loss: 0.234487, acc.: 89.06%] [G loss: 5.209786]\n",
      "epoch:5 step:4496 [D loss: 0.095254, acc.: 96.09%] [G loss: 6.164102]\n",
      "epoch:5 step:4497 [D loss: 0.126449, acc.: 97.66%] [G loss: 3.883072]\n",
      "epoch:5 step:4498 [D loss: 0.041058, acc.: 100.00%] [G loss: 2.432161]\n",
      "epoch:5 step:4499 [D loss: 0.073303, acc.: 98.44%] [G loss: 1.481674]\n",
      "epoch:5 step:4500 [D loss: 0.101664, acc.: 96.88%] [G loss: 1.834895]\n",
      "epoch:5 step:4501 [D loss: 0.196448, acc.: 96.09%] [G loss: 1.215958]\n",
      "epoch:5 step:4502 [D loss: 0.108682, acc.: 98.44%] [G loss: 1.346639]\n",
      "epoch:5 step:4503 [D loss: 0.092973, acc.: 97.66%] [G loss: 1.139650]\n",
      "epoch:5 step:4504 [D loss: 0.192260, acc.: 93.75%] [G loss: 1.277381]\n",
      "epoch:5 step:4505 [D loss: 0.151378, acc.: 95.31%] [G loss: 0.643816]\n",
      "epoch:5 step:4506 [D loss: 0.080783, acc.: 97.66%] [G loss: 1.781711]\n",
      "epoch:5 step:4507 [D loss: 0.013951, acc.: 100.00%] [G loss: 0.332546]\n",
      "epoch:5 step:4508 [D loss: 0.616699, acc.: 71.88%] [G loss: 6.198831]\n",
      "epoch:5 step:4509 [D loss: 0.747410, acc.: 62.50%] [G loss: 2.989461]\n",
      "epoch:5 step:4510 [D loss: 0.202423, acc.: 92.97%] [G loss: 3.282289]\n",
      "epoch:5 step:4511 [D loss: 0.021786, acc.: 100.00%] [G loss: 3.709289]\n",
      "epoch:5 step:4512 [D loss: 0.356173, acc.: 82.03%] [G loss: 3.367749]\n",
      "epoch:5 step:4513 [D loss: 0.263618, acc.: 88.28%] [G loss: 2.305202]\n",
      "epoch:5 step:4514 [D loss: 0.283126, acc.: 89.84%] [G loss: 4.313730]\n",
      "epoch:5 step:4515 [D loss: 0.360545, acc.: 82.03%] [G loss: 1.967457]\n",
      "epoch:5 step:4516 [D loss: 0.068804, acc.: 99.22%] [G loss: 2.574450]\n",
      "epoch:5 step:4517 [D loss: 0.096114, acc.: 96.09%] [G loss: 5.361753]\n",
      "epoch:5 step:4518 [D loss: 0.172635, acc.: 92.19%] [G loss: 2.942067]\n",
      "epoch:5 step:4519 [D loss: 0.190028, acc.: 93.75%] [G loss: 6.079731]\n",
      "epoch:5 step:4520 [D loss: 0.196803, acc.: 89.84%] [G loss: 2.957477]\n",
      "epoch:5 step:4521 [D loss: 0.030822, acc.: 100.00%] [G loss: 2.236845]\n",
      "epoch:5 step:4522 [D loss: 0.023826, acc.: 100.00%] [G loss: 0.826542]\n",
      "epoch:5 step:4523 [D loss: 0.010105, acc.: 100.00%] [G loss: 0.361845]\n",
      "epoch:5 step:4524 [D loss: 0.079492, acc.: 99.22%] [G loss: 0.739875]\n",
      "epoch:5 step:4525 [D loss: 0.010312, acc.: 100.00%] [G loss: 0.651908]\n",
      "epoch:5 step:4526 [D loss: 0.036198, acc.: 100.00%] [G loss: 0.803284]\n",
      "epoch:5 step:4527 [D loss: 0.064707, acc.: 99.22%] [G loss: 0.074317]\n",
      "epoch:5 step:4528 [D loss: 0.339363, acc.: 84.38%] [G loss: 3.298346]\n",
      "epoch:5 step:4529 [D loss: 0.247916, acc.: 85.94%] [G loss: 3.342216]\n",
      "epoch:5 step:4530 [D loss: 0.127810, acc.: 94.53%] [G loss: 0.799921]\n",
      "epoch:5 step:4531 [D loss: 0.151654, acc.: 93.75%] [G loss: 2.354275]\n",
      "epoch:5 step:4532 [D loss: 0.055119, acc.: 100.00%] [G loss: 1.703389]\n",
      "epoch:5 step:4533 [D loss: 0.847967, acc.: 57.81%] [G loss: 5.476160]\n",
      "epoch:5 step:4534 [D loss: 0.198220, acc.: 92.19%] [G loss: 5.720246]\n",
      "epoch:5 step:4535 [D loss: 0.106830, acc.: 95.31%] [G loss: 2.442909]\n",
      "epoch:5 step:4536 [D loss: 0.011938, acc.: 100.00%] [G loss: 1.598004]\n",
      "epoch:5 step:4537 [D loss: 0.017403, acc.: 100.00%] [G loss: 1.097236]\n",
      "epoch:5 step:4538 [D loss: 0.496691, acc.: 73.44%] [G loss: 5.400388]\n",
      "epoch:5 step:4539 [D loss: 0.549610, acc.: 71.88%] [G loss: 1.954443]\n",
      "epoch:5 step:4540 [D loss: 0.120750, acc.: 93.75%] [G loss: 2.153060]\n",
      "epoch:5 step:4541 [D loss: 0.026920, acc.: 100.00%] [G loss: 2.108749]\n",
      "epoch:5 step:4542 [D loss: 0.312212, acc.: 82.81%] [G loss: 4.736110]\n",
      "epoch:5 step:4543 [D loss: 0.245610, acc.: 91.41%] [G loss: 3.037016]\n",
      "epoch:5 step:4544 [D loss: 0.333418, acc.: 86.72%] [G loss: 5.593622]\n",
      "epoch:5 step:4545 [D loss: 0.101358, acc.: 99.22%] [G loss: 4.901091]\n",
      "epoch:5 step:4546 [D loss: 0.096728, acc.: 99.22%] [G loss: 3.093293]\n",
      "epoch:5 step:4547 [D loss: 0.055098, acc.: 99.22%] [G loss: 1.741683]\n",
      "epoch:5 step:4548 [D loss: 0.139610, acc.: 96.09%] [G loss: 3.897935]\n",
      "epoch:5 step:4549 [D loss: 0.052554, acc.: 100.00%] [G loss: 3.787636]\n",
      "epoch:5 step:4550 [D loss: 0.074788, acc.: 99.22%] [G loss: 1.509326]\n",
      "epoch:5 step:4551 [D loss: 0.172604, acc.: 96.88%] [G loss: 0.859474]\n",
      "epoch:5 step:4552 [D loss: 0.086265, acc.: 98.44%] [G loss: 1.610538]\n",
      "epoch:5 step:4553 [D loss: 0.066055, acc.: 97.66%] [G loss: 0.918256]\n",
      "epoch:5 step:4554 [D loss: 0.035566, acc.: 100.00%] [G loss: 0.335644]\n",
      "epoch:5 step:4555 [D loss: 0.276112, acc.: 86.72%] [G loss: 4.694880]\n",
      "epoch:5 step:4556 [D loss: 0.113984, acc.: 96.09%] [G loss: 5.165676]\n",
      "epoch:5 step:4557 [D loss: 0.725302, acc.: 65.62%] [G loss: 3.488463]\n",
      "epoch:5 step:4558 [D loss: 0.052427, acc.: 98.44%] [G loss: 4.362296]\n",
      "epoch:5 step:4559 [D loss: 0.076706, acc.: 96.88%] [G loss: 1.149398]\n",
      "epoch:5 step:4560 [D loss: 0.048667, acc.: 99.22%] [G loss: 1.286321]\n",
      "epoch:5 step:4561 [D loss: 0.062602, acc.: 99.22%] [G loss: 1.351785]\n",
      "epoch:5 step:4562 [D loss: 0.111308, acc.: 96.88%] [G loss: 3.593179]\n",
      "epoch:5 step:4563 [D loss: 0.137743, acc.: 93.75%] [G loss: 1.073863]\n",
      "epoch:5 step:4564 [D loss: 0.020548, acc.: 100.00%] [G loss: 0.456292]\n",
      "epoch:5 step:4565 [D loss: 0.374335, acc.: 78.12%] [G loss: 7.314265]\n",
      "epoch:5 step:4566 [D loss: 0.839117, acc.: 62.50%] [G loss: 2.481740]\n",
      "epoch:5 step:4567 [D loss: 0.145817, acc.: 94.53%] [G loss: 3.246459]\n",
      "epoch:5 step:4568 [D loss: 0.010548, acc.: 100.00%] [G loss: 2.964230]\n",
      "epoch:5 step:4569 [D loss: 0.033393, acc.: 100.00%] [G loss: 2.148342]\n",
      "epoch:5 step:4570 [D loss: 0.359695, acc.: 83.59%] [G loss: 5.422943]\n",
      "epoch:5 step:4571 [D loss: 0.393637, acc.: 85.16%] [G loss: 3.628904]\n",
      "epoch:5 step:4572 [D loss: 0.249144, acc.: 88.28%] [G loss: 4.963914]\n",
      "epoch:5 step:4573 [D loss: 0.107611, acc.: 96.09%] [G loss: 2.966986]\n",
      "epoch:5 step:4574 [D loss: 0.241231, acc.: 89.84%] [G loss: 2.209979]\n",
      "epoch:5 step:4575 [D loss: 0.037687, acc.: 100.00%] [G loss: 3.372836]\n",
      "epoch:5 step:4576 [D loss: 0.343784, acc.: 85.94%] [G loss: 3.815395]\n",
      "epoch:5 step:4577 [D loss: 0.511386, acc.: 71.88%] [G loss: 5.872514]\n",
      "epoch:5 step:4578 [D loss: 0.046825, acc.: 100.00%] [G loss: 5.805179]\n",
      "epoch:5 step:4579 [D loss: 0.111853, acc.: 96.09%] [G loss: 2.101202]\n",
      "epoch:5 step:4580 [D loss: 0.221144, acc.: 87.50%] [G loss: 4.366111]\n",
      "epoch:5 step:4581 [D loss: 0.145550, acc.: 93.75%] [G loss: 4.979486]\n",
      "epoch:5 step:4582 [D loss: 0.093154, acc.: 98.44%] [G loss: 4.998121]\n",
      "epoch:5 step:4583 [D loss: 0.192130, acc.: 94.53%] [G loss: 2.427541]\n",
      "epoch:5 step:4584 [D loss: 0.057187, acc.: 99.22%] [G loss: 2.536443]\n",
      "epoch:5 step:4585 [D loss: 0.037108, acc.: 100.00%] [G loss: 0.828152]\n",
      "epoch:5 step:4586 [D loss: 0.114049, acc.: 96.88%] [G loss: 1.077703]\n",
      "epoch:5 step:4587 [D loss: 0.027283, acc.: 100.00%] [G loss: 2.510719]\n",
      "epoch:5 step:4588 [D loss: 0.245563, acc.: 86.72%] [G loss: 3.620772]\n",
      "epoch:5 step:4589 [D loss: 0.512673, acc.: 73.44%] [G loss: 0.126276]\n",
      "epoch:5 step:4590 [D loss: 0.065127, acc.: 98.44%] [G loss: 0.370077]\n",
      "epoch:5 step:4591 [D loss: 0.010037, acc.: 100.00%] [G loss: 0.802722]\n",
      "epoch:5 step:4592 [D loss: 0.032609, acc.: 99.22%] [G loss: 0.158922]\n",
      "epoch:5 step:4593 [D loss: 0.006675, acc.: 100.00%] [G loss: 0.584086]\n",
      "epoch:5 step:4594 [D loss: 0.013984, acc.: 100.00%] [G loss: 0.239051]\n",
      "epoch:5 step:4595 [D loss: 0.012887, acc.: 100.00%] [G loss: 0.265168]\n",
      "epoch:5 step:4596 [D loss: 0.085980, acc.: 96.88%] [G loss: 0.306299]\n",
      "epoch:5 step:4597 [D loss: 0.163560, acc.: 92.97%] [G loss: 0.215577]\n",
      "epoch:5 step:4598 [D loss: 0.027292, acc.: 99.22%] [G loss: 0.076713]\n",
      "epoch:5 step:4599 [D loss: 0.002840, acc.: 100.00%] [G loss: 0.384670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4600 [D loss: 0.307201, acc.: 83.59%] [G loss: 6.306193]\n",
      "epoch:5 step:4601 [D loss: 0.546130, acc.: 77.34%] [G loss: 1.662173]\n",
      "epoch:5 step:4602 [D loss: 0.007487, acc.: 100.00%] [G loss: 1.145977]\n",
      "epoch:5 step:4603 [D loss: 0.201987, acc.: 92.97%] [G loss: 3.078987]\n",
      "epoch:5 step:4604 [D loss: 0.020890, acc.: 100.00%] [G loss: 2.820746]\n",
      "epoch:5 step:4605 [D loss: 0.828503, acc.: 50.00%] [G loss: 5.414520]\n",
      "epoch:5 step:4606 [D loss: 0.145340, acc.: 96.09%] [G loss: 4.017788]\n",
      "epoch:5 step:4607 [D loss: 0.321208, acc.: 86.72%] [G loss: 0.153494]\n",
      "epoch:5 step:4608 [D loss: 0.645479, acc.: 71.09%] [G loss: 5.560325]\n",
      "epoch:5 step:4609 [D loss: 0.713212, acc.: 70.31%] [G loss: 1.557534]\n",
      "epoch:5 step:4610 [D loss: 0.142650, acc.: 92.19%] [G loss: 2.528826]\n",
      "epoch:5 step:4611 [D loss: 0.071958, acc.: 99.22%] [G loss: 2.769198]\n",
      "epoch:5 step:4612 [D loss: 0.058265, acc.: 99.22%] [G loss: 2.129524]\n",
      "epoch:5 step:4613 [D loss: 0.199143, acc.: 92.97%] [G loss: 4.025756]\n",
      "epoch:5 step:4614 [D loss: 0.060367, acc.: 100.00%] [G loss: 4.055748]\n",
      "epoch:5 step:4615 [D loss: 0.192800, acc.: 89.84%] [G loss: 2.658204]\n",
      "epoch:5 step:4616 [D loss: 0.164131, acc.: 96.88%] [G loss: 4.828400]\n",
      "epoch:5 step:4617 [D loss: 0.040781, acc.: 99.22%] [G loss: 5.408779]\n",
      "epoch:5 step:4618 [D loss: 1.592035, acc.: 26.56%] [G loss: 7.698204]\n",
      "epoch:5 step:4619 [D loss: 0.072341, acc.: 97.66%] [G loss: 8.725163]\n",
      "epoch:5 step:4620 [D loss: 0.209009, acc.: 91.41%] [G loss: 7.431509]\n",
      "epoch:5 step:4621 [D loss: 0.011323, acc.: 100.00%] [G loss: 6.907313]\n",
      "epoch:5 step:4622 [D loss: 0.008654, acc.: 100.00%] [G loss: 5.536490]\n",
      "epoch:5 step:4623 [D loss: 0.009350, acc.: 100.00%] [G loss: 4.211102]\n",
      "epoch:5 step:4624 [D loss: 0.041245, acc.: 99.22%] [G loss: 3.107002]\n",
      "epoch:5 step:4625 [D loss: 0.005384, acc.: 100.00%] [G loss: 2.099962]\n",
      "epoch:5 step:4626 [D loss: 0.036695, acc.: 99.22%] [G loss: 0.247865]\n",
      "epoch:5 step:4627 [D loss: 0.031194, acc.: 100.00%] [G loss: 0.338275]\n",
      "epoch:5 step:4628 [D loss: 0.011807, acc.: 100.00%] [G loss: 0.053657]\n",
      "epoch:5 step:4629 [D loss: 0.271830, acc.: 87.50%] [G loss: 1.931944]\n",
      "epoch:5 step:4630 [D loss: 0.181988, acc.: 92.97%] [G loss: 1.136986]\n",
      "epoch:5 step:4631 [D loss: 0.088080, acc.: 97.66%] [G loss: 0.263798]\n",
      "epoch:5 step:4632 [D loss: 0.041531, acc.: 100.00%] [G loss: 0.272492]\n",
      "epoch:5 step:4633 [D loss: 0.125255, acc.: 94.53%] [G loss: 0.891040]\n",
      "epoch:5 step:4634 [D loss: 0.030286, acc.: 100.00%] [G loss: 0.416158]\n",
      "epoch:5 step:4635 [D loss: 0.026762, acc.: 99.22%] [G loss: 0.783068]\n",
      "epoch:5 step:4636 [D loss: 0.273945, acc.: 91.41%] [G loss: 0.424120]\n",
      "epoch:5 step:4637 [D loss: 0.090375, acc.: 96.88%] [G loss: 1.829241]\n",
      "epoch:5 step:4638 [D loss: 0.725212, acc.: 64.84%] [G loss: 0.742583]\n",
      "epoch:5 step:4639 [D loss: 0.005632, acc.: 100.00%] [G loss: 0.924873]\n",
      "epoch:5 step:4640 [D loss: 1.018013, acc.: 53.12%] [G loss: 5.566094]\n",
      "epoch:5 step:4641 [D loss: 0.665033, acc.: 65.62%] [G loss: 3.780758]\n",
      "epoch:5 step:4642 [D loss: 0.151265, acc.: 96.88%] [G loss: 3.535688]\n",
      "epoch:5 step:4643 [D loss: 0.110983, acc.: 96.09%] [G loss: 2.949321]\n",
      "epoch:5 step:4644 [D loss: 0.051327, acc.: 100.00%] [G loss: 3.851783]\n",
      "epoch:5 step:4645 [D loss: 0.145767, acc.: 96.88%] [G loss: 3.872033]\n",
      "epoch:5 step:4646 [D loss: 0.189474, acc.: 95.31%] [G loss: 4.588572]\n",
      "epoch:5 step:4647 [D loss: 0.258207, acc.: 87.50%] [G loss: 3.041730]\n",
      "epoch:5 step:4648 [D loss: 0.067022, acc.: 100.00%] [G loss: 3.060071]\n",
      "epoch:5 step:4649 [D loss: 0.204575, acc.: 92.97%] [G loss: 2.714498]\n",
      "epoch:5 step:4650 [D loss: 0.187136, acc.: 92.97%] [G loss: 0.656931]\n",
      "epoch:5 step:4651 [D loss: 0.052456, acc.: 100.00%] [G loss: 0.542100]\n",
      "epoch:5 step:4652 [D loss: 0.208479, acc.: 90.62%] [G loss: 1.733105]\n",
      "epoch:5 step:4653 [D loss: 0.155352, acc.: 94.53%] [G loss: 1.259973]\n",
      "epoch:5 step:4654 [D loss: 0.086846, acc.: 99.22%] [G loss: 0.406985]\n",
      "epoch:5 step:4655 [D loss: 0.075406, acc.: 99.22%] [G loss: 0.554916]\n",
      "epoch:5 step:4656 [D loss: 0.065264, acc.: 99.22%] [G loss: 1.141073]\n",
      "epoch:5 step:4657 [D loss: 0.018222, acc.: 100.00%] [G loss: 0.718897]\n",
      "epoch:5 step:4658 [D loss: 0.092804, acc.: 98.44%] [G loss: 0.300198]\n",
      "epoch:5 step:4659 [D loss: 0.142217, acc.: 96.09%] [G loss: 0.357133]\n",
      "epoch:5 step:4660 [D loss: 0.080311, acc.: 100.00%] [G loss: 1.722829]\n",
      "epoch:5 step:4661 [D loss: 0.097811, acc.: 98.44%] [G loss: 1.973387]\n",
      "epoch:5 step:4662 [D loss: 0.312778, acc.: 88.28%] [G loss: 0.628878]\n",
      "epoch:5 step:4663 [D loss: 0.151520, acc.: 95.31%] [G loss: 2.932181]\n",
      "epoch:5 step:4664 [D loss: 0.711616, acc.: 67.97%] [G loss: 5.229115]\n",
      "epoch:5 step:4665 [D loss: 0.044311, acc.: 98.44%] [G loss: 5.839324]\n",
      "epoch:5 step:4666 [D loss: 0.029087, acc.: 100.00%] [G loss: 4.672750]\n",
      "epoch:5 step:4667 [D loss: 0.909620, acc.: 60.16%] [G loss: 5.626748]\n",
      "epoch:5 step:4668 [D loss: 0.089907, acc.: 96.09%] [G loss: 6.085661]\n",
      "epoch:5 step:4669 [D loss: 0.110859, acc.: 96.88%] [G loss: 4.625760]\n",
      "epoch:5 step:4670 [D loss: 0.133910, acc.: 95.31%] [G loss: 3.935863]\n",
      "epoch:5 step:4671 [D loss: 0.150536, acc.: 96.88%] [G loss: 4.712208]\n",
      "epoch:5 step:4672 [D loss: 0.147567, acc.: 96.88%] [G loss: 4.194645]\n",
      "epoch:5 step:4673 [D loss: 0.330787, acc.: 85.16%] [G loss: 2.722002]\n",
      "epoch:5 step:4674 [D loss: 0.193796, acc.: 92.97%] [G loss: 5.765834]\n",
      "epoch:5 step:4675 [D loss: 0.293146, acc.: 85.94%] [G loss: 2.405506]\n",
      "epoch:5 step:4676 [D loss: 0.195214, acc.: 93.75%] [G loss: 2.948147]\n",
      "epoch:5 step:4677 [D loss: 0.081886, acc.: 98.44%] [G loss: 5.187469]\n",
      "epoch:5 step:4678 [D loss: 0.118173, acc.: 96.88%] [G loss: 2.690013]\n",
      "epoch:5 step:4679 [D loss: 0.193550, acc.: 93.75%] [G loss: 3.297692]\n",
      "epoch:5 step:4680 [D loss: 0.129699, acc.: 95.31%] [G loss: 2.911010]\n",
      "epoch:5 step:4681 [D loss: 0.148104, acc.: 96.09%] [G loss: 2.289059]\n",
      "epoch:5 step:4682 [D loss: 0.785668, acc.: 60.16%] [G loss: 9.080200]\n",
      "epoch:5 step:4683 [D loss: 1.123996, acc.: 56.25%] [G loss: 3.757080]\n",
      "epoch:5 step:4684 [D loss: 0.063885, acc.: 100.00%] [G loss: 1.757112]\n",
      "epoch:5 step:4685 [D loss: 0.207255, acc.: 90.62%] [G loss: 2.231351]\n",
      "epoch:5 step:4686 [D loss: 0.009957, acc.: 100.00%] [G loss: 2.900056]\n",
      "epoch:6 step:4687 [D loss: 0.053590, acc.: 99.22%] [G loss: 1.628603]\n",
      "epoch:6 step:4688 [D loss: 0.109157, acc.: 98.44%] [G loss: 1.037895]\n",
      "epoch:6 step:4689 [D loss: 0.136241, acc.: 95.31%] [G loss: 1.284199]\n",
      "epoch:6 step:4690 [D loss: 0.071811, acc.: 100.00%] [G loss: 1.826671]\n",
      "epoch:6 step:4691 [D loss: 1.062960, acc.: 50.00%] [G loss: 5.744367]\n",
      "epoch:6 step:4692 [D loss: 0.320111, acc.: 81.25%] [G loss: 4.700846]\n",
      "epoch:6 step:4693 [D loss: 0.068776, acc.: 99.22%] [G loss: 3.226294]\n",
      "epoch:6 step:4694 [D loss: 0.117270, acc.: 98.44%] [G loss: 2.715305]\n",
      "epoch:6 step:4695 [D loss: 0.252518, acc.: 89.84%] [G loss: 4.747087]\n",
      "epoch:6 step:4696 [D loss: 0.143799, acc.: 96.88%] [G loss: 2.485950]\n",
      "epoch:6 step:4697 [D loss: 0.234284, acc.: 91.41%] [G loss: 4.013763]\n",
      "epoch:6 step:4698 [D loss: 0.879104, acc.: 53.12%] [G loss: 5.453287]\n",
      "epoch:6 step:4699 [D loss: 0.018051, acc.: 100.00%] [G loss: 5.803876]\n",
      "epoch:6 step:4700 [D loss: 0.055433, acc.: 99.22%] [G loss: 3.890060]\n",
      "epoch:6 step:4701 [D loss: 0.060548, acc.: 97.66%] [G loss: 1.888984]\n",
      "epoch:6 step:4702 [D loss: 0.166220, acc.: 92.97%] [G loss: 2.039226]\n",
      "epoch:6 step:4703 [D loss: 0.051909, acc.: 100.00%] [G loss: 3.114145]\n",
      "epoch:6 step:4704 [D loss: 0.156872, acc.: 96.88%] [G loss: 1.020317]\n",
      "epoch:6 step:4705 [D loss: 0.149069, acc.: 97.66%] [G loss: 0.924873]\n",
      "epoch:6 step:4706 [D loss: 0.057966, acc.: 99.22%] [G loss: 1.395059]\n",
      "epoch:6 step:4707 [D loss: 0.276184, acc.: 89.06%] [G loss: 3.023398]\n",
      "epoch:6 step:4708 [D loss: 0.100741, acc.: 97.66%] [G loss: 2.998248]\n",
      "epoch:6 step:4709 [D loss: 2.231121, acc.: 28.12%] [G loss: 8.816875]\n",
      "epoch:6 step:4710 [D loss: 1.028789, acc.: 61.72%] [G loss: 5.283605]\n",
      "epoch:6 step:4711 [D loss: 0.115284, acc.: 96.09%] [G loss: 3.683988]\n",
      "epoch:6 step:4712 [D loss: 0.043607, acc.: 100.00%] [G loss: 4.306942]\n",
      "epoch:6 step:4713 [D loss: 0.191434, acc.: 96.09%] [G loss: 3.393114]\n",
      "epoch:6 step:4714 [D loss: 0.074200, acc.: 99.22%] [G loss: 4.036213]\n",
      "epoch:6 step:4715 [D loss: 0.868394, acc.: 53.12%] [G loss: 6.668566]\n",
      "epoch:6 step:4716 [D loss: 1.063844, acc.: 57.03%] [G loss: 2.014207]\n",
      "epoch:6 step:4717 [D loss: 0.362760, acc.: 83.59%] [G loss: 4.139652]\n",
      "epoch:6 step:4718 [D loss: 0.204491, acc.: 94.53%] [G loss: 3.533546]\n",
      "epoch:6 step:4719 [D loss: 0.328534, acc.: 89.84%] [G loss: 3.618486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4720 [D loss: 0.131539, acc.: 96.09%] [G loss: 3.405546]\n",
      "epoch:6 step:4721 [D loss: 0.253816, acc.: 92.97%] [G loss: 3.877890]\n",
      "epoch:6 step:4722 [D loss: 0.607741, acc.: 69.53%] [G loss: 3.792679]\n",
      "epoch:6 step:4723 [D loss: 0.247241, acc.: 90.62%] [G loss: 4.814352]\n",
      "epoch:6 step:4724 [D loss: 0.713383, acc.: 64.84%] [G loss: 3.548624]\n",
      "epoch:6 step:4725 [D loss: 0.052113, acc.: 100.00%] [G loss: 4.688023]\n",
      "epoch:6 step:4726 [D loss: 0.150210, acc.: 93.75%] [G loss: 1.920311]\n",
      "epoch:6 step:4727 [D loss: 0.128304, acc.: 95.31%] [G loss: 1.888765]\n",
      "epoch:6 step:4728 [D loss: 0.083264, acc.: 98.44%] [G loss: 1.949109]\n",
      "epoch:6 step:4729 [D loss: 0.861613, acc.: 50.78%] [G loss: 6.088886]\n",
      "epoch:6 step:4730 [D loss: 1.159374, acc.: 53.91%] [G loss: 1.489195]\n",
      "epoch:6 step:4731 [D loss: 0.405611, acc.: 80.47%] [G loss: 4.036666]\n",
      "epoch:6 step:4732 [D loss: 0.058928, acc.: 99.22%] [G loss: 2.965435]\n",
      "epoch:6 step:4733 [D loss: 0.379513, acc.: 82.81%] [G loss: 2.541279]\n",
      "epoch:6 step:4734 [D loss: 0.218131, acc.: 93.75%] [G loss: 3.635942]\n",
      "epoch:6 step:4735 [D loss: 0.154631, acc.: 96.09%] [G loss: 3.372934]\n",
      "epoch:6 step:4736 [D loss: 0.282904, acc.: 89.84%] [G loss: 4.185886]\n",
      "epoch:6 step:4737 [D loss: 0.124786, acc.: 98.44%] [G loss: 4.487011]\n",
      "epoch:6 step:4738 [D loss: 0.224374, acc.: 92.97%] [G loss: 2.604724]\n",
      "epoch:6 step:4739 [D loss: 0.184263, acc.: 94.53%] [G loss: 2.871530]\n",
      "epoch:6 step:4740 [D loss: 0.123548, acc.: 97.66%] [G loss: 4.323721]\n",
      "epoch:6 step:4741 [D loss: 0.467685, acc.: 75.78%] [G loss: 3.018348]\n",
      "epoch:6 step:4742 [D loss: 0.129349, acc.: 98.44%] [G loss: 3.684035]\n",
      "epoch:6 step:4743 [D loss: 0.320074, acc.: 87.50%] [G loss: 2.564242]\n",
      "epoch:6 step:4744 [D loss: 0.394301, acc.: 82.03%] [G loss: 3.598630]\n",
      "epoch:6 step:4745 [D loss: 0.117444, acc.: 97.66%] [G loss: 2.246006]\n",
      "epoch:6 step:4746 [D loss: 0.217937, acc.: 90.62%] [G loss: 3.312474]\n",
      "epoch:6 step:4747 [D loss: 0.694347, acc.: 62.50%] [G loss: 4.528066]\n",
      "epoch:6 step:4748 [D loss: 0.231874, acc.: 90.62%] [G loss: 3.114757]\n",
      "epoch:6 step:4749 [D loss: 0.073483, acc.: 100.00%] [G loss: 3.155905]\n",
      "epoch:6 step:4750 [D loss: 0.076109, acc.: 99.22%] [G loss: 2.531786]\n",
      "epoch:6 step:4751 [D loss: 0.437827, acc.: 78.12%] [G loss: 4.079947]\n",
      "epoch:6 step:4752 [D loss: 0.187092, acc.: 90.62%] [G loss: 3.181800]\n",
      "epoch:6 step:4753 [D loss: 0.425069, acc.: 77.34%] [G loss: 1.999013]\n",
      "epoch:6 step:4754 [D loss: 0.228974, acc.: 90.62%] [G loss: 4.304295]\n",
      "epoch:6 step:4755 [D loss: 0.368889, acc.: 82.81%] [G loss: 3.604888]\n",
      "epoch:6 step:4756 [D loss: 0.212578, acc.: 94.53%] [G loss: 3.353646]\n",
      "epoch:6 step:4757 [D loss: 0.139662, acc.: 95.31%] [G loss: 2.589891]\n",
      "epoch:6 step:4758 [D loss: 0.125938, acc.: 96.88%] [G loss: 2.470874]\n",
      "epoch:6 step:4759 [D loss: 0.141517, acc.: 96.88%] [G loss: 3.435707]\n",
      "epoch:6 step:4760 [D loss: 0.119330, acc.: 99.22%] [G loss: 3.067250]\n",
      "epoch:6 step:4761 [D loss: 0.091364, acc.: 97.66%] [G loss: 2.663551]\n",
      "epoch:6 step:4762 [D loss: 0.348138, acc.: 85.94%] [G loss: 3.220866]\n",
      "epoch:6 step:4763 [D loss: 0.342463, acc.: 87.50%] [G loss: 1.374177]\n",
      "epoch:6 step:4764 [D loss: 0.190720, acc.: 91.41%] [G loss: 3.197222]\n",
      "epoch:6 step:4765 [D loss: 0.097368, acc.: 96.09%] [G loss: 2.400604]\n",
      "epoch:6 step:4766 [D loss: 0.134092, acc.: 96.88%] [G loss: 0.814583]\n",
      "epoch:6 step:4767 [D loss: 0.259812, acc.: 87.50%] [G loss: 4.284773]\n",
      "epoch:6 step:4768 [D loss: 0.171622, acc.: 93.75%] [G loss: 3.639885]\n",
      "epoch:6 step:4769 [D loss: 0.103443, acc.: 96.88%] [G loss: 2.880589]\n",
      "epoch:6 step:4770 [D loss: 0.769159, acc.: 61.72%] [G loss: 7.927883]\n",
      "epoch:6 step:4771 [D loss: 0.751002, acc.: 66.41%] [G loss: 5.197558]\n",
      "epoch:6 step:4772 [D loss: 0.082762, acc.: 98.44%] [G loss: 4.007741]\n",
      "epoch:6 step:4773 [D loss: 0.083853, acc.: 98.44%] [G loss: 3.827036]\n",
      "epoch:6 step:4774 [D loss: 0.024017, acc.: 100.00%] [G loss: 3.362021]\n",
      "epoch:6 step:4775 [D loss: 0.083466, acc.: 97.66%] [G loss: 3.994290]\n",
      "epoch:6 step:4776 [D loss: 0.230868, acc.: 91.41%] [G loss: 4.529870]\n",
      "epoch:6 step:4777 [D loss: 0.199620, acc.: 92.97%] [G loss: 2.925571]\n",
      "epoch:6 step:4778 [D loss: 0.125646, acc.: 96.88%] [G loss: 3.722971]\n",
      "epoch:6 step:4779 [D loss: 0.183248, acc.: 95.31%] [G loss: 2.603010]\n",
      "epoch:6 step:4780 [D loss: 0.601968, acc.: 71.88%] [G loss: 5.924473]\n",
      "epoch:6 step:4781 [D loss: 0.347996, acc.: 82.81%] [G loss: 4.871350]\n",
      "epoch:6 step:4782 [D loss: 0.080226, acc.: 99.22%] [G loss: 2.128116]\n",
      "epoch:6 step:4783 [D loss: 0.072347, acc.: 99.22%] [G loss: 2.709773]\n",
      "epoch:6 step:4784 [D loss: 0.095469, acc.: 96.88%] [G loss: 0.462275]\n",
      "epoch:6 step:4785 [D loss: 0.110021, acc.: 96.88%] [G loss: 0.675821]\n",
      "epoch:6 step:4786 [D loss: 0.105374, acc.: 99.22%] [G loss: 1.919114]\n",
      "epoch:6 step:4787 [D loss: 0.215883, acc.: 89.84%] [G loss: 0.354306]\n",
      "epoch:6 step:4788 [D loss: 0.108423, acc.: 96.88%] [G loss: 1.056307]\n",
      "epoch:6 step:4789 [D loss: 0.092498, acc.: 98.44%] [G loss: 0.438928]\n",
      "epoch:6 step:4790 [D loss: 0.160757, acc.: 92.19%] [G loss: 3.540334]\n",
      "epoch:6 step:4791 [D loss: 0.113809, acc.: 97.66%] [G loss: 2.100578]\n",
      "epoch:6 step:4792 [D loss: 0.135616, acc.: 95.31%] [G loss: 0.981919]\n",
      "epoch:6 step:4793 [D loss: 0.184144, acc.: 96.09%] [G loss: 2.312209]\n",
      "epoch:6 step:4794 [D loss: 0.203481, acc.: 92.97%] [G loss: 4.062984]\n",
      "epoch:6 step:4795 [D loss: 0.231080, acc.: 91.41%] [G loss: 3.748500]\n",
      "epoch:6 step:4796 [D loss: 0.079424, acc.: 98.44%] [G loss: 3.297468]\n",
      "epoch:6 step:4797 [D loss: 1.178925, acc.: 48.44%] [G loss: 9.065255]\n",
      "epoch:6 step:4798 [D loss: 0.451038, acc.: 79.69%] [G loss: 8.008190]\n",
      "epoch:6 step:4799 [D loss: 0.032878, acc.: 99.22%] [G loss: 6.175747]\n",
      "epoch:6 step:4800 [D loss: 0.031658, acc.: 100.00%] [G loss: 4.676002]\n",
      "epoch:6 step:4801 [D loss: 0.038450, acc.: 99.22%] [G loss: 2.905530]\n",
      "epoch:6 step:4802 [D loss: 0.422917, acc.: 78.12%] [G loss: 8.221623]\n",
      "epoch:6 step:4803 [D loss: 0.708224, acc.: 65.62%] [G loss: 3.959961]\n",
      "epoch:6 step:4804 [D loss: 0.127521, acc.: 96.09%] [G loss: 3.011736]\n",
      "epoch:6 step:4805 [D loss: 0.054482, acc.: 98.44%] [G loss: 3.991247]\n",
      "epoch:6 step:4806 [D loss: 0.064706, acc.: 99.22%] [G loss: 3.217481]\n",
      "epoch:6 step:4807 [D loss: 0.225342, acc.: 92.97%] [G loss: 5.556774]\n",
      "epoch:6 step:4808 [D loss: 0.342199, acc.: 86.72%] [G loss: 3.184609]\n",
      "epoch:6 step:4809 [D loss: 0.632411, acc.: 67.19%] [G loss: 6.997616]\n",
      "epoch:6 step:4810 [D loss: 0.801422, acc.: 64.84%] [G loss: 3.484520]\n",
      "epoch:6 step:4811 [D loss: 0.212151, acc.: 93.75%] [G loss: 3.543149]\n",
      "epoch:6 step:4812 [D loss: 0.069608, acc.: 99.22%] [G loss: 3.818151]\n",
      "epoch:6 step:4813 [D loss: 0.148841, acc.: 96.88%] [G loss: 3.690255]\n",
      "epoch:6 step:4814 [D loss: 0.133263, acc.: 95.31%] [G loss: 3.080094]\n",
      "epoch:6 step:4815 [D loss: 0.114995, acc.: 97.66%] [G loss: 2.117281]\n",
      "epoch:6 step:4816 [D loss: 0.131552, acc.: 96.09%] [G loss: 4.140641]\n",
      "epoch:6 step:4817 [D loss: 0.315312, acc.: 89.06%] [G loss: 5.223625]\n",
      "epoch:6 step:4818 [D loss: 0.803791, acc.: 60.16%] [G loss: 3.567754]\n",
      "epoch:6 step:4819 [D loss: 0.047976, acc.: 100.00%] [G loss: 4.709979]\n",
      "epoch:6 step:4820 [D loss: 0.070330, acc.: 100.00%] [G loss: 3.118145]\n",
      "epoch:6 step:4821 [D loss: 0.133641, acc.: 97.66%] [G loss: 3.678443]\n",
      "epoch:6 step:4822 [D loss: 0.361571, acc.: 85.94%] [G loss: 3.271382]\n",
      "epoch:6 step:4823 [D loss: 0.118633, acc.: 97.66%] [G loss: 2.250331]\n",
      "epoch:6 step:4824 [D loss: 0.155109, acc.: 95.31%] [G loss: 1.038179]\n",
      "epoch:6 step:4825 [D loss: 0.080471, acc.: 98.44%] [G loss: 0.955737]\n",
      "epoch:6 step:4826 [D loss: 0.052941, acc.: 100.00%] [G loss: 0.917639]\n",
      "epoch:6 step:4827 [D loss: 0.040272, acc.: 99.22%] [G loss: 0.266863]\n",
      "epoch:6 step:4828 [D loss: 0.442298, acc.: 76.56%] [G loss: 6.537429]\n",
      "epoch:6 step:4829 [D loss: 0.950788, acc.: 59.38%] [G loss: 2.199013]\n",
      "epoch:6 step:4830 [D loss: 0.137035, acc.: 94.53%] [G loss: 1.658491]\n",
      "epoch:6 step:4831 [D loss: 0.013948, acc.: 100.00%] [G loss: 1.982118]\n",
      "epoch:6 step:4832 [D loss: 0.069323, acc.: 100.00%] [G loss: 2.207197]\n",
      "epoch:6 step:4833 [D loss: 0.086018, acc.: 98.44%] [G loss: 2.237392]\n",
      "epoch:6 step:4834 [D loss: 0.186047, acc.: 95.31%] [G loss: 1.271276]\n",
      "epoch:6 step:4835 [D loss: 0.168520, acc.: 94.53%] [G loss: 1.668545]\n",
      "epoch:6 step:4836 [D loss: 0.047732, acc.: 100.00%] [G loss: 0.900841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4837 [D loss: 0.018161, acc.: 100.00%] [G loss: 0.487508]\n",
      "epoch:6 step:4838 [D loss: 0.064024, acc.: 99.22%] [G loss: 1.399704]\n",
      "epoch:6 step:4839 [D loss: 0.076185, acc.: 100.00%] [G loss: 0.979012]\n",
      "epoch:6 step:4840 [D loss: 0.031503, acc.: 100.00%] [G loss: 1.263868]\n",
      "epoch:6 step:4841 [D loss: 0.099438, acc.: 99.22%] [G loss: 0.698933]\n",
      "epoch:6 step:4842 [D loss: 0.112075, acc.: 96.88%] [G loss: 0.434411]\n",
      "epoch:6 step:4843 [D loss: 0.115891, acc.: 97.66%] [G loss: 2.224189]\n",
      "epoch:6 step:4844 [D loss: 0.249825, acc.: 89.06%] [G loss: 1.191701]\n",
      "epoch:6 step:4845 [D loss: 0.294522, acc.: 85.16%] [G loss: 3.668252]\n",
      "epoch:6 step:4846 [D loss: 1.400795, acc.: 37.50%] [G loss: 4.609816]\n",
      "epoch:6 step:4847 [D loss: 0.079902, acc.: 97.66%] [G loss: 4.926710]\n",
      "epoch:6 step:4848 [D loss: 0.054460, acc.: 99.22%] [G loss: 4.076737]\n",
      "epoch:6 step:4849 [D loss: 0.175452, acc.: 95.31%] [G loss: 4.716394]\n",
      "epoch:6 step:4850 [D loss: 0.127598, acc.: 97.66%] [G loss: 4.907122]\n",
      "epoch:6 step:4851 [D loss: 0.148979, acc.: 96.88%] [G loss: 4.724564]\n",
      "epoch:6 step:4852 [D loss: 0.270414, acc.: 86.72%] [G loss: 4.119831]\n",
      "epoch:6 step:4853 [D loss: 0.095429, acc.: 98.44%] [G loss: 3.673961]\n",
      "epoch:6 step:4854 [D loss: 0.060748, acc.: 100.00%] [G loss: 3.899101]\n",
      "epoch:6 step:4855 [D loss: 0.155295, acc.: 92.97%] [G loss: 4.802032]\n",
      "epoch:6 step:4856 [D loss: 0.403857, acc.: 83.59%] [G loss: 3.397486]\n",
      "epoch:6 step:4857 [D loss: 0.072907, acc.: 100.00%] [G loss: 2.854816]\n",
      "epoch:6 step:4858 [D loss: 0.200001, acc.: 90.62%] [G loss: 4.655777]\n",
      "epoch:6 step:4859 [D loss: 2.250621, acc.: 10.94%] [G loss: 6.855865]\n",
      "epoch:6 step:4860 [D loss: 1.081051, acc.: 55.47%] [G loss: 3.780198]\n",
      "epoch:6 step:4861 [D loss: 0.106006, acc.: 96.88%] [G loss: 2.970212]\n",
      "epoch:6 step:4862 [D loss: 0.127983, acc.: 96.09%] [G loss: 2.121198]\n",
      "epoch:6 step:4863 [D loss: 0.074379, acc.: 100.00%] [G loss: 2.445475]\n",
      "epoch:6 step:4864 [D loss: 0.097678, acc.: 99.22%] [G loss: 1.326062]\n",
      "epoch:6 step:4865 [D loss: 0.324124, acc.: 85.16%] [G loss: 2.563352]\n",
      "epoch:6 step:4866 [D loss: 0.110551, acc.: 98.44%] [G loss: 3.033741]\n",
      "epoch:6 step:4867 [D loss: 0.109330, acc.: 96.09%] [G loss: 2.716976]\n",
      "epoch:6 step:4868 [D loss: 0.099175, acc.: 98.44%] [G loss: 2.202878]\n",
      "epoch:6 step:4869 [D loss: 0.157919, acc.: 94.53%] [G loss: 2.641436]\n",
      "epoch:6 step:4870 [D loss: 0.260001, acc.: 91.41%] [G loss: 1.796166]\n",
      "epoch:6 step:4871 [D loss: 0.301692, acc.: 85.16%] [G loss: 3.293910]\n",
      "epoch:6 step:4872 [D loss: 0.130168, acc.: 97.66%] [G loss: 3.935071]\n",
      "epoch:6 step:4873 [D loss: 0.287151, acc.: 89.84%] [G loss: 2.099156]\n",
      "epoch:6 step:4874 [D loss: 0.100908, acc.: 99.22%] [G loss: 3.029423]\n",
      "epoch:6 step:4875 [D loss: 0.126629, acc.: 96.88%] [G loss: 2.194160]\n",
      "epoch:6 step:4876 [D loss: 0.197352, acc.: 92.19%] [G loss: 4.467439]\n",
      "epoch:6 step:4877 [D loss: 0.210862, acc.: 93.75%] [G loss: 4.318781]\n",
      "epoch:6 step:4878 [D loss: 0.197410, acc.: 96.09%] [G loss: 1.747737]\n",
      "epoch:6 step:4879 [D loss: 0.071373, acc.: 99.22%] [G loss: 1.639349]\n",
      "epoch:6 step:4880 [D loss: 0.338260, acc.: 83.59%] [G loss: 6.121527]\n",
      "epoch:6 step:4881 [D loss: 0.916460, acc.: 58.59%] [G loss: 1.398938]\n",
      "epoch:6 step:4882 [D loss: 0.773692, acc.: 63.28%] [G loss: 7.278550]\n",
      "epoch:6 step:4883 [D loss: 0.191467, acc.: 89.84%] [G loss: 7.268659]\n",
      "epoch:6 step:4884 [D loss: 0.575145, acc.: 67.19%] [G loss: 1.183373]\n",
      "epoch:6 step:4885 [D loss: 0.436194, acc.: 81.25%] [G loss: 4.246368]\n",
      "epoch:6 step:4886 [D loss: 0.019716, acc.: 100.00%] [G loss: 5.015601]\n",
      "epoch:6 step:4887 [D loss: 0.195235, acc.: 92.19%] [G loss: 2.469795]\n",
      "epoch:6 step:4888 [D loss: 0.535471, acc.: 75.00%] [G loss: 1.062666]\n",
      "epoch:6 step:4889 [D loss: 0.058900, acc.: 100.00%] [G loss: 1.438109]\n",
      "epoch:6 step:4890 [D loss: 0.113573, acc.: 95.31%] [G loss: 0.986334]\n",
      "epoch:6 step:4891 [D loss: 0.200485, acc.: 92.19%] [G loss: 0.689728]\n",
      "epoch:6 step:4892 [D loss: 0.217213, acc.: 91.41%] [G loss: 1.702644]\n",
      "epoch:6 step:4893 [D loss: 0.104007, acc.: 97.66%] [G loss: 2.749084]\n",
      "epoch:6 step:4894 [D loss: 0.045448, acc.: 98.44%] [G loss: 2.812122]\n",
      "epoch:6 step:4895 [D loss: 0.142888, acc.: 96.09%] [G loss: 2.568262]\n",
      "epoch:6 step:4896 [D loss: 0.148635, acc.: 95.31%] [G loss: 4.076525]\n",
      "epoch:6 step:4897 [D loss: 0.381819, acc.: 86.72%] [G loss: 3.427492]\n",
      "epoch:6 step:4898 [D loss: 0.179714, acc.: 94.53%] [G loss: 3.629502]\n",
      "epoch:6 step:4899 [D loss: 0.690667, acc.: 60.94%] [G loss: 3.839177]\n",
      "epoch:6 step:4900 [D loss: 0.080822, acc.: 98.44%] [G loss: 3.514521]\n",
      "epoch:6 step:4901 [D loss: 0.059134, acc.: 99.22%] [G loss: 2.419633]\n",
      "epoch:6 step:4902 [D loss: 0.102647, acc.: 97.66%] [G loss: 2.947803]\n",
      "epoch:6 step:4903 [D loss: 0.079208, acc.: 97.66%] [G loss: 2.360094]\n",
      "epoch:6 step:4904 [D loss: 0.092052, acc.: 99.22%] [G loss: 0.675084]\n",
      "epoch:6 step:4905 [D loss: 0.040804, acc.: 100.00%] [G loss: 0.701633]\n",
      "epoch:6 step:4906 [D loss: 0.064136, acc.: 99.22%] [G loss: 0.983379]\n",
      "epoch:6 step:4907 [D loss: 0.017696, acc.: 100.00%] [G loss: 0.247389]\n",
      "epoch:6 step:4908 [D loss: 0.053789, acc.: 100.00%] [G loss: 0.196875]\n",
      "epoch:6 step:4909 [D loss: 0.036557, acc.: 100.00%] [G loss: 0.221087]\n",
      "epoch:6 step:4910 [D loss: 0.098258, acc.: 98.44%] [G loss: 0.580310]\n",
      "epoch:6 step:4911 [D loss: 0.048892, acc.: 100.00%] [G loss: 0.912100]\n",
      "epoch:6 step:4912 [D loss: 0.085110, acc.: 97.66%] [G loss: 0.456665]\n",
      "epoch:6 step:4913 [D loss: 0.043638, acc.: 100.00%] [G loss: 0.439764]\n",
      "epoch:6 step:4914 [D loss: 0.016287, acc.: 100.00%] [G loss: 0.311317]\n",
      "epoch:6 step:4915 [D loss: 0.068623, acc.: 99.22%] [G loss: 0.278648]\n",
      "epoch:6 step:4916 [D loss: 0.311259, acc.: 85.94%] [G loss: 2.814211]\n",
      "epoch:6 step:4917 [D loss: 0.036492, acc.: 99.22%] [G loss: 3.973351]\n",
      "epoch:6 step:4918 [D loss: 0.100417, acc.: 98.44%] [G loss: 1.698717]\n",
      "epoch:6 step:4919 [D loss: 0.037011, acc.: 100.00%] [G loss: 0.584757]\n",
      "epoch:6 step:4920 [D loss: 0.075530, acc.: 99.22%] [G loss: 0.935688]\n",
      "epoch:6 step:4921 [D loss: 0.060276, acc.: 99.22%] [G loss: 0.828304]\n",
      "epoch:6 step:4922 [D loss: 2.588976, acc.: 25.00%] [G loss: 7.725456]\n",
      "epoch:6 step:4923 [D loss: 0.595110, acc.: 68.75%] [G loss: 6.238200]\n",
      "epoch:6 step:4924 [D loss: 0.365117, acc.: 82.03%] [G loss: 2.494815]\n",
      "epoch:6 step:4925 [D loss: 0.295699, acc.: 87.50%] [G loss: 4.028534]\n",
      "epoch:6 step:4926 [D loss: 0.115566, acc.: 96.09%] [G loss: 3.642821]\n",
      "epoch:6 step:4927 [D loss: 0.159592, acc.: 93.75%] [G loss: 2.070995]\n",
      "epoch:6 step:4928 [D loss: 0.148045, acc.: 95.31%] [G loss: 4.553813]\n",
      "epoch:6 step:4929 [D loss: 0.187410, acc.: 90.62%] [G loss: 3.101939]\n",
      "epoch:6 step:4930 [D loss: 0.227791, acc.: 90.62%] [G loss: 3.286234]\n",
      "epoch:6 step:4931 [D loss: 0.065153, acc.: 98.44%] [G loss: 2.987296]\n",
      "epoch:6 step:4932 [D loss: 0.330183, acc.: 84.38%] [G loss: 4.296180]\n",
      "epoch:6 step:4933 [D loss: 0.480696, acc.: 78.12%] [G loss: 2.871178]\n",
      "epoch:6 step:4934 [D loss: 0.064977, acc.: 100.00%] [G loss: 2.414351]\n",
      "epoch:6 step:4935 [D loss: 0.159189, acc.: 92.97%] [G loss: 3.312929]\n",
      "epoch:6 step:4936 [D loss: 0.172247, acc.: 93.75%] [G loss: 1.994366]\n",
      "epoch:6 step:4937 [D loss: 0.226159, acc.: 90.62%] [G loss: 4.494435]\n",
      "epoch:6 step:4938 [D loss: 0.168688, acc.: 92.97%] [G loss: 3.239826]\n",
      "epoch:6 step:4939 [D loss: 0.257692, acc.: 92.19%] [G loss: 2.807231]\n",
      "epoch:6 step:4940 [D loss: 0.101519, acc.: 97.66%] [G loss: 4.149304]\n",
      "epoch:6 step:4941 [D loss: 0.266559, acc.: 90.62%] [G loss: 3.277187]\n",
      "epoch:6 step:4942 [D loss: 0.123042, acc.: 97.66%] [G loss: 3.835569]\n",
      "epoch:6 step:4943 [D loss: 0.461184, acc.: 78.91%] [G loss: 2.176008]\n",
      "epoch:6 step:4944 [D loss: 0.257873, acc.: 89.06%] [G loss: 5.575814]\n",
      "epoch:6 step:4945 [D loss: 0.097672, acc.: 96.88%] [G loss: 5.157900]\n",
      "epoch:6 step:4946 [D loss: 0.164660, acc.: 92.19%] [G loss: 3.436098]\n",
      "epoch:6 step:4947 [D loss: 0.301617, acc.: 85.94%] [G loss: 4.617718]\n",
      "epoch:6 step:4948 [D loss: 0.100592, acc.: 97.66%] [G loss: 4.528935]\n",
      "epoch:6 step:4949 [D loss: 0.269401, acc.: 89.06%] [G loss: 3.111668]\n",
      "epoch:6 step:4950 [D loss: 0.256733, acc.: 89.06%] [G loss: 3.655130]\n",
      "epoch:6 step:4951 [D loss: 0.100685, acc.: 98.44%] [G loss: 4.990305]\n",
      "epoch:6 step:4952 [D loss: 0.274454, acc.: 89.84%] [G loss: 3.007144]\n",
      "epoch:6 step:4953 [D loss: 0.457787, acc.: 78.12%] [G loss: 3.685796]\n",
      "epoch:6 step:4954 [D loss: 0.055671, acc.: 99.22%] [G loss: 5.067429]\n",
      "epoch:6 step:4955 [D loss: 0.209989, acc.: 92.19%] [G loss: 5.446891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4956 [D loss: 0.114513, acc.: 97.66%] [G loss: 3.584926]\n",
      "epoch:6 step:4957 [D loss: 0.120912, acc.: 96.09%] [G loss: 2.933735]\n",
      "epoch:6 step:4958 [D loss: 0.268671, acc.: 89.06%] [G loss: 5.668396]\n",
      "epoch:6 step:4959 [D loss: 0.240377, acc.: 89.84%] [G loss: 3.427032]\n",
      "epoch:6 step:4960 [D loss: 0.198036, acc.: 95.31%] [G loss: 3.715801]\n",
      "epoch:6 step:4961 [D loss: 0.245152, acc.: 88.28%] [G loss: 3.561289]\n",
      "epoch:6 step:4962 [D loss: 0.304651, acc.: 87.50%] [G loss: 3.495045]\n",
      "epoch:6 step:4963 [D loss: 0.055953, acc.: 100.00%] [G loss: 3.420949]\n",
      "epoch:6 step:4964 [D loss: 0.112496, acc.: 99.22%] [G loss: 3.920987]\n",
      "epoch:6 step:4965 [D loss: 0.464490, acc.: 78.12%] [G loss: 2.634480]\n",
      "epoch:6 step:4966 [D loss: 0.124926, acc.: 97.66%] [G loss: 4.157093]\n",
      "epoch:6 step:4967 [D loss: 0.202814, acc.: 94.53%] [G loss: 4.432437]\n",
      "epoch:6 step:4968 [D loss: 0.060868, acc.: 99.22%] [G loss: 4.586649]\n",
      "epoch:6 step:4969 [D loss: 0.495403, acc.: 71.88%] [G loss: 6.371666]\n",
      "epoch:6 step:4970 [D loss: 0.447106, acc.: 78.91%] [G loss: 2.628543]\n",
      "epoch:6 step:4971 [D loss: 0.321097, acc.: 86.72%] [G loss: 6.585546]\n",
      "epoch:6 step:4972 [D loss: 0.568768, acc.: 78.12%] [G loss: 3.687834]\n",
      "epoch:6 step:4973 [D loss: 0.063285, acc.: 99.22%] [G loss: 2.461518]\n",
      "epoch:6 step:4974 [D loss: 0.212175, acc.: 87.50%] [G loss: 5.005981]\n",
      "epoch:6 step:4975 [D loss: 0.104224, acc.: 97.66%] [G loss: 4.941738]\n",
      "epoch:6 step:4976 [D loss: 0.157288, acc.: 94.53%] [G loss: 2.102947]\n",
      "epoch:6 step:4977 [D loss: 0.108242, acc.: 97.66%] [G loss: 1.994939]\n",
      "epoch:6 step:4978 [D loss: 0.085385, acc.: 100.00%] [G loss: 1.927109]\n",
      "epoch:6 step:4979 [D loss: 0.046572, acc.: 98.44%] [G loss: 1.222532]\n",
      "epoch:6 step:4980 [D loss: 0.035146, acc.: 100.00%] [G loss: 0.825890]\n",
      "epoch:6 step:4981 [D loss: 0.146431, acc.: 96.09%] [G loss: 0.376682]\n",
      "epoch:6 step:4982 [D loss: 0.004973, acc.: 100.00%] [G loss: 0.314992]\n",
      "epoch:6 step:4983 [D loss: 0.143910, acc.: 96.09%] [G loss: 1.755708]\n",
      "epoch:6 step:4984 [D loss: 0.106527, acc.: 97.66%] [G loss: 2.061554]\n",
      "epoch:6 step:4985 [D loss: 0.091061, acc.: 100.00%] [G loss: 2.375621]\n",
      "epoch:6 step:4986 [D loss: 1.730487, acc.: 32.81%] [G loss: 7.189968]\n",
      "epoch:6 step:4987 [D loss: 0.708002, acc.: 68.75%] [G loss: 6.192346]\n",
      "epoch:6 step:4988 [D loss: 0.022083, acc.: 100.00%] [G loss: 4.673151]\n",
      "epoch:6 step:4989 [D loss: 0.109466, acc.: 95.31%] [G loss: 4.512649]\n",
      "epoch:6 step:4990 [D loss: 0.079236, acc.: 97.66%] [G loss: 4.392508]\n",
      "epoch:6 step:4991 [D loss: 0.054526, acc.: 99.22%] [G loss: 3.455426]\n",
      "epoch:6 step:4992 [D loss: 0.122379, acc.: 98.44%] [G loss: 2.640193]\n",
      "epoch:6 step:4993 [D loss: 0.166141, acc.: 95.31%] [G loss: 3.992512]\n",
      "epoch:6 step:4994 [D loss: 0.175617, acc.: 92.97%] [G loss: 4.313417]\n",
      "epoch:6 step:4995 [D loss: 0.747026, acc.: 63.28%] [G loss: 6.208952]\n",
      "epoch:6 step:4996 [D loss: 0.421641, acc.: 78.12%] [G loss: 3.925086]\n",
      "epoch:6 step:4997 [D loss: 0.092372, acc.: 97.66%] [G loss: 3.639911]\n",
      "epoch:6 step:4998 [D loss: 0.054493, acc.: 100.00%] [G loss: 2.669721]\n",
      "epoch:6 step:4999 [D loss: 0.122365, acc.: 96.88%] [G loss: 3.986831]\n",
      "epoch:6 step:5000 [D loss: 0.068078, acc.: 100.00%] [G loss: 3.261041]\n",
      "epoch:6 step:5001 [D loss: 1.940919, acc.: 28.12%] [G loss: 6.710271]\n",
      "epoch:6 step:5002 [D loss: 0.735393, acc.: 67.19%] [G loss: 4.913158]\n",
      "epoch:6 step:5003 [D loss: 0.160389, acc.: 94.53%] [G loss: 1.860313]\n",
      "epoch:6 step:5004 [D loss: 0.256701, acc.: 88.28%] [G loss: 3.273043]\n",
      "epoch:6 step:5005 [D loss: 0.075490, acc.: 98.44%] [G loss: 3.907876]\n",
      "epoch:6 step:5006 [D loss: 0.176632, acc.: 92.97%] [G loss: 2.003700]\n",
      "epoch:6 step:5007 [D loss: 0.176269, acc.: 94.53%] [G loss: 2.120636]\n",
      "epoch:6 step:5008 [D loss: 0.126070, acc.: 96.09%] [G loss: 1.566142]\n",
      "epoch:6 step:5009 [D loss: 0.133320, acc.: 96.09%] [G loss: 0.995955]\n",
      "epoch:6 step:5010 [D loss: 0.068076, acc.: 98.44%] [G loss: 0.976457]\n",
      "epoch:6 step:5011 [D loss: 0.190390, acc.: 95.31%] [G loss: 1.779812]\n",
      "epoch:6 step:5012 [D loss: 0.107812, acc.: 96.88%] [G loss: 1.797881]\n",
      "epoch:6 step:5013 [D loss: 0.265690, acc.: 90.62%] [G loss: 3.197761]\n",
      "epoch:6 step:5014 [D loss: 0.463178, acc.: 77.34%] [G loss: 1.010918]\n",
      "epoch:6 step:5015 [D loss: 0.224673, acc.: 92.19%] [G loss: 2.655699]\n",
      "epoch:6 step:5016 [D loss: 0.051427, acc.: 99.22%] [G loss: 4.107333]\n",
      "epoch:6 step:5017 [D loss: 0.285295, acc.: 88.28%] [G loss: 0.767604]\n",
      "epoch:6 step:5018 [D loss: 0.453909, acc.: 73.44%] [G loss: 3.878297]\n",
      "epoch:6 step:5019 [D loss: 0.259993, acc.: 86.72%] [G loss: 3.973362]\n",
      "epoch:6 step:5020 [D loss: 0.274334, acc.: 86.72%] [G loss: 2.797299]\n",
      "epoch:6 step:5021 [D loss: 0.074037, acc.: 98.44%] [G loss: 2.401472]\n",
      "epoch:6 step:5022 [D loss: 0.132820, acc.: 98.44%] [G loss: 3.874992]\n",
      "epoch:6 step:5023 [D loss: 0.105957, acc.: 98.44%] [G loss: 2.910037]\n",
      "epoch:6 step:5024 [D loss: 0.261015, acc.: 88.28%] [G loss: 4.214949]\n",
      "epoch:6 step:5025 [D loss: 0.411020, acc.: 82.81%] [G loss: 3.966239]\n",
      "epoch:6 step:5026 [D loss: 0.040140, acc.: 100.00%] [G loss: 3.062178]\n",
      "epoch:6 step:5027 [D loss: 0.194895, acc.: 92.97%] [G loss: 4.355017]\n",
      "epoch:6 step:5028 [D loss: 0.215075, acc.: 90.62%] [G loss: 2.328671]\n",
      "epoch:6 step:5029 [D loss: 0.046776, acc.: 99.22%] [G loss: 1.028260]\n",
      "epoch:6 step:5030 [D loss: 0.171339, acc.: 92.19%] [G loss: 1.576750]\n",
      "epoch:6 step:5031 [D loss: 0.028946, acc.: 100.00%] [G loss: 1.682828]\n",
      "epoch:6 step:5032 [D loss: 0.097488, acc.: 98.44%] [G loss: 0.670867]\n",
      "epoch:6 step:5033 [D loss: 0.093184, acc.: 99.22%] [G loss: 0.952788]\n",
      "epoch:6 step:5034 [D loss: 0.097973, acc.: 97.66%] [G loss: 0.281474]\n",
      "epoch:6 step:5035 [D loss: 0.092846, acc.: 97.66%] [G loss: 0.787601]\n",
      "epoch:6 step:5036 [D loss: 0.652304, acc.: 65.62%] [G loss: 1.544884]\n",
      "epoch:6 step:5037 [D loss: 0.004142, acc.: 100.00%] [G loss: 2.736768]\n",
      "epoch:6 step:5038 [D loss: 0.126865, acc.: 96.09%] [G loss: 2.155532]\n",
      "epoch:6 step:5039 [D loss: 0.120865, acc.: 97.66%] [G loss: 0.935197]\n",
      "epoch:6 step:5040 [D loss: 0.103563, acc.: 99.22%] [G loss: 2.950691]\n",
      "epoch:6 step:5041 [D loss: 0.045823, acc.: 99.22%] [G loss: 2.696264]\n",
      "epoch:6 step:5042 [D loss: 0.417546, acc.: 79.69%] [G loss: 4.849893]\n",
      "epoch:6 step:5043 [D loss: 0.091002, acc.: 96.88%] [G loss: 5.585221]\n",
      "epoch:6 step:5044 [D loss: 0.633238, acc.: 69.53%] [G loss: 3.260398]\n",
      "epoch:6 step:5045 [D loss: 0.015149, acc.: 100.00%] [G loss: 5.401954]\n",
      "epoch:6 step:5046 [D loss: 0.101322, acc.: 97.66%] [G loss: 2.880482]\n",
      "epoch:6 step:5047 [D loss: 0.429299, acc.: 76.56%] [G loss: 5.279448]\n",
      "epoch:6 step:5048 [D loss: 0.047008, acc.: 99.22%] [G loss: 5.587257]\n",
      "epoch:6 step:5049 [D loss: 0.220260, acc.: 89.84%] [G loss: 2.798431]\n",
      "epoch:6 step:5050 [D loss: 0.405918, acc.: 82.03%] [G loss: 6.623772]\n",
      "epoch:6 step:5051 [D loss: 0.217693, acc.: 90.62%] [G loss: 5.145779]\n",
      "epoch:6 step:5052 [D loss: 0.265057, acc.: 86.72%] [G loss: 2.734778]\n",
      "epoch:6 step:5053 [D loss: 0.046103, acc.: 99.22%] [G loss: 2.646348]\n",
      "epoch:6 step:5054 [D loss: 0.193494, acc.: 90.62%] [G loss: 6.027977]\n",
      "epoch:6 step:5055 [D loss: 0.075019, acc.: 97.66%] [G loss: 5.377107]\n",
      "epoch:6 step:5056 [D loss: 0.488686, acc.: 75.78%] [G loss: 5.426362]\n",
      "epoch:6 step:5057 [D loss: 0.065863, acc.: 97.66%] [G loss: 5.576399]\n",
      "epoch:6 step:5058 [D loss: 0.109264, acc.: 96.09%] [G loss: 4.577612]\n",
      "epoch:6 step:5059 [D loss: 0.670242, acc.: 67.19%] [G loss: 7.273932]\n",
      "epoch:6 step:5060 [D loss: 0.183137, acc.: 91.41%] [G loss: 7.061054]\n",
      "epoch:6 step:5061 [D loss: 0.212030, acc.: 89.84%] [G loss: 2.935759]\n",
      "epoch:6 step:5062 [D loss: 0.146891, acc.: 95.31%] [G loss: 5.050066]\n",
      "epoch:6 step:5063 [D loss: 0.011843, acc.: 100.00%] [G loss: 4.671907]\n",
      "epoch:6 step:5064 [D loss: 0.330330, acc.: 87.50%] [G loss: 3.583958]\n",
      "epoch:6 step:5065 [D loss: 0.090093, acc.: 98.44%] [G loss: 4.013071]\n",
      "epoch:6 step:5066 [D loss: 0.169096, acc.: 93.75%] [G loss: 1.775969]\n",
      "epoch:6 step:5067 [D loss: 0.112840, acc.: 95.31%] [G loss: 2.648247]\n",
      "epoch:6 step:5068 [D loss: 0.518481, acc.: 71.88%] [G loss: 5.439461]\n",
      "epoch:6 step:5069 [D loss: 0.506598, acc.: 74.22%] [G loss: 1.315343]\n",
      "epoch:6 step:5070 [D loss: 0.155811, acc.: 92.19%] [G loss: 4.166639]\n",
      "epoch:6 step:5071 [D loss: 0.087423, acc.: 96.88%] [G loss: 3.120258]\n",
      "epoch:6 step:5072 [D loss: 0.040559, acc.: 100.00%] [G loss: 3.149757]\n",
      "epoch:6 step:5073 [D loss: 0.177122, acc.: 92.97%] [G loss: 4.058007]\n",
      "epoch:6 step:5074 [D loss: 0.053128, acc.: 100.00%] [G loss: 4.989363]\n",
      "epoch:6 step:5075 [D loss: 0.099474, acc.: 98.44%] [G loss: 2.925868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5076 [D loss: 0.313691, acc.: 90.62%] [G loss: 5.208365]\n",
      "epoch:6 step:5077 [D loss: 0.829781, acc.: 59.38%] [G loss: 7.317184]\n",
      "epoch:6 step:5078 [D loss: 0.634122, acc.: 71.88%] [G loss: 3.151581]\n",
      "epoch:6 step:5079 [D loss: 0.430631, acc.: 78.91%] [G loss: 6.372470]\n",
      "epoch:6 step:5080 [D loss: 0.255342, acc.: 87.50%] [G loss: 4.922731]\n",
      "epoch:6 step:5081 [D loss: 0.299499, acc.: 88.28%] [G loss: 4.279748]\n",
      "epoch:6 step:5082 [D loss: 0.030346, acc.: 100.00%] [G loss: 2.954931]\n",
      "epoch:6 step:5083 [D loss: 0.025362, acc.: 100.00%] [G loss: 3.917463]\n",
      "epoch:6 step:5084 [D loss: 0.145201, acc.: 96.88%] [G loss: 3.873158]\n",
      "epoch:6 step:5085 [D loss: 0.186791, acc.: 92.97%] [G loss: 2.047325]\n",
      "epoch:6 step:5086 [D loss: 0.038061, acc.: 99.22%] [G loss: 1.554168]\n",
      "epoch:6 step:5087 [D loss: 0.173994, acc.: 93.75%] [G loss: 3.251691]\n",
      "epoch:6 step:5088 [D loss: 0.156192, acc.: 94.53%] [G loss: 1.012396]\n",
      "epoch:6 step:5089 [D loss: 0.651074, acc.: 71.09%] [G loss: 7.139743]\n",
      "epoch:6 step:5090 [D loss: 0.802145, acc.: 60.94%] [G loss: 3.643621]\n",
      "epoch:6 step:5091 [D loss: 0.161306, acc.: 94.53%] [G loss: 0.909278]\n",
      "epoch:6 step:5092 [D loss: 0.326920, acc.: 82.03%] [G loss: 3.656651]\n",
      "epoch:6 step:5093 [D loss: 0.048503, acc.: 99.22%] [G loss: 4.538960]\n",
      "epoch:6 step:5094 [D loss: 0.083618, acc.: 98.44%] [G loss: 2.864264]\n",
      "epoch:6 step:5095 [D loss: 0.237192, acc.: 91.41%] [G loss: 2.347033]\n",
      "epoch:6 step:5096 [D loss: 0.133657, acc.: 95.31%] [G loss: 4.102830]\n",
      "epoch:6 step:5097 [D loss: 1.697707, acc.: 28.12%] [G loss: 3.738249]\n",
      "epoch:6 step:5098 [D loss: 0.045070, acc.: 100.00%] [G loss: 3.696249]\n",
      "epoch:6 step:5099 [D loss: 0.098661, acc.: 99.22%] [G loss: 3.788032]\n",
      "epoch:6 step:5100 [D loss: 0.044414, acc.: 100.00%] [G loss: 2.814093]\n",
      "epoch:6 step:5101 [D loss: 0.162034, acc.: 95.31%] [G loss: 2.682031]\n",
      "epoch:6 step:5102 [D loss: 0.263731, acc.: 91.41%] [G loss: 2.260297]\n",
      "epoch:6 step:5103 [D loss: 0.222463, acc.: 92.19%] [G loss: 3.035620]\n",
      "epoch:6 step:5104 [D loss: 0.075664, acc.: 98.44%] [G loss: 2.632912]\n",
      "epoch:6 step:5105 [D loss: 0.130712, acc.: 98.44%] [G loss: 2.919198]\n",
      "epoch:6 step:5106 [D loss: 0.399605, acc.: 83.59%] [G loss: 4.592230]\n",
      "epoch:6 step:5107 [D loss: 0.206298, acc.: 93.75%] [G loss: 3.609452]\n",
      "epoch:6 step:5108 [D loss: 0.130587, acc.: 98.44%] [G loss: 3.339558]\n",
      "epoch:6 step:5109 [D loss: 0.059038, acc.: 99.22%] [G loss: 3.189591]\n",
      "epoch:6 step:5110 [D loss: 0.104062, acc.: 99.22%] [G loss: 2.792500]\n",
      "epoch:6 step:5111 [D loss: 0.152882, acc.: 94.53%] [G loss: 2.980113]\n",
      "epoch:6 step:5112 [D loss: 0.076226, acc.: 98.44%] [G loss: 1.692545]\n",
      "epoch:6 step:5113 [D loss: 0.192665, acc.: 92.97%] [G loss: 4.534806]\n",
      "epoch:6 step:5114 [D loss: 0.968956, acc.: 57.03%] [G loss: 1.289294]\n",
      "epoch:6 step:5115 [D loss: 0.173368, acc.: 93.75%] [G loss: 3.624331]\n",
      "epoch:6 step:5116 [D loss: 0.421453, acc.: 80.47%] [G loss: 3.200757]\n",
      "epoch:6 step:5117 [D loss: 0.069480, acc.: 99.22%] [G loss: 3.060776]\n",
      "epoch:6 step:5118 [D loss: 0.205317, acc.: 95.31%] [G loss: 3.293083]\n",
      "epoch:6 step:5119 [D loss: 0.030207, acc.: 100.00%] [G loss: 4.493895]\n",
      "epoch:6 step:5120 [D loss: 0.290971, acc.: 89.06%] [G loss: 3.269801]\n",
      "epoch:6 step:5121 [D loss: 0.135914, acc.: 96.88%] [G loss: 3.866500]\n",
      "epoch:6 step:5122 [D loss: 0.519607, acc.: 71.88%] [G loss: 4.160210]\n",
      "epoch:6 step:5123 [D loss: 0.075336, acc.: 97.66%] [G loss: 4.379060]\n",
      "epoch:6 step:5124 [D loss: 0.052917, acc.: 99.22%] [G loss: 4.727416]\n",
      "epoch:6 step:5125 [D loss: 0.056062, acc.: 100.00%] [G loss: 3.769087]\n",
      "epoch:6 step:5126 [D loss: 0.163378, acc.: 92.97%] [G loss: 3.711869]\n",
      "epoch:6 step:5127 [D loss: 0.350193, acc.: 85.16%] [G loss: 2.642442]\n",
      "epoch:6 step:5128 [D loss: 0.191793, acc.: 92.97%] [G loss: 3.766556]\n",
      "epoch:6 step:5129 [D loss: 0.106519, acc.: 98.44%] [G loss: 3.993104]\n",
      "epoch:6 step:5130 [D loss: 0.100886, acc.: 97.66%] [G loss: 4.076388]\n",
      "epoch:6 step:5131 [D loss: 0.044862, acc.: 99.22%] [G loss: 2.742299]\n",
      "epoch:6 step:5132 [D loss: 0.184834, acc.: 93.75%] [G loss: 4.211761]\n",
      "epoch:6 step:5133 [D loss: 0.453056, acc.: 79.69%] [G loss: 1.127240]\n",
      "epoch:6 step:5134 [D loss: 0.263687, acc.: 85.16%] [G loss: 4.984059]\n",
      "epoch:6 step:5135 [D loss: 0.550267, acc.: 75.00%] [G loss: 3.019886]\n",
      "epoch:6 step:5136 [D loss: 0.039172, acc.: 100.00%] [G loss: 2.546667]\n",
      "epoch:6 step:5137 [D loss: 0.365709, acc.: 82.81%] [G loss: 6.338355]\n",
      "epoch:6 step:5138 [D loss: 0.239010, acc.: 87.50%] [G loss: 4.120356]\n",
      "epoch:6 step:5139 [D loss: 0.322034, acc.: 89.84%] [G loss: 5.594993]\n",
      "epoch:6 step:5140 [D loss: 0.068775, acc.: 100.00%] [G loss: 5.480409]\n",
      "epoch:6 step:5141 [D loss: 0.334957, acc.: 86.72%] [G loss: 5.346991]\n",
      "epoch:6 step:5142 [D loss: 0.099282, acc.: 96.09%] [G loss: 3.354757]\n",
      "epoch:6 step:5143 [D loss: 0.011004, acc.: 100.00%] [G loss: 2.929580]\n",
      "epoch:6 step:5144 [D loss: 0.146364, acc.: 95.31%] [G loss: 2.799821]\n",
      "epoch:6 step:5145 [D loss: 0.316808, acc.: 82.03%] [G loss: 3.873137]\n",
      "epoch:6 step:5146 [D loss: 0.552544, acc.: 75.00%] [G loss: 0.609024]\n",
      "epoch:6 step:5147 [D loss: 0.175994, acc.: 92.97%] [G loss: 2.573947]\n",
      "epoch:6 step:5148 [D loss: 0.021713, acc.: 100.00%] [G loss: 3.865203]\n",
      "epoch:6 step:5149 [D loss: 0.902436, acc.: 53.12%] [G loss: 6.403677]\n",
      "epoch:6 step:5150 [D loss: 0.139089, acc.: 93.75%] [G loss: 6.832916]\n",
      "epoch:6 step:5151 [D loss: 0.084839, acc.: 96.88%] [G loss: 3.352635]\n",
      "epoch:6 step:5152 [D loss: 0.273060, acc.: 89.84%] [G loss: 5.707915]\n",
      "epoch:6 step:5153 [D loss: 0.491466, acc.: 78.12%] [G loss: 5.489583]\n",
      "epoch:6 step:5154 [D loss: 0.053458, acc.: 98.44%] [G loss: 5.341947]\n",
      "epoch:6 step:5155 [D loss: 0.144033, acc.: 93.75%] [G loss: 3.717262]\n",
      "epoch:6 step:5156 [D loss: 0.189869, acc.: 93.75%] [G loss: 4.787342]\n",
      "epoch:6 step:5157 [D loss: 0.316581, acc.: 85.16%] [G loss: 2.794713]\n",
      "epoch:6 step:5158 [D loss: 0.609866, acc.: 68.75%] [G loss: 9.716705]\n",
      "epoch:6 step:5159 [D loss: 0.768100, acc.: 63.28%] [G loss: 5.843410]\n",
      "epoch:6 step:5160 [D loss: 0.299572, acc.: 89.06%] [G loss: 5.083495]\n",
      "epoch:6 step:5161 [D loss: 0.021954, acc.: 100.00%] [G loss: 4.548772]\n",
      "epoch:6 step:5162 [D loss: 0.028147, acc.: 100.00%] [G loss: 3.140023]\n",
      "epoch:6 step:5163 [D loss: 0.151346, acc.: 95.31%] [G loss: 4.693476]\n",
      "epoch:6 step:5164 [D loss: 0.291201, acc.: 92.19%] [G loss: 3.345289]\n",
      "epoch:6 step:5165 [D loss: 0.182625, acc.: 94.53%] [G loss: 4.888524]\n",
      "epoch:6 step:5166 [D loss: 0.131370, acc.: 95.31%] [G loss: 4.657833]\n",
      "epoch:6 step:5167 [D loss: 0.534016, acc.: 78.91%] [G loss: 4.043241]\n",
      "epoch:6 step:5168 [D loss: 0.406867, acc.: 80.47%] [G loss: 3.502182]\n",
      "epoch:6 step:5169 [D loss: 0.089165, acc.: 99.22%] [G loss: 3.144544]\n",
      "epoch:6 step:5170 [D loss: 0.134830, acc.: 96.88%] [G loss: 3.567154]\n",
      "epoch:6 step:5171 [D loss: 0.265015, acc.: 88.28%] [G loss: 4.964737]\n",
      "epoch:6 step:5172 [D loss: 0.029054, acc.: 99.22%] [G loss: 4.045384]\n",
      "epoch:6 step:5173 [D loss: 0.083035, acc.: 100.00%] [G loss: 2.209352]\n",
      "epoch:6 step:5174 [D loss: 0.197734, acc.: 92.97%] [G loss: 4.809842]\n",
      "epoch:6 step:5175 [D loss: 0.308731, acc.: 85.16%] [G loss: 4.071282]\n",
      "epoch:6 step:5176 [D loss: 0.093783, acc.: 96.09%] [G loss: 3.280750]\n",
      "epoch:6 step:5177 [D loss: 0.164487, acc.: 95.31%] [G loss: 2.459388]\n",
      "epoch:6 step:5178 [D loss: 0.383223, acc.: 82.81%] [G loss: 6.554231]\n",
      "epoch:6 step:5179 [D loss: 0.212661, acc.: 91.41%] [G loss: 4.104889]\n",
      "epoch:6 step:5180 [D loss: 0.124907, acc.: 95.31%] [G loss: 3.644110]\n",
      "epoch:6 step:5181 [D loss: 0.081935, acc.: 98.44%] [G loss: 4.182387]\n",
      "epoch:6 step:5182 [D loss: 0.124777, acc.: 97.66%] [G loss: 4.348471]\n",
      "epoch:6 step:5183 [D loss: 0.492442, acc.: 75.78%] [G loss: 6.877544]\n",
      "epoch:6 step:5184 [D loss: 0.480612, acc.: 74.22%] [G loss: 3.364565]\n",
      "epoch:6 step:5185 [D loss: 0.316506, acc.: 81.25%] [G loss: 5.800307]\n",
      "epoch:6 step:5186 [D loss: 0.159586, acc.: 94.53%] [G loss: 5.231240]\n",
      "epoch:6 step:5187 [D loss: 0.510499, acc.: 74.22%] [G loss: 3.645098]\n",
      "epoch:6 step:5188 [D loss: 0.033818, acc.: 100.00%] [G loss: 3.670890]\n",
      "epoch:6 step:5189 [D loss: 0.060771, acc.: 98.44%] [G loss: 3.837400]\n",
      "epoch:6 step:5190 [D loss: 0.099141, acc.: 97.66%] [G loss: 2.802786]\n",
      "epoch:6 step:5191 [D loss: 0.038709, acc.: 100.00%] [G loss: 3.044991]\n",
      "epoch:6 step:5192 [D loss: 0.117906, acc.: 97.66%] [G loss: 2.781219]\n",
      "epoch:6 step:5193 [D loss: 0.045773, acc.: 100.00%] [G loss: 3.212233]\n",
      "epoch:6 step:5194 [D loss: 0.233748, acc.: 91.41%] [G loss: 2.479227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5195 [D loss: 0.152036, acc.: 96.09%] [G loss: 0.305591]\n",
      "epoch:6 step:5196 [D loss: 0.204007, acc.: 90.62%] [G loss: 3.482891]\n",
      "epoch:6 step:5197 [D loss: 0.273249, acc.: 84.38%] [G loss: 1.810210]\n",
      "epoch:6 step:5198 [D loss: 0.168908, acc.: 92.97%] [G loss: 3.436358]\n",
      "epoch:6 step:5199 [D loss: 0.159028, acc.: 92.97%] [G loss: 2.945122]\n",
      "epoch:6 step:5200 [D loss: 0.090669, acc.: 96.88%] [G loss: 3.059719]\n",
      "epoch:6 step:5201 [D loss: 0.030368, acc.: 100.00%] [G loss: 1.594652]\n",
      "epoch:6 step:5202 [D loss: 0.308565, acc.: 86.72%] [G loss: 5.296116]\n",
      "epoch:6 step:5203 [D loss: 0.796088, acc.: 61.72%] [G loss: 1.522547]\n",
      "epoch:6 step:5204 [D loss: 0.326320, acc.: 82.81%] [G loss: 5.244636]\n",
      "epoch:6 step:5205 [D loss: 0.203416, acc.: 88.28%] [G loss: 3.258591]\n",
      "epoch:6 step:5206 [D loss: 0.117037, acc.: 96.88%] [G loss: 3.607702]\n",
      "epoch:6 step:5207 [D loss: 0.147111, acc.: 92.97%] [G loss: 5.564737]\n",
      "epoch:6 step:5208 [D loss: 0.403511, acc.: 82.81%] [G loss: 3.492039]\n",
      "epoch:6 step:5209 [D loss: 0.030365, acc.: 100.00%] [G loss: 3.909345]\n",
      "epoch:6 step:5210 [D loss: 0.281598, acc.: 89.06%] [G loss: 3.615919]\n",
      "epoch:6 step:5211 [D loss: 0.038574, acc.: 100.00%] [G loss: 5.567650]\n",
      "epoch:6 step:5212 [D loss: 0.709935, acc.: 64.06%] [G loss: 1.655605]\n",
      "epoch:6 step:5213 [D loss: 0.124600, acc.: 95.31%] [G loss: 4.812236]\n",
      "epoch:6 step:5214 [D loss: 0.022693, acc.: 100.00%] [G loss: 4.646312]\n",
      "epoch:6 step:5215 [D loss: 0.422436, acc.: 79.69%] [G loss: 6.440130]\n",
      "epoch:6 step:5216 [D loss: 0.979126, acc.: 52.34%] [G loss: 1.922847]\n",
      "epoch:6 step:5217 [D loss: 0.020029, acc.: 100.00%] [G loss: 3.146809]\n",
      "epoch:6 step:5218 [D loss: 0.038414, acc.: 99.22%] [G loss: 2.401945]\n",
      "epoch:6 step:5219 [D loss: 0.019997, acc.: 100.00%] [G loss: 1.381054]\n",
      "epoch:6 step:5220 [D loss: 0.469846, acc.: 80.47%] [G loss: 6.528551]\n",
      "epoch:6 step:5221 [D loss: 0.475130, acc.: 71.88%] [G loss: 3.139106]\n",
      "epoch:6 step:5222 [D loss: 0.232183, acc.: 89.06%] [G loss: 3.734970]\n",
      "epoch:6 step:5223 [D loss: 0.044131, acc.: 99.22%] [G loss: 3.398086]\n",
      "epoch:6 step:5224 [D loss: 0.087028, acc.: 96.09%] [G loss: 1.978679]\n",
      "epoch:6 step:5225 [D loss: 0.450080, acc.: 78.91%] [G loss: 6.598569]\n",
      "epoch:6 step:5226 [D loss: 0.490104, acc.: 73.44%] [G loss: 3.866276]\n",
      "epoch:6 step:5227 [D loss: 0.074233, acc.: 98.44%] [G loss: 4.726583]\n",
      "epoch:6 step:5228 [D loss: 0.251571, acc.: 90.62%] [G loss: 4.123477]\n",
      "epoch:6 step:5229 [D loss: 0.188442, acc.: 91.41%] [G loss: 5.047416]\n",
      "epoch:6 step:5230 [D loss: 0.106755, acc.: 95.31%] [G loss: 2.100865]\n",
      "epoch:6 step:5231 [D loss: 0.305641, acc.: 85.16%] [G loss: 5.875469]\n",
      "epoch:6 step:5232 [D loss: 0.097812, acc.: 97.66%] [G loss: 6.159116]\n",
      "epoch:6 step:5233 [D loss: 0.534528, acc.: 75.00%] [G loss: 3.631036]\n",
      "epoch:6 step:5234 [D loss: 0.092203, acc.: 96.88%] [G loss: 5.156169]\n",
      "epoch:6 step:5235 [D loss: 0.054018, acc.: 99.22%] [G loss: 4.750027]\n",
      "epoch:6 step:5236 [D loss: 0.137211, acc.: 96.88%] [G loss: 2.753885]\n",
      "epoch:6 step:5237 [D loss: 0.069795, acc.: 98.44%] [G loss: 3.217676]\n",
      "epoch:6 step:5238 [D loss: 0.161866, acc.: 94.53%] [G loss: 2.469664]\n",
      "epoch:6 step:5239 [D loss: 0.091639, acc.: 97.66%] [G loss: 2.886128]\n",
      "epoch:6 step:5240 [D loss: 0.513466, acc.: 75.00%] [G loss: 3.626804]\n",
      "epoch:6 step:5241 [D loss: 0.079506, acc.: 98.44%] [G loss: 3.699099]\n",
      "epoch:6 step:5242 [D loss: 0.112048, acc.: 98.44%] [G loss: 3.495694]\n",
      "epoch:6 step:5243 [D loss: 0.622375, acc.: 70.31%] [G loss: 7.355334]\n",
      "epoch:6 step:5244 [D loss: 0.360727, acc.: 82.03%] [G loss: 6.032140]\n",
      "epoch:6 step:5245 [D loss: 0.074092, acc.: 97.66%] [G loss: 4.341559]\n",
      "epoch:6 step:5246 [D loss: 0.036625, acc.: 99.22%] [G loss: 4.726652]\n",
      "epoch:6 step:5247 [D loss: 0.149263, acc.: 96.09%] [G loss: 1.941989]\n",
      "epoch:6 step:5248 [D loss: 0.154585, acc.: 93.75%] [G loss: 3.386976]\n",
      "epoch:6 step:5249 [D loss: 0.061780, acc.: 99.22%] [G loss: 5.579012]\n",
      "epoch:6 step:5250 [D loss: 0.076429, acc.: 98.44%] [G loss: 3.704210]\n",
      "epoch:6 step:5251 [D loss: 0.156711, acc.: 95.31%] [G loss: 4.947286]\n",
      "epoch:6 step:5252 [D loss: 0.529527, acc.: 71.88%] [G loss: 6.227265]\n",
      "epoch:6 step:5253 [D loss: 0.108844, acc.: 96.09%] [G loss: 6.458138]\n",
      "epoch:6 step:5254 [D loss: 0.116861, acc.: 95.31%] [G loss: 2.138455]\n",
      "epoch:6 step:5255 [D loss: 0.088343, acc.: 96.88%] [G loss: 3.654195]\n",
      "epoch:6 step:5256 [D loss: 0.114833, acc.: 96.88%] [G loss: 5.247836]\n",
      "epoch:6 step:5257 [D loss: 0.621301, acc.: 68.75%] [G loss: 5.459048]\n",
      "epoch:6 step:5258 [D loss: 0.045820, acc.: 100.00%] [G loss: 4.481936]\n",
      "epoch:6 step:5259 [D loss: 0.349172, acc.: 83.59%] [G loss: 3.925146]\n",
      "epoch:6 step:5260 [D loss: 0.139371, acc.: 96.09%] [G loss: 5.422593]\n",
      "epoch:6 step:5261 [D loss: 0.453021, acc.: 79.69%] [G loss: 2.749484]\n",
      "epoch:6 step:5262 [D loss: 0.041612, acc.: 99.22%] [G loss: 2.514824]\n",
      "epoch:6 step:5263 [D loss: 0.266105, acc.: 92.97%] [G loss: 4.621792]\n",
      "epoch:6 step:5264 [D loss: 0.230266, acc.: 90.62%] [G loss: 2.183872]\n",
      "epoch:6 step:5265 [D loss: 0.268284, acc.: 88.28%] [G loss: 4.546465]\n",
      "epoch:6 step:5266 [D loss: 0.119355, acc.: 94.53%] [G loss: 3.296088]\n",
      "epoch:6 step:5267 [D loss: 0.284923, acc.: 86.72%] [G loss: 4.944925]\n",
      "epoch:6 step:5268 [D loss: 0.341415, acc.: 84.38%] [G loss: 6.464852]\n",
      "epoch:6 step:5269 [D loss: 0.276811, acc.: 86.72%] [G loss: 4.377332]\n",
      "epoch:6 step:5270 [D loss: 0.203104, acc.: 92.97%] [G loss: 7.542508]\n",
      "epoch:6 step:5271 [D loss: 0.045172, acc.: 99.22%] [G loss: 7.622767]\n",
      "epoch:6 step:5272 [D loss: 0.070983, acc.: 98.44%] [G loss: 7.002929]\n",
      "epoch:6 step:5273 [D loss: 0.098271, acc.: 97.66%] [G loss: 7.057584]\n",
      "epoch:6 step:5274 [D loss: 0.070898, acc.: 98.44%] [G loss: 5.679998]\n",
      "epoch:6 step:5275 [D loss: 0.042739, acc.: 99.22%] [G loss: 3.497774]\n",
      "epoch:6 step:5276 [D loss: 0.168232, acc.: 91.41%] [G loss: 7.072909]\n",
      "epoch:6 step:5277 [D loss: 0.203607, acc.: 89.84%] [G loss: 4.246829]\n",
      "epoch:6 step:5278 [D loss: 0.180602, acc.: 90.62%] [G loss: 6.684941]\n",
      "epoch:6 step:5279 [D loss: 0.189543, acc.: 89.84%] [G loss: 5.652841]\n",
      "epoch:6 step:5280 [D loss: 0.039757, acc.: 100.00%] [G loss: 3.324198]\n",
      "epoch:6 step:5281 [D loss: 0.114558, acc.: 96.09%] [G loss: 4.524962]\n",
      "epoch:6 step:5282 [D loss: 0.117816, acc.: 98.44%] [G loss: 3.708144]\n",
      "epoch:6 step:5283 [D loss: 0.117549, acc.: 99.22%] [G loss: 4.454155]\n",
      "epoch:6 step:5284 [D loss: 0.235525, acc.: 92.19%] [G loss: 0.820546]\n",
      "epoch:6 step:5285 [D loss: 0.101641, acc.: 97.66%] [G loss: 0.839015]\n",
      "epoch:6 step:5286 [D loss: 0.020392, acc.: 100.00%] [G loss: 1.921858]\n",
      "epoch:6 step:5287 [D loss: 0.024636, acc.: 100.00%] [G loss: 0.517831]\n",
      "epoch:6 step:5288 [D loss: 0.072436, acc.: 97.66%] [G loss: 1.010659]\n",
      "epoch:6 step:5289 [D loss: 0.845538, acc.: 60.16%] [G loss: 10.174627]\n",
      "epoch:6 step:5290 [D loss: 1.614780, acc.: 51.56%] [G loss: 5.536145]\n",
      "epoch:6 step:5291 [D loss: 0.068909, acc.: 98.44%] [G loss: 2.957152]\n",
      "epoch:6 step:5292 [D loss: 0.047974, acc.: 99.22%] [G loss: 2.177673]\n",
      "epoch:6 step:5293 [D loss: 0.119668, acc.: 96.09%] [G loss: 3.523848]\n",
      "epoch:6 step:5294 [D loss: 0.139358, acc.: 94.53%] [G loss: 2.241367]\n",
      "epoch:6 step:5295 [D loss: 0.456269, acc.: 78.12%] [G loss: 5.187671]\n",
      "epoch:6 step:5296 [D loss: 0.346642, acc.: 85.16%] [G loss: 2.036576]\n",
      "epoch:6 step:5297 [D loss: 0.295081, acc.: 87.50%] [G loss: 3.086909]\n",
      "epoch:6 step:5298 [D loss: 0.106966, acc.: 96.88%] [G loss: 4.435984]\n",
      "epoch:6 step:5299 [D loss: 0.665123, acc.: 67.19%] [G loss: 4.718691]\n",
      "epoch:6 step:5300 [D loss: 0.038685, acc.: 100.00%] [G loss: 4.903659]\n",
      "epoch:6 step:5301 [D loss: 0.105831, acc.: 97.66%] [G loss: 3.298069]\n",
      "epoch:6 step:5302 [D loss: 0.210438, acc.: 92.97%] [G loss: 3.615243]\n",
      "epoch:6 step:5303 [D loss: 0.060557, acc.: 100.00%] [G loss: 4.046907]\n",
      "epoch:6 step:5304 [D loss: 0.122737, acc.: 95.31%] [G loss: 2.612497]\n",
      "epoch:6 step:5305 [D loss: 0.502807, acc.: 76.56%] [G loss: 6.126812]\n",
      "epoch:6 step:5306 [D loss: 0.332061, acc.: 84.38%] [G loss: 4.000278]\n",
      "epoch:6 step:5307 [D loss: 0.033317, acc.: 100.00%] [G loss: 1.638939]\n",
      "epoch:6 step:5308 [D loss: 0.096301, acc.: 98.44%] [G loss: 2.857097]\n",
      "epoch:6 step:5309 [D loss: 0.125520, acc.: 98.44%] [G loss: 2.118693]\n",
      "epoch:6 step:5310 [D loss: 0.051505, acc.: 98.44%] [G loss: 3.416934]\n",
      "epoch:6 step:5311 [D loss: 0.202502, acc.: 95.31%] [G loss: 1.489974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5312 [D loss: 0.089166, acc.: 98.44%] [G loss: 2.178885]\n",
      "epoch:6 step:5313 [D loss: 0.688664, acc.: 67.97%] [G loss: 4.459602]\n",
      "epoch:6 step:5314 [D loss: 0.484526, acc.: 77.34%] [G loss: 1.075065]\n",
      "epoch:6 step:5315 [D loss: 0.119603, acc.: 96.88%] [G loss: 1.711530]\n",
      "epoch:6 step:5316 [D loss: 0.007404, acc.: 100.00%] [G loss: 0.582114]\n",
      "epoch:6 step:5317 [D loss: 0.368541, acc.: 85.16%] [G loss: 5.164653]\n",
      "epoch:6 step:5318 [D loss: 0.370134, acc.: 81.25%] [G loss: 3.064707]\n",
      "epoch:6 step:5319 [D loss: 0.058790, acc.: 98.44%] [G loss: 1.628227]\n",
      "epoch:6 step:5320 [D loss: 0.027000, acc.: 100.00%] [G loss: 1.172500]\n",
      "epoch:6 step:5321 [D loss: 0.089007, acc.: 97.66%] [G loss: 0.978642]\n",
      "epoch:6 step:5322 [D loss: 0.014571, acc.: 100.00%] [G loss: 1.015285]\n",
      "epoch:6 step:5323 [D loss: 0.167038, acc.: 95.31%] [G loss: 0.301364]\n",
      "epoch:6 step:5324 [D loss: 0.046717, acc.: 99.22%] [G loss: 0.331086]\n",
      "epoch:6 step:5325 [D loss: 0.055825, acc.: 99.22%] [G loss: 0.281233]\n",
      "epoch:6 step:5326 [D loss: 0.772284, acc.: 66.41%] [G loss: 8.445410]\n",
      "epoch:6 step:5327 [D loss: 1.505861, acc.: 53.91%] [G loss: 3.480164]\n",
      "epoch:6 step:5328 [D loss: 0.189114, acc.: 92.97%] [G loss: 4.489715]\n",
      "epoch:6 step:5329 [D loss: 0.054113, acc.: 99.22%] [G loss: 4.849635]\n",
      "epoch:6 step:5330 [D loss: 0.064012, acc.: 99.22%] [G loss: 3.630216]\n",
      "epoch:6 step:5331 [D loss: 0.134908, acc.: 97.66%] [G loss: 3.023780]\n",
      "epoch:6 step:5332 [D loss: 0.799356, acc.: 61.72%] [G loss: 5.187234]\n",
      "epoch:6 step:5333 [D loss: 0.139390, acc.: 95.31%] [G loss: 5.064579]\n",
      "epoch:6 step:5334 [D loss: 0.070028, acc.: 98.44%] [G loss: 4.709958]\n",
      "epoch:6 step:5335 [D loss: 0.275934, acc.: 87.50%] [G loss: 3.297421]\n",
      "epoch:6 step:5336 [D loss: 0.137998, acc.: 96.88%] [G loss: 3.757977]\n",
      "epoch:6 step:5337 [D loss: 0.066444, acc.: 99.22%] [G loss: 4.550814]\n",
      "epoch:6 step:5338 [D loss: 0.111295, acc.: 96.88%] [G loss: 2.866907]\n",
      "epoch:6 step:5339 [D loss: 0.133716, acc.: 96.88%] [G loss: 4.828654]\n",
      "epoch:6 step:5340 [D loss: 0.174323, acc.: 92.97%] [G loss: 2.393216]\n",
      "epoch:6 step:5341 [D loss: 0.251470, acc.: 89.06%] [G loss: 3.203263]\n",
      "epoch:6 step:5342 [D loss: 0.450817, acc.: 79.69%] [G loss: 3.289993]\n",
      "epoch:6 step:5343 [D loss: 0.023331, acc.: 100.00%] [G loss: 3.711855]\n",
      "epoch:6 step:5344 [D loss: 0.027938, acc.: 100.00%] [G loss: 3.325455]\n",
      "epoch:6 step:5345 [D loss: 0.149207, acc.: 93.75%] [G loss: 4.029644]\n",
      "epoch:6 step:5346 [D loss: 0.206743, acc.: 93.75%] [G loss: 4.120292]\n",
      "epoch:6 step:5347 [D loss: 0.075517, acc.: 98.44%] [G loss: 3.330373]\n",
      "epoch:6 step:5348 [D loss: 0.161692, acc.: 96.09%] [G loss: 5.893504]\n",
      "epoch:6 step:5349 [D loss: 0.165132, acc.: 95.31%] [G loss: 4.636986]\n",
      "epoch:6 step:5350 [D loss: 0.069688, acc.: 99.22%] [G loss: 5.123194]\n",
      "epoch:6 step:5351 [D loss: 0.349077, acc.: 82.81%] [G loss: 6.083075]\n",
      "epoch:6 step:5352 [D loss: 0.064938, acc.: 98.44%] [G loss: 5.494042]\n",
      "epoch:6 step:5353 [D loss: 0.319157, acc.: 85.94%] [G loss: 2.263553]\n",
      "epoch:6 step:5354 [D loss: 0.098540, acc.: 98.44%] [G loss: 3.273980]\n",
      "epoch:6 step:5355 [D loss: 0.079519, acc.: 99.22%] [G loss: 5.443105]\n",
      "epoch:6 step:5356 [D loss: 0.142780, acc.: 94.53%] [G loss: 4.024544]\n",
      "epoch:6 step:5357 [D loss: 0.479637, acc.: 76.56%] [G loss: 7.930583]\n",
      "epoch:6 step:5358 [D loss: 0.178353, acc.: 92.97%] [G loss: 6.502010]\n",
      "epoch:6 step:5359 [D loss: 0.143263, acc.: 94.53%] [G loss: 5.253988]\n",
      "epoch:6 step:5360 [D loss: 0.303868, acc.: 85.16%] [G loss: 8.374454]\n",
      "epoch:6 step:5361 [D loss: 0.208565, acc.: 90.62%] [G loss: 6.466034]\n",
      "epoch:6 step:5362 [D loss: 0.093456, acc.: 97.66%] [G loss: 3.304596]\n",
      "epoch:6 step:5363 [D loss: 0.133175, acc.: 95.31%] [G loss: 3.974615]\n",
      "epoch:6 step:5364 [D loss: 0.032439, acc.: 100.00%] [G loss: 4.190055]\n",
      "epoch:6 step:5365 [D loss: 0.461338, acc.: 75.00%] [G loss: 7.155417]\n",
      "epoch:6 step:5366 [D loss: 0.667502, acc.: 64.06%] [G loss: 3.412861]\n",
      "epoch:6 step:5367 [D loss: 0.071455, acc.: 97.66%] [G loss: 3.002580]\n",
      "epoch:6 step:5368 [D loss: 0.274426, acc.: 90.62%] [G loss: 5.927964]\n",
      "epoch:6 step:5369 [D loss: 1.294598, acc.: 34.38%] [G loss: 7.411478]\n",
      "epoch:6 step:5370 [D loss: 0.078587, acc.: 98.44%] [G loss: 6.992273]\n",
      "epoch:6 step:5371 [D loss: 0.544744, acc.: 77.34%] [G loss: 1.767348]\n",
      "epoch:6 step:5372 [D loss: 0.854674, acc.: 61.72%] [G loss: 8.745175]\n",
      "epoch:6 step:5373 [D loss: 0.877692, acc.: 62.50%] [G loss: 2.656721]\n",
      "epoch:6 step:5374 [D loss: 0.493660, acc.: 73.44%] [G loss: 3.319598]\n",
      "epoch:6 step:5375 [D loss: 0.183353, acc.: 93.75%] [G loss: 3.585091]\n",
      "epoch:6 step:5376 [D loss: 0.192934, acc.: 92.97%] [G loss: 4.079890]\n",
      "epoch:6 step:5377 [D loss: 0.254397, acc.: 88.28%] [G loss: 2.294193]\n",
      "epoch:6 step:5378 [D loss: 1.654725, acc.: 33.59%] [G loss: 6.837330]\n",
      "epoch:6 step:5379 [D loss: 1.124279, acc.: 55.47%] [G loss: 3.389114]\n",
      "epoch:6 step:5380 [D loss: 0.253605, acc.: 88.28%] [G loss: 3.486718]\n",
      "epoch:6 step:5381 [D loss: 0.215581, acc.: 92.97%] [G loss: 3.250184]\n",
      "epoch:6 step:5382 [D loss: 0.150829, acc.: 96.88%] [G loss: 2.556754]\n",
      "epoch:6 step:5383 [D loss: 0.072809, acc.: 99.22%] [G loss: 2.358215]\n",
      "epoch:6 step:5384 [D loss: 0.166924, acc.: 96.88%] [G loss: 3.031787]\n",
      "epoch:6 step:5385 [D loss: 0.081068, acc.: 98.44%] [G loss: 2.324164]\n",
      "epoch:6 step:5386 [D loss: 0.155245, acc.: 98.44%] [G loss: 2.185550]\n",
      "epoch:6 step:5387 [D loss: 0.120109, acc.: 97.66%] [G loss: 2.745641]\n",
      "epoch:6 step:5388 [D loss: 0.179407, acc.: 94.53%] [G loss: 2.177340]\n",
      "epoch:6 step:5389 [D loss: 0.336564, acc.: 87.50%] [G loss: 1.346609]\n",
      "epoch:6 step:5390 [D loss: 0.381269, acc.: 81.25%] [G loss: 4.161582]\n",
      "epoch:6 step:5391 [D loss: 0.191674, acc.: 92.19%] [G loss: 4.242725]\n",
      "epoch:6 step:5392 [D loss: 0.169407, acc.: 95.31%] [G loss: 1.975459]\n",
      "epoch:6 step:5393 [D loss: 0.119586, acc.: 98.44%] [G loss: 0.705198]\n",
      "epoch:6 step:5394 [D loss: 0.376651, acc.: 80.47%] [G loss: 5.690698]\n",
      "epoch:6 step:5395 [D loss: 0.426895, acc.: 75.78%] [G loss: 3.542157]\n",
      "epoch:6 step:5396 [D loss: 0.016468, acc.: 100.00%] [G loss: 2.057480]\n",
      "epoch:6 step:5397 [D loss: 0.030769, acc.: 100.00%] [G loss: 2.106511]\n",
      "epoch:6 step:5398 [D loss: 0.164328, acc.: 93.75%] [G loss: 3.750532]\n",
      "epoch:6 step:5399 [D loss: 0.527674, acc.: 77.34%] [G loss: 1.626165]\n",
      "epoch:6 step:5400 [D loss: 0.439671, acc.: 76.56%] [G loss: 5.726731]\n",
      "epoch:6 step:5401 [D loss: 0.595802, acc.: 71.88%] [G loss: 3.286324]\n",
      "epoch:6 step:5402 [D loss: 0.171418, acc.: 90.62%] [G loss: 4.097735]\n",
      "epoch:6 step:5403 [D loss: 0.044097, acc.: 100.00%] [G loss: 4.638998]\n",
      "epoch:6 step:5404 [D loss: 0.134501, acc.: 96.09%] [G loss: 2.975516]\n",
      "epoch:6 step:5405 [D loss: 0.226960, acc.: 91.41%] [G loss: 4.230061]\n",
      "epoch:6 step:5406 [D loss: 0.040256, acc.: 100.00%] [G loss: 4.880179]\n",
      "epoch:6 step:5407 [D loss: 0.126056, acc.: 96.09%] [G loss: 2.299959]\n",
      "epoch:6 step:5408 [D loss: 0.247848, acc.: 88.28%] [G loss: 4.896521]\n",
      "epoch:6 step:5409 [D loss: 0.056507, acc.: 99.22%] [G loss: 5.237095]\n",
      "epoch:6 step:5410 [D loss: 0.271023, acc.: 87.50%] [G loss: 1.786268]\n",
      "epoch:6 step:5411 [D loss: 0.364143, acc.: 82.81%] [G loss: 4.528392]\n",
      "epoch:6 step:5412 [D loss: 0.044174, acc.: 98.44%] [G loss: 4.901395]\n",
      "epoch:6 step:5413 [D loss: 0.258257, acc.: 90.62%] [G loss: 1.071650]\n",
      "epoch:6 step:5414 [D loss: 0.202533, acc.: 92.97%] [G loss: 2.138947]\n",
      "epoch:6 step:5415 [D loss: 0.033884, acc.: 100.00%] [G loss: 2.476779]\n",
      "epoch:6 step:5416 [D loss: 0.030343, acc.: 99.22%] [G loss: 0.682759]\n",
      "epoch:6 step:5417 [D loss: 0.036463, acc.: 100.00%] [G loss: 0.532294]\n",
      "epoch:6 step:5418 [D loss: 0.037961, acc.: 99.22%] [G loss: 0.157310]\n",
      "epoch:6 step:5419 [D loss: 0.079243, acc.: 97.66%] [G loss: 0.108729]\n",
      "epoch:6 step:5420 [D loss: 0.047384, acc.: 100.00%] [G loss: 1.885052]\n",
      "epoch:6 step:5421 [D loss: 0.472147, acc.: 75.78%] [G loss: 0.213532]\n",
      "epoch:6 step:5422 [D loss: 0.165626, acc.: 92.97%] [G loss: 1.723411]\n",
      "epoch:6 step:5423 [D loss: 0.012517, acc.: 100.00%] [G loss: 3.827055]\n",
      "epoch:6 step:5424 [D loss: 0.141234, acc.: 93.75%] [G loss: 1.343912]\n",
      "epoch:6 step:5425 [D loss: 0.084911, acc.: 99.22%] [G loss: 0.414553]\n",
      "epoch:6 step:5426 [D loss: 0.087018, acc.: 96.88%] [G loss: 1.029114]\n",
      "epoch:6 step:5427 [D loss: 0.063576, acc.: 99.22%] [G loss: 1.982922]\n",
      "epoch:6 step:5428 [D loss: 0.030208, acc.: 99.22%] [G loss: 1.424225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5429 [D loss: 0.042836, acc.: 99.22%] [G loss: 0.985263]\n",
      "epoch:6 step:5430 [D loss: 0.573431, acc.: 68.75%] [G loss: 7.491090]\n",
      "epoch:6 step:5431 [D loss: 1.146860, acc.: 53.91%] [G loss: 3.782720]\n",
      "epoch:6 step:5432 [D loss: 0.511464, acc.: 75.78%] [G loss: 5.910812]\n",
      "epoch:6 step:5433 [D loss: 0.150963, acc.: 94.53%] [G loss: 6.660172]\n",
      "epoch:6 step:5434 [D loss: 0.056865, acc.: 100.00%] [G loss: 6.157906]\n",
      "epoch:6 step:5435 [D loss: 0.094278, acc.: 98.44%] [G loss: 3.919496]\n",
      "epoch:6 step:5436 [D loss: 0.120277, acc.: 97.66%] [G loss: 3.419089]\n",
      "epoch:6 step:5437 [D loss: 0.052632, acc.: 99.22%] [G loss: 3.776316]\n",
      "epoch:6 step:5438 [D loss: 0.153705, acc.: 95.31%] [G loss: 4.096599]\n",
      "epoch:6 step:5439 [D loss: 0.146924, acc.: 95.31%] [G loss: 2.852087]\n",
      "epoch:6 step:5440 [D loss: 0.135594, acc.: 97.66%] [G loss: 2.876055]\n",
      "epoch:6 step:5441 [D loss: 0.113945, acc.: 97.66%] [G loss: 3.775100]\n",
      "epoch:6 step:5442 [D loss: 0.153647, acc.: 96.88%] [G loss: 2.716645]\n",
      "epoch:6 step:5443 [D loss: 0.132523, acc.: 93.75%] [G loss: 2.109943]\n",
      "epoch:6 step:5444 [D loss: 0.127402, acc.: 95.31%] [G loss: 2.398443]\n",
      "epoch:6 step:5445 [D loss: 0.147702, acc.: 94.53%] [G loss: 2.593663]\n",
      "epoch:6 step:5446 [D loss: 0.191513, acc.: 92.19%] [G loss: 2.449605]\n",
      "epoch:6 step:5447 [D loss: 0.105706, acc.: 98.44%] [G loss: 1.896240]\n",
      "epoch:6 step:5448 [D loss: 0.162534, acc.: 94.53%] [G loss: 0.521744]\n",
      "epoch:6 step:5449 [D loss: 0.577348, acc.: 71.88%] [G loss: 4.415201]\n",
      "epoch:6 step:5450 [D loss: 0.320383, acc.: 83.59%] [G loss: 5.591775]\n",
      "epoch:6 step:5451 [D loss: 0.697790, acc.: 65.62%] [G loss: 0.451215]\n",
      "epoch:6 step:5452 [D loss: 1.287972, acc.: 55.47%] [G loss: 6.207717]\n",
      "epoch:6 step:5453 [D loss: 0.453730, acc.: 76.56%] [G loss: 5.706183]\n",
      "epoch:6 step:5454 [D loss: 0.622564, acc.: 71.09%] [G loss: 0.867485]\n",
      "epoch:6 step:5455 [D loss: 0.446966, acc.: 78.12%] [G loss: 2.972757]\n",
      "epoch:6 step:5456 [D loss: 0.038694, acc.: 100.00%] [G loss: 3.363242]\n",
      "epoch:6 step:5457 [D loss: 0.412144, acc.: 79.69%] [G loss: 1.405989]\n",
      "epoch:6 step:5458 [D loss: 0.574885, acc.: 68.75%] [G loss: 3.439118]\n",
      "epoch:6 step:5459 [D loss: 0.120675, acc.: 97.66%] [G loss: 4.061999]\n",
      "epoch:6 step:5460 [D loss: 0.270743, acc.: 88.28%] [G loss: 2.329022]\n",
      "epoch:6 step:5461 [D loss: 0.104810, acc.: 99.22%] [G loss: 2.929056]\n",
      "epoch:6 step:5462 [D loss: 0.237561, acc.: 92.19%] [G loss: 3.173610]\n",
      "epoch:6 step:5463 [D loss: 0.524111, acc.: 71.09%] [G loss: 3.166924]\n",
      "epoch:6 step:5464 [D loss: 0.196073, acc.: 93.75%] [G loss: 3.702791]\n",
      "epoch:6 step:5465 [D loss: 0.236905, acc.: 87.50%] [G loss: 2.649640]\n",
      "epoch:6 step:5466 [D loss: 0.334336, acc.: 84.38%] [G loss: 4.278091]\n",
      "epoch:6 step:5467 [D loss: 0.067522, acc.: 99.22%] [G loss: 4.006076]\n",
      "epoch:7 step:5468 [D loss: 0.465232, acc.: 81.25%] [G loss: 3.081695]\n",
      "epoch:7 step:5469 [D loss: 0.062297, acc.: 100.00%] [G loss: 1.008419]\n",
      "epoch:7 step:5470 [D loss: 0.175487, acc.: 94.53%] [G loss: 3.190893]\n",
      "epoch:7 step:5471 [D loss: 0.161243, acc.: 95.31%] [G loss: 3.126562]\n",
      "epoch:7 step:5472 [D loss: 0.109796, acc.: 98.44%] [G loss: 0.341022]\n",
      "epoch:7 step:5473 [D loss: 0.080943, acc.: 98.44%] [G loss: 0.927969]\n",
      "epoch:7 step:5474 [D loss: 0.117504, acc.: 96.09%] [G loss: 1.248103]\n",
      "epoch:7 step:5475 [D loss: 0.351997, acc.: 82.81%] [G loss: 3.544423]\n",
      "epoch:7 step:5476 [D loss: 0.673952, acc.: 66.41%] [G loss: 1.906245]\n",
      "epoch:7 step:5477 [D loss: 0.087068, acc.: 97.66%] [G loss: 0.189874]\n",
      "epoch:7 step:5478 [D loss: 0.384164, acc.: 82.03%] [G loss: 3.289693]\n",
      "epoch:7 step:5479 [D loss: 0.383241, acc.: 79.69%] [G loss: 0.912265]\n",
      "epoch:7 step:5480 [D loss: 0.115974, acc.: 97.66%] [G loss: 2.049282]\n",
      "epoch:7 step:5481 [D loss: 0.077017, acc.: 98.44%] [G loss: 1.701945]\n",
      "epoch:7 step:5482 [D loss: 0.035635, acc.: 100.00%] [G loss: 0.747683]\n",
      "epoch:7 step:5483 [D loss: 0.979262, acc.: 57.03%] [G loss: 4.298921]\n",
      "epoch:7 step:5484 [D loss: 0.547094, acc.: 75.00%] [G loss: 4.625216]\n",
      "epoch:7 step:5485 [D loss: 0.174488, acc.: 94.53%] [G loss: 2.826663]\n",
      "epoch:7 step:5486 [D loss: 0.404927, acc.: 85.16%] [G loss: 4.007078]\n",
      "epoch:7 step:5487 [D loss: 0.252021, acc.: 86.72%] [G loss: 3.091714]\n",
      "epoch:7 step:5488 [D loss: 0.209526, acc.: 92.19%] [G loss: 2.996422]\n",
      "epoch:7 step:5489 [D loss: 0.181971, acc.: 94.53%] [G loss: 3.974958]\n",
      "epoch:7 step:5490 [D loss: 0.273238, acc.: 85.16%] [G loss: 1.891872]\n",
      "epoch:7 step:5491 [D loss: 0.652135, acc.: 64.06%] [G loss: 6.208190]\n",
      "epoch:7 step:5492 [D loss: 0.312234, acc.: 82.81%] [G loss: 6.086467]\n",
      "epoch:7 step:5493 [D loss: 0.524032, acc.: 74.22%] [G loss: 1.497585]\n",
      "epoch:7 step:5494 [D loss: 0.183445, acc.: 93.75%] [G loss: 3.448523]\n",
      "epoch:7 step:5495 [D loss: 0.034651, acc.: 99.22%] [G loss: 2.473889]\n",
      "epoch:7 step:5496 [D loss: 0.170997, acc.: 95.31%] [G loss: 2.209242]\n",
      "epoch:7 step:5497 [D loss: 0.116412, acc.: 97.66%] [G loss: 2.793869]\n",
      "epoch:7 step:5498 [D loss: 0.157660, acc.: 97.66%] [G loss: 1.667590]\n",
      "epoch:7 step:5499 [D loss: 0.177316, acc.: 96.88%] [G loss: 2.244381]\n",
      "epoch:7 step:5500 [D loss: 0.087793, acc.: 96.88%] [G loss: 3.189991]\n",
      "epoch:7 step:5501 [D loss: 0.071914, acc.: 100.00%] [G loss: 1.833966]\n",
      "epoch:7 step:5502 [D loss: 0.694450, acc.: 67.19%] [G loss: 5.012168]\n",
      "epoch:7 step:5503 [D loss: 0.545496, acc.: 73.44%] [G loss: 4.149907]\n",
      "epoch:7 step:5504 [D loss: 0.195400, acc.: 93.75%] [G loss: 2.979618]\n",
      "epoch:7 step:5505 [D loss: 0.232281, acc.: 89.84%] [G loss: 3.276803]\n",
      "epoch:7 step:5506 [D loss: 0.120719, acc.: 95.31%] [G loss: 3.271686]\n",
      "epoch:7 step:5507 [D loss: 0.207280, acc.: 91.41%] [G loss: 3.165593]\n",
      "epoch:7 step:5508 [D loss: 0.141562, acc.: 96.88%] [G loss: 3.931016]\n",
      "epoch:7 step:5509 [D loss: 0.122718, acc.: 96.88%] [G loss: 2.983490]\n",
      "epoch:7 step:5510 [D loss: 0.294505, acc.: 88.28%] [G loss: 2.152715]\n",
      "epoch:7 step:5511 [D loss: 0.313998, acc.: 87.50%] [G loss: 4.541237]\n",
      "epoch:7 step:5512 [D loss: 0.750949, acc.: 60.94%] [G loss: 3.970899]\n",
      "epoch:7 step:5513 [D loss: 0.106760, acc.: 96.88%] [G loss: 3.712761]\n",
      "epoch:7 step:5514 [D loss: 0.133693, acc.: 98.44%] [G loss: 2.522253]\n",
      "epoch:7 step:5515 [D loss: 0.211375, acc.: 93.75%] [G loss: 3.368238]\n",
      "epoch:7 step:5516 [D loss: 0.099000, acc.: 100.00%] [G loss: 3.701638]\n",
      "epoch:7 step:5517 [D loss: 0.345291, acc.: 88.28%] [G loss: 1.788654]\n",
      "epoch:7 step:5518 [D loss: 0.225834, acc.: 92.97%] [G loss: 4.910379]\n",
      "epoch:7 step:5519 [D loss: 0.214903, acc.: 90.62%] [G loss: 4.000167]\n",
      "epoch:7 step:5520 [D loss: 0.126624, acc.: 96.88%] [G loss: 3.617546]\n",
      "epoch:7 step:5521 [D loss: 0.379889, acc.: 79.69%] [G loss: 5.575444]\n",
      "epoch:7 step:5522 [D loss: 0.111994, acc.: 97.66%] [G loss: 5.276460]\n",
      "epoch:7 step:5523 [D loss: 0.817355, acc.: 57.81%] [G loss: 3.826820]\n",
      "epoch:7 step:5524 [D loss: 0.032086, acc.: 99.22%] [G loss: 4.557506]\n",
      "epoch:7 step:5525 [D loss: 0.051469, acc.: 100.00%] [G loss: 4.139683]\n",
      "epoch:7 step:5526 [D loss: 0.082744, acc.: 97.66%] [G loss: 2.176569]\n",
      "epoch:7 step:5527 [D loss: 0.108717, acc.: 96.88%] [G loss: 2.134976]\n",
      "epoch:7 step:5528 [D loss: 0.261977, acc.: 89.84%] [G loss: 3.386746]\n",
      "epoch:7 step:5529 [D loss: 0.154671, acc.: 96.09%] [G loss: 2.699592]\n",
      "epoch:7 step:5530 [D loss: 0.084066, acc.: 98.44%] [G loss: 1.398220]\n",
      "epoch:7 step:5531 [D loss: 0.199406, acc.: 92.97%] [G loss: 1.762009]\n",
      "epoch:7 step:5532 [D loss: 0.526529, acc.: 75.00%] [G loss: 4.071115]\n",
      "epoch:7 step:5533 [D loss: 0.068950, acc.: 99.22%] [G loss: 4.082825]\n",
      "epoch:7 step:5534 [D loss: 0.067304, acc.: 99.22%] [G loss: 3.566178]\n",
      "epoch:7 step:5535 [D loss: 0.341560, acc.: 86.72%] [G loss: 5.212965]\n",
      "epoch:7 step:5536 [D loss: 0.087683, acc.: 96.88%] [G loss: 4.406432]\n",
      "epoch:7 step:5537 [D loss: 0.121019, acc.: 97.66%] [G loss: 2.149987]\n",
      "epoch:7 step:5538 [D loss: 0.096891, acc.: 97.66%] [G loss: 3.631417]\n",
      "epoch:7 step:5539 [D loss: 0.130807, acc.: 96.88%] [G loss: 4.270150]\n",
      "epoch:7 step:5540 [D loss: 0.287689, acc.: 88.28%] [G loss: 3.016792]\n",
      "epoch:7 step:5541 [D loss: 0.150430, acc.: 93.75%] [G loss: 4.681255]\n",
      "epoch:7 step:5542 [D loss: 0.858790, acc.: 56.25%] [G loss: 6.256652]\n",
      "epoch:7 step:5543 [D loss: 0.834405, acc.: 58.59%] [G loss: 2.482508]\n",
      "epoch:7 step:5544 [D loss: 0.214214, acc.: 90.62%] [G loss: 4.665687]\n",
      "epoch:7 step:5545 [D loss: 0.056635, acc.: 98.44%] [G loss: 5.026455]\n",
      "epoch:7 step:5546 [D loss: 0.165916, acc.: 94.53%] [G loss: 3.628170]\n",
      "epoch:7 step:5547 [D loss: 0.057102, acc.: 100.00%] [G loss: 3.483853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5548 [D loss: 0.106364, acc.: 98.44%] [G loss: 2.301839]\n",
      "epoch:7 step:5549 [D loss: 0.129371, acc.: 95.31%] [G loss: 4.237959]\n",
      "epoch:7 step:5550 [D loss: 1.290705, acc.: 41.41%] [G loss: 8.418022]\n",
      "epoch:7 step:5551 [D loss: 1.451425, acc.: 52.34%] [G loss: 3.981085]\n",
      "epoch:7 step:5552 [D loss: 0.173275, acc.: 92.97%] [G loss: 3.187669]\n",
      "epoch:7 step:5553 [D loss: 0.104369, acc.: 97.66%] [G loss: 3.746896]\n",
      "epoch:7 step:5554 [D loss: 0.242803, acc.: 91.41%] [G loss: 4.995567]\n",
      "epoch:7 step:5555 [D loss: 0.148168, acc.: 96.88%] [G loss: 2.988574]\n",
      "epoch:7 step:5556 [D loss: 0.156158, acc.: 95.31%] [G loss: 1.759298]\n",
      "epoch:7 step:5557 [D loss: 0.794658, acc.: 57.03%] [G loss: 2.678167]\n",
      "epoch:7 step:5558 [D loss: 0.054623, acc.: 99.22%] [G loss: 4.496644]\n",
      "epoch:7 step:5559 [D loss: 0.172938, acc.: 94.53%] [G loss: 3.685235]\n",
      "epoch:7 step:5560 [D loss: 0.032686, acc.: 100.00%] [G loss: 2.083638]\n",
      "epoch:7 step:5561 [D loss: 0.412251, acc.: 79.69%] [G loss: 5.442903]\n",
      "epoch:7 step:5562 [D loss: 0.286666, acc.: 85.94%] [G loss: 4.026814]\n",
      "epoch:7 step:5563 [D loss: 0.183388, acc.: 96.09%] [G loss: 3.501210]\n",
      "epoch:7 step:5564 [D loss: 0.081133, acc.: 100.00%] [G loss: 2.972196]\n",
      "epoch:7 step:5565 [D loss: 0.220063, acc.: 94.53%] [G loss: 3.867927]\n",
      "epoch:7 step:5566 [D loss: 0.225853, acc.: 92.97%] [G loss: 3.329394]\n",
      "epoch:7 step:5567 [D loss: 0.248099, acc.: 92.97%] [G loss: 2.105671]\n",
      "epoch:7 step:5568 [D loss: 0.071572, acc.: 99.22%] [G loss: 3.751928]\n",
      "epoch:7 step:5569 [D loss: 0.129323, acc.: 97.66%] [G loss: 2.640306]\n",
      "epoch:7 step:5570 [D loss: 0.124895, acc.: 96.09%] [G loss: 0.820935]\n",
      "epoch:7 step:5571 [D loss: 0.537499, acc.: 73.44%] [G loss: 6.001622]\n",
      "epoch:7 step:5572 [D loss: 1.095960, acc.: 52.34%] [G loss: 1.819721]\n",
      "epoch:7 step:5573 [D loss: 0.122143, acc.: 96.09%] [G loss: 0.688063]\n",
      "epoch:7 step:5574 [D loss: 0.083211, acc.: 99.22%] [G loss: 2.933264]\n",
      "epoch:7 step:5575 [D loss: 0.149649, acc.: 96.88%] [G loss: 1.555401]\n",
      "epoch:7 step:5576 [D loss: 0.052230, acc.: 100.00%] [G loss: 2.179253]\n",
      "epoch:7 step:5577 [D loss: 0.190230, acc.: 92.19%] [G loss: 0.780362]\n",
      "epoch:7 step:5578 [D loss: 0.165845, acc.: 96.88%] [G loss: 2.753397]\n",
      "epoch:7 step:5579 [D loss: 0.147539, acc.: 96.88%] [G loss: 1.703164]\n",
      "epoch:7 step:5580 [D loss: 0.124959, acc.: 96.09%] [G loss: 2.144364]\n",
      "epoch:7 step:5581 [D loss: 0.191598, acc.: 95.31%] [G loss: 1.452952]\n",
      "epoch:7 step:5582 [D loss: 0.183405, acc.: 92.97%] [G loss: 5.340737]\n",
      "epoch:7 step:5583 [D loss: 0.464147, acc.: 75.78%] [G loss: 2.666590]\n",
      "epoch:7 step:5584 [D loss: 0.114253, acc.: 99.22%] [G loss: 1.222233]\n",
      "epoch:7 step:5585 [D loss: 0.108842, acc.: 97.66%] [G loss: 1.968091]\n",
      "epoch:7 step:5586 [D loss: 0.022780, acc.: 99.22%] [G loss: 2.686332]\n",
      "epoch:7 step:5587 [D loss: 0.356506, acc.: 82.03%] [G loss: 5.027313]\n",
      "epoch:7 step:5588 [D loss: 0.188021, acc.: 91.41%] [G loss: 3.680995]\n",
      "epoch:7 step:5589 [D loss: 0.346469, acc.: 90.62%] [G loss: 4.527305]\n",
      "epoch:7 step:5590 [D loss: 0.107525, acc.: 96.09%] [G loss: 3.986473]\n",
      "epoch:7 step:5591 [D loss: 0.142408, acc.: 96.88%] [G loss: 2.993561]\n",
      "epoch:7 step:5592 [D loss: 0.090529, acc.: 99.22%] [G loss: 3.184627]\n",
      "epoch:7 step:5593 [D loss: 0.586960, acc.: 70.31%] [G loss: 5.469256]\n",
      "epoch:7 step:5594 [D loss: 0.317495, acc.: 85.94%] [G loss: 4.713866]\n",
      "epoch:7 step:5595 [D loss: 0.068123, acc.: 99.22%] [G loss: 3.156018]\n",
      "epoch:7 step:5596 [D loss: 0.221258, acc.: 91.41%] [G loss: 5.485739]\n",
      "epoch:7 step:5597 [D loss: 0.159554, acc.: 95.31%] [G loss: 4.509024]\n",
      "epoch:7 step:5598 [D loss: 0.041359, acc.: 100.00%] [G loss: 4.015631]\n",
      "epoch:7 step:5599 [D loss: 0.081045, acc.: 97.66%] [G loss: 1.909711]\n",
      "epoch:7 step:5600 [D loss: 0.204731, acc.: 91.41%] [G loss: 3.676180]\n",
      "epoch:7 step:5601 [D loss: 0.077386, acc.: 98.44%] [G loss: 4.864053]\n",
      "epoch:7 step:5602 [D loss: 0.456869, acc.: 78.12%] [G loss: 4.380055]\n",
      "epoch:7 step:5603 [D loss: 0.057679, acc.: 99.22%] [G loss: 3.603252]\n",
      "epoch:7 step:5604 [D loss: 0.064767, acc.: 100.00%] [G loss: 1.771516]\n",
      "epoch:7 step:5605 [D loss: 0.170914, acc.: 92.97%] [G loss: 2.902204]\n",
      "epoch:7 step:5606 [D loss: 0.135192, acc.: 95.31%] [G loss: 3.947854]\n",
      "epoch:7 step:5607 [D loss: 0.500890, acc.: 75.00%] [G loss: 5.171436]\n",
      "epoch:7 step:5608 [D loss: 0.517342, acc.: 76.56%] [G loss: 1.883400]\n",
      "epoch:7 step:5609 [D loss: 0.265689, acc.: 85.16%] [G loss: 3.371083]\n",
      "epoch:7 step:5610 [D loss: 0.195414, acc.: 93.75%] [G loss: 3.736755]\n",
      "epoch:7 step:5611 [D loss: 0.071764, acc.: 97.66%] [G loss: 2.512781]\n",
      "epoch:7 step:5612 [D loss: 0.069891, acc.: 99.22%] [G loss: 1.935510]\n",
      "epoch:7 step:5613 [D loss: 0.232857, acc.: 91.41%] [G loss: 4.078734]\n",
      "epoch:7 step:5614 [D loss: 0.265814, acc.: 89.06%] [G loss: 3.086322]\n",
      "epoch:7 step:5615 [D loss: 0.275586, acc.: 89.06%] [G loss: 8.166979]\n",
      "epoch:7 step:5616 [D loss: 0.240267, acc.: 88.28%] [G loss: 4.805541]\n",
      "epoch:7 step:5617 [D loss: 0.045542, acc.: 100.00%] [G loss: 2.962659]\n",
      "epoch:7 step:5618 [D loss: 0.083950, acc.: 99.22%] [G loss: 3.461917]\n",
      "epoch:7 step:5619 [D loss: 0.016215, acc.: 100.00%] [G loss: 3.914351]\n",
      "epoch:7 step:5620 [D loss: 0.511748, acc.: 72.66%] [G loss: 6.697696]\n",
      "epoch:7 step:5621 [D loss: 0.249873, acc.: 89.06%] [G loss: 5.018863]\n",
      "epoch:7 step:5622 [D loss: 0.160026, acc.: 94.53%] [G loss: 4.813783]\n",
      "epoch:7 step:5623 [D loss: 0.392810, acc.: 78.12%] [G loss: 4.721600]\n",
      "epoch:7 step:5624 [D loss: 0.021762, acc.: 100.00%] [G loss: 5.952478]\n",
      "epoch:7 step:5625 [D loss: 0.362442, acc.: 85.94%] [G loss: 3.691641]\n",
      "epoch:7 step:5626 [D loss: 0.065688, acc.: 98.44%] [G loss: 5.394970]\n",
      "epoch:7 step:5627 [D loss: 0.169254, acc.: 94.53%] [G loss: 4.910020]\n",
      "epoch:7 step:5628 [D loss: 0.069422, acc.: 99.22%] [G loss: 4.628873]\n",
      "epoch:7 step:5629 [D loss: 0.028034, acc.: 100.00%] [G loss: 4.583238]\n",
      "epoch:7 step:5630 [D loss: 0.288910, acc.: 88.28%] [G loss: 5.541683]\n",
      "epoch:7 step:5631 [D loss: 0.120302, acc.: 93.75%] [G loss: 4.160601]\n",
      "epoch:7 step:5632 [D loss: 0.026835, acc.: 100.00%] [G loss: 2.496315]\n",
      "epoch:7 step:5633 [D loss: 0.026416, acc.: 100.00%] [G loss: 2.306351]\n",
      "epoch:7 step:5634 [D loss: 0.029899, acc.: 100.00%] [G loss: 2.122187]\n",
      "epoch:7 step:5635 [D loss: 0.119496, acc.: 97.66%] [G loss: 1.538370]\n",
      "epoch:7 step:5636 [D loss: 0.064695, acc.: 98.44%] [G loss: 4.684877]\n",
      "epoch:7 step:5637 [D loss: 0.095304, acc.: 96.88%] [G loss: 2.200836]\n",
      "epoch:7 step:5638 [D loss: 0.034591, acc.: 100.00%] [G loss: 1.493649]\n",
      "epoch:7 step:5639 [D loss: 0.374409, acc.: 79.69%] [G loss: 9.502862]\n",
      "epoch:7 step:5640 [D loss: 1.382996, acc.: 53.12%] [G loss: 4.782917]\n",
      "epoch:7 step:5641 [D loss: 0.068834, acc.: 98.44%] [G loss: 3.691948]\n",
      "epoch:7 step:5642 [D loss: 0.052690, acc.: 100.00%] [G loss: 4.503712]\n",
      "epoch:7 step:5643 [D loss: 0.070135, acc.: 99.22%] [G loss: 4.925047]\n",
      "epoch:7 step:5644 [D loss: 0.134139, acc.: 95.31%] [G loss: 4.633765]\n",
      "epoch:7 step:5645 [D loss: 0.141471, acc.: 96.09%] [G loss: 4.573541]\n",
      "epoch:7 step:5646 [D loss: 1.258661, acc.: 42.19%] [G loss: 7.674616]\n",
      "epoch:7 step:5647 [D loss: 0.279180, acc.: 85.16%] [G loss: 7.252623]\n",
      "epoch:7 step:5648 [D loss: 0.115336, acc.: 95.31%] [G loss: 5.914234]\n",
      "epoch:7 step:5649 [D loss: 0.040313, acc.: 100.00%] [G loss: 3.831413]\n",
      "epoch:7 step:5650 [D loss: 0.069332, acc.: 98.44%] [G loss: 3.738968]\n",
      "epoch:7 step:5651 [D loss: 0.124245, acc.: 95.31%] [G loss: 4.783285]\n",
      "epoch:7 step:5652 [D loss: 0.116697, acc.: 99.22%] [G loss: 4.373059]\n",
      "epoch:7 step:5653 [D loss: 0.099611, acc.: 99.22%] [G loss: 3.783279]\n",
      "epoch:7 step:5654 [D loss: 0.869314, acc.: 57.81%] [G loss: 7.205494]\n",
      "epoch:7 step:5655 [D loss: 1.182310, acc.: 53.91%] [G loss: 4.093770]\n",
      "epoch:7 step:5656 [D loss: 0.072240, acc.: 99.22%] [G loss: 3.724388]\n",
      "epoch:7 step:5657 [D loss: 0.082503, acc.: 99.22%] [G loss: 3.100488]\n",
      "epoch:7 step:5658 [D loss: 0.052713, acc.: 100.00%] [G loss: 3.449441]\n",
      "epoch:7 step:5659 [D loss: 0.099905, acc.: 98.44%] [G loss: 3.387359]\n",
      "epoch:7 step:5660 [D loss: 0.034991, acc.: 100.00%] [G loss: 2.911790]\n",
      "epoch:7 step:5661 [D loss: 0.077502, acc.: 99.22%] [G loss: 3.191161]\n",
      "epoch:7 step:5662 [D loss: 0.088664, acc.: 99.22%] [G loss: 3.381211]\n",
      "epoch:7 step:5663 [D loss: 0.105828, acc.: 95.31%] [G loss: 2.269131]\n",
      "epoch:7 step:5664 [D loss: 0.141375, acc.: 96.09%] [G loss: 3.472371]\n",
      "epoch:7 step:5665 [D loss: 0.041131, acc.: 100.00%] [G loss: 3.342739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5666 [D loss: 0.215414, acc.: 92.19%] [G loss: 3.806916]\n",
      "epoch:7 step:5667 [D loss: 0.090279, acc.: 98.44%] [G loss: 3.246991]\n",
      "epoch:7 step:5668 [D loss: 0.181137, acc.: 92.19%] [G loss: 4.020555]\n",
      "epoch:7 step:5669 [D loss: 0.396940, acc.: 84.38%] [G loss: 2.017580]\n",
      "epoch:7 step:5670 [D loss: 0.063607, acc.: 99.22%] [G loss: 2.698735]\n",
      "epoch:7 step:5671 [D loss: 0.036018, acc.: 100.00%] [G loss: 2.980758]\n",
      "epoch:7 step:5672 [D loss: 1.263687, acc.: 42.19%] [G loss: 6.583061]\n",
      "epoch:7 step:5673 [D loss: 1.036055, acc.: 56.25%] [G loss: 3.706316]\n",
      "epoch:7 step:5674 [D loss: 0.064495, acc.: 99.22%] [G loss: 2.516321]\n",
      "epoch:7 step:5675 [D loss: 0.108943, acc.: 96.88%] [G loss: 1.672139]\n",
      "epoch:7 step:5676 [D loss: 0.174668, acc.: 94.53%] [G loss: 3.044564]\n",
      "epoch:7 step:5677 [D loss: 0.236265, acc.: 92.97%] [G loss: 2.874851]\n",
      "epoch:7 step:5678 [D loss: 0.031801, acc.: 100.00%] [G loss: 3.290395]\n",
      "epoch:7 step:5679 [D loss: 0.089594, acc.: 98.44%] [G loss: 2.880432]\n",
      "epoch:7 step:5680 [D loss: 0.353428, acc.: 81.25%] [G loss: 5.027875]\n",
      "epoch:7 step:5681 [D loss: 0.525796, acc.: 73.44%] [G loss: 2.272018]\n",
      "epoch:7 step:5682 [D loss: 0.577308, acc.: 66.41%] [G loss: 6.091616]\n",
      "epoch:7 step:5683 [D loss: 0.517902, acc.: 72.66%] [G loss: 3.411579]\n",
      "epoch:7 step:5684 [D loss: 0.148208, acc.: 96.09%] [G loss: 4.196410]\n",
      "epoch:7 step:5685 [D loss: 0.115991, acc.: 99.22%] [G loss: 2.751541]\n",
      "epoch:7 step:5686 [D loss: 0.079276, acc.: 99.22%] [G loss: 3.251943]\n",
      "epoch:7 step:5687 [D loss: 0.160099, acc.: 94.53%] [G loss: 2.826900]\n",
      "epoch:7 step:5688 [D loss: 0.108542, acc.: 96.09%] [G loss: 4.225363]\n",
      "epoch:7 step:5689 [D loss: 0.138714, acc.: 96.09%] [G loss: 2.992950]\n",
      "epoch:7 step:5690 [D loss: 0.128947, acc.: 96.88%] [G loss: 2.536120]\n",
      "epoch:7 step:5691 [D loss: 0.162910, acc.: 94.53%] [G loss: 3.701623]\n",
      "epoch:7 step:5692 [D loss: 0.042833, acc.: 100.00%] [G loss: 3.260988]\n",
      "epoch:7 step:5693 [D loss: 0.385807, acc.: 82.03%] [G loss: 4.746636]\n",
      "epoch:7 step:5694 [D loss: 0.143797, acc.: 95.31%] [G loss: 5.763788]\n",
      "epoch:7 step:5695 [D loss: 0.093790, acc.: 99.22%] [G loss: 4.226256]\n",
      "epoch:7 step:5696 [D loss: 0.199967, acc.: 93.75%] [G loss: 3.301485]\n",
      "epoch:7 step:5697 [D loss: 0.216958, acc.: 92.97%] [G loss: 3.150181]\n",
      "epoch:7 step:5698 [D loss: 0.148639, acc.: 97.66%] [G loss: 4.608893]\n",
      "epoch:7 step:5699 [D loss: 0.485717, acc.: 75.00%] [G loss: 4.291935]\n",
      "epoch:7 step:5700 [D loss: 0.108353, acc.: 97.66%] [G loss: 3.857017]\n",
      "epoch:7 step:5701 [D loss: 0.122036, acc.: 97.66%] [G loss: 3.375671]\n",
      "epoch:7 step:5702 [D loss: 0.038315, acc.: 99.22%] [G loss: 4.981944]\n",
      "epoch:7 step:5703 [D loss: 0.133136, acc.: 94.53%] [G loss: 4.696162]\n",
      "epoch:7 step:5704 [D loss: 0.072071, acc.: 99.22%] [G loss: 4.208512]\n",
      "epoch:7 step:5705 [D loss: 0.377299, acc.: 79.69%] [G loss: 5.527841]\n",
      "epoch:7 step:5706 [D loss: 0.141518, acc.: 94.53%] [G loss: 5.329207]\n",
      "epoch:7 step:5707 [D loss: 0.551380, acc.: 67.19%] [G loss: 5.003684]\n",
      "epoch:7 step:5708 [D loss: 0.028525, acc.: 99.22%] [G loss: 6.807813]\n",
      "epoch:7 step:5709 [D loss: 0.128243, acc.: 96.09%] [G loss: 3.596581]\n",
      "epoch:7 step:5710 [D loss: 0.008477, acc.: 100.00%] [G loss: 4.293112]\n",
      "epoch:7 step:5711 [D loss: 0.120022, acc.: 96.09%] [G loss: 3.752844]\n",
      "epoch:7 step:5712 [D loss: 0.038261, acc.: 100.00%] [G loss: 3.719734]\n",
      "epoch:7 step:5713 [D loss: 0.037721, acc.: 99.22%] [G loss: 2.034574]\n",
      "epoch:7 step:5714 [D loss: 0.220165, acc.: 93.75%] [G loss: 1.613913]\n",
      "epoch:7 step:5715 [D loss: 0.091207, acc.: 97.66%] [G loss: 1.464941]\n",
      "epoch:7 step:5716 [D loss: 0.022220, acc.: 99.22%] [G loss: 2.815551]\n",
      "epoch:7 step:5717 [D loss: 0.316465, acc.: 87.50%] [G loss: 4.084901]\n",
      "epoch:7 step:5718 [D loss: 0.033254, acc.: 99.22%] [G loss: 3.457599]\n",
      "epoch:7 step:5719 [D loss: 0.114543, acc.: 95.31%] [G loss: 0.449944]\n",
      "epoch:7 step:5720 [D loss: 0.019059, acc.: 100.00%] [G loss: 0.332315]\n",
      "epoch:7 step:5721 [D loss: 0.361869, acc.: 82.81%] [G loss: 7.842965]\n",
      "epoch:7 step:5722 [D loss: 1.976679, acc.: 51.56%] [G loss: 1.075626]\n",
      "epoch:7 step:5723 [D loss: 0.131527, acc.: 96.09%] [G loss: 2.264365]\n",
      "epoch:7 step:5724 [D loss: 0.104969, acc.: 97.66%] [G loss: 3.486629]\n",
      "epoch:7 step:5725 [D loss: 0.107130, acc.: 98.44%] [G loss: 3.903428]\n",
      "epoch:7 step:5726 [D loss: 0.212221, acc.: 92.97%] [G loss: 3.257633]\n",
      "epoch:7 step:5727 [D loss: 0.178197, acc.: 96.09%] [G loss: 4.261493]\n",
      "epoch:7 step:5728 [D loss: 0.410960, acc.: 78.91%] [G loss: 2.972580]\n",
      "epoch:7 step:5729 [D loss: 0.051434, acc.: 100.00%] [G loss: 4.853356]\n",
      "epoch:7 step:5730 [D loss: 0.378359, acc.: 79.69%] [G loss: 6.193259]\n",
      "epoch:7 step:5731 [D loss: 0.267021, acc.: 86.72%] [G loss: 2.941905]\n",
      "epoch:7 step:5732 [D loss: 0.188438, acc.: 92.19%] [G loss: 3.937904]\n",
      "epoch:7 step:5733 [D loss: 0.048639, acc.: 98.44%] [G loss: 3.808282]\n",
      "epoch:7 step:5734 [D loss: 0.662626, acc.: 69.53%] [G loss: 3.226847]\n",
      "epoch:7 step:5735 [D loss: 0.029655, acc.: 100.00%] [G loss: 3.486782]\n",
      "epoch:7 step:5736 [D loss: 0.092061, acc.: 98.44%] [G loss: 2.192813]\n",
      "epoch:7 step:5737 [D loss: 0.694150, acc.: 67.97%] [G loss: 7.438234]\n",
      "epoch:7 step:5738 [D loss: 0.665037, acc.: 72.66%] [G loss: 4.954757]\n",
      "epoch:7 step:5739 [D loss: 0.166652, acc.: 93.75%] [G loss: 2.290495]\n",
      "epoch:7 step:5740 [D loss: 0.532861, acc.: 77.34%] [G loss: 5.764002]\n",
      "epoch:7 step:5741 [D loss: 0.200777, acc.: 89.84%] [G loss: 5.182619]\n",
      "epoch:7 step:5742 [D loss: 0.188018, acc.: 92.19%] [G loss: 2.181911]\n",
      "epoch:7 step:5743 [D loss: 0.303541, acc.: 89.84%] [G loss: 6.342705]\n",
      "epoch:7 step:5744 [D loss: 0.469717, acc.: 75.78%] [G loss: 2.374402]\n",
      "epoch:7 step:5745 [D loss: 0.394035, acc.: 80.47%] [G loss: 5.530941]\n",
      "epoch:7 step:5746 [D loss: 0.706354, acc.: 67.97%] [G loss: 2.204978]\n",
      "epoch:7 step:5747 [D loss: 0.260258, acc.: 90.62%] [G loss: 3.852484]\n",
      "epoch:7 step:5748 [D loss: 0.060520, acc.: 98.44%] [G loss: 4.170420]\n",
      "epoch:7 step:5749 [D loss: 0.250623, acc.: 93.75%] [G loss: 3.028614]\n",
      "epoch:7 step:5750 [D loss: 0.244260, acc.: 91.41%] [G loss: 4.958611]\n",
      "epoch:7 step:5751 [D loss: 0.168510, acc.: 93.75%] [G loss: 4.419339]\n",
      "epoch:7 step:5752 [D loss: 0.182349, acc.: 96.09%] [G loss: 4.332117]\n",
      "epoch:7 step:5753 [D loss: 0.600277, acc.: 67.19%] [G loss: 6.492961]\n",
      "epoch:7 step:5754 [D loss: 0.269906, acc.: 85.16%] [G loss: 4.948209]\n",
      "epoch:7 step:5755 [D loss: 0.098155, acc.: 97.66%] [G loss: 3.578614]\n",
      "epoch:7 step:5756 [D loss: 0.076090, acc.: 99.22%] [G loss: 3.077158]\n",
      "epoch:7 step:5757 [D loss: 0.083904, acc.: 99.22%] [G loss: 3.436579]\n",
      "epoch:7 step:5758 [D loss: 0.392050, acc.: 82.81%] [G loss: 4.415317]\n",
      "epoch:7 step:5759 [D loss: 0.165318, acc.: 94.53%] [G loss: 4.273116]\n",
      "epoch:7 step:5760 [D loss: 0.390576, acc.: 80.47%] [G loss: 5.381581]\n",
      "epoch:7 step:5761 [D loss: 0.232792, acc.: 90.62%] [G loss: 3.817499]\n",
      "epoch:7 step:5762 [D loss: 0.374416, acc.: 82.81%] [G loss: 4.820981]\n",
      "epoch:7 step:5763 [D loss: 0.057312, acc.: 98.44%] [G loss: 3.951475]\n",
      "epoch:7 step:5764 [D loss: 0.065072, acc.: 99.22%] [G loss: 3.101426]\n",
      "epoch:7 step:5765 [D loss: 0.187570, acc.: 92.97%] [G loss: 4.677339]\n",
      "epoch:7 step:5766 [D loss: 0.085280, acc.: 98.44%] [G loss: 3.592528]\n",
      "epoch:7 step:5767 [D loss: 0.690483, acc.: 62.50%] [G loss: 5.105363]\n",
      "epoch:7 step:5768 [D loss: 0.055902, acc.: 98.44%] [G loss: 5.707234]\n",
      "epoch:7 step:5769 [D loss: 0.636906, acc.: 66.41%] [G loss: 3.776056]\n",
      "epoch:7 step:5770 [D loss: 0.047039, acc.: 100.00%] [G loss: 3.450681]\n",
      "epoch:7 step:5771 [D loss: 0.036735, acc.: 100.00%] [G loss: 2.686224]\n",
      "epoch:7 step:5772 [D loss: 0.223591, acc.: 90.62%] [G loss: 4.626710]\n",
      "epoch:7 step:5773 [D loss: 0.189331, acc.: 92.97%] [G loss: 2.590146]\n",
      "epoch:7 step:5774 [D loss: 0.356255, acc.: 80.47%] [G loss: 3.198272]\n",
      "epoch:7 step:5775 [D loss: 0.076821, acc.: 100.00%] [G loss: 4.891310]\n",
      "epoch:7 step:5776 [D loss: 0.318313, acc.: 86.72%] [G loss: 3.785759]\n",
      "epoch:7 step:5777 [D loss: 0.077136, acc.: 99.22%] [G loss: 3.789650]\n",
      "epoch:7 step:5778 [D loss: 0.553650, acc.: 75.78%] [G loss: 5.817764]\n",
      "epoch:7 step:5779 [D loss: 0.703053, acc.: 60.16%] [G loss: 3.572111]\n",
      "epoch:7 step:5780 [D loss: 0.026359, acc.: 100.00%] [G loss: 3.760987]\n",
      "epoch:7 step:5781 [D loss: 0.065017, acc.: 100.00%] [G loss: 3.367836]\n",
      "epoch:7 step:5782 [D loss: 0.215349, acc.: 92.97%] [G loss: 2.254271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5783 [D loss: 0.051529, acc.: 100.00%] [G loss: 2.832966]\n",
      "epoch:7 step:5784 [D loss: 0.212196, acc.: 88.28%] [G loss: 5.169340]\n",
      "epoch:7 step:5785 [D loss: 0.146330, acc.: 95.31%] [G loss: 3.946190]\n",
      "epoch:7 step:5786 [D loss: 0.111483, acc.: 97.66%] [G loss: 3.866246]\n",
      "epoch:7 step:5787 [D loss: 0.298476, acc.: 87.50%] [G loss: 3.736120]\n",
      "epoch:7 step:5788 [D loss: 0.210518, acc.: 92.19%] [G loss: 2.335719]\n",
      "epoch:7 step:5789 [D loss: 0.057686, acc.: 100.00%] [G loss: 2.738569]\n",
      "epoch:7 step:5790 [D loss: 0.070057, acc.: 100.00%] [G loss: 3.086847]\n",
      "epoch:7 step:5791 [D loss: 0.096396, acc.: 97.66%] [G loss: 1.091937]\n",
      "epoch:7 step:5792 [D loss: 0.156725, acc.: 96.09%] [G loss: 3.570994]\n",
      "epoch:7 step:5793 [D loss: 0.094706, acc.: 97.66%] [G loss: 4.053012]\n",
      "epoch:7 step:5794 [D loss: 0.349516, acc.: 82.81%] [G loss: 0.695231]\n",
      "epoch:7 step:5795 [D loss: 0.061713, acc.: 99.22%] [G loss: 1.166671]\n",
      "epoch:7 step:5796 [D loss: 0.018963, acc.: 100.00%] [G loss: 2.110611]\n",
      "epoch:7 step:5797 [D loss: 0.243757, acc.: 91.41%] [G loss: 4.071911]\n",
      "epoch:7 step:5798 [D loss: 0.262372, acc.: 88.28%] [G loss: 1.042892]\n",
      "epoch:7 step:5799 [D loss: 0.302797, acc.: 86.72%] [G loss: 4.453012]\n",
      "epoch:7 step:5800 [D loss: 0.295365, acc.: 83.59%] [G loss: 4.573992]\n",
      "epoch:7 step:5801 [D loss: 0.063838, acc.: 100.00%] [G loss: 1.990412]\n",
      "epoch:7 step:5802 [D loss: 0.050503, acc.: 99.22%] [G loss: 4.197807]\n",
      "epoch:7 step:5803 [D loss: 0.123015, acc.: 97.66%] [G loss: 2.391731]\n",
      "epoch:7 step:5804 [D loss: 0.312918, acc.: 87.50%] [G loss: 5.006336]\n",
      "epoch:7 step:5805 [D loss: 0.064222, acc.: 99.22%] [G loss: 5.482718]\n",
      "epoch:7 step:5806 [D loss: 0.120438, acc.: 97.66%] [G loss: 1.982324]\n",
      "epoch:7 step:5807 [D loss: 0.716213, acc.: 61.72%] [G loss: 8.508591]\n",
      "epoch:7 step:5808 [D loss: 1.138864, acc.: 52.34%] [G loss: 3.387216]\n",
      "epoch:7 step:5809 [D loss: 0.236318, acc.: 90.62%] [G loss: 1.251405]\n",
      "epoch:7 step:5810 [D loss: 0.134165, acc.: 96.09%] [G loss: 2.489645]\n",
      "epoch:7 step:5811 [D loss: 0.130200, acc.: 96.88%] [G loss: 4.282888]\n",
      "epoch:7 step:5812 [D loss: 0.050966, acc.: 99.22%] [G loss: 2.656322]\n",
      "epoch:7 step:5813 [D loss: 0.097263, acc.: 96.88%] [G loss: 1.736333]\n",
      "epoch:7 step:5814 [D loss: 0.133495, acc.: 94.53%] [G loss: 1.704228]\n",
      "epoch:7 step:5815 [D loss: 0.200818, acc.: 89.84%] [G loss: 1.339423]\n",
      "epoch:7 step:5816 [D loss: 0.240978, acc.: 92.97%] [G loss: 4.168902]\n",
      "epoch:7 step:5817 [D loss: 0.633368, acc.: 67.97%] [G loss: 0.895702]\n",
      "epoch:7 step:5818 [D loss: 0.417845, acc.: 77.34%] [G loss: 3.682207]\n",
      "epoch:7 step:5819 [D loss: 0.124071, acc.: 92.97%] [G loss: 3.519478]\n",
      "epoch:7 step:5820 [D loss: 1.342941, acc.: 38.28%] [G loss: 3.706178]\n",
      "epoch:7 step:5821 [D loss: 0.022648, acc.: 100.00%] [G loss: 4.884508]\n",
      "epoch:7 step:5822 [D loss: 0.092688, acc.: 98.44%] [G loss: 3.925739]\n",
      "epoch:7 step:5823 [D loss: 0.049756, acc.: 100.00%] [G loss: 2.045163]\n",
      "epoch:7 step:5824 [D loss: 0.092958, acc.: 96.09%] [G loss: 1.978455]\n",
      "epoch:7 step:5825 [D loss: 0.119865, acc.: 99.22%] [G loss: 2.713067]\n",
      "epoch:7 step:5826 [D loss: 0.232589, acc.: 89.84%] [G loss: 3.739675]\n",
      "epoch:7 step:5827 [D loss: 0.087736, acc.: 98.44%] [G loss: 3.888867]\n",
      "epoch:7 step:5828 [D loss: 0.559405, acc.: 69.53%] [G loss: 3.696434]\n",
      "epoch:7 step:5829 [D loss: 0.036589, acc.: 100.00%] [G loss: 5.519469]\n",
      "epoch:7 step:5830 [D loss: 0.040680, acc.: 100.00%] [G loss: 4.433688]\n",
      "epoch:7 step:5831 [D loss: 0.041036, acc.: 100.00%] [G loss: 4.354816]\n",
      "epoch:7 step:5832 [D loss: 0.021439, acc.: 100.00%] [G loss: 3.721317]\n",
      "epoch:7 step:5833 [D loss: 0.112380, acc.: 98.44%] [G loss: 3.482999]\n",
      "epoch:7 step:5834 [D loss: 0.085811, acc.: 97.66%] [G loss: 3.083398]\n",
      "epoch:7 step:5835 [D loss: 0.030357, acc.: 100.00%] [G loss: 4.157600]\n",
      "epoch:7 step:5836 [D loss: 0.104475, acc.: 96.88%] [G loss: 1.955427]\n",
      "epoch:7 step:5837 [D loss: 0.253595, acc.: 89.06%] [G loss: 4.263678]\n",
      "epoch:7 step:5838 [D loss: 0.079344, acc.: 96.09%] [G loss: 3.690184]\n",
      "epoch:7 step:5839 [D loss: 0.202295, acc.: 91.41%] [G loss: 0.580102]\n",
      "epoch:7 step:5840 [D loss: 0.706313, acc.: 70.31%] [G loss: 6.924971]\n",
      "epoch:7 step:5841 [D loss: 0.833143, acc.: 64.06%] [G loss: 3.306022]\n",
      "epoch:7 step:5842 [D loss: 0.063373, acc.: 99.22%] [G loss: 1.412505]\n",
      "epoch:7 step:5843 [D loss: 0.174079, acc.: 95.31%] [G loss: 3.126767]\n",
      "epoch:7 step:5844 [D loss: 0.364049, acc.: 80.47%] [G loss: 0.423088]\n",
      "epoch:7 step:5845 [D loss: 0.476237, acc.: 74.22%] [G loss: 5.380767]\n",
      "epoch:7 step:5846 [D loss: 0.427920, acc.: 81.25%] [G loss: 3.271125]\n",
      "epoch:7 step:5847 [D loss: 0.109774, acc.: 96.88%] [G loss: 4.165025]\n",
      "epoch:7 step:5848 [D loss: 0.042502, acc.: 100.00%] [G loss: 4.073906]\n",
      "epoch:7 step:5849 [D loss: 0.348478, acc.: 84.38%] [G loss: 4.945515]\n",
      "epoch:7 step:5850 [D loss: 0.207282, acc.: 91.41%] [G loss: 5.093626]\n",
      "epoch:7 step:5851 [D loss: 0.173754, acc.: 94.53%] [G loss: 4.423919]\n",
      "epoch:7 step:5852 [D loss: 0.224697, acc.: 95.31%] [G loss: 2.230848]\n",
      "epoch:7 step:5853 [D loss: 0.181283, acc.: 92.97%] [G loss: 4.206881]\n",
      "epoch:7 step:5854 [D loss: 0.087619, acc.: 99.22%] [G loss: 4.728401]\n",
      "epoch:7 step:5855 [D loss: 0.251503, acc.: 89.84%] [G loss: 2.545948]\n",
      "epoch:7 step:5856 [D loss: 0.482770, acc.: 77.34%] [G loss: 7.367631]\n",
      "epoch:7 step:5857 [D loss: 0.389702, acc.: 77.34%] [G loss: 4.070341]\n",
      "epoch:7 step:5858 [D loss: 0.161860, acc.: 93.75%] [G loss: 1.545802]\n",
      "epoch:7 step:5859 [D loss: 0.126924, acc.: 96.09%] [G loss: 2.963920]\n",
      "epoch:7 step:5860 [D loss: 0.079400, acc.: 99.22%] [G loss: 3.005256]\n",
      "epoch:7 step:5861 [D loss: 0.128858, acc.: 96.09%] [G loss: 1.678340]\n",
      "epoch:7 step:5862 [D loss: 0.078520, acc.: 98.44%] [G loss: 1.000797]\n",
      "epoch:7 step:5863 [D loss: 0.130167, acc.: 97.66%] [G loss: 1.793038]\n",
      "epoch:7 step:5864 [D loss: 0.107007, acc.: 96.88%] [G loss: 3.279657]\n",
      "epoch:7 step:5865 [D loss: 0.117821, acc.: 96.09%] [G loss: 1.311822]\n",
      "epoch:7 step:5866 [D loss: 0.064793, acc.: 99.22%] [G loss: 1.285347]\n",
      "epoch:7 step:5867 [D loss: 0.067022, acc.: 97.66%] [G loss: 2.641108]\n",
      "epoch:7 step:5868 [D loss: 0.542474, acc.: 73.44%] [G loss: 5.540521]\n",
      "epoch:7 step:5869 [D loss: 0.558659, acc.: 69.53%] [G loss: 1.632457]\n",
      "epoch:7 step:5870 [D loss: 0.381948, acc.: 75.78%] [G loss: 5.509625]\n",
      "epoch:7 step:5871 [D loss: 0.211872, acc.: 89.84%] [G loss: 5.026079]\n",
      "epoch:7 step:5872 [D loss: 0.156789, acc.: 95.31%] [G loss: 1.712545]\n",
      "epoch:7 step:5873 [D loss: 0.385796, acc.: 80.47%] [G loss: 7.052629]\n",
      "epoch:7 step:5874 [D loss: 0.177264, acc.: 91.41%] [G loss: 7.097938]\n",
      "epoch:7 step:5875 [D loss: 0.137144, acc.: 95.31%] [G loss: 4.068155]\n",
      "epoch:7 step:5876 [D loss: 0.072351, acc.: 97.66%] [G loss: 4.022637]\n",
      "epoch:7 step:5877 [D loss: 0.049544, acc.: 99.22%] [G loss: 5.255738]\n",
      "epoch:7 step:5878 [D loss: 0.280872, acc.: 86.72%] [G loss: 2.845375]\n",
      "epoch:7 step:5879 [D loss: 0.412858, acc.: 77.34%] [G loss: 7.122438]\n",
      "epoch:7 step:5880 [D loss: 0.196741, acc.: 92.19%] [G loss: 7.501393]\n",
      "epoch:7 step:5881 [D loss: 0.196791, acc.: 92.19%] [G loss: 3.166895]\n",
      "epoch:7 step:5882 [D loss: 0.010483, acc.: 100.00%] [G loss: 2.175342]\n",
      "epoch:7 step:5883 [D loss: 0.070372, acc.: 97.66%] [G loss: 1.852623]\n",
      "epoch:7 step:5884 [D loss: 0.020163, acc.: 100.00%] [G loss: 2.298037]\n",
      "epoch:7 step:5885 [D loss: 0.021061, acc.: 100.00%] [G loss: 1.480678]\n",
      "epoch:7 step:5886 [D loss: 0.237691, acc.: 89.84%] [G loss: 4.534989]\n",
      "epoch:7 step:5887 [D loss: 0.270369, acc.: 87.50%] [G loss: 4.804824]\n",
      "epoch:7 step:5888 [D loss: 0.409290, acc.: 79.69%] [G loss: 3.839597]\n",
      "epoch:7 step:5889 [D loss: 0.062827, acc.: 98.44%] [G loss: 5.319263]\n",
      "epoch:7 step:5890 [D loss: 0.197978, acc.: 91.41%] [G loss: 1.134500]\n",
      "epoch:7 step:5891 [D loss: 0.452182, acc.: 79.69%] [G loss: 6.190073]\n",
      "epoch:7 step:5892 [D loss: 0.490981, acc.: 71.09%] [G loss: 2.625616]\n",
      "epoch:7 step:5893 [D loss: 0.276810, acc.: 91.41%] [G loss: 4.080911]\n",
      "epoch:7 step:5894 [D loss: 0.032145, acc.: 100.00%] [G loss: 4.748057]\n",
      "epoch:7 step:5895 [D loss: 0.452842, acc.: 81.25%] [G loss: 3.168331]\n",
      "epoch:7 step:5896 [D loss: 0.076346, acc.: 99.22%] [G loss: 2.529098]\n",
      "epoch:7 step:5897 [D loss: 0.303754, acc.: 84.38%] [G loss: 5.427890]\n",
      "epoch:7 step:5898 [D loss: 0.487463, acc.: 78.12%] [G loss: 1.692481]\n",
      "epoch:7 step:5899 [D loss: 0.222518, acc.: 92.97%] [G loss: 4.255665]\n",
      "epoch:7 step:5900 [D loss: 0.053089, acc.: 99.22%] [G loss: 4.588746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5901 [D loss: 0.114748, acc.: 97.66%] [G loss: 2.437230]\n",
      "epoch:7 step:5902 [D loss: 0.089055, acc.: 97.66%] [G loss: 3.684378]\n",
      "epoch:7 step:5903 [D loss: 0.063369, acc.: 99.22%] [G loss: 1.977997]\n",
      "epoch:7 step:5904 [D loss: 0.204164, acc.: 92.19%] [G loss: 0.765782]\n",
      "epoch:7 step:5905 [D loss: 0.216130, acc.: 90.62%] [G loss: 3.856591]\n",
      "epoch:7 step:5906 [D loss: 0.091347, acc.: 97.66%] [G loss: 2.601435]\n",
      "epoch:7 step:5907 [D loss: 0.086570, acc.: 98.44%] [G loss: 1.840117]\n",
      "epoch:7 step:5908 [D loss: 0.196620, acc.: 92.19%] [G loss: 1.499496]\n",
      "epoch:7 step:5909 [D loss: 0.005695, acc.: 100.00%] [G loss: 1.438473]\n",
      "epoch:7 step:5910 [D loss: 0.122658, acc.: 96.09%] [G loss: 1.441861]\n",
      "epoch:7 step:5911 [D loss: 0.988452, acc.: 46.88%] [G loss: 4.857401]\n",
      "epoch:7 step:5912 [D loss: 0.964588, acc.: 60.94%] [G loss: 2.342405]\n",
      "epoch:7 step:5913 [D loss: 0.220075, acc.: 89.84%] [G loss: 3.142642]\n",
      "epoch:7 step:5914 [D loss: 0.018600, acc.: 100.00%] [G loss: 3.600402]\n",
      "epoch:7 step:5915 [D loss: 0.556710, acc.: 67.97%] [G loss: 2.179701]\n",
      "epoch:7 step:5916 [D loss: 0.172678, acc.: 92.97%] [G loss: 4.339510]\n",
      "epoch:7 step:5917 [D loss: 0.093438, acc.: 98.44%] [G loss: 4.959015]\n",
      "epoch:7 step:5918 [D loss: 0.129494, acc.: 97.66%] [G loss: 4.533842]\n",
      "epoch:7 step:5919 [D loss: 0.143139, acc.: 95.31%] [G loss: 3.684282]\n",
      "epoch:7 step:5920 [D loss: 0.204580, acc.: 93.75%] [G loss: 3.718288]\n",
      "epoch:7 step:5921 [D loss: 0.097898, acc.: 97.66%] [G loss: 2.879024]\n",
      "epoch:7 step:5922 [D loss: 0.172493, acc.: 93.75%] [G loss: 4.128081]\n",
      "epoch:7 step:5923 [D loss: 0.187900, acc.: 93.75%] [G loss: 4.559375]\n",
      "epoch:7 step:5924 [D loss: 0.030858, acc.: 100.00%] [G loss: 3.517232]\n",
      "epoch:7 step:5925 [D loss: 0.239219, acc.: 91.41%] [G loss: 0.709448]\n",
      "epoch:7 step:5926 [D loss: 0.499896, acc.: 75.78%] [G loss: 6.903063]\n",
      "epoch:7 step:5927 [D loss: 0.377445, acc.: 76.56%] [G loss: 5.586074]\n",
      "epoch:7 step:5928 [D loss: 0.070673, acc.: 99.22%] [G loss: 2.641642]\n",
      "epoch:7 step:5929 [D loss: 0.042134, acc.: 100.00%] [G loss: 1.672039]\n",
      "epoch:7 step:5930 [D loss: 0.095417, acc.: 97.66%] [G loss: 4.852388]\n",
      "epoch:7 step:5931 [D loss: 0.012398, acc.: 100.00%] [G loss: 3.415302]\n",
      "epoch:7 step:5932 [D loss: 0.466786, acc.: 76.56%] [G loss: 6.051523]\n",
      "epoch:7 step:5933 [D loss: 0.204631, acc.: 92.19%] [G loss: 4.089296]\n",
      "epoch:7 step:5934 [D loss: 0.171274, acc.: 94.53%] [G loss: 5.062171]\n",
      "epoch:7 step:5935 [D loss: 0.250738, acc.: 89.84%] [G loss: 3.475967]\n",
      "epoch:7 step:5936 [D loss: 0.077274, acc.: 96.88%] [G loss: 4.308876]\n",
      "epoch:7 step:5937 [D loss: 0.127807, acc.: 94.53%] [G loss: 3.943926]\n",
      "epoch:7 step:5938 [D loss: 0.497407, acc.: 78.12%] [G loss: 6.618606]\n",
      "epoch:7 step:5939 [D loss: 0.277429, acc.: 83.59%] [G loss: 5.975505]\n",
      "epoch:7 step:5940 [D loss: 0.060889, acc.: 100.00%] [G loss: 4.935153]\n",
      "epoch:7 step:5941 [D loss: 0.122743, acc.: 96.09%] [G loss: 3.959593]\n",
      "epoch:7 step:5942 [D loss: 0.091795, acc.: 97.66%] [G loss: 3.181680]\n",
      "epoch:7 step:5943 [D loss: 0.030670, acc.: 100.00%] [G loss: 2.894266]\n",
      "epoch:7 step:5944 [D loss: 0.222507, acc.: 89.84%] [G loss: 5.011248]\n",
      "epoch:7 step:5945 [D loss: 0.436991, acc.: 81.25%] [G loss: 2.814255]\n",
      "epoch:7 step:5946 [D loss: 0.979996, acc.: 60.94%] [G loss: 8.484266]\n",
      "epoch:7 step:5947 [D loss: 1.111785, acc.: 59.38%] [G loss: 5.396667]\n",
      "epoch:7 step:5948 [D loss: 0.102465, acc.: 96.09%] [G loss: 3.984101]\n",
      "epoch:7 step:5949 [D loss: 0.079235, acc.: 98.44%] [G loss: 4.024675]\n",
      "epoch:7 step:5950 [D loss: 0.090994, acc.: 97.66%] [G loss: 4.968623]\n",
      "epoch:7 step:5951 [D loss: 0.091311, acc.: 98.44%] [G loss: 4.389318]\n",
      "epoch:7 step:5952 [D loss: 0.143591, acc.: 97.66%] [G loss: 4.444851]\n",
      "epoch:7 step:5953 [D loss: 0.121681, acc.: 96.09%] [G loss: 4.330229]\n",
      "epoch:7 step:5954 [D loss: 0.110458, acc.: 96.88%] [G loss: 2.578114]\n",
      "epoch:7 step:5955 [D loss: 0.085547, acc.: 100.00%] [G loss: 2.986820]\n",
      "epoch:7 step:5956 [D loss: 0.263710, acc.: 89.06%] [G loss: 4.613113]\n",
      "epoch:7 step:5957 [D loss: 0.085333, acc.: 96.88%] [G loss: 4.403584]\n",
      "epoch:7 step:5958 [D loss: 0.602167, acc.: 71.88%] [G loss: 4.620695]\n",
      "epoch:7 step:5959 [D loss: 0.153847, acc.: 93.75%] [G loss: 4.338419]\n",
      "epoch:7 step:5960 [D loss: 0.141994, acc.: 94.53%] [G loss: 3.298935]\n",
      "epoch:7 step:5961 [D loss: 0.117723, acc.: 95.31%] [G loss: 3.859999]\n",
      "epoch:7 step:5962 [D loss: 0.541499, acc.: 72.66%] [G loss: 5.697326]\n",
      "epoch:7 step:5963 [D loss: 0.312258, acc.: 83.59%] [G loss: 3.132840]\n",
      "epoch:7 step:5964 [D loss: 0.064680, acc.: 100.00%] [G loss: 2.758501]\n",
      "epoch:7 step:5965 [D loss: 0.141631, acc.: 96.09%] [G loss: 4.080224]\n",
      "epoch:7 step:5966 [D loss: 1.416847, acc.: 32.03%] [G loss: 7.992185]\n",
      "epoch:7 step:5967 [D loss: 1.167626, acc.: 56.25%] [G loss: 5.131743]\n",
      "epoch:7 step:5968 [D loss: 0.294051, acc.: 88.28%] [G loss: 2.353952]\n",
      "epoch:7 step:5969 [D loss: 0.277469, acc.: 88.28%] [G loss: 3.812347]\n",
      "epoch:7 step:5970 [D loss: 0.037662, acc.: 100.00%] [G loss: 4.913277]\n",
      "epoch:7 step:5971 [D loss: 0.091930, acc.: 98.44%] [G loss: 4.301006]\n",
      "epoch:7 step:5972 [D loss: 0.098950, acc.: 99.22%] [G loss: 3.821917]\n",
      "epoch:7 step:5973 [D loss: 0.087745, acc.: 97.66%] [G loss: 3.239904]\n",
      "epoch:7 step:5974 [D loss: 0.069903, acc.: 97.66%] [G loss: 3.902270]\n",
      "epoch:7 step:5975 [D loss: 0.112862, acc.: 97.66%] [G loss: 4.115156]\n",
      "epoch:7 step:5976 [D loss: 0.656771, acc.: 69.53%] [G loss: 3.939236]\n",
      "epoch:7 step:5977 [D loss: 0.092595, acc.: 97.66%] [G loss: 4.668850]\n",
      "epoch:7 step:5978 [D loss: 0.328920, acc.: 85.94%] [G loss: 3.627861]\n",
      "epoch:7 step:5979 [D loss: 0.084395, acc.: 98.44%] [G loss: 4.294155]\n",
      "epoch:7 step:5980 [D loss: 0.113739, acc.: 99.22%] [G loss: 4.046922]\n",
      "epoch:7 step:5981 [D loss: 0.405896, acc.: 79.69%] [G loss: 4.734731]\n",
      "epoch:7 step:5982 [D loss: 0.191913, acc.: 90.62%] [G loss: 3.023219]\n",
      "epoch:7 step:5983 [D loss: 0.105907, acc.: 97.66%] [G loss: 1.540446]\n",
      "epoch:7 step:5984 [D loss: 0.010942, acc.: 100.00%] [G loss: 1.457588]\n",
      "epoch:7 step:5985 [D loss: 0.060968, acc.: 100.00%] [G loss: 0.728159]\n",
      "epoch:7 step:5986 [D loss: 0.195121, acc.: 92.97%] [G loss: 1.855122]\n",
      "epoch:7 step:5987 [D loss: 0.018797, acc.: 100.00%] [G loss: 2.103972]\n",
      "epoch:7 step:5988 [D loss: 0.175676, acc.: 93.75%] [G loss: 1.436470]\n",
      "epoch:7 step:5989 [D loss: 0.072278, acc.: 100.00%] [G loss: 0.829173]\n",
      "epoch:7 step:5990 [D loss: 0.145858, acc.: 97.66%] [G loss: 1.050802]\n",
      "epoch:7 step:5991 [D loss: 0.087899, acc.: 98.44%] [G loss: 0.971316]\n",
      "epoch:7 step:5992 [D loss: 0.080245, acc.: 98.44%] [G loss: 0.187650]\n",
      "epoch:7 step:5993 [D loss: 0.359423, acc.: 86.72%] [G loss: 3.249822]\n",
      "epoch:7 step:5994 [D loss: 0.814171, acc.: 57.81%] [G loss: 3.642648]\n",
      "epoch:7 step:5995 [D loss: 0.085441, acc.: 98.44%] [G loss: 3.051569]\n",
      "epoch:7 step:5996 [D loss: 0.060714, acc.: 98.44%] [G loss: 3.976320]\n",
      "epoch:7 step:5997 [D loss: 0.033564, acc.: 100.00%] [G loss: 4.192940]\n",
      "epoch:7 step:5998 [D loss: 0.149656, acc.: 95.31%] [G loss: 3.960966]\n",
      "epoch:7 step:5999 [D loss: 0.149892, acc.: 97.66%] [G loss: 5.024400]\n",
      "epoch:7 step:6000 [D loss: 0.084439, acc.: 99.22%] [G loss: 4.247047]\n",
      "epoch:7 step:6001 [D loss: 0.143474, acc.: 97.66%] [G loss: 3.841439]\n",
      "epoch:7 step:6002 [D loss: 0.100155, acc.: 98.44%] [G loss: 3.882666]\n",
      "epoch:7 step:6003 [D loss: 0.092471, acc.: 100.00%] [G loss: 3.433180]\n",
      "epoch:7 step:6004 [D loss: 0.435939, acc.: 82.03%] [G loss: 5.216477]\n",
      "epoch:7 step:6005 [D loss: 0.086554, acc.: 97.66%] [G loss: 5.485697]\n",
      "epoch:7 step:6006 [D loss: 0.338140, acc.: 84.38%] [G loss: 3.431550]\n",
      "epoch:7 step:6007 [D loss: 0.024328, acc.: 99.22%] [G loss: 3.169953]\n",
      "epoch:7 step:6008 [D loss: 0.027503, acc.: 99.22%] [G loss: 1.477599]\n",
      "epoch:7 step:6009 [D loss: 1.191312, acc.: 51.56%] [G loss: 7.749761]\n",
      "epoch:7 step:6010 [D loss: 1.127407, acc.: 56.25%] [G loss: 3.566972]\n",
      "epoch:7 step:6011 [D loss: 0.486202, acc.: 78.91%] [G loss: 4.991346]\n",
      "epoch:7 step:6012 [D loss: 0.132946, acc.: 93.75%] [G loss: 4.568530]\n",
      "epoch:7 step:6013 [D loss: 0.417233, acc.: 82.03%] [G loss: 4.620702]\n",
      "epoch:7 step:6014 [D loss: 0.212170, acc.: 92.19%] [G loss: 4.271017]\n",
      "epoch:7 step:6015 [D loss: 0.077455, acc.: 97.66%] [G loss: 2.435037]\n",
      "epoch:7 step:6016 [D loss: 0.078125, acc.: 96.88%] [G loss: 1.866565]\n",
      "epoch:7 step:6017 [D loss: 0.200433, acc.: 92.19%] [G loss: 3.413703]\n",
      "epoch:7 step:6018 [D loss: 0.070159, acc.: 99.22%] [G loss: 3.122129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6019 [D loss: 1.001476, acc.: 51.56%] [G loss: 5.072899]\n",
      "epoch:7 step:6020 [D loss: 0.054458, acc.: 99.22%] [G loss: 6.395020]\n",
      "epoch:7 step:6021 [D loss: 0.645161, acc.: 69.53%] [G loss: 4.120794]\n",
      "epoch:7 step:6022 [D loss: 0.064655, acc.: 98.44%] [G loss: 5.104270]\n",
      "epoch:7 step:6023 [D loss: 0.082486, acc.: 96.88%] [G loss: 4.783935]\n",
      "epoch:7 step:6024 [D loss: 0.246177, acc.: 91.41%] [G loss: 3.791630]\n",
      "epoch:7 step:6025 [D loss: 0.156570, acc.: 96.88%] [G loss: 3.328257]\n",
      "epoch:7 step:6026 [D loss: 0.046938, acc.: 100.00%] [G loss: 3.538061]\n",
      "epoch:7 step:6027 [D loss: 0.250859, acc.: 92.19%] [G loss: 2.756147]\n",
      "epoch:7 step:6028 [D loss: 0.097612, acc.: 96.88%] [G loss: 1.410079]\n",
      "epoch:7 step:6029 [D loss: 0.353562, acc.: 86.72%] [G loss: 3.216671]\n",
      "epoch:7 step:6030 [D loss: 0.018517, acc.: 100.00%] [G loss: 2.902965]\n",
      "epoch:7 step:6031 [D loss: 0.226847, acc.: 90.62%] [G loss: 2.214167]\n",
      "epoch:7 step:6032 [D loss: 0.050911, acc.: 100.00%] [G loss: 0.956592]\n",
      "epoch:7 step:6033 [D loss: 0.344448, acc.: 83.59%] [G loss: 6.169927]\n",
      "epoch:7 step:6034 [D loss: 0.086595, acc.: 96.88%] [G loss: 7.038964]\n",
      "epoch:7 step:6035 [D loss: 0.110703, acc.: 96.09%] [G loss: 4.760932]\n",
      "epoch:7 step:6036 [D loss: 0.123638, acc.: 96.88%] [G loss: 4.580239]\n",
      "epoch:7 step:6037 [D loss: 0.062824, acc.: 100.00%] [G loss: 3.718209]\n",
      "epoch:7 step:6038 [D loss: 0.078633, acc.: 100.00%] [G loss: 2.486062]\n",
      "epoch:7 step:6039 [D loss: 0.079990, acc.: 99.22%] [G loss: 3.949079]\n",
      "epoch:7 step:6040 [D loss: 0.559972, acc.: 73.44%] [G loss: 5.908414]\n",
      "epoch:7 step:6041 [D loss: 0.571830, acc.: 71.88%] [G loss: 3.304923]\n",
      "epoch:7 step:6042 [D loss: 0.109468, acc.: 96.88%] [G loss: 4.704014]\n",
      "epoch:7 step:6043 [D loss: 0.025336, acc.: 100.00%] [G loss: 4.208140]\n",
      "epoch:7 step:6044 [D loss: 0.152638, acc.: 94.53%] [G loss: 3.596355]\n",
      "epoch:7 step:6045 [D loss: 0.034760, acc.: 99.22%] [G loss: 4.223498]\n",
      "epoch:7 step:6046 [D loss: 0.211838, acc.: 95.31%] [G loss: 5.859742]\n",
      "epoch:7 step:6047 [D loss: 0.361278, acc.: 85.94%] [G loss: 2.257198]\n",
      "epoch:7 step:6048 [D loss: 0.143510, acc.: 95.31%] [G loss: 4.732594]\n",
      "epoch:7 step:6049 [D loss: 0.017448, acc.: 100.00%] [G loss: 5.841804]\n",
      "epoch:7 step:6050 [D loss: 0.079642, acc.: 96.88%] [G loss: 4.144959]\n",
      "epoch:7 step:6051 [D loss: 0.281174, acc.: 86.72%] [G loss: 7.116094]\n",
      "epoch:7 step:6052 [D loss: 0.833589, acc.: 58.59%] [G loss: 4.486643]\n",
      "epoch:7 step:6053 [D loss: 0.008875, acc.: 100.00%] [G loss: 5.970772]\n",
      "epoch:7 step:6054 [D loss: 0.052965, acc.: 99.22%] [G loss: 4.114220]\n",
      "epoch:7 step:6055 [D loss: 0.117008, acc.: 96.09%] [G loss: 4.002660]\n",
      "epoch:7 step:6056 [D loss: 0.212333, acc.: 93.75%] [G loss: 3.548031]\n",
      "epoch:7 step:6057 [D loss: 0.119337, acc.: 95.31%] [G loss: 4.122231]\n",
      "epoch:7 step:6058 [D loss: 0.077470, acc.: 99.22%] [G loss: 3.057166]\n",
      "epoch:7 step:6059 [D loss: 0.155383, acc.: 99.22%] [G loss: 3.693030]\n",
      "epoch:7 step:6060 [D loss: 0.250162, acc.: 91.41%] [G loss: 3.802298]\n",
      "epoch:7 step:6061 [D loss: 0.124540, acc.: 95.31%] [G loss: 1.316757]\n",
      "epoch:7 step:6062 [D loss: 0.118426, acc.: 96.09%] [G loss: 1.233944]\n",
      "epoch:7 step:6063 [D loss: 0.452213, acc.: 75.78%] [G loss: 8.913531]\n",
      "epoch:7 step:6064 [D loss: 0.977760, acc.: 57.03%] [G loss: 2.786410]\n",
      "epoch:7 step:6065 [D loss: 0.151441, acc.: 92.97%] [G loss: 2.533775]\n",
      "epoch:7 step:6066 [D loss: 0.079334, acc.: 98.44%] [G loss: 4.873678]\n",
      "epoch:7 step:6067 [D loss: 0.068480, acc.: 97.66%] [G loss: 4.344190]\n",
      "epoch:7 step:6068 [D loss: 0.058378, acc.: 99.22%] [G loss: 3.535893]\n",
      "epoch:7 step:6069 [D loss: 0.143523, acc.: 94.53%] [G loss: 4.958919]\n",
      "epoch:7 step:6070 [D loss: 0.542521, acc.: 76.56%] [G loss: 3.776009]\n",
      "epoch:7 step:6071 [D loss: 0.040357, acc.: 99.22%] [G loss: 4.941383]\n",
      "epoch:7 step:6072 [D loss: 0.062210, acc.: 98.44%] [G loss: 4.724809]\n",
      "epoch:7 step:6073 [D loss: 0.049366, acc.: 100.00%] [G loss: 3.450743]\n",
      "epoch:7 step:6074 [D loss: 0.034276, acc.: 100.00%] [G loss: 2.977560]\n",
      "epoch:7 step:6075 [D loss: 0.091623, acc.: 98.44%] [G loss: 3.409242]\n",
      "epoch:7 step:6076 [D loss: 0.299214, acc.: 87.50%] [G loss: 4.977805]\n",
      "epoch:7 step:6077 [D loss: 0.084792, acc.: 98.44%] [G loss: 4.339703]\n",
      "epoch:7 step:6078 [D loss: 0.049302, acc.: 99.22%] [G loss: 3.164040]\n",
      "epoch:7 step:6079 [D loss: 0.052058, acc.: 99.22%] [G loss: 1.768257]\n",
      "epoch:7 step:6080 [D loss: 1.664942, acc.: 31.25%] [G loss: 6.420601]\n",
      "epoch:7 step:6081 [D loss: 0.164636, acc.: 92.97%] [G loss: 7.711651]\n",
      "epoch:7 step:6082 [D loss: 0.364547, acc.: 79.69%] [G loss: 4.659903]\n",
      "epoch:7 step:6083 [D loss: 0.072447, acc.: 97.66%] [G loss: 4.657355]\n",
      "epoch:7 step:6084 [D loss: 0.028695, acc.: 100.00%] [G loss: 3.439262]\n",
      "epoch:7 step:6085 [D loss: 0.098660, acc.: 96.09%] [G loss: 2.761789]\n",
      "epoch:7 step:6086 [D loss: 0.054026, acc.: 100.00%] [G loss: 3.946526]\n",
      "epoch:7 step:6087 [D loss: 0.205949, acc.: 92.97%] [G loss: 5.345785]\n",
      "epoch:7 step:6088 [D loss: 0.190487, acc.: 93.75%] [G loss: 4.322666]\n",
      "epoch:7 step:6089 [D loss: 0.266990, acc.: 89.84%] [G loss: 5.180551]\n",
      "epoch:7 step:6090 [D loss: 1.483556, acc.: 24.22%] [G loss: 7.147849]\n",
      "epoch:7 step:6091 [D loss: 0.296036, acc.: 84.38%] [G loss: 6.709837]\n",
      "epoch:7 step:6092 [D loss: 0.289409, acc.: 83.59%] [G loss: 3.194972]\n",
      "epoch:7 step:6093 [D loss: 0.094403, acc.: 96.88%] [G loss: 2.169385]\n",
      "epoch:7 step:6094 [D loss: 0.106528, acc.: 96.09%] [G loss: 1.010100]\n",
      "epoch:7 step:6095 [D loss: 0.017770, acc.: 100.00%] [G loss: 2.841447]\n",
      "epoch:7 step:6096 [D loss: 0.114171, acc.: 99.22%] [G loss: 3.786501]\n",
      "epoch:7 step:6097 [D loss: 0.045238, acc.: 100.00%] [G loss: 2.940287]\n",
      "epoch:7 step:6098 [D loss: 0.241331, acc.: 92.97%] [G loss: 0.820447]\n",
      "epoch:7 step:6099 [D loss: 0.773320, acc.: 64.06%] [G loss: 7.809612]\n",
      "epoch:7 step:6100 [D loss: 1.270533, acc.: 55.47%] [G loss: 4.901515]\n",
      "epoch:7 step:6101 [D loss: 0.189239, acc.: 92.97%] [G loss: 2.783442]\n",
      "epoch:7 step:6102 [D loss: 0.137965, acc.: 96.88%] [G loss: 3.659083]\n",
      "epoch:7 step:6103 [D loss: 0.330502, acc.: 85.16%] [G loss: 3.938382]\n",
      "epoch:7 step:6104 [D loss: 0.184850, acc.: 92.19%] [G loss: 4.388880]\n",
      "epoch:7 step:6105 [D loss: 0.185417, acc.: 93.75%] [G loss: 3.393050]\n",
      "epoch:7 step:6106 [D loss: 0.173497, acc.: 96.09%] [G loss: 3.082834]\n",
      "epoch:7 step:6107 [D loss: 0.097405, acc.: 98.44%] [G loss: 4.003134]\n",
      "epoch:7 step:6108 [D loss: 0.058589, acc.: 98.44%] [G loss: 2.856780]\n",
      "epoch:7 step:6109 [D loss: 0.485766, acc.: 75.00%] [G loss: 2.242043]\n",
      "epoch:7 step:6110 [D loss: 0.035604, acc.: 99.22%] [G loss: 1.411469]\n",
      "epoch:7 step:6111 [D loss: 0.060064, acc.: 100.00%] [G loss: 0.820298]\n",
      "epoch:7 step:6112 [D loss: 0.548095, acc.: 69.53%] [G loss: 5.531063]\n",
      "epoch:7 step:6113 [D loss: 0.950845, acc.: 59.38%] [G loss: 1.372662]\n",
      "epoch:7 step:6114 [D loss: 0.399018, acc.: 79.69%] [G loss: 4.942046]\n",
      "epoch:7 step:6115 [D loss: 0.119677, acc.: 94.53%] [G loss: 5.821349]\n",
      "epoch:7 step:6116 [D loss: 0.261601, acc.: 88.28%] [G loss: 3.235738]\n",
      "epoch:7 step:6117 [D loss: 0.198893, acc.: 91.41%] [G loss: 3.451007]\n",
      "epoch:7 step:6118 [D loss: 0.037813, acc.: 100.00%] [G loss: 4.188371]\n",
      "epoch:7 step:6119 [D loss: 0.166777, acc.: 95.31%] [G loss: 3.522833]\n",
      "epoch:7 step:6120 [D loss: 0.095790, acc.: 96.09%] [G loss: 3.011835]\n",
      "epoch:7 step:6121 [D loss: 0.184547, acc.: 91.41%] [G loss: 3.987566]\n",
      "epoch:7 step:6122 [D loss: 0.199947, acc.: 92.19%] [G loss: 2.113516]\n",
      "epoch:7 step:6123 [D loss: 0.602829, acc.: 67.97%] [G loss: 7.214707]\n",
      "epoch:7 step:6124 [D loss: 0.607569, acc.: 70.31%] [G loss: 4.198673]\n",
      "epoch:7 step:6125 [D loss: 0.060041, acc.: 99.22%] [G loss: 3.686425]\n",
      "epoch:7 step:6126 [D loss: 0.127399, acc.: 97.66%] [G loss: 4.069949]\n",
      "epoch:7 step:6127 [D loss: 0.117100, acc.: 96.09%] [G loss: 4.456155]\n",
      "epoch:7 step:6128 [D loss: 0.092148, acc.: 98.44%] [G loss: 3.636500]\n",
      "epoch:7 step:6129 [D loss: 0.140966, acc.: 95.31%] [G loss: 2.982994]\n",
      "epoch:7 step:6130 [D loss: 0.163160, acc.: 95.31%] [G loss: 4.803705]\n",
      "epoch:7 step:6131 [D loss: 0.407520, acc.: 82.81%] [G loss: 4.015218]\n",
      "epoch:7 step:6132 [D loss: 0.174506, acc.: 93.75%] [G loss: 2.997609]\n",
      "epoch:7 step:6133 [D loss: 0.038811, acc.: 99.22%] [G loss: 2.099165]\n",
      "epoch:7 step:6134 [D loss: 0.325059, acc.: 85.16%] [G loss: 4.801337]\n",
      "epoch:7 step:6135 [D loss: 0.363331, acc.: 82.03%] [G loss: 2.873815]\n",
      "epoch:7 step:6136 [D loss: 0.118951, acc.: 95.31%] [G loss: 2.941756]\n",
      "epoch:7 step:6137 [D loss: 0.081318, acc.: 98.44%] [G loss: 3.148323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6138 [D loss: 0.120562, acc.: 95.31%] [G loss: 2.093532]\n",
      "epoch:7 step:6139 [D loss: 0.423037, acc.: 78.91%] [G loss: 4.893966]\n",
      "epoch:7 step:6140 [D loss: 0.316403, acc.: 86.72%] [G loss: 3.764334]\n",
      "epoch:7 step:6141 [D loss: 0.186513, acc.: 92.97%] [G loss: 3.373703]\n",
      "epoch:7 step:6142 [D loss: 0.044462, acc.: 100.00%] [G loss: 4.896323]\n",
      "epoch:7 step:6143 [D loss: 0.099293, acc.: 99.22%] [G loss: 3.907866]\n",
      "epoch:7 step:6144 [D loss: 0.115857, acc.: 98.44%] [G loss: 3.264036]\n",
      "epoch:7 step:6145 [D loss: 0.177537, acc.: 95.31%] [G loss: 3.892444]\n",
      "epoch:7 step:6146 [D loss: 0.167123, acc.: 96.09%] [G loss: 3.277658]\n",
      "epoch:7 step:6147 [D loss: 0.055275, acc.: 99.22%] [G loss: 3.336927]\n",
      "epoch:7 step:6148 [D loss: 0.124282, acc.: 97.66%] [G loss: 3.584599]\n",
      "epoch:7 step:6149 [D loss: 0.221833, acc.: 92.97%] [G loss: 4.184879]\n",
      "epoch:7 step:6150 [D loss: 0.190806, acc.: 92.97%] [G loss: 2.067091]\n",
      "epoch:7 step:6151 [D loss: 0.161055, acc.: 95.31%] [G loss: 3.570376]\n",
      "epoch:7 step:6152 [D loss: 0.365558, acc.: 82.03%] [G loss: 5.460406]\n",
      "epoch:7 step:6153 [D loss: 0.303742, acc.: 85.94%] [G loss: 2.749397]\n",
      "epoch:7 step:6154 [D loss: 0.332499, acc.: 89.06%] [G loss: 5.888249]\n",
      "epoch:7 step:6155 [D loss: 0.337198, acc.: 81.25%] [G loss: 3.095488]\n",
      "epoch:7 step:6156 [D loss: 0.492468, acc.: 77.34%] [G loss: 6.728064]\n",
      "epoch:7 step:6157 [D loss: 0.279474, acc.: 83.59%] [G loss: 5.236629]\n",
      "epoch:7 step:6158 [D loss: 0.168002, acc.: 93.75%] [G loss: 4.460327]\n",
      "epoch:7 step:6159 [D loss: 0.142254, acc.: 97.66%] [G loss: 4.273843]\n",
      "epoch:7 step:6160 [D loss: 0.063314, acc.: 98.44%] [G loss: 3.242527]\n",
      "epoch:7 step:6161 [D loss: 0.073695, acc.: 97.66%] [G loss: 2.170489]\n",
      "epoch:7 step:6162 [D loss: 0.158268, acc.: 92.97%] [G loss: 4.334222]\n",
      "epoch:7 step:6163 [D loss: 0.201194, acc.: 94.53%] [G loss: 5.033768]\n",
      "epoch:7 step:6164 [D loss: 0.142533, acc.: 96.09%] [G loss: 4.631946]\n",
      "epoch:7 step:6165 [D loss: 0.065026, acc.: 99.22%] [G loss: 4.221244]\n",
      "epoch:7 step:6166 [D loss: 0.238130, acc.: 90.62%] [G loss: 3.124649]\n",
      "epoch:7 step:6167 [D loss: 0.034863, acc.: 99.22%] [G loss: 2.898689]\n",
      "epoch:7 step:6168 [D loss: 0.124607, acc.: 94.53%] [G loss: 3.537406]\n",
      "epoch:7 step:6169 [D loss: 0.072556, acc.: 98.44%] [G loss: 4.685700]\n",
      "epoch:7 step:6170 [D loss: 0.140304, acc.: 93.75%] [G loss: 4.656886]\n",
      "epoch:7 step:6171 [D loss: 0.259874, acc.: 89.84%] [G loss: 5.103001]\n",
      "epoch:7 step:6172 [D loss: 0.077113, acc.: 97.66%] [G loss: 4.821033]\n",
      "epoch:7 step:6173 [D loss: 0.074452, acc.: 98.44%] [G loss: 2.641838]\n",
      "epoch:7 step:6174 [D loss: 0.327755, acc.: 87.50%] [G loss: 6.084490]\n",
      "epoch:7 step:6175 [D loss: 0.113859, acc.: 96.88%] [G loss: 5.430416]\n",
      "epoch:7 step:6176 [D loss: 0.182917, acc.: 95.31%] [G loss: 2.648158]\n",
      "epoch:7 step:6177 [D loss: 0.063025, acc.: 99.22%] [G loss: 2.578930]\n",
      "epoch:7 step:6178 [D loss: 0.127284, acc.: 96.09%] [G loss: 5.625659]\n",
      "epoch:7 step:6179 [D loss: 0.200065, acc.: 96.88%] [G loss: 5.600332]\n",
      "epoch:7 step:6180 [D loss: 2.075980, acc.: 21.09%] [G loss: 6.821702]\n",
      "epoch:7 step:6181 [D loss: 0.366783, acc.: 80.47%] [G loss: 5.521027]\n",
      "epoch:7 step:6182 [D loss: 0.439181, acc.: 75.00%] [G loss: 2.836399]\n",
      "epoch:7 step:6183 [D loss: 0.091783, acc.: 96.88%] [G loss: 3.316277]\n",
      "epoch:7 step:6184 [D loss: 0.093669, acc.: 96.88%] [G loss: 4.423286]\n",
      "epoch:7 step:6185 [D loss: 0.015937, acc.: 100.00%] [G loss: 4.465132]\n",
      "epoch:7 step:6186 [D loss: 0.109236, acc.: 96.88%] [G loss: 4.949377]\n",
      "epoch:7 step:6187 [D loss: 0.064863, acc.: 99.22%] [G loss: 4.128987]\n",
      "epoch:7 step:6188 [D loss: 0.143318, acc.: 94.53%] [G loss: 2.108235]\n",
      "epoch:7 step:6189 [D loss: 0.183751, acc.: 94.53%] [G loss: 4.111938]\n",
      "epoch:7 step:6190 [D loss: 0.218966, acc.: 90.62%] [G loss: 2.200748]\n",
      "epoch:7 step:6191 [D loss: 0.173886, acc.: 95.31%] [G loss: 4.628138]\n",
      "epoch:7 step:6192 [D loss: 0.389998, acc.: 83.59%] [G loss: 4.153323]\n",
      "epoch:7 step:6193 [D loss: 0.023883, acc.: 100.00%] [G loss: 4.048873]\n",
      "epoch:7 step:6194 [D loss: 0.519910, acc.: 77.34%] [G loss: 6.024365]\n",
      "epoch:7 step:6195 [D loss: 0.276411, acc.: 87.50%] [G loss: 5.297730]\n",
      "epoch:7 step:6196 [D loss: 0.044071, acc.: 98.44%] [G loss: 4.104509]\n",
      "epoch:7 step:6197 [D loss: 0.072382, acc.: 98.44%] [G loss: 4.810573]\n",
      "epoch:7 step:6198 [D loss: 0.035584, acc.: 99.22%] [G loss: 4.621140]\n",
      "epoch:7 step:6199 [D loss: 0.134749, acc.: 96.88%] [G loss: 3.602789]\n",
      "epoch:7 step:6200 [D loss: 0.084555, acc.: 98.44%] [G loss: 3.797732]\n",
      "epoch:7 step:6201 [D loss: 0.033917, acc.: 100.00%] [G loss: 4.506134]\n",
      "epoch:7 step:6202 [D loss: 0.370119, acc.: 85.16%] [G loss: 2.360202]\n",
      "epoch:7 step:6203 [D loss: 0.224324, acc.: 91.41%] [G loss: 5.427357]\n",
      "epoch:7 step:6204 [D loss: 0.159069, acc.: 96.88%] [G loss: 3.181119]\n",
      "epoch:7 step:6205 [D loss: 0.208904, acc.: 92.97%] [G loss: 3.259064]\n",
      "epoch:7 step:6206 [D loss: 0.247999, acc.: 89.84%] [G loss: 6.528072]\n",
      "epoch:7 step:6207 [D loss: 0.398897, acc.: 78.12%] [G loss: 1.875368]\n",
      "epoch:7 step:6208 [D loss: 0.432378, acc.: 78.12%] [G loss: 5.842890]\n",
      "epoch:7 step:6209 [D loss: 0.129584, acc.: 94.53%] [G loss: 5.685034]\n",
      "epoch:7 step:6210 [D loss: 0.305305, acc.: 85.16%] [G loss: 1.142586]\n",
      "epoch:7 step:6211 [D loss: 0.576066, acc.: 71.88%] [G loss: 5.020915]\n",
      "epoch:7 step:6212 [D loss: 0.224538, acc.: 88.28%] [G loss: 5.053214]\n",
      "epoch:7 step:6213 [D loss: 0.242841, acc.: 87.50%] [G loss: 1.671766]\n",
      "epoch:7 step:6214 [D loss: 0.318203, acc.: 84.38%] [G loss: 5.663724]\n",
      "epoch:7 step:6215 [D loss: 0.065156, acc.: 98.44%] [G loss: 6.451690]\n",
      "epoch:7 step:6216 [D loss: 0.245494, acc.: 87.50%] [G loss: 2.384154]\n",
      "epoch:7 step:6217 [D loss: 0.261535, acc.: 89.06%] [G loss: 4.213167]\n",
      "epoch:7 step:6218 [D loss: 0.057786, acc.: 99.22%] [G loss: 5.150963]\n",
      "epoch:7 step:6219 [D loss: 0.040733, acc.: 100.00%] [G loss: 4.211815]\n",
      "epoch:7 step:6220 [D loss: 0.303489, acc.: 85.94%] [G loss: 2.776660]\n",
      "epoch:7 step:6221 [D loss: 0.395754, acc.: 82.03%] [G loss: 5.171134]\n",
      "epoch:7 step:6222 [D loss: 0.134145, acc.: 93.75%] [G loss: 4.163274]\n",
      "epoch:7 step:6223 [D loss: 0.177744, acc.: 94.53%] [G loss: 4.903073]\n",
      "epoch:7 step:6224 [D loss: 0.110642, acc.: 96.88%] [G loss: 4.305313]\n",
      "epoch:7 step:6225 [D loss: 0.085614, acc.: 99.22%] [G loss: 4.125975]\n",
      "epoch:7 step:6226 [D loss: 0.268038, acc.: 89.06%] [G loss: 6.864506]\n",
      "epoch:7 step:6227 [D loss: 0.810654, acc.: 64.84%] [G loss: 4.137742]\n",
      "epoch:7 step:6228 [D loss: 0.014870, acc.: 100.00%] [G loss: 5.201542]\n",
      "epoch:7 step:6229 [D loss: 0.175527, acc.: 92.97%] [G loss: 3.212498]\n",
      "epoch:7 step:6230 [D loss: 0.139236, acc.: 94.53%] [G loss: 5.144765]\n",
      "epoch:7 step:6231 [D loss: 0.045869, acc.: 99.22%] [G loss: 5.243197]\n",
      "epoch:7 step:6232 [D loss: 0.055982, acc.: 99.22%] [G loss: 3.905459]\n",
      "epoch:7 step:6233 [D loss: 0.044950, acc.: 98.44%] [G loss: 2.346596]\n",
      "epoch:7 step:6234 [D loss: 0.862548, acc.: 62.50%] [G loss: 9.321810]\n",
      "epoch:7 step:6235 [D loss: 1.851477, acc.: 50.00%] [G loss: 3.672895]\n",
      "epoch:7 step:6236 [D loss: 0.210650, acc.: 92.19%] [G loss: 3.071646]\n",
      "epoch:7 step:6237 [D loss: 0.134205, acc.: 97.66%] [G loss: 4.327758]\n",
      "epoch:7 step:6238 [D loss: 0.104421, acc.: 96.88%] [G loss: 4.095911]\n",
      "epoch:7 step:6239 [D loss: 0.171833, acc.: 95.31%] [G loss: 3.226019]\n",
      "epoch:7 step:6240 [D loss: 0.056190, acc.: 100.00%] [G loss: 3.827609]\n",
      "epoch:7 step:6241 [D loss: 0.135929, acc.: 96.88%] [G loss: 3.663126]\n",
      "epoch:7 step:6242 [D loss: 0.050621, acc.: 100.00%] [G loss: 3.687838]\n",
      "epoch:7 step:6243 [D loss: 0.092827, acc.: 98.44%] [G loss: 2.046081]\n",
      "epoch:7 step:6244 [D loss: 1.205169, acc.: 45.31%] [G loss: 6.392296]\n",
      "epoch:7 step:6245 [D loss: 0.994897, acc.: 56.25%] [G loss: 3.978056]\n",
      "epoch:7 step:6246 [D loss: 0.100341, acc.: 96.88%] [G loss: 2.322736]\n",
      "epoch:7 step:6247 [D loss: 0.296934, acc.: 84.38%] [G loss: 3.801517]\n",
      "epoch:7 step:6248 [D loss: 0.015787, acc.: 100.00%] [G loss: 4.918476]\n",
      "epoch:8 step:6249 [D loss: 0.180438, acc.: 93.75%] [G loss: 3.476423]\n",
      "epoch:8 step:6250 [D loss: 0.072394, acc.: 99.22%] [G loss: 2.356569]\n",
      "epoch:8 step:6251 [D loss: 0.281312, acc.: 89.84%] [G loss: 3.416846]\n",
      "epoch:8 step:6252 [D loss: 0.044460, acc.: 99.22%] [G loss: 4.288228]\n",
      "epoch:8 step:6253 [D loss: 0.190171, acc.: 94.53%] [G loss: 2.956140]\n",
      "epoch:8 step:6254 [D loss: 0.145458, acc.: 97.66%] [G loss: 3.352022]\n",
      "epoch:8 step:6255 [D loss: 0.172545, acc.: 96.09%] [G loss: 2.560998]\n",
      "epoch:8 step:6256 [D loss: 0.067692, acc.: 100.00%] [G loss: 3.093601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6257 [D loss: 0.064117, acc.: 100.00%] [G loss: 2.822397]\n",
      "epoch:8 step:6258 [D loss: 0.421945, acc.: 81.25%] [G loss: 3.682772]\n",
      "epoch:8 step:6259 [D loss: 0.230574, acc.: 90.62%] [G loss: 4.644469]\n",
      "epoch:8 step:6260 [D loss: 0.164319, acc.: 94.53%] [G loss: 0.895665]\n",
      "epoch:8 step:6261 [D loss: 0.156879, acc.: 94.53%] [G loss: 1.663298]\n",
      "epoch:8 step:6262 [D loss: 0.016326, acc.: 100.00%] [G loss: 2.045641]\n",
      "epoch:8 step:6263 [D loss: 0.095616, acc.: 98.44%] [G loss: 3.802999]\n",
      "epoch:8 step:6264 [D loss: 0.143453, acc.: 93.75%] [G loss: 1.713484]\n",
      "epoch:8 step:6265 [D loss: 0.385973, acc.: 80.47%] [G loss: 5.455261]\n",
      "epoch:8 step:6266 [D loss: 0.509053, acc.: 75.00%] [G loss: 2.500968]\n",
      "epoch:8 step:6267 [D loss: 0.175449, acc.: 93.75%] [G loss: 3.708552]\n",
      "epoch:8 step:6268 [D loss: 0.035016, acc.: 100.00%] [G loss: 4.513843]\n",
      "epoch:8 step:6269 [D loss: 0.133357, acc.: 96.09%] [G loss: 3.545412]\n",
      "epoch:8 step:6270 [D loss: 0.120877, acc.: 96.88%] [G loss: 3.905884]\n",
      "epoch:8 step:6271 [D loss: 0.132297, acc.: 96.88%] [G loss: 2.068054]\n",
      "epoch:8 step:6272 [D loss: 0.226299, acc.: 91.41%] [G loss: 2.978304]\n",
      "epoch:8 step:6273 [D loss: 0.104073, acc.: 97.66%] [G loss: 3.508857]\n",
      "epoch:8 step:6274 [D loss: 1.424062, acc.: 35.16%] [G loss: 6.599030]\n",
      "epoch:8 step:6275 [D loss: 0.888902, acc.: 63.28%] [G loss: 5.510331]\n",
      "epoch:8 step:6276 [D loss: 0.147114, acc.: 96.09%] [G loss: 2.417444]\n",
      "epoch:8 step:6277 [D loss: 0.147874, acc.: 94.53%] [G loss: 3.593985]\n",
      "epoch:8 step:6278 [D loss: 0.083216, acc.: 98.44%] [G loss: 4.448453]\n",
      "epoch:8 step:6279 [D loss: 0.108677, acc.: 97.66%] [G loss: 2.794783]\n",
      "epoch:8 step:6280 [D loss: 0.151442, acc.: 92.97%] [G loss: 2.008018]\n",
      "epoch:8 step:6281 [D loss: 0.194555, acc.: 92.97%] [G loss: 3.148016]\n",
      "epoch:8 step:6282 [D loss: 0.064240, acc.: 98.44%] [G loss: 4.542149]\n",
      "epoch:8 step:6283 [D loss: 0.192847, acc.: 92.19%] [G loss: 2.214103]\n",
      "epoch:8 step:6284 [D loss: 0.125434, acc.: 98.44%] [G loss: 3.433406]\n",
      "epoch:8 step:6285 [D loss: 0.192497, acc.: 95.31%] [G loss: 3.548398]\n",
      "epoch:8 step:6286 [D loss: 0.140036, acc.: 96.88%] [G loss: 3.870441]\n",
      "epoch:8 step:6287 [D loss: 0.114452, acc.: 96.88%] [G loss: 4.052651]\n",
      "epoch:8 step:6288 [D loss: 0.300360, acc.: 86.72%] [G loss: 4.081947]\n",
      "epoch:8 step:6289 [D loss: 0.125275, acc.: 96.88%] [G loss: 3.064278]\n",
      "epoch:8 step:6290 [D loss: 0.093915, acc.: 99.22%] [G loss: 3.869067]\n",
      "epoch:8 step:6291 [D loss: 0.468179, acc.: 78.12%] [G loss: 4.051082]\n",
      "epoch:8 step:6292 [D loss: 0.157200, acc.: 95.31%] [G loss: 4.569674]\n",
      "epoch:8 step:6293 [D loss: 0.118677, acc.: 98.44%] [G loss: 4.297281]\n",
      "epoch:8 step:6294 [D loss: 0.263036, acc.: 89.06%] [G loss: 4.318622]\n",
      "epoch:8 step:6295 [D loss: 0.055395, acc.: 98.44%] [G loss: 4.119232]\n",
      "epoch:8 step:6296 [D loss: 0.065474, acc.: 98.44%] [G loss: 2.941953]\n",
      "epoch:8 step:6297 [D loss: 0.128949, acc.: 96.09%] [G loss: 4.585677]\n",
      "epoch:8 step:6298 [D loss: 0.286842, acc.: 85.94%] [G loss: 1.882138]\n",
      "epoch:8 step:6299 [D loss: 0.070170, acc.: 99.22%] [G loss: 2.793930]\n",
      "epoch:8 step:6300 [D loss: 0.152577, acc.: 95.31%] [G loss: 4.682435]\n",
      "epoch:8 step:6301 [D loss: 0.053199, acc.: 98.44%] [G loss: 3.608995]\n",
      "epoch:8 step:6302 [D loss: 0.308233, acc.: 85.94%] [G loss: 4.869580]\n",
      "epoch:8 step:6303 [D loss: 0.073203, acc.: 99.22%] [G loss: 3.793817]\n",
      "epoch:8 step:6304 [D loss: 0.553972, acc.: 73.44%] [G loss: 3.838966]\n",
      "epoch:8 step:6305 [D loss: 0.020791, acc.: 100.00%] [G loss: 5.002828]\n",
      "epoch:8 step:6306 [D loss: 0.027360, acc.: 100.00%] [G loss: 4.515944]\n",
      "epoch:8 step:6307 [D loss: 0.026259, acc.: 100.00%] [G loss: 4.086833]\n",
      "epoch:8 step:6308 [D loss: 0.094713, acc.: 98.44%] [G loss: 2.188438]\n",
      "epoch:8 step:6309 [D loss: 0.313116, acc.: 84.38%] [G loss: 6.889444]\n",
      "epoch:8 step:6310 [D loss: 0.439395, acc.: 77.34%] [G loss: 3.338192]\n",
      "epoch:8 step:6311 [D loss: 0.259552, acc.: 89.06%] [G loss: 5.601250]\n",
      "epoch:8 step:6312 [D loss: 0.311012, acc.: 85.94%] [G loss: 2.635992]\n",
      "epoch:8 step:6313 [D loss: 0.286848, acc.: 85.94%] [G loss: 5.125793]\n",
      "epoch:8 step:6314 [D loss: 0.027092, acc.: 99.22%] [G loss: 5.798885]\n",
      "epoch:8 step:6315 [D loss: 0.054222, acc.: 98.44%] [G loss: 5.234459]\n",
      "epoch:8 step:6316 [D loss: 0.689220, acc.: 66.41%] [G loss: 7.135850]\n",
      "epoch:8 step:6317 [D loss: 0.280138, acc.: 82.81%] [G loss: 6.154103]\n",
      "epoch:8 step:6318 [D loss: 0.056252, acc.: 99.22%] [G loss: 4.142705]\n",
      "epoch:8 step:6319 [D loss: 0.091966, acc.: 95.31%] [G loss: 4.674233]\n",
      "epoch:8 step:6320 [D loss: 0.027799, acc.: 100.00%] [G loss: 4.958470]\n",
      "epoch:8 step:6321 [D loss: 0.152412, acc.: 95.31%] [G loss: 4.271196]\n",
      "epoch:8 step:6322 [D loss: 0.075251, acc.: 97.66%] [G loss: 4.651246]\n",
      "epoch:8 step:6323 [D loss: 0.182945, acc.: 93.75%] [G loss: 2.339367]\n",
      "epoch:8 step:6324 [D loss: 0.404068, acc.: 82.03%] [G loss: 6.683644]\n",
      "epoch:8 step:6325 [D loss: 0.940893, acc.: 60.94%] [G loss: 1.283528]\n",
      "epoch:8 step:6326 [D loss: 0.486449, acc.: 74.22%] [G loss: 6.882139]\n",
      "epoch:8 step:6327 [D loss: 0.535730, acc.: 73.44%] [G loss: 3.034157]\n",
      "epoch:8 step:6328 [D loss: 0.050428, acc.: 99.22%] [G loss: 2.818587]\n",
      "epoch:8 step:6329 [D loss: 0.090431, acc.: 97.66%] [G loss: 3.665040]\n",
      "epoch:8 step:6330 [D loss: 0.019747, acc.: 100.00%] [G loss: 2.060033]\n",
      "epoch:8 step:6331 [D loss: 0.065101, acc.: 100.00%] [G loss: 2.067453]\n",
      "epoch:8 step:6332 [D loss: 0.075519, acc.: 98.44%] [G loss: 0.436593]\n",
      "epoch:8 step:6333 [D loss: 0.260316, acc.: 90.62%] [G loss: 3.306363]\n",
      "epoch:8 step:6334 [D loss: 0.187794, acc.: 92.19%] [G loss: 3.275620]\n",
      "epoch:8 step:6335 [D loss: 0.016839, acc.: 100.00%] [G loss: 2.287676]\n",
      "epoch:8 step:6336 [D loss: 0.139722, acc.: 95.31%] [G loss: 3.389050]\n",
      "epoch:8 step:6337 [D loss: 0.070689, acc.: 99.22%] [G loss: 4.093951]\n",
      "epoch:8 step:6338 [D loss: 0.551015, acc.: 72.66%] [G loss: 5.036097]\n",
      "epoch:8 step:6339 [D loss: 0.313988, acc.: 82.81%] [G loss: 2.859781]\n",
      "epoch:8 step:6340 [D loss: 0.208310, acc.: 89.84%] [G loss: 6.377161]\n",
      "epoch:8 step:6341 [D loss: 0.049756, acc.: 99.22%] [G loss: 6.200425]\n",
      "epoch:8 step:6342 [D loss: 0.301354, acc.: 85.94%] [G loss: 4.523828]\n",
      "epoch:8 step:6343 [D loss: 0.033092, acc.: 100.00%] [G loss: 3.566169]\n",
      "epoch:8 step:6344 [D loss: 0.069456, acc.: 98.44%] [G loss: 4.816483]\n",
      "epoch:8 step:6345 [D loss: 0.014187, acc.: 100.00%] [G loss: 3.728869]\n",
      "epoch:8 step:6346 [D loss: 0.093494, acc.: 96.09%] [G loss: 3.150376]\n",
      "epoch:8 step:6347 [D loss: 0.022438, acc.: 100.00%] [G loss: 1.783688]\n",
      "epoch:8 step:6348 [D loss: 0.084440, acc.: 98.44%] [G loss: 1.124626]\n",
      "epoch:8 step:6349 [D loss: 0.010229, acc.: 100.00%] [G loss: 0.804859]\n",
      "epoch:8 step:6350 [D loss: 0.047091, acc.: 100.00%] [G loss: 1.143816]\n",
      "epoch:8 step:6351 [D loss: 0.062694, acc.: 98.44%] [G loss: 0.317936]\n",
      "epoch:8 step:6352 [D loss: 0.087206, acc.: 97.66%] [G loss: 0.791850]\n",
      "epoch:8 step:6353 [D loss: 0.038896, acc.: 99.22%] [G loss: 1.910863]\n",
      "epoch:8 step:6354 [D loss: 0.025639, acc.: 99.22%] [G loss: 2.092599]\n",
      "epoch:8 step:6355 [D loss: 0.161374, acc.: 97.66%] [G loss: 1.678966]\n",
      "epoch:8 step:6356 [D loss: 0.146903, acc.: 96.88%] [G loss: 2.183050]\n",
      "epoch:8 step:6357 [D loss: 0.009140, acc.: 100.00%] [G loss: 1.858883]\n",
      "epoch:8 step:6358 [D loss: 0.052794, acc.: 100.00%] [G loss: 1.864416]\n",
      "epoch:8 step:6359 [D loss: 0.523213, acc.: 74.22%] [G loss: 5.860197]\n",
      "epoch:8 step:6360 [D loss: 0.327393, acc.: 82.81%] [G loss: 4.147803]\n",
      "epoch:8 step:6361 [D loss: 0.008451, acc.: 100.00%] [G loss: 3.228079]\n",
      "epoch:8 step:6362 [D loss: 0.108222, acc.: 96.09%] [G loss: 3.525509]\n",
      "epoch:8 step:6363 [D loss: 0.182323, acc.: 93.75%] [G loss: 5.445844]\n",
      "epoch:8 step:6364 [D loss: 0.443069, acc.: 79.69%] [G loss: 5.305502]\n",
      "epoch:8 step:6365 [D loss: 0.079265, acc.: 97.66%] [G loss: 4.930439]\n",
      "epoch:8 step:6366 [D loss: 0.003615, acc.: 100.00%] [G loss: 2.957833]\n",
      "epoch:8 step:6367 [D loss: 0.012424, acc.: 100.00%] [G loss: 3.828870]\n",
      "epoch:8 step:6368 [D loss: 0.041213, acc.: 99.22%] [G loss: 3.089279]\n",
      "epoch:8 step:6369 [D loss: 0.130983, acc.: 94.53%] [G loss: 4.738803]\n",
      "epoch:8 step:6370 [D loss: 0.161886, acc.: 93.75%] [G loss: 3.708380]\n",
      "epoch:8 step:6371 [D loss: 0.245147, acc.: 89.84%] [G loss: 6.693428]\n",
      "epoch:8 step:6372 [D loss: 0.163622, acc.: 90.62%] [G loss: 5.081516]\n",
      "epoch:8 step:6373 [D loss: 0.133873, acc.: 94.53%] [G loss: 4.264801]\n",
      "epoch:8 step:6374 [D loss: 0.267069, acc.: 89.84%] [G loss: 2.672145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6375 [D loss: 0.087700, acc.: 96.88%] [G loss: 4.500075]\n",
      "epoch:8 step:6376 [D loss: 0.030463, acc.: 99.22%] [G loss: 4.234978]\n",
      "epoch:8 step:6377 [D loss: 0.034706, acc.: 99.22%] [G loss: 3.604638]\n",
      "epoch:8 step:6378 [D loss: 0.733103, acc.: 63.28%] [G loss: 8.664369]\n",
      "epoch:8 step:6379 [D loss: 0.377561, acc.: 81.25%] [G loss: 4.642677]\n",
      "epoch:8 step:6380 [D loss: 0.057106, acc.: 100.00%] [G loss: 3.757617]\n",
      "epoch:8 step:6381 [D loss: 0.103831, acc.: 96.88%] [G loss: 4.759992]\n",
      "epoch:8 step:6382 [D loss: 0.032718, acc.: 100.00%] [G loss: 3.743980]\n",
      "epoch:8 step:6383 [D loss: 0.044310, acc.: 100.00%] [G loss: 3.560127]\n",
      "epoch:8 step:6384 [D loss: 0.696630, acc.: 61.72%] [G loss: 9.211802]\n",
      "epoch:8 step:6385 [D loss: 1.297334, acc.: 52.34%] [G loss: 3.565121]\n",
      "epoch:8 step:6386 [D loss: 0.194891, acc.: 90.62%] [G loss: 5.280046]\n",
      "epoch:8 step:6387 [D loss: 0.154236, acc.: 93.75%] [G loss: 5.241662]\n",
      "epoch:8 step:6388 [D loss: 0.050169, acc.: 98.44%] [G loss: 2.744977]\n",
      "epoch:8 step:6389 [D loss: 0.025364, acc.: 100.00%] [G loss: 3.136624]\n",
      "epoch:8 step:6390 [D loss: 0.199793, acc.: 89.84%] [G loss: 4.759768]\n",
      "epoch:8 step:6391 [D loss: 0.067855, acc.: 98.44%] [G loss: 3.631999]\n",
      "epoch:8 step:6392 [D loss: 0.780106, acc.: 59.38%] [G loss: 5.818092]\n",
      "epoch:8 step:6393 [D loss: 0.101877, acc.: 97.66%] [G loss: 6.517232]\n",
      "epoch:8 step:6394 [D loss: 0.457941, acc.: 77.34%] [G loss: 0.393486]\n",
      "epoch:8 step:6395 [D loss: 0.432605, acc.: 78.12%] [G loss: 2.753447]\n",
      "epoch:8 step:6396 [D loss: 0.053656, acc.: 98.44%] [G loss: 4.225317]\n",
      "epoch:8 step:6397 [D loss: 0.136016, acc.: 94.53%] [G loss: 1.763438]\n",
      "epoch:8 step:6398 [D loss: 0.055561, acc.: 99.22%] [G loss: 1.508739]\n",
      "epoch:8 step:6399 [D loss: 0.105831, acc.: 96.88%] [G loss: 4.074887]\n",
      "epoch:8 step:6400 [D loss: 0.073156, acc.: 100.00%] [G loss: 3.359982]\n",
      "epoch:8 step:6401 [D loss: 0.391611, acc.: 79.69%] [G loss: 0.461648]\n",
      "epoch:8 step:6402 [D loss: 1.584336, acc.: 53.91%] [G loss: 7.874815]\n",
      "epoch:8 step:6403 [D loss: 0.930654, acc.: 60.16%] [G loss: 5.345704]\n",
      "epoch:8 step:6404 [D loss: 0.044284, acc.: 100.00%] [G loss: 4.108125]\n",
      "epoch:8 step:6405 [D loss: 0.144407, acc.: 95.31%] [G loss: 3.675008]\n",
      "epoch:8 step:6406 [D loss: 0.358440, acc.: 85.16%] [G loss: 3.115731]\n",
      "epoch:8 step:6407 [D loss: 0.125936, acc.: 96.09%] [G loss: 4.017459]\n",
      "epoch:8 step:6408 [D loss: 0.069517, acc.: 99.22%] [G loss: 4.829599]\n",
      "epoch:8 step:6409 [D loss: 0.255643, acc.: 91.41%] [G loss: 3.471398]\n",
      "epoch:8 step:6410 [D loss: 0.069582, acc.: 99.22%] [G loss: 2.831326]\n",
      "epoch:8 step:6411 [D loss: 0.547417, acc.: 71.09%] [G loss: 6.271440]\n",
      "epoch:8 step:6412 [D loss: 0.644461, acc.: 70.31%] [G loss: 3.912218]\n",
      "epoch:8 step:6413 [D loss: 0.097961, acc.: 100.00%] [G loss: 3.250558]\n",
      "epoch:8 step:6414 [D loss: 0.091216, acc.: 97.66%] [G loss: 3.209764]\n",
      "epoch:8 step:6415 [D loss: 0.167492, acc.: 91.41%] [G loss: 3.955084]\n",
      "epoch:8 step:6416 [D loss: 0.091433, acc.: 97.66%] [G loss: 5.023193]\n",
      "epoch:8 step:6417 [D loss: 0.099926, acc.: 96.88%] [G loss: 4.906466]\n",
      "epoch:8 step:6418 [D loss: 0.117194, acc.: 96.09%] [G loss: 3.225830]\n",
      "epoch:8 step:6419 [D loss: 0.173190, acc.: 92.19%] [G loss: 4.197410]\n",
      "epoch:8 step:6420 [D loss: 0.074511, acc.: 97.66%] [G loss: 4.764973]\n",
      "epoch:8 step:6421 [D loss: 0.059888, acc.: 98.44%] [G loss: 2.925392]\n",
      "epoch:8 step:6422 [D loss: 0.179677, acc.: 94.53%] [G loss: 4.558218]\n",
      "epoch:8 step:6423 [D loss: 0.053086, acc.: 99.22%] [G loss: 4.348094]\n",
      "epoch:8 step:6424 [D loss: 0.842036, acc.: 59.38%] [G loss: 0.991501]\n",
      "epoch:8 step:6425 [D loss: 0.412390, acc.: 78.12%] [G loss: 5.798925]\n",
      "epoch:8 step:6426 [D loss: 0.200494, acc.: 90.62%] [G loss: 4.430892]\n",
      "epoch:8 step:6427 [D loss: 0.169623, acc.: 92.97%] [G loss: 1.600037]\n",
      "epoch:8 step:6428 [D loss: 0.009760, acc.: 100.00%] [G loss: 0.625986]\n",
      "epoch:8 step:6429 [D loss: 0.042506, acc.: 100.00%] [G loss: 0.475131]\n",
      "epoch:8 step:6430 [D loss: 0.059807, acc.: 99.22%] [G loss: 1.111776]\n",
      "epoch:8 step:6431 [D loss: 0.026112, acc.: 100.00%] [G loss: 1.631602]\n",
      "epoch:8 step:6432 [D loss: 0.009001, acc.: 100.00%] [G loss: 0.321666]\n",
      "epoch:8 step:6433 [D loss: 0.662623, acc.: 67.19%] [G loss: 6.317523]\n",
      "epoch:8 step:6434 [D loss: 0.729044, acc.: 64.84%] [G loss: 4.232059]\n",
      "epoch:8 step:6435 [D loss: 0.057542, acc.: 100.00%] [G loss: 2.494439]\n",
      "epoch:8 step:6436 [D loss: 0.147372, acc.: 95.31%] [G loss: 4.220853]\n",
      "epoch:8 step:6437 [D loss: 0.055509, acc.: 98.44%] [G loss: 4.248480]\n",
      "epoch:8 step:6438 [D loss: 0.131222, acc.: 97.66%] [G loss: 3.417162]\n",
      "epoch:8 step:6439 [D loss: 0.130363, acc.: 96.88%] [G loss: 2.358427]\n",
      "epoch:8 step:6440 [D loss: 0.107832, acc.: 96.09%] [G loss: 3.455017]\n",
      "epoch:8 step:6441 [D loss: 0.137130, acc.: 96.88%] [G loss: 4.308316]\n",
      "epoch:8 step:6442 [D loss: 0.123181, acc.: 96.88%] [G loss: 3.779170]\n",
      "epoch:8 step:6443 [D loss: 0.214518, acc.: 92.19%] [G loss: 2.454843]\n",
      "epoch:8 step:6444 [D loss: 0.104791, acc.: 99.22%] [G loss: 3.011797]\n",
      "epoch:8 step:6445 [D loss: 0.153968, acc.: 94.53%] [G loss: 5.077616]\n",
      "epoch:8 step:6446 [D loss: 0.086077, acc.: 96.88%] [G loss: 4.784966]\n",
      "epoch:8 step:6447 [D loss: 0.121533, acc.: 94.53%] [G loss: 3.291008]\n",
      "epoch:8 step:6448 [D loss: 0.157769, acc.: 95.31%] [G loss: 4.037093]\n",
      "epoch:8 step:6449 [D loss: 0.066351, acc.: 99.22%] [G loss: 4.413668]\n",
      "epoch:8 step:6450 [D loss: 0.430975, acc.: 75.78%] [G loss: 1.388161]\n",
      "epoch:8 step:6451 [D loss: 0.046093, acc.: 98.44%] [G loss: 2.876467]\n",
      "epoch:8 step:6452 [D loss: 0.045618, acc.: 99.22%] [G loss: 3.228544]\n",
      "epoch:8 step:6453 [D loss: 0.039124, acc.: 100.00%] [G loss: 2.208576]\n",
      "epoch:8 step:6454 [D loss: 0.120845, acc.: 96.09%] [G loss: 1.671164]\n",
      "epoch:8 step:6455 [D loss: 0.206338, acc.: 91.41%] [G loss: 3.045275]\n",
      "epoch:8 step:6456 [D loss: 0.238770, acc.: 87.50%] [G loss: 2.053264]\n",
      "epoch:8 step:6457 [D loss: 0.748450, acc.: 68.75%] [G loss: 6.059323]\n",
      "epoch:8 step:6458 [D loss: 0.227512, acc.: 89.06%] [G loss: 7.649573]\n",
      "epoch:8 step:6459 [D loss: 0.142847, acc.: 96.09%] [G loss: 4.580617]\n",
      "epoch:8 step:6460 [D loss: 0.062006, acc.: 99.22%] [G loss: 3.709762]\n",
      "epoch:8 step:6461 [D loss: 0.071659, acc.: 100.00%] [G loss: 3.116293]\n",
      "epoch:8 step:6462 [D loss: 0.323609, acc.: 87.50%] [G loss: 6.086231]\n",
      "epoch:8 step:6463 [D loss: 0.073670, acc.: 97.66%] [G loss: 5.789511]\n",
      "epoch:8 step:6464 [D loss: 0.119004, acc.: 96.09%] [G loss: 3.543801]\n",
      "epoch:8 step:6465 [D loss: 0.055377, acc.: 99.22%] [G loss: 3.079858]\n",
      "epoch:8 step:6466 [D loss: 0.163498, acc.: 94.53%] [G loss: 2.357040]\n",
      "epoch:8 step:6467 [D loss: 0.074478, acc.: 99.22%] [G loss: 2.456093]\n",
      "epoch:8 step:6468 [D loss: 0.253897, acc.: 89.06%] [G loss: 4.604715]\n",
      "epoch:8 step:6469 [D loss: 0.046611, acc.: 99.22%] [G loss: 4.515688]\n",
      "epoch:8 step:6470 [D loss: 1.340552, acc.: 35.16%] [G loss: 4.696797]\n",
      "epoch:8 step:6471 [D loss: 0.023413, acc.: 100.00%] [G loss: 5.900113]\n",
      "epoch:8 step:6472 [D loss: 0.073481, acc.: 99.22%] [G loss: 3.970320]\n",
      "epoch:8 step:6473 [D loss: 0.061346, acc.: 99.22%] [G loss: 4.084913]\n",
      "epoch:8 step:6474 [D loss: 0.110978, acc.: 95.31%] [G loss: 4.986544]\n",
      "epoch:8 step:6475 [D loss: 0.062165, acc.: 99.22%] [G loss: 4.945403]\n",
      "epoch:8 step:6476 [D loss: 0.089299, acc.: 97.66%] [G loss: 3.806485]\n",
      "epoch:8 step:6477 [D loss: 0.261115, acc.: 90.62%] [G loss: 5.173253]\n",
      "epoch:8 step:6478 [D loss: 0.236439, acc.: 91.41%] [G loss: 4.119959]\n",
      "epoch:8 step:6479 [D loss: 0.048819, acc.: 98.44%] [G loss: 1.752388]\n",
      "epoch:8 step:6480 [D loss: 0.536390, acc.: 73.44%] [G loss: 7.788120]\n",
      "epoch:8 step:6481 [D loss: 0.615867, acc.: 66.41%] [G loss: 3.232702]\n",
      "epoch:8 step:6482 [D loss: 0.153368, acc.: 96.09%] [G loss: 2.809180]\n",
      "epoch:8 step:6483 [D loss: 0.081467, acc.: 97.66%] [G loss: 4.098135]\n",
      "epoch:8 step:6484 [D loss: 0.074137, acc.: 97.66%] [G loss: 3.412115]\n",
      "epoch:8 step:6485 [D loss: 0.018151, acc.: 100.00%] [G loss: 2.272867]\n",
      "epoch:8 step:6486 [D loss: 0.298825, acc.: 83.59%] [G loss: 5.352290]\n",
      "epoch:8 step:6487 [D loss: 0.280364, acc.: 86.72%] [G loss: 1.497147]\n",
      "epoch:8 step:6488 [D loss: 0.352680, acc.: 81.25%] [G loss: 5.641895]\n",
      "epoch:8 step:6489 [D loss: 0.120352, acc.: 96.09%] [G loss: 4.814949]\n",
      "epoch:8 step:6490 [D loss: 0.329899, acc.: 84.38%] [G loss: 0.790220]\n",
      "epoch:8 step:6491 [D loss: 1.023983, acc.: 57.03%] [G loss: 8.085888]\n",
      "epoch:8 step:6492 [D loss: 1.033625, acc.: 57.03%] [G loss: 4.292732]\n",
      "epoch:8 step:6493 [D loss: 0.181123, acc.: 93.75%] [G loss: 2.712702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6494 [D loss: 0.046539, acc.: 100.00%] [G loss: 4.177326]\n",
      "epoch:8 step:6495 [D loss: 0.368007, acc.: 83.59%] [G loss: 6.334635]\n",
      "epoch:8 step:6496 [D loss: 0.203576, acc.: 90.62%] [G loss: 4.998358]\n",
      "epoch:8 step:6497 [D loss: 0.116932, acc.: 97.66%] [G loss: 3.091897]\n",
      "epoch:8 step:6498 [D loss: 0.206944, acc.: 92.19%] [G loss: 4.518404]\n",
      "epoch:8 step:6499 [D loss: 0.056585, acc.: 99.22%] [G loss: 4.049282]\n",
      "epoch:8 step:6500 [D loss: 0.160958, acc.: 96.88%] [G loss: 4.493197]\n",
      "epoch:8 step:6501 [D loss: 0.200026, acc.: 92.97%] [G loss: 2.870436]\n",
      "epoch:8 step:6502 [D loss: 0.072394, acc.: 99.22%] [G loss: 2.875660]\n",
      "epoch:8 step:6503 [D loss: 0.045970, acc.: 100.00%] [G loss: 3.948797]\n",
      "epoch:8 step:6504 [D loss: 0.147270, acc.: 96.09%] [G loss: 3.219616]\n",
      "epoch:8 step:6505 [D loss: 0.380627, acc.: 82.81%] [G loss: 6.415634]\n",
      "epoch:8 step:6506 [D loss: 0.283003, acc.: 82.03%] [G loss: 5.473672]\n",
      "epoch:8 step:6507 [D loss: 0.033852, acc.: 100.00%] [G loss: 4.387844]\n",
      "epoch:8 step:6508 [D loss: 0.062350, acc.: 98.44%] [G loss: 4.344956]\n",
      "epoch:8 step:6509 [D loss: 0.094545, acc.: 95.31%] [G loss: 2.439850]\n",
      "epoch:8 step:6510 [D loss: 0.370645, acc.: 79.69%] [G loss: 6.883294]\n",
      "epoch:8 step:6511 [D loss: 0.055822, acc.: 98.44%] [G loss: 7.799296]\n",
      "epoch:8 step:6512 [D loss: 1.194806, acc.: 53.91%] [G loss: 1.033447]\n",
      "epoch:8 step:6513 [D loss: 1.292236, acc.: 54.69%] [G loss: 5.826324]\n",
      "epoch:8 step:6514 [D loss: 0.468856, acc.: 75.78%] [G loss: 4.632818]\n",
      "epoch:8 step:6515 [D loss: 0.305070, acc.: 86.72%] [G loss: 2.952258]\n",
      "epoch:8 step:6516 [D loss: 0.563264, acc.: 75.00%] [G loss: 5.250190]\n",
      "epoch:8 step:6517 [D loss: 0.193762, acc.: 89.84%] [G loss: 5.857665]\n",
      "epoch:8 step:6518 [D loss: 0.368333, acc.: 85.16%] [G loss: 0.742962]\n",
      "epoch:8 step:6519 [D loss: 0.296646, acc.: 86.72%] [G loss: 3.735502]\n",
      "epoch:8 step:6520 [D loss: 0.051070, acc.: 97.66%] [G loss: 3.161237]\n",
      "epoch:8 step:6521 [D loss: 0.317480, acc.: 84.38%] [G loss: 2.022534]\n",
      "epoch:8 step:6522 [D loss: 0.151439, acc.: 94.53%] [G loss: 2.027461]\n",
      "epoch:8 step:6523 [D loss: 0.150660, acc.: 93.75%] [G loss: 3.106222]\n",
      "epoch:8 step:6524 [D loss: 0.079208, acc.: 99.22%] [G loss: 2.605273]\n",
      "epoch:8 step:6525 [D loss: 0.147324, acc.: 96.88%] [G loss: 3.896984]\n",
      "epoch:8 step:6526 [D loss: 0.106899, acc.: 96.88%] [G loss: 2.740497]\n",
      "epoch:8 step:6527 [D loss: 0.130715, acc.: 95.31%] [G loss: 2.627578]\n",
      "epoch:8 step:6528 [D loss: 0.167743, acc.: 95.31%] [G loss: 1.286759]\n",
      "epoch:8 step:6529 [D loss: 0.430959, acc.: 76.56%] [G loss: 5.847972]\n",
      "epoch:8 step:6530 [D loss: 0.334315, acc.: 79.69%] [G loss: 4.977643]\n",
      "epoch:8 step:6531 [D loss: 0.083961, acc.: 98.44%] [G loss: 3.467809]\n",
      "epoch:8 step:6532 [D loss: 0.115337, acc.: 96.09%] [G loss: 3.590044]\n",
      "epoch:8 step:6533 [D loss: 0.034516, acc.: 100.00%] [G loss: 4.715193]\n",
      "epoch:8 step:6534 [D loss: 0.392449, acc.: 81.25%] [G loss: 1.485506]\n",
      "epoch:8 step:6535 [D loss: 0.174897, acc.: 93.75%] [G loss: 3.378943]\n",
      "epoch:8 step:6536 [D loss: 0.028065, acc.: 100.00%] [G loss: 4.307360]\n",
      "epoch:8 step:6537 [D loss: 0.056361, acc.: 99.22%] [G loss: 2.329765]\n",
      "epoch:8 step:6538 [D loss: 0.084869, acc.: 100.00%] [G loss: 3.745264]\n",
      "epoch:8 step:6539 [D loss: 0.119083, acc.: 99.22%] [G loss: 3.015426]\n",
      "epoch:8 step:6540 [D loss: 0.206057, acc.: 93.75%] [G loss: 1.987914]\n",
      "epoch:8 step:6541 [D loss: 0.136755, acc.: 94.53%] [G loss: 3.820735]\n",
      "epoch:8 step:6542 [D loss: 0.033169, acc.: 100.00%] [G loss: 3.892606]\n",
      "epoch:8 step:6543 [D loss: 0.948777, acc.: 57.81%] [G loss: 5.422389]\n",
      "epoch:8 step:6544 [D loss: 0.503372, acc.: 74.22%] [G loss: 3.499815]\n",
      "epoch:8 step:6545 [D loss: 0.155310, acc.: 92.97%] [G loss: 3.777000]\n",
      "epoch:8 step:6546 [D loss: 0.025325, acc.: 100.00%] [G loss: 4.143964]\n",
      "epoch:8 step:6547 [D loss: 0.154557, acc.: 96.09%] [G loss: 4.892177]\n",
      "epoch:8 step:6548 [D loss: 0.141399, acc.: 95.31%] [G loss: 4.238672]\n",
      "epoch:8 step:6549 [D loss: 0.087363, acc.: 97.66%] [G loss: 4.852261]\n",
      "epoch:8 step:6550 [D loss: 0.047406, acc.: 99.22%] [G loss: 4.206345]\n",
      "epoch:8 step:6551 [D loss: 0.420271, acc.: 79.69%] [G loss: 4.989615]\n",
      "epoch:8 step:6552 [D loss: 0.035993, acc.: 99.22%] [G loss: 6.199615]\n",
      "epoch:8 step:6553 [D loss: 0.083718, acc.: 98.44%] [G loss: 3.844650]\n",
      "epoch:8 step:6554 [D loss: 0.110359, acc.: 96.09%] [G loss: 3.820017]\n",
      "epoch:8 step:6555 [D loss: 0.051333, acc.: 99.22%] [G loss: 3.238357]\n",
      "epoch:8 step:6556 [D loss: 0.091893, acc.: 98.44%] [G loss: 4.845049]\n",
      "epoch:8 step:6557 [D loss: 0.772384, acc.: 54.69%] [G loss: 5.806629]\n",
      "epoch:8 step:6558 [D loss: 0.429659, acc.: 75.78%] [G loss: 2.908195]\n",
      "epoch:8 step:6559 [D loss: 0.192858, acc.: 92.19%] [G loss: 5.340698]\n",
      "epoch:8 step:6560 [D loss: 0.193100, acc.: 92.19%] [G loss: 3.818195]\n",
      "epoch:8 step:6561 [D loss: 0.215587, acc.: 89.84%] [G loss: 4.193940]\n",
      "epoch:8 step:6562 [D loss: 0.062652, acc.: 99.22%] [G loss: 4.696442]\n",
      "epoch:8 step:6563 [D loss: 0.786274, acc.: 60.94%] [G loss: 4.476083]\n",
      "epoch:8 step:6564 [D loss: 0.050620, acc.: 98.44%] [G loss: 5.235622]\n",
      "epoch:8 step:6565 [D loss: 0.802654, acc.: 61.72%] [G loss: 5.995914]\n",
      "epoch:8 step:6566 [D loss: 0.101168, acc.: 97.66%] [G loss: 5.898973]\n",
      "epoch:8 step:6567 [D loss: 0.262450, acc.: 88.28%] [G loss: 2.589042]\n",
      "epoch:8 step:6568 [D loss: 0.255485, acc.: 89.84%] [G loss: 4.224586]\n",
      "epoch:8 step:6569 [D loss: 0.046038, acc.: 100.00%] [G loss: 5.028214]\n",
      "epoch:8 step:6570 [D loss: 0.031890, acc.: 99.22%] [G loss: 4.962269]\n",
      "epoch:8 step:6571 [D loss: 0.046548, acc.: 100.00%] [G loss: 2.670482]\n",
      "epoch:8 step:6572 [D loss: 0.089932, acc.: 98.44%] [G loss: 2.665901]\n",
      "epoch:8 step:6573 [D loss: 0.086716, acc.: 98.44%] [G loss: 3.029645]\n",
      "epoch:8 step:6574 [D loss: 0.037497, acc.: 100.00%] [G loss: 4.445086]\n",
      "epoch:8 step:6575 [D loss: 0.045235, acc.: 99.22%] [G loss: 3.092883]\n",
      "epoch:8 step:6576 [D loss: 0.158042, acc.: 96.09%] [G loss: 3.523449]\n",
      "epoch:8 step:6577 [D loss: 0.237724, acc.: 91.41%] [G loss: 0.602037]\n",
      "epoch:8 step:6578 [D loss: 0.447231, acc.: 77.34%] [G loss: 3.297423]\n",
      "epoch:8 step:6579 [D loss: 1.166183, acc.: 51.56%] [G loss: 0.800905]\n",
      "epoch:8 step:6580 [D loss: 0.102416, acc.: 96.88%] [G loss: 3.914455]\n",
      "epoch:8 step:6581 [D loss: 0.044124, acc.: 100.00%] [G loss: 2.207880]\n",
      "epoch:8 step:6582 [D loss: 0.219377, acc.: 92.19%] [G loss: 1.370660]\n",
      "epoch:8 step:6583 [D loss: 0.051049, acc.: 100.00%] [G loss: 1.242165]\n",
      "epoch:8 step:6584 [D loss: 0.157062, acc.: 95.31%] [G loss: 2.782763]\n",
      "epoch:8 step:6585 [D loss: 0.108960, acc.: 98.44%] [G loss: 1.550981]\n",
      "epoch:8 step:6586 [D loss: 0.263254, acc.: 89.06%] [G loss: 3.272046]\n",
      "epoch:8 step:6587 [D loss: 0.048203, acc.: 98.44%] [G loss: 4.298865]\n",
      "epoch:8 step:6588 [D loss: 1.514119, acc.: 33.59%] [G loss: 4.168579]\n",
      "epoch:8 step:6589 [D loss: 0.058715, acc.: 97.66%] [G loss: 4.490382]\n",
      "epoch:8 step:6590 [D loss: 0.619287, acc.: 73.44%] [G loss: 1.890882]\n",
      "epoch:8 step:6591 [D loss: 0.261528, acc.: 88.28%] [G loss: 3.536444]\n",
      "epoch:8 step:6592 [D loss: 0.024927, acc.: 100.00%] [G loss: 4.447094]\n",
      "epoch:8 step:6593 [D loss: 0.103159, acc.: 99.22%] [G loss: 3.821339]\n",
      "epoch:8 step:6594 [D loss: 0.069991, acc.: 100.00%] [G loss: 3.079091]\n",
      "epoch:8 step:6595 [D loss: 0.236202, acc.: 91.41%] [G loss: 4.615559]\n",
      "epoch:8 step:6596 [D loss: 0.104995, acc.: 97.66%] [G loss: 4.294572]\n",
      "epoch:8 step:6597 [D loss: 0.069061, acc.: 99.22%] [G loss: 2.927171]\n",
      "epoch:8 step:6598 [D loss: 0.211579, acc.: 91.41%] [G loss: 2.592371]\n",
      "epoch:8 step:6599 [D loss: 0.057320, acc.: 99.22%] [G loss: 3.078254]\n",
      "epoch:8 step:6600 [D loss: 0.163540, acc.: 95.31%] [G loss: 3.156264]\n",
      "epoch:8 step:6601 [D loss: 0.083135, acc.: 99.22%] [G loss: 3.115931]\n",
      "epoch:8 step:6602 [D loss: 0.217929, acc.: 90.62%] [G loss: 6.261065]\n",
      "epoch:8 step:6603 [D loss: 0.596660, acc.: 74.22%] [G loss: 1.879664]\n",
      "epoch:8 step:6604 [D loss: 0.422099, acc.: 80.47%] [G loss: 5.263521]\n",
      "epoch:8 step:6605 [D loss: 0.165172, acc.: 92.19%] [G loss: 6.412710]\n",
      "epoch:8 step:6606 [D loss: 0.241741, acc.: 88.28%] [G loss: 3.433250]\n",
      "epoch:8 step:6607 [D loss: 0.093216, acc.: 97.66%] [G loss: 3.199870]\n",
      "epoch:8 step:6608 [D loss: 0.145719, acc.: 96.09%] [G loss: 3.873079]\n",
      "epoch:8 step:6609 [D loss: 0.128504, acc.: 96.88%] [G loss: 2.985009]\n",
      "epoch:8 step:6610 [D loss: 0.121315, acc.: 96.09%] [G loss: 4.235116]\n",
      "epoch:8 step:6611 [D loss: 0.028539, acc.: 100.00%] [G loss: 4.353052]\n",
      "epoch:8 step:6612 [D loss: 0.209442, acc.: 93.75%] [G loss: 3.824668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6613 [D loss: 0.090735, acc.: 98.44%] [G loss: 4.725101]\n",
      "epoch:8 step:6614 [D loss: 0.246343, acc.: 92.19%] [G loss: 4.188073]\n",
      "epoch:8 step:6615 [D loss: 0.207450, acc.: 92.19%] [G loss: 2.960789]\n",
      "epoch:8 step:6616 [D loss: 0.081894, acc.: 97.66%] [G loss: 4.053790]\n",
      "epoch:8 step:6617 [D loss: 0.065415, acc.: 99.22%] [G loss: 3.422844]\n",
      "epoch:8 step:6618 [D loss: 0.256982, acc.: 89.06%] [G loss: 4.478730]\n",
      "epoch:8 step:6619 [D loss: 0.027153, acc.: 100.00%] [G loss: 4.915534]\n",
      "epoch:8 step:6620 [D loss: 0.091570, acc.: 96.88%] [G loss: 2.729387]\n",
      "epoch:8 step:6621 [D loss: 0.323501, acc.: 86.72%] [G loss: 4.629272]\n",
      "epoch:8 step:6622 [D loss: 0.121816, acc.: 96.88%] [G loss: 4.308286]\n",
      "epoch:8 step:6623 [D loss: 0.433343, acc.: 80.47%] [G loss: 4.116631]\n",
      "epoch:8 step:6624 [D loss: 0.059194, acc.: 99.22%] [G loss: 4.688130]\n",
      "epoch:8 step:6625 [D loss: 0.057897, acc.: 98.44%] [G loss: 4.244537]\n",
      "epoch:8 step:6626 [D loss: 0.129609, acc.: 95.31%] [G loss: 4.714632]\n",
      "epoch:8 step:6627 [D loss: 0.039431, acc.: 99.22%] [G loss: 5.437240]\n",
      "epoch:8 step:6628 [D loss: 0.179954, acc.: 92.97%] [G loss: 5.302953]\n",
      "epoch:8 step:6629 [D loss: 0.077087, acc.: 98.44%] [G loss: 4.831300]\n",
      "epoch:8 step:6630 [D loss: 0.126181, acc.: 97.66%] [G loss: 1.781930]\n",
      "epoch:8 step:6631 [D loss: 0.106633, acc.: 96.88%] [G loss: 4.594662]\n",
      "epoch:8 step:6632 [D loss: 0.026549, acc.: 100.00%] [G loss: 4.301428]\n",
      "epoch:8 step:6633 [D loss: 1.103749, acc.: 45.31%] [G loss: 7.392034]\n",
      "epoch:8 step:6634 [D loss: 0.825530, acc.: 60.16%] [G loss: 2.645204]\n",
      "epoch:8 step:6635 [D loss: 0.044642, acc.: 99.22%] [G loss: 5.236331]\n",
      "epoch:8 step:6636 [D loss: 0.009010, acc.: 100.00%] [G loss: 5.354006]\n",
      "epoch:8 step:6637 [D loss: 0.168338, acc.: 94.53%] [G loss: 2.702908]\n",
      "epoch:8 step:6638 [D loss: 0.118187, acc.: 98.44%] [G loss: 5.955302]\n",
      "epoch:8 step:6639 [D loss: 0.143476, acc.: 93.75%] [G loss: 2.729815]\n",
      "epoch:8 step:6640 [D loss: 0.084155, acc.: 97.66%] [G loss: 4.431298]\n",
      "epoch:8 step:6641 [D loss: 0.098840, acc.: 97.66%] [G loss: 5.816600]\n",
      "epoch:8 step:6642 [D loss: 0.094954, acc.: 98.44%] [G loss: 5.073909]\n",
      "epoch:8 step:6643 [D loss: 0.423541, acc.: 81.25%] [G loss: 1.600903]\n",
      "epoch:8 step:6644 [D loss: 0.115754, acc.: 96.88%] [G loss: 6.700669]\n",
      "epoch:8 step:6645 [D loss: 0.043746, acc.: 98.44%] [G loss: 5.075851]\n",
      "epoch:8 step:6646 [D loss: 0.489706, acc.: 75.78%] [G loss: 4.865640]\n",
      "epoch:8 step:6647 [D loss: 0.063373, acc.: 98.44%] [G loss: 7.035213]\n",
      "epoch:8 step:6648 [D loss: 0.010023, acc.: 100.00%] [G loss: 4.493806]\n",
      "epoch:8 step:6649 [D loss: 0.040802, acc.: 100.00%] [G loss: 4.321590]\n",
      "epoch:8 step:6650 [D loss: 0.060897, acc.: 98.44%] [G loss: 2.768688]\n",
      "epoch:8 step:6651 [D loss: 0.204580, acc.: 92.97%] [G loss: 3.789723]\n",
      "epoch:8 step:6652 [D loss: 0.061957, acc.: 97.66%] [G loss: 4.549697]\n",
      "epoch:8 step:6653 [D loss: 0.095028, acc.: 98.44%] [G loss: 2.595885]\n",
      "epoch:8 step:6654 [D loss: 0.262314, acc.: 87.50%] [G loss: 5.445282]\n",
      "epoch:8 step:6655 [D loss: 0.121953, acc.: 97.66%] [G loss: 4.812686]\n",
      "epoch:8 step:6656 [D loss: 0.080355, acc.: 98.44%] [G loss: 4.058248]\n",
      "epoch:8 step:6657 [D loss: 0.059814, acc.: 100.00%] [G loss: 2.807511]\n",
      "epoch:8 step:6658 [D loss: 0.056647, acc.: 99.22%] [G loss: 5.414300]\n",
      "epoch:8 step:6659 [D loss: 1.468000, acc.: 35.94%] [G loss: 10.534048]\n",
      "epoch:8 step:6660 [D loss: 1.292636, acc.: 60.16%] [G loss: 4.580564]\n",
      "epoch:8 step:6661 [D loss: 0.051750, acc.: 98.44%] [G loss: 2.622372]\n",
      "epoch:8 step:6662 [D loss: 0.107708, acc.: 93.75%] [G loss: 3.374825]\n",
      "epoch:8 step:6663 [D loss: 0.284910, acc.: 90.62%] [G loss: 6.755478]\n",
      "epoch:8 step:6664 [D loss: 0.452930, acc.: 82.81%] [G loss: 3.169239]\n",
      "epoch:8 step:6665 [D loss: 0.114552, acc.: 95.31%] [G loss: 1.887368]\n",
      "epoch:8 step:6666 [D loss: 0.097676, acc.: 97.66%] [G loss: 2.273096]\n",
      "epoch:8 step:6667 [D loss: 0.128967, acc.: 94.53%] [G loss: 0.506161]\n",
      "epoch:8 step:6668 [D loss: 0.082391, acc.: 99.22%] [G loss: 0.852986]\n",
      "epoch:8 step:6669 [D loss: 0.127942, acc.: 96.09%] [G loss: 3.920676]\n",
      "epoch:8 step:6670 [D loss: 0.314080, acc.: 87.50%] [G loss: 2.596342]\n",
      "epoch:8 step:6671 [D loss: 0.053890, acc.: 98.44%] [G loss: 1.655061]\n",
      "epoch:8 step:6672 [D loss: 1.098773, acc.: 53.91%] [G loss: 8.033648]\n",
      "epoch:8 step:6673 [D loss: 2.566862, acc.: 50.00%] [G loss: 4.207708]\n",
      "epoch:8 step:6674 [D loss: 0.238718, acc.: 92.19%] [G loss: 2.587713]\n",
      "epoch:8 step:6675 [D loss: 0.157354, acc.: 96.09%] [G loss: 3.093487]\n",
      "epoch:8 step:6676 [D loss: 0.174958, acc.: 94.53%] [G loss: 2.742038]\n",
      "epoch:8 step:6677 [D loss: 0.135310, acc.: 96.88%] [G loss: 2.721894]\n",
      "epoch:8 step:6678 [D loss: 0.231392, acc.: 93.75%] [G loss: 2.152775]\n",
      "epoch:8 step:6679 [D loss: 0.296322, acc.: 86.72%] [G loss: 5.364580]\n",
      "epoch:8 step:6680 [D loss: 0.127387, acc.: 95.31%] [G loss: 5.437743]\n",
      "epoch:8 step:6681 [D loss: 0.080526, acc.: 98.44%] [G loss: 4.320099]\n",
      "epoch:8 step:6682 [D loss: 0.074731, acc.: 98.44%] [G loss: 2.977723]\n",
      "epoch:8 step:6683 [D loss: 0.052575, acc.: 99.22%] [G loss: 1.604202]\n",
      "epoch:8 step:6684 [D loss: 0.026109, acc.: 100.00%] [G loss: 1.397364]\n",
      "epoch:8 step:6685 [D loss: 0.190964, acc.: 92.97%] [G loss: 3.037500]\n",
      "epoch:8 step:6686 [D loss: 0.051701, acc.: 100.00%] [G loss: 3.676985]\n",
      "epoch:8 step:6687 [D loss: 0.198975, acc.: 91.41%] [G loss: 0.497272]\n",
      "epoch:8 step:6688 [D loss: 0.071004, acc.: 99.22%] [G loss: 0.457748]\n",
      "epoch:8 step:6689 [D loss: 0.058567, acc.: 99.22%] [G loss: 0.634561]\n",
      "epoch:8 step:6690 [D loss: 0.039032, acc.: 99.22%] [G loss: 1.042282]\n",
      "epoch:8 step:6691 [D loss: 0.235370, acc.: 89.06%] [G loss: 4.256267]\n",
      "epoch:8 step:6692 [D loss: 0.111422, acc.: 97.66%] [G loss: 3.582015]\n",
      "epoch:8 step:6693 [D loss: 0.099490, acc.: 97.66%] [G loss: 2.447450]\n",
      "epoch:8 step:6694 [D loss: 0.061267, acc.: 99.22%] [G loss: 0.852400]\n",
      "epoch:8 step:6695 [D loss: 0.460657, acc.: 82.03%] [G loss: 6.252157]\n",
      "epoch:8 step:6696 [D loss: 0.744498, acc.: 67.19%] [G loss: 2.374833]\n",
      "epoch:8 step:6697 [D loss: 0.247218, acc.: 89.84%] [G loss: 3.104172]\n",
      "epoch:8 step:6698 [D loss: 0.036473, acc.: 100.00%] [G loss: 3.963050]\n",
      "epoch:8 step:6699 [D loss: 0.117049, acc.: 96.09%] [G loss: 2.668892]\n",
      "epoch:8 step:6700 [D loss: 0.093610, acc.: 97.66%] [G loss: 2.917955]\n",
      "epoch:8 step:6701 [D loss: 0.220622, acc.: 93.75%] [G loss: 4.349106]\n",
      "epoch:8 step:6702 [D loss: 0.143479, acc.: 97.66%] [G loss: 3.913635]\n",
      "epoch:8 step:6703 [D loss: 0.583552, acc.: 67.97%] [G loss: 4.930318]\n",
      "epoch:8 step:6704 [D loss: 0.122083, acc.: 96.09%] [G loss: 5.639846]\n",
      "epoch:8 step:6705 [D loss: 0.073730, acc.: 97.66%] [G loss: 3.648538]\n",
      "epoch:8 step:6706 [D loss: 0.066618, acc.: 98.44%] [G loss: 2.858644]\n",
      "epoch:8 step:6707 [D loss: 0.054523, acc.: 100.00%] [G loss: 1.569199]\n",
      "epoch:8 step:6708 [D loss: 0.083056, acc.: 99.22%] [G loss: 2.301695]\n",
      "epoch:8 step:6709 [D loss: 0.074473, acc.: 99.22%] [G loss: 2.279126]\n",
      "epoch:8 step:6710 [D loss: 0.132689, acc.: 96.09%] [G loss: 4.343193]\n",
      "epoch:8 step:6711 [D loss: 0.121121, acc.: 94.53%] [G loss: 2.555643]\n",
      "epoch:8 step:6712 [D loss: 0.322104, acc.: 86.72%] [G loss: 5.402912]\n",
      "epoch:8 step:6713 [D loss: 0.202057, acc.: 91.41%] [G loss: 3.225750]\n",
      "epoch:8 step:6714 [D loss: 0.030754, acc.: 100.00%] [G loss: 2.236408]\n",
      "epoch:8 step:6715 [D loss: 0.044160, acc.: 100.00%] [G loss: 2.340471]\n",
      "epoch:8 step:6716 [D loss: 0.088403, acc.: 97.66%] [G loss: 2.311408]\n",
      "epoch:8 step:6717 [D loss: 0.068862, acc.: 98.44%] [G loss: 2.487254]\n",
      "epoch:8 step:6718 [D loss: 0.037674, acc.: 100.00%] [G loss: 3.399933]\n",
      "epoch:8 step:6719 [D loss: 0.632319, acc.: 67.97%] [G loss: 4.698315]\n",
      "epoch:8 step:6720 [D loss: 0.413013, acc.: 79.69%] [G loss: 3.307133]\n",
      "epoch:8 step:6721 [D loss: 0.020505, acc.: 100.00%] [G loss: 2.137791]\n",
      "epoch:8 step:6722 [D loss: 0.146223, acc.: 96.09%] [G loss: 3.677565]\n",
      "epoch:8 step:6723 [D loss: 0.114963, acc.: 96.88%] [G loss: 2.761098]\n",
      "epoch:8 step:6724 [D loss: 0.071873, acc.: 98.44%] [G loss: 3.557987]\n",
      "epoch:8 step:6725 [D loss: 0.019646, acc.: 100.00%] [G loss: 3.730980]\n",
      "epoch:8 step:6726 [D loss: 0.025137, acc.: 99.22%] [G loss: 2.867081]\n",
      "epoch:8 step:6727 [D loss: 0.194644, acc.: 91.41%] [G loss: 2.883824]\n",
      "epoch:8 step:6728 [D loss: 0.067230, acc.: 96.09%] [G loss: 3.916614]\n",
      "epoch:8 step:6729 [D loss: 0.142814, acc.: 94.53%] [G loss: 2.605076]\n",
      "epoch:8 step:6730 [D loss: 1.914498, acc.: 36.72%] [G loss: 7.897386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6731 [D loss: 1.582060, acc.: 52.34%] [G loss: 5.718308]\n",
      "epoch:8 step:6732 [D loss: 0.467905, acc.: 76.56%] [G loss: 2.517880]\n",
      "epoch:8 step:6733 [D loss: 0.110619, acc.: 97.66%] [G loss: 2.297484]\n",
      "epoch:8 step:6734 [D loss: 0.186136, acc.: 91.41%] [G loss: 4.077852]\n",
      "epoch:8 step:6735 [D loss: 0.077147, acc.: 99.22%] [G loss: 3.898983]\n",
      "epoch:8 step:6736 [D loss: 0.120586, acc.: 96.09%] [G loss: 3.348946]\n",
      "epoch:8 step:6737 [D loss: 0.074694, acc.: 98.44%] [G loss: 2.871091]\n",
      "epoch:8 step:6738 [D loss: 0.041171, acc.: 100.00%] [G loss: 3.199005]\n",
      "epoch:8 step:6739 [D loss: 0.105489, acc.: 97.66%] [G loss: 2.806819]\n",
      "epoch:8 step:6740 [D loss: 0.213310, acc.: 92.97%] [G loss: 4.277673]\n",
      "epoch:8 step:6741 [D loss: 0.149792, acc.: 92.97%] [G loss: 3.293042]\n",
      "epoch:8 step:6742 [D loss: 0.098225, acc.: 100.00%] [G loss: 3.353961]\n",
      "epoch:8 step:6743 [D loss: 0.082066, acc.: 99.22%] [G loss: 1.452647]\n",
      "epoch:8 step:6744 [D loss: 0.153480, acc.: 96.09%] [G loss: 4.263563]\n",
      "epoch:8 step:6745 [D loss: 0.110049, acc.: 96.88%] [G loss: 3.968230]\n",
      "epoch:8 step:6746 [D loss: 0.135286, acc.: 96.88%] [G loss: 2.327352]\n",
      "epoch:8 step:6747 [D loss: 0.052516, acc.: 100.00%] [G loss: 1.485138]\n",
      "epoch:8 step:6748 [D loss: 0.056074, acc.: 100.00%] [G loss: 3.704897]\n",
      "epoch:8 step:6749 [D loss: 0.278233, acc.: 89.84%] [G loss: 0.394806]\n",
      "epoch:8 step:6750 [D loss: 0.106944, acc.: 97.66%] [G loss: 1.665055]\n",
      "epoch:8 step:6751 [D loss: 0.017418, acc.: 100.00%] [G loss: 1.527175]\n",
      "epoch:8 step:6752 [D loss: 0.074351, acc.: 100.00%] [G loss: 2.031835]\n",
      "epoch:8 step:6753 [D loss: 0.106055, acc.: 97.66%] [G loss: 3.808367]\n",
      "epoch:8 step:6754 [D loss: 0.074349, acc.: 98.44%] [G loss: 1.053978]\n",
      "epoch:8 step:6755 [D loss: 0.169467, acc.: 94.53%] [G loss: 3.493978]\n",
      "epoch:8 step:6756 [D loss: 0.771280, acc.: 49.22%] [G loss: 3.158803]\n",
      "epoch:8 step:6757 [D loss: 0.136007, acc.: 94.53%] [G loss: 2.620843]\n",
      "epoch:8 step:6758 [D loss: 0.223766, acc.: 92.97%] [G loss: 4.823317]\n",
      "epoch:8 step:6759 [D loss: 0.146369, acc.: 94.53%] [G loss: 3.565662]\n",
      "epoch:8 step:6760 [D loss: 0.137215, acc.: 98.44%] [G loss: 3.534942]\n",
      "epoch:8 step:6761 [D loss: 0.045861, acc.: 100.00%] [G loss: 3.627105]\n",
      "epoch:8 step:6762 [D loss: 0.054652, acc.: 100.00%] [G loss: 1.697156]\n",
      "epoch:8 step:6763 [D loss: 0.460442, acc.: 75.78%] [G loss: 4.918604]\n",
      "epoch:8 step:6764 [D loss: 0.377412, acc.: 78.91%] [G loss: 3.827108]\n",
      "epoch:8 step:6765 [D loss: 0.279510, acc.: 88.28%] [G loss: 4.851849]\n",
      "epoch:8 step:6766 [D loss: 0.066543, acc.: 97.66%] [G loss: 4.338477]\n",
      "epoch:8 step:6767 [D loss: 0.739354, acc.: 59.38%] [G loss: 5.587688]\n",
      "epoch:8 step:6768 [D loss: 0.096955, acc.: 97.66%] [G loss: 5.256548]\n",
      "epoch:8 step:6769 [D loss: 0.109713, acc.: 95.31%] [G loss: 2.759016]\n",
      "epoch:8 step:6770 [D loss: 0.077828, acc.: 100.00%] [G loss: 1.857459]\n",
      "epoch:8 step:6771 [D loss: 0.263219, acc.: 87.50%] [G loss: 3.779812]\n",
      "epoch:8 step:6772 [D loss: 0.782720, acc.: 63.28%] [G loss: 3.005044]\n",
      "epoch:8 step:6773 [D loss: 0.138656, acc.: 95.31%] [G loss: 3.186160]\n",
      "epoch:8 step:6774 [D loss: 0.521642, acc.: 71.88%] [G loss: 5.174213]\n",
      "epoch:8 step:6775 [D loss: 0.072070, acc.: 97.66%] [G loss: 4.523516]\n",
      "epoch:8 step:6776 [D loss: 0.086396, acc.: 96.88%] [G loss: 3.423395]\n",
      "epoch:8 step:6777 [D loss: 0.328779, acc.: 83.59%] [G loss: 5.213249]\n",
      "epoch:8 step:6778 [D loss: 0.463786, acc.: 76.56%] [G loss: 2.433736]\n",
      "epoch:8 step:6779 [D loss: 0.146095, acc.: 94.53%] [G loss: 2.611367]\n",
      "epoch:8 step:6780 [D loss: 0.037457, acc.: 100.00%] [G loss: 1.442149]\n",
      "epoch:8 step:6781 [D loss: 0.053943, acc.: 98.44%] [G loss: 1.485049]\n",
      "epoch:8 step:6782 [D loss: 0.100172, acc.: 97.66%] [G loss: 1.326390]\n",
      "epoch:8 step:6783 [D loss: 0.164208, acc.: 92.97%] [G loss: 1.925894]\n",
      "epoch:8 step:6784 [D loss: 0.051033, acc.: 97.66%] [G loss: 3.103569]\n",
      "epoch:8 step:6785 [D loss: 0.830076, acc.: 55.47%] [G loss: 6.720597]\n",
      "epoch:8 step:6786 [D loss: 0.041024, acc.: 100.00%] [G loss: 6.000244]\n",
      "epoch:8 step:6787 [D loss: 0.402395, acc.: 83.59%] [G loss: 2.951751]\n",
      "epoch:8 step:6788 [D loss: 0.061069, acc.: 98.44%] [G loss: 2.097463]\n",
      "epoch:8 step:6789 [D loss: 0.025006, acc.: 100.00%] [G loss: 2.975184]\n",
      "epoch:8 step:6790 [D loss: 0.199681, acc.: 92.19%] [G loss: 2.305721]\n",
      "epoch:8 step:6791 [D loss: 0.065007, acc.: 97.66%] [G loss: 3.567083]\n",
      "epoch:8 step:6792 [D loss: 0.087335, acc.: 96.88%] [G loss: 4.281955]\n",
      "epoch:8 step:6793 [D loss: 0.046079, acc.: 100.00%] [G loss: 3.294438]\n",
      "epoch:8 step:6794 [D loss: 0.052967, acc.: 98.44%] [G loss: 3.967563]\n",
      "epoch:8 step:6795 [D loss: 0.417386, acc.: 79.69%] [G loss: 4.854517]\n",
      "epoch:8 step:6796 [D loss: 0.030357, acc.: 100.00%] [G loss: 6.496121]\n",
      "epoch:8 step:6797 [D loss: 0.068490, acc.: 96.09%] [G loss: 4.456025]\n",
      "epoch:8 step:6798 [D loss: 0.018048, acc.: 100.00%] [G loss: 3.944274]\n",
      "epoch:8 step:6799 [D loss: 0.018645, acc.: 100.00%] [G loss: 2.639554]\n",
      "epoch:8 step:6800 [D loss: 0.021588, acc.: 100.00%] [G loss: 1.981722]\n",
      "epoch:8 step:6801 [D loss: 0.041226, acc.: 99.22%] [G loss: 2.705185]\n",
      "epoch:8 step:6802 [D loss: 0.089988, acc.: 98.44%] [G loss: 3.122826]\n",
      "epoch:8 step:6803 [D loss: 0.030394, acc.: 100.00%] [G loss: 3.952393]\n",
      "epoch:8 step:6804 [D loss: 0.167875, acc.: 90.62%] [G loss: 5.970033]\n",
      "epoch:8 step:6805 [D loss: 0.399481, acc.: 78.12%] [G loss: 3.255185]\n",
      "epoch:8 step:6806 [D loss: 0.134954, acc.: 95.31%] [G loss: 5.267780]\n",
      "epoch:8 step:6807 [D loss: 0.025604, acc.: 100.00%] [G loss: 5.827953]\n",
      "epoch:8 step:6808 [D loss: 0.020123, acc.: 100.00%] [G loss: 3.914784]\n",
      "epoch:8 step:6809 [D loss: 0.126154, acc.: 97.66%] [G loss: 3.952279]\n",
      "epoch:8 step:6810 [D loss: 0.074350, acc.: 99.22%] [G loss: 3.936719]\n",
      "epoch:8 step:6811 [D loss: 0.099802, acc.: 99.22%] [G loss: 5.044360]\n",
      "epoch:8 step:6812 [D loss: 0.041295, acc.: 100.00%] [G loss: 5.178504]\n",
      "epoch:8 step:6813 [D loss: 1.567955, acc.: 33.59%] [G loss: 9.432938]\n",
      "epoch:8 step:6814 [D loss: 2.505839, acc.: 50.00%] [G loss: 4.027681]\n",
      "epoch:8 step:6815 [D loss: 0.183588, acc.: 94.53%] [G loss: 2.301982]\n",
      "epoch:8 step:6816 [D loss: 0.134341, acc.: 96.88%] [G loss: 3.067931]\n",
      "epoch:8 step:6817 [D loss: 0.154976, acc.: 96.09%] [G loss: 3.441052]\n",
      "epoch:8 step:6818 [D loss: 0.119265, acc.: 99.22%] [G loss: 3.448444]\n",
      "epoch:8 step:6819 [D loss: 0.098860, acc.: 97.66%] [G loss: 3.211334]\n",
      "epoch:8 step:6820 [D loss: 0.062789, acc.: 100.00%] [G loss: 3.251956]\n",
      "epoch:8 step:6821 [D loss: 0.142234, acc.: 96.88%] [G loss: 2.951317]\n",
      "epoch:8 step:6822 [D loss: 0.343361, acc.: 87.50%] [G loss: 4.331938]\n",
      "epoch:8 step:6823 [D loss: 0.134109, acc.: 96.09%] [G loss: 4.005340]\n",
      "epoch:8 step:6824 [D loss: 0.065562, acc.: 99.22%] [G loss: 3.590014]\n",
      "epoch:8 step:6825 [D loss: 0.085736, acc.: 98.44%] [G loss: 3.610082]\n",
      "epoch:8 step:6826 [D loss: 0.036825, acc.: 100.00%] [G loss: 3.323349]\n",
      "epoch:8 step:6827 [D loss: 0.128614, acc.: 96.09%] [G loss: 4.424392]\n",
      "epoch:8 step:6828 [D loss: 0.168729, acc.: 94.53%] [G loss: 4.137722]\n",
      "epoch:8 step:6829 [D loss: 0.198426, acc.: 93.75%] [G loss: 3.794734]\n",
      "epoch:8 step:6830 [D loss: 0.070956, acc.: 99.22%] [G loss: 4.240566]\n",
      "epoch:8 step:6831 [D loss: 0.105846, acc.: 96.88%] [G loss: 3.402881]\n",
      "epoch:8 step:6832 [D loss: 0.367816, acc.: 79.69%] [G loss: 6.018500]\n",
      "epoch:8 step:6833 [D loss: 0.333151, acc.: 80.47%] [G loss: 4.303417]\n",
      "epoch:8 step:6834 [D loss: 0.025984, acc.: 100.00%] [G loss: 3.130894]\n",
      "epoch:8 step:6835 [D loss: 0.065242, acc.: 99.22%] [G loss: 1.730393]\n",
      "epoch:8 step:6836 [D loss: 0.048094, acc.: 99.22%] [G loss: 1.451810]\n",
      "epoch:8 step:6837 [D loss: 0.156717, acc.: 96.09%] [G loss: 2.441660]\n",
      "epoch:8 step:6838 [D loss: 0.065302, acc.: 99.22%] [G loss: 2.320848]\n",
      "epoch:8 step:6839 [D loss: 0.385597, acc.: 80.47%] [G loss: 2.471793]\n",
      "epoch:8 step:6840 [D loss: 0.018488, acc.: 100.00%] [G loss: 4.085163]\n",
      "epoch:8 step:6841 [D loss: 0.150606, acc.: 92.97%] [G loss: 1.314137]\n",
      "epoch:8 step:6842 [D loss: 0.251502, acc.: 89.06%] [G loss: 4.670452]\n",
      "epoch:8 step:6843 [D loss: 0.043097, acc.: 100.00%] [G loss: 6.116499]\n",
      "epoch:8 step:6844 [D loss: 0.321957, acc.: 85.16%] [G loss: 0.860256]\n",
      "epoch:8 step:6845 [D loss: 0.245924, acc.: 90.62%] [G loss: 4.368349]\n",
      "epoch:8 step:6846 [D loss: 0.009183, acc.: 100.00%] [G loss: 6.424127]\n",
      "epoch:8 step:6847 [D loss: 0.358118, acc.: 84.38%] [G loss: 2.367245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6848 [D loss: 0.181485, acc.: 92.97%] [G loss: 4.406958]\n",
      "epoch:8 step:6849 [D loss: 0.039326, acc.: 99.22%] [G loss: 4.135164]\n",
      "epoch:8 step:6850 [D loss: 0.076982, acc.: 98.44%] [G loss: 2.915601]\n",
      "epoch:8 step:6851 [D loss: 0.199545, acc.: 90.62%] [G loss: 6.018828]\n",
      "epoch:8 step:6852 [D loss: 1.886573, acc.: 16.41%] [G loss: 7.442090]\n",
      "epoch:8 step:6853 [D loss: 0.064923, acc.: 98.44%] [G loss: 8.531604]\n",
      "epoch:8 step:6854 [D loss: 0.618047, acc.: 64.84%] [G loss: 4.491727]\n",
      "epoch:8 step:6855 [D loss: 0.062591, acc.: 98.44%] [G loss: 3.008616]\n",
      "epoch:8 step:6856 [D loss: 0.090452, acc.: 97.66%] [G loss: 3.865976]\n",
      "epoch:8 step:6857 [D loss: 0.021080, acc.: 100.00%] [G loss: 2.749224]\n",
      "epoch:8 step:6858 [D loss: 0.036524, acc.: 99.22%] [G loss: 2.738824]\n",
      "epoch:8 step:6859 [D loss: 0.026353, acc.: 100.00%] [G loss: 1.222461]\n",
      "epoch:8 step:6860 [D loss: 0.014136, acc.: 100.00%] [G loss: 1.314684]\n",
      "epoch:8 step:6861 [D loss: 0.187831, acc.: 92.97%] [G loss: 3.370465]\n",
      "epoch:8 step:6862 [D loss: 0.051678, acc.: 99.22%] [G loss: 2.240021]\n",
      "epoch:8 step:6863 [D loss: 0.120458, acc.: 94.53%] [G loss: 1.260131]\n",
      "epoch:8 step:6864 [D loss: 0.107004, acc.: 98.44%] [G loss: 0.924941]\n",
      "epoch:8 step:6865 [D loss: 0.043943, acc.: 100.00%] [G loss: 0.742092]\n",
      "epoch:8 step:6866 [D loss: 0.189234, acc.: 93.75%] [G loss: 2.788462]\n",
      "epoch:8 step:6867 [D loss: 1.011955, acc.: 54.69%] [G loss: 0.365451]\n",
      "epoch:8 step:6868 [D loss: 0.106733, acc.: 97.66%] [G loss: 2.958825]\n",
      "epoch:8 step:6869 [D loss: 0.008681, acc.: 100.00%] [G loss: 4.517599]\n",
      "epoch:8 step:6870 [D loss: 0.066254, acc.: 98.44%] [G loss: 2.522317]\n",
      "epoch:8 step:6871 [D loss: 0.084912, acc.: 99.22%] [G loss: 2.937993]\n",
      "epoch:8 step:6872 [D loss: 0.041188, acc.: 100.00%] [G loss: 2.144863]\n",
      "epoch:8 step:6873 [D loss: 0.120134, acc.: 96.88%] [G loss: 4.914995]\n",
      "epoch:8 step:6874 [D loss: 0.173368, acc.: 96.09%] [G loss: 3.903406]\n",
      "epoch:8 step:6875 [D loss: 0.937043, acc.: 50.00%] [G loss: 3.837430]\n",
      "epoch:8 step:6876 [D loss: 0.127654, acc.: 97.66%] [G loss: 4.801697]\n",
      "epoch:8 step:6877 [D loss: 0.172418, acc.: 94.53%] [G loss: 3.004486]\n",
      "epoch:8 step:6878 [D loss: 0.036367, acc.: 99.22%] [G loss: 2.536486]\n",
      "epoch:8 step:6879 [D loss: 0.140910, acc.: 99.22%] [G loss: 1.742766]\n",
      "epoch:8 step:6880 [D loss: 0.034696, acc.: 99.22%] [G loss: 3.175087]\n",
      "epoch:8 step:6881 [D loss: 0.127543, acc.: 96.88%] [G loss: 1.320113]\n",
      "epoch:8 step:6882 [D loss: 0.044807, acc.: 100.00%] [G loss: 1.234052]\n",
      "epoch:8 step:6883 [D loss: 0.044282, acc.: 99.22%] [G loss: 0.818014]\n",
      "epoch:8 step:6884 [D loss: 0.056731, acc.: 100.00%] [G loss: 0.681808]\n",
      "epoch:8 step:6885 [D loss: 0.139377, acc.: 96.09%] [G loss: 1.571645]\n",
      "epoch:8 step:6886 [D loss: 0.044870, acc.: 97.66%] [G loss: 0.957156]\n",
      "epoch:8 step:6887 [D loss: 0.357377, acc.: 85.16%] [G loss: 2.335392]\n",
      "epoch:8 step:6888 [D loss: 0.043686, acc.: 100.00%] [G loss: 2.596719]\n",
      "epoch:8 step:6889 [D loss: 0.048441, acc.: 100.00%] [G loss: 2.289693]\n",
      "epoch:8 step:6890 [D loss: 0.359205, acc.: 85.16%] [G loss: 5.101345]\n",
      "epoch:8 step:6891 [D loss: 0.388510, acc.: 79.69%] [G loss: 3.423969]\n",
      "epoch:8 step:6892 [D loss: 0.037661, acc.: 100.00%] [G loss: 2.422555]\n",
      "epoch:8 step:6893 [D loss: 0.015777, acc.: 100.00%] [G loss: 1.580304]\n",
      "epoch:8 step:6894 [D loss: 0.316502, acc.: 85.94%] [G loss: 4.394328]\n",
      "epoch:8 step:6895 [D loss: 0.087375, acc.: 96.09%] [G loss: 3.572160]\n",
      "epoch:8 step:6896 [D loss: 0.149982, acc.: 96.88%] [G loss: 3.869060]\n",
      "epoch:8 step:6897 [D loss: 0.413224, acc.: 78.91%] [G loss: 1.486586]\n",
      "epoch:8 step:6898 [D loss: 0.279347, acc.: 87.50%] [G loss: 6.031110]\n",
      "epoch:8 step:6899 [D loss: 0.073015, acc.: 98.44%] [G loss: 5.436509]\n",
      "epoch:8 step:6900 [D loss: 0.058542, acc.: 98.44%] [G loss: 4.655869]\n",
      "epoch:8 step:6901 [D loss: 0.184548, acc.: 96.09%] [G loss: 5.419995]\n",
      "epoch:8 step:6902 [D loss: 0.216370, acc.: 90.62%] [G loss: 3.247350]\n",
      "epoch:8 step:6903 [D loss: 0.261474, acc.: 87.50%] [G loss: 6.944587]\n",
      "epoch:8 step:6904 [D loss: 0.208292, acc.: 91.41%] [G loss: 5.516150]\n",
      "epoch:8 step:6905 [D loss: 0.027810, acc.: 99.22%] [G loss: 5.182232]\n",
      "epoch:8 step:6906 [D loss: 0.028671, acc.: 100.00%] [G loss: 4.033312]\n",
      "epoch:8 step:6907 [D loss: 0.059709, acc.: 100.00%] [G loss: 4.957870]\n",
      "epoch:8 step:6908 [D loss: 0.072770, acc.: 100.00%] [G loss: 4.169653]\n",
      "epoch:8 step:6909 [D loss: 0.182134, acc.: 94.53%] [G loss: 2.896454]\n",
      "epoch:8 step:6910 [D loss: 0.030653, acc.: 100.00%] [G loss: 3.560341]\n",
      "epoch:8 step:6911 [D loss: 0.061929, acc.: 99.22%] [G loss: 3.405712]\n",
      "epoch:8 step:6912 [D loss: 0.016331, acc.: 100.00%] [G loss: 4.208236]\n",
      "epoch:8 step:6913 [D loss: 0.198919, acc.: 91.41%] [G loss: 0.135824]\n",
      "epoch:8 step:6914 [D loss: 0.398472, acc.: 79.69%] [G loss: 6.893802]\n",
      "epoch:8 step:6915 [D loss: 1.386923, acc.: 53.91%] [G loss: 1.591421]\n",
      "epoch:8 step:6916 [D loss: 0.722151, acc.: 67.19%] [G loss: 7.211673]\n",
      "epoch:8 step:6917 [D loss: 0.773040, acc.: 63.28%] [G loss: 2.567000]\n",
      "epoch:8 step:6918 [D loss: 0.848939, acc.: 61.72%] [G loss: 6.019667]\n",
      "epoch:8 step:6919 [D loss: 0.568382, acc.: 67.97%] [G loss: 6.104711]\n",
      "epoch:8 step:6920 [D loss: 0.409965, acc.: 78.91%] [G loss: 2.438064]\n",
      "epoch:8 step:6921 [D loss: 0.214153, acc.: 89.84%] [G loss: 3.901661]\n",
      "epoch:8 step:6922 [D loss: 0.048287, acc.: 99.22%] [G loss: 4.167242]\n",
      "epoch:8 step:6923 [D loss: 0.066869, acc.: 99.22%] [G loss: 4.164109]\n",
      "epoch:8 step:6924 [D loss: 0.069001, acc.: 99.22%] [G loss: 3.823122]\n",
      "epoch:8 step:6925 [D loss: 0.109773, acc.: 98.44%] [G loss: 3.490320]\n",
      "epoch:8 step:6926 [D loss: 0.417583, acc.: 78.91%] [G loss: 4.296528]\n",
      "epoch:8 step:6927 [D loss: 0.087057, acc.: 97.66%] [G loss: 3.697015]\n",
      "epoch:8 step:6928 [D loss: 0.249595, acc.: 90.62%] [G loss: 2.480817]\n",
      "epoch:8 step:6929 [D loss: 0.184345, acc.: 94.53%] [G loss: 3.348324]\n",
      "epoch:8 step:6930 [D loss: 0.197169, acc.: 92.97%] [G loss: 3.548955]\n",
      "epoch:8 step:6931 [D loss: 0.086103, acc.: 98.44%] [G loss: 3.310220]\n",
      "epoch:8 step:6932 [D loss: 0.072072, acc.: 100.00%] [G loss: 3.684521]\n",
      "epoch:8 step:6933 [D loss: 0.361816, acc.: 83.59%] [G loss: 3.478943]\n",
      "epoch:8 step:6934 [D loss: 0.079198, acc.: 98.44%] [G loss: 3.070096]\n",
      "epoch:8 step:6935 [D loss: 0.141043, acc.: 96.88%] [G loss: 2.488652]\n",
      "epoch:8 step:6936 [D loss: 0.153043, acc.: 96.88%] [G loss: 2.485610]\n",
      "epoch:8 step:6937 [D loss: 0.062962, acc.: 100.00%] [G loss: 4.620555]\n",
      "epoch:8 step:6938 [D loss: 0.172564, acc.: 92.19%] [G loss: 1.607183]\n",
      "epoch:8 step:6939 [D loss: 0.095823, acc.: 98.44%] [G loss: 1.734786]\n",
      "epoch:8 step:6940 [D loss: 0.148522, acc.: 97.66%] [G loss: 2.981961]\n",
      "epoch:8 step:6941 [D loss: 0.164481, acc.: 93.75%] [G loss: 3.154726]\n",
      "epoch:8 step:6942 [D loss: 0.156328, acc.: 93.75%] [G loss: 4.438845]\n",
      "epoch:8 step:6943 [D loss: 0.066193, acc.: 96.88%] [G loss: 5.054658]\n",
      "epoch:8 step:6944 [D loss: 0.115937, acc.: 96.09%] [G loss: 2.273900]\n",
      "epoch:8 step:6945 [D loss: 0.063803, acc.: 99.22%] [G loss: 4.260628]\n",
      "epoch:8 step:6946 [D loss: 0.043595, acc.: 100.00%] [G loss: 3.508420]\n",
      "epoch:8 step:6947 [D loss: 0.174753, acc.: 93.75%] [G loss: 3.900572]\n",
      "epoch:8 step:6948 [D loss: 0.247797, acc.: 89.84%] [G loss: 5.069457]\n",
      "epoch:8 step:6949 [D loss: 0.317118, acc.: 88.28%] [G loss: 4.159341]\n",
      "epoch:8 step:6950 [D loss: 0.030304, acc.: 100.00%] [G loss: 5.186536]\n",
      "epoch:8 step:6951 [D loss: 0.059494, acc.: 100.00%] [G loss: 4.436189]\n",
      "epoch:8 step:6952 [D loss: 0.061582, acc.: 99.22%] [G loss: 3.120310]\n",
      "epoch:8 step:6953 [D loss: 0.072182, acc.: 100.00%] [G loss: 4.952674]\n",
      "epoch:8 step:6954 [D loss: 0.041043, acc.: 100.00%] [G loss: 4.159251]\n",
      "epoch:8 step:6955 [D loss: 0.968509, acc.: 49.22%] [G loss: 7.727571]\n",
      "epoch:8 step:6956 [D loss: 0.261952, acc.: 85.94%] [G loss: 5.809832]\n",
      "epoch:8 step:6957 [D loss: 0.255535, acc.: 89.06%] [G loss: 2.201227]\n",
      "epoch:8 step:6958 [D loss: 0.087873, acc.: 96.88%] [G loss: 3.069321]\n",
      "epoch:8 step:6959 [D loss: 0.004479, acc.: 100.00%] [G loss: 4.101108]\n",
      "epoch:8 step:6960 [D loss: 0.062142, acc.: 100.00%] [G loss: 4.598189]\n",
      "epoch:8 step:6961 [D loss: 0.853006, acc.: 57.03%] [G loss: 7.534674]\n",
      "epoch:8 step:6962 [D loss: 0.818544, acc.: 66.41%] [G loss: 2.480434]\n",
      "epoch:8 step:6963 [D loss: 0.242973, acc.: 90.62%] [G loss: 4.535119]\n",
      "epoch:8 step:6964 [D loss: 0.028581, acc.: 100.00%] [G loss: 5.790129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6965 [D loss: 0.019983, acc.: 100.00%] [G loss: 4.762514]\n",
      "epoch:8 step:6966 [D loss: 0.047872, acc.: 100.00%] [G loss: 1.710887]\n",
      "epoch:8 step:6967 [D loss: 0.148865, acc.: 94.53%] [G loss: 3.270034]\n",
      "epoch:8 step:6968 [D loss: 0.007880, acc.: 100.00%] [G loss: 3.327562]\n",
      "epoch:8 step:6969 [D loss: 0.120105, acc.: 98.44%] [G loss: 4.582617]\n",
      "epoch:8 step:6970 [D loss: 0.048916, acc.: 99.22%] [G loss: 1.885996]\n",
      "epoch:8 step:6971 [D loss: 0.145504, acc.: 95.31%] [G loss: 3.461264]\n",
      "epoch:8 step:6972 [D loss: 0.175256, acc.: 91.41%] [G loss: 1.260900]\n",
      "epoch:8 step:6973 [D loss: 1.268665, acc.: 50.78%] [G loss: 8.922548]\n",
      "epoch:8 step:6974 [D loss: 1.296906, acc.: 53.12%] [G loss: 2.657308]\n",
      "epoch:8 step:6975 [D loss: 0.098552, acc.: 97.66%] [G loss: 2.347571]\n",
      "epoch:8 step:6976 [D loss: 0.329474, acc.: 83.59%] [G loss: 5.612740]\n",
      "epoch:8 step:6977 [D loss: 0.132597, acc.: 95.31%] [G loss: 5.406884]\n",
      "epoch:8 step:6978 [D loss: 0.085904, acc.: 97.66%] [G loss: 3.976227]\n",
      "epoch:8 step:6979 [D loss: 0.080284, acc.: 98.44%] [G loss: 2.518435]\n",
      "epoch:8 step:6980 [D loss: 0.139291, acc.: 96.09%] [G loss: 4.639592]\n",
      "epoch:8 step:6981 [D loss: 0.147122, acc.: 92.19%] [G loss: 2.838709]\n",
      "epoch:8 step:6982 [D loss: 0.078068, acc.: 98.44%] [G loss: 2.371729]\n",
      "epoch:8 step:6983 [D loss: 0.241441, acc.: 91.41%] [G loss: 2.612680]\n",
      "epoch:8 step:6984 [D loss: 0.122225, acc.: 96.88%] [G loss: 4.532685]\n",
      "epoch:8 step:6985 [D loss: 0.096533, acc.: 96.88%] [G loss: 3.880308]\n",
      "epoch:8 step:6986 [D loss: 0.158920, acc.: 94.53%] [G loss: 2.991268]\n",
      "epoch:8 step:6987 [D loss: 0.311801, acc.: 90.62%] [G loss: 4.985355]\n",
      "epoch:8 step:6988 [D loss: 0.169812, acc.: 92.19%] [G loss: 3.712861]\n",
      "epoch:8 step:6989 [D loss: 0.065994, acc.: 98.44%] [G loss: 1.985677]\n",
      "epoch:8 step:6990 [D loss: 0.164323, acc.: 92.97%] [G loss: 4.544847]\n",
      "epoch:8 step:6991 [D loss: 0.119513, acc.: 96.88%] [G loss: 3.662370]\n",
      "epoch:8 step:6992 [D loss: 0.163088, acc.: 95.31%] [G loss: 3.558834]\n",
      "epoch:8 step:6993 [D loss: 0.073118, acc.: 100.00%] [G loss: 3.828458]\n",
      "epoch:8 step:6994 [D loss: 0.078884, acc.: 99.22%] [G loss: 2.972700]\n",
      "epoch:8 step:6995 [D loss: 0.596104, acc.: 70.31%] [G loss: 4.161871]\n",
      "epoch:8 step:6996 [D loss: 0.062739, acc.: 100.00%] [G loss: 4.682033]\n",
      "epoch:8 step:6997 [D loss: 0.256468, acc.: 89.06%] [G loss: 1.934994]\n",
      "epoch:8 step:6998 [D loss: 0.063185, acc.: 98.44%] [G loss: 2.424364]\n",
      "epoch:8 step:6999 [D loss: 0.125092, acc.: 96.09%] [G loss: 3.941510]\n",
      "epoch:8 step:7000 [D loss: 0.021040, acc.: 100.00%] [G loss: 4.329233]\n",
      "epoch:8 step:7001 [D loss: 0.163116, acc.: 95.31%] [G loss: 2.344512]\n",
      "epoch:8 step:7002 [D loss: 0.364568, acc.: 82.81%] [G loss: 5.288104]\n",
      "epoch:8 step:7003 [D loss: 0.110723, acc.: 94.53%] [G loss: 5.372638]\n",
      "epoch:8 step:7004 [D loss: 0.262927, acc.: 89.84%] [G loss: 2.647666]\n",
      "epoch:8 step:7005 [D loss: 0.103803, acc.: 96.88%] [G loss: 2.871211]\n",
      "epoch:8 step:7006 [D loss: 0.028953, acc.: 100.00%] [G loss: 3.192879]\n",
      "epoch:8 step:7007 [D loss: 0.129373, acc.: 96.88%] [G loss: 4.475482]\n",
      "epoch:8 step:7008 [D loss: 0.096880, acc.: 98.44%] [G loss: 3.483493]\n",
      "epoch:8 step:7009 [D loss: 0.068480, acc.: 98.44%] [G loss: 1.893235]\n",
      "epoch:8 step:7010 [D loss: 0.135967, acc.: 96.09%] [G loss: 0.256168]\n",
      "epoch:8 step:7011 [D loss: 0.229223, acc.: 88.28%] [G loss: 3.912533]\n",
      "epoch:8 step:7012 [D loss: 0.088286, acc.: 97.66%] [G loss: 5.229532]\n",
      "epoch:8 step:7013 [D loss: 0.225670, acc.: 94.53%] [G loss: 2.225766]\n",
      "epoch:8 step:7014 [D loss: 0.194627, acc.: 92.19%] [G loss: 6.320271]\n",
      "epoch:8 step:7015 [D loss: 0.062030, acc.: 98.44%] [G loss: 6.141715]\n",
      "epoch:8 step:7016 [D loss: 0.194636, acc.: 90.62%] [G loss: 1.644331]\n",
      "epoch:8 step:7017 [D loss: 0.016598, acc.: 100.00%] [G loss: 1.000657]\n",
      "epoch:8 step:7018 [D loss: 0.145658, acc.: 94.53%] [G loss: 3.678320]\n",
      "epoch:8 step:7019 [D loss: 0.219253, acc.: 92.19%] [G loss: 1.973046]\n",
      "epoch:8 step:7020 [D loss: 0.066712, acc.: 99.22%] [G loss: 3.077299]\n",
      "epoch:8 step:7021 [D loss: 0.036160, acc.: 99.22%] [G loss: 3.361166]\n",
      "epoch:8 step:7022 [D loss: 0.049801, acc.: 99.22%] [G loss: 4.086010]\n",
      "epoch:8 step:7023 [D loss: 0.045031, acc.: 99.22%] [G loss: 3.538666]\n",
      "epoch:8 step:7024 [D loss: 0.075506, acc.: 97.66%] [G loss: 3.863382]\n",
      "epoch:8 step:7025 [D loss: 0.315523, acc.: 89.06%] [G loss: 5.000405]\n",
      "epoch:8 step:7026 [D loss: 0.149408, acc.: 92.97%] [G loss: 4.057249]\n",
      "epoch:8 step:7027 [D loss: 0.106667, acc.: 96.09%] [G loss: 4.954399]\n",
      "epoch:8 step:7028 [D loss: 0.027707, acc.: 100.00%] [G loss: 4.475821]\n",
      "epoch:8 step:7029 [D loss: 0.035305, acc.: 100.00%] [G loss: 5.253279]\n",
      "epoch:9 step:7030 [D loss: 0.103434, acc.: 96.88%] [G loss: 4.166637]\n",
      "epoch:9 step:7031 [D loss: 0.073644, acc.: 98.44%] [G loss: 4.474686]\n",
      "epoch:9 step:7032 [D loss: 0.229410, acc.: 92.19%] [G loss: 3.839919]\n",
      "epoch:9 step:7033 [D loss: 0.032572, acc.: 100.00%] [G loss: 4.536938]\n",
      "epoch:9 step:7034 [D loss: 0.016681, acc.: 100.00%] [G loss: 4.672920]\n",
      "epoch:9 step:7035 [D loss: 0.066915, acc.: 99.22%] [G loss: 1.357330]\n",
      "epoch:9 step:7036 [D loss: 0.046648, acc.: 100.00%] [G loss: 0.937336]\n",
      "epoch:9 step:7037 [D loss: 0.021273, acc.: 100.00%] [G loss: 0.581212]\n",
      "epoch:9 step:7038 [D loss: 0.257117, acc.: 90.62%] [G loss: 7.250815]\n",
      "epoch:9 step:7039 [D loss: 0.991885, acc.: 60.16%] [G loss: 0.199756]\n",
      "epoch:9 step:7040 [D loss: 0.558572, acc.: 73.44%] [G loss: 7.611634]\n",
      "epoch:9 step:7041 [D loss: 1.402657, acc.: 50.78%] [G loss: 2.096738]\n",
      "epoch:9 step:7042 [D loss: 0.147745, acc.: 96.09%] [G loss: 2.201698]\n",
      "epoch:9 step:7043 [D loss: 0.025593, acc.: 99.22%] [G loss: 1.919035]\n",
      "epoch:9 step:7044 [D loss: 0.019924, acc.: 100.00%] [G loss: 1.695672]\n",
      "epoch:9 step:7045 [D loss: 0.413628, acc.: 78.91%] [G loss: 4.675678]\n",
      "epoch:9 step:7046 [D loss: 0.274532, acc.: 86.72%] [G loss: 3.775340]\n",
      "epoch:9 step:7047 [D loss: 0.033593, acc.: 99.22%] [G loss: 1.939646]\n",
      "epoch:9 step:7048 [D loss: 0.239860, acc.: 89.06%] [G loss: 5.116254]\n",
      "epoch:9 step:7049 [D loss: 0.123523, acc.: 96.09%] [G loss: 4.577704]\n",
      "epoch:9 step:7050 [D loss: 0.097878, acc.: 98.44%] [G loss: 5.040019]\n",
      "epoch:9 step:7051 [D loss: 0.128625, acc.: 96.88%] [G loss: 2.591774]\n",
      "epoch:9 step:7052 [D loss: 0.279634, acc.: 86.72%] [G loss: 7.044063]\n",
      "epoch:9 step:7053 [D loss: 0.393781, acc.: 78.91%] [G loss: 3.415766]\n",
      "epoch:9 step:7054 [D loss: 0.453298, acc.: 74.22%] [G loss: 7.926302]\n",
      "epoch:9 step:7055 [D loss: 0.437110, acc.: 78.12%] [G loss: 5.840237]\n",
      "epoch:9 step:7056 [D loss: 0.032278, acc.: 100.00%] [G loss: 3.911268]\n",
      "epoch:9 step:7057 [D loss: 0.068254, acc.: 97.66%] [G loss: 3.220690]\n",
      "epoch:9 step:7058 [D loss: 0.195081, acc.: 90.62%] [G loss: 4.755748]\n",
      "epoch:9 step:7059 [D loss: 0.089077, acc.: 96.88%] [G loss: 3.779571]\n",
      "epoch:9 step:7060 [D loss: 0.400401, acc.: 84.38%] [G loss: 4.823498]\n",
      "epoch:9 step:7061 [D loss: 0.035920, acc.: 100.00%] [G loss: 6.030973]\n",
      "epoch:9 step:7062 [D loss: 0.715147, acc.: 64.84%] [G loss: 6.844077]\n",
      "epoch:9 step:7063 [D loss: 0.152676, acc.: 92.97%] [G loss: 6.601492]\n",
      "epoch:9 step:7064 [D loss: 0.439729, acc.: 80.47%] [G loss: 4.345617]\n",
      "epoch:9 step:7065 [D loss: 0.038032, acc.: 100.00%] [G loss: 4.148366]\n",
      "epoch:9 step:7066 [D loss: 0.021117, acc.: 100.00%] [G loss: 3.894436]\n",
      "epoch:9 step:7067 [D loss: 0.097823, acc.: 97.66%] [G loss: 4.455682]\n",
      "epoch:9 step:7068 [D loss: 0.118631, acc.: 96.09%] [G loss: 5.758742]\n",
      "epoch:9 step:7069 [D loss: 0.057171, acc.: 97.66%] [G loss: 5.856449]\n",
      "epoch:9 step:7070 [D loss: 0.368786, acc.: 85.94%] [G loss: 5.120759]\n",
      "epoch:9 step:7071 [D loss: 0.044505, acc.: 99.22%] [G loss: 5.062194]\n",
      "epoch:9 step:7072 [D loss: 0.083146, acc.: 97.66%] [G loss: 2.046408]\n",
      "epoch:9 step:7073 [D loss: 0.233613, acc.: 90.62%] [G loss: 5.157547]\n",
      "epoch:9 step:7074 [D loss: 0.174259, acc.: 91.41%] [G loss: 4.667872]\n",
      "epoch:9 step:7075 [D loss: 0.404088, acc.: 85.16%] [G loss: 6.087401]\n",
      "epoch:9 step:7076 [D loss: 0.012747, acc.: 100.00%] [G loss: 7.334312]\n",
      "epoch:9 step:7077 [D loss: 0.158734, acc.: 92.97%] [G loss: 2.991541]\n",
      "epoch:9 step:7078 [D loss: 0.110928, acc.: 96.09%] [G loss: 3.316087]\n",
      "epoch:9 step:7079 [D loss: 0.126549, acc.: 96.88%] [G loss: 4.934426]\n",
      "epoch:9 step:7080 [D loss: 0.214207, acc.: 90.62%] [G loss: 4.827405]\n",
      "epoch:9 step:7081 [D loss: 0.170315, acc.: 93.75%] [G loss: 7.015944]\n",
      "epoch:9 step:7082 [D loss: 0.534127, acc.: 75.00%] [G loss: 6.323595]\n",
      "epoch:9 step:7083 [D loss: 0.065001, acc.: 97.66%] [G loss: 5.859640]\n",
      "epoch:9 step:7084 [D loss: 0.044162, acc.: 98.44%] [G loss: 5.175234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7085 [D loss: 0.209259, acc.: 93.75%] [G loss: 2.820122]\n",
      "epoch:9 step:7086 [D loss: 0.107714, acc.: 96.88%] [G loss: 5.952109]\n",
      "epoch:9 step:7087 [D loss: 0.101033, acc.: 96.09%] [G loss: 3.834338]\n",
      "epoch:9 step:7088 [D loss: 0.053297, acc.: 99.22%] [G loss: 4.515583]\n",
      "epoch:9 step:7089 [D loss: 0.152182, acc.: 95.31%] [G loss: 5.637513]\n",
      "epoch:9 step:7090 [D loss: 1.039599, acc.: 49.22%] [G loss: 8.785749]\n",
      "epoch:9 step:7091 [D loss: 1.210084, acc.: 54.69%] [G loss: 3.187328]\n",
      "epoch:9 step:7092 [D loss: 0.143073, acc.: 95.31%] [G loss: 3.621166]\n",
      "epoch:9 step:7093 [D loss: 0.036738, acc.: 99.22%] [G loss: 3.727356]\n",
      "epoch:9 step:7094 [D loss: 0.156213, acc.: 94.53%] [G loss: 1.497944]\n",
      "epoch:9 step:7095 [D loss: 0.211173, acc.: 91.41%] [G loss: 6.977448]\n",
      "epoch:9 step:7096 [D loss: 0.019707, acc.: 100.00%] [G loss: 5.157009]\n",
      "epoch:9 step:7097 [D loss: 0.450220, acc.: 79.69%] [G loss: 4.676980]\n",
      "epoch:9 step:7098 [D loss: 0.075133, acc.: 97.66%] [G loss: 5.936526]\n",
      "epoch:9 step:7099 [D loss: 0.054556, acc.: 98.44%] [G loss: 3.380157]\n",
      "epoch:9 step:7100 [D loss: 0.053166, acc.: 100.00%] [G loss: 4.289970]\n",
      "epoch:9 step:7101 [D loss: 0.078386, acc.: 97.66%] [G loss: 3.135911]\n",
      "epoch:9 step:7102 [D loss: 0.083808, acc.: 97.66%] [G loss: 2.565657]\n",
      "epoch:9 step:7103 [D loss: 0.174814, acc.: 95.31%] [G loss: 4.205861]\n",
      "epoch:9 step:7104 [D loss: 0.099922, acc.: 96.88%] [G loss: 5.563776]\n",
      "epoch:9 step:7105 [D loss: 0.920771, acc.: 55.47%] [G loss: 5.879830]\n",
      "epoch:9 step:7106 [D loss: 0.235823, acc.: 87.50%] [G loss: 6.015854]\n",
      "epoch:9 step:7107 [D loss: 0.046393, acc.: 100.00%] [G loss: 3.453206]\n",
      "epoch:9 step:7108 [D loss: 0.139067, acc.: 96.09%] [G loss: 4.533156]\n",
      "epoch:9 step:7109 [D loss: 0.018646, acc.: 100.00%] [G loss: 4.582957]\n",
      "epoch:9 step:7110 [D loss: 0.093133, acc.: 96.88%] [G loss: 4.676253]\n",
      "epoch:9 step:7111 [D loss: 0.184590, acc.: 91.41%] [G loss: 5.439269]\n",
      "epoch:9 step:7112 [D loss: 0.245940, acc.: 89.06%] [G loss: 3.432152]\n",
      "epoch:9 step:7113 [D loss: 0.217181, acc.: 89.06%] [G loss: 6.374717]\n",
      "epoch:9 step:7114 [D loss: 0.111396, acc.: 97.66%] [G loss: 5.872496]\n",
      "epoch:9 step:7115 [D loss: 0.062888, acc.: 99.22%] [G loss: 3.243237]\n",
      "epoch:9 step:7116 [D loss: 0.069317, acc.: 97.66%] [G loss: 4.903293]\n",
      "epoch:9 step:7117 [D loss: 0.038375, acc.: 99.22%] [G loss: 3.002618]\n",
      "epoch:9 step:7118 [D loss: 0.062428, acc.: 99.22%] [G loss: 3.625265]\n",
      "epoch:9 step:7119 [D loss: 1.254601, acc.: 42.19%] [G loss: 7.384244]\n",
      "epoch:9 step:7120 [D loss: 0.988268, acc.: 60.94%] [G loss: 3.904718]\n",
      "epoch:9 step:7121 [D loss: 0.232285, acc.: 91.41%] [G loss: 3.030230]\n",
      "epoch:9 step:7122 [D loss: 0.056142, acc.: 100.00%] [G loss: 2.484351]\n",
      "epoch:9 step:7123 [D loss: 0.088523, acc.: 98.44%] [G loss: 3.018049]\n",
      "epoch:9 step:7124 [D loss: 0.041266, acc.: 100.00%] [G loss: 3.244399]\n",
      "epoch:9 step:7125 [D loss: 0.086677, acc.: 99.22%] [G loss: 3.307040]\n",
      "epoch:9 step:7126 [D loss: 0.144972, acc.: 95.31%] [G loss: 3.928077]\n",
      "epoch:9 step:7127 [D loss: 0.159086, acc.: 94.53%] [G loss: 2.905375]\n",
      "epoch:9 step:7128 [D loss: 0.059759, acc.: 99.22%] [G loss: 2.050726]\n",
      "epoch:9 step:7129 [D loss: 0.222918, acc.: 91.41%] [G loss: 4.612738]\n",
      "epoch:9 step:7130 [D loss: 0.075825, acc.: 96.88%] [G loss: 4.781607]\n",
      "epoch:9 step:7131 [D loss: 0.095869, acc.: 96.09%] [G loss: 3.385771]\n",
      "epoch:9 step:7132 [D loss: 0.067273, acc.: 97.66%] [G loss: 2.355999]\n",
      "epoch:9 step:7133 [D loss: 0.171879, acc.: 92.19%] [G loss: 5.122241]\n",
      "epoch:9 step:7134 [D loss: 0.075643, acc.: 96.88%] [G loss: 6.172475]\n",
      "epoch:9 step:7135 [D loss: 0.057235, acc.: 98.44%] [G loss: 3.896850]\n",
      "epoch:9 step:7136 [D loss: 0.091969, acc.: 97.66%] [G loss: 4.100786]\n",
      "epoch:9 step:7137 [D loss: 0.059477, acc.: 99.22%] [G loss: 4.060554]\n",
      "epoch:9 step:7138 [D loss: 0.019732, acc.: 100.00%] [G loss: 4.231135]\n",
      "epoch:9 step:7139 [D loss: 0.023359, acc.: 100.00%] [G loss: 3.076758]\n",
      "epoch:9 step:7140 [D loss: 0.137570, acc.: 96.88%] [G loss: 3.921797]\n",
      "epoch:9 step:7141 [D loss: 0.060747, acc.: 100.00%] [G loss: 2.665252]\n",
      "epoch:9 step:7142 [D loss: 0.031473, acc.: 100.00%] [G loss: 3.945098]\n",
      "epoch:9 step:7143 [D loss: 0.106708, acc.: 97.66%] [G loss: 5.554912]\n",
      "epoch:9 step:7144 [D loss: 0.038908, acc.: 99.22%] [G loss: 2.611484]\n",
      "epoch:9 step:7145 [D loss: 0.546706, acc.: 71.09%] [G loss: 1.706147]\n",
      "epoch:9 step:7146 [D loss: 0.008085, acc.: 100.00%] [G loss: 5.623722]\n",
      "epoch:9 step:7147 [D loss: 0.069781, acc.: 97.66%] [G loss: 2.049514]\n",
      "epoch:9 step:7148 [D loss: 0.085309, acc.: 99.22%] [G loss: 4.891920]\n",
      "epoch:9 step:7149 [D loss: 0.021339, acc.: 100.00%] [G loss: 1.952035]\n",
      "epoch:9 step:7150 [D loss: 0.024952, acc.: 100.00%] [G loss: 1.153603]\n",
      "epoch:9 step:7151 [D loss: 0.069906, acc.: 100.00%] [G loss: 3.299665]\n",
      "epoch:9 step:7152 [D loss: 0.075329, acc.: 98.44%] [G loss: 2.569337]\n",
      "epoch:9 step:7153 [D loss: 0.033590, acc.: 100.00%] [G loss: 0.647289]\n",
      "epoch:9 step:7154 [D loss: 0.450506, acc.: 78.91%] [G loss: 6.728414]\n",
      "epoch:9 step:7155 [D loss: 0.896394, acc.: 58.59%] [G loss: 4.540313]\n",
      "epoch:9 step:7156 [D loss: 0.030668, acc.: 99.22%] [G loss: 3.873955]\n",
      "epoch:9 step:7157 [D loss: 0.026666, acc.: 100.00%] [G loss: 5.076773]\n",
      "epoch:9 step:7158 [D loss: 0.036762, acc.: 100.00%] [G loss: 3.576532]\n",
      "epoch:9 step:7159 [D loss: 0.066969, acc.: 99.22%] [G loss: 4.545125]\n",
      "epoch:9 step:7160 [D loss: 0.039028, acc.: 100.00%] [G loss: 2.388084]\n",
      "epoch:9 step:7161 [D loss: 0.216279, acc.: 90.62%] [G loss: 3.949162]\n",
      "epoch:9 step:7162 [D loss: 0.279460, acc.: 87.50%] [G loss: 2.961595]\n",
      "epoch:9 step:7163 [D loss: 0.126185, acc.: 93.75%] [G loss: 4.681263]\n",
      "epoch:9 step:7164 [D loss: 0.026819, acc.: 100.00%] [G loss: 4.927514]\n",
      "epoch:9 step:7165 [D loss: 0.063499, acc.: 97.66%] [G loss: 3.842769]\n",
      "epoch:9 step:7166 [D loss: 0.132334, acc.: 95.31%] [G loss: 4.891154]\n",
      "epoch:9 step:7167 [D loss: 0.254801, acc.: 89.84%] [G loss: 4.161112]\n",
      "epoch:9 step:7168 [D loss: 0.045783, acc.: 99.22%] [G loss: 4.016350]\n",
      "epoch:9 step:7169 [D loss: 0.069355, acc.: 99.22%] [G loss: 3.061141]\n",
      "epoch:9 step:7170 [D loss: 0.011946, acc.: 100.00%] [G loss: 1.813936]\n",
      "epoch:9 step:7171 [D loss: 0.057578, acc.: 99.22%] [G loss: 4.396851]\n",
      "epoch:9 step:7172 [D loss: 0.275925, acc.: 90.62%] [G loss: 4.111004]\n",
      "epoch:9 step:7173 [D loss: 0.048886, acc.: 98.44%] [G loss: 6.374060]\n",
      "epoch:9 step:7174 [D loss: 0.076079, acc.: 97.66%] [G loss: 3.531422]\n",
      "epoch:9 step:7175 [D loss: 0.029935, acc.: 99.22%] [G loss: 0.867561]\n",
      "epoch:9 step:7176 [D loss: 0.035180, acc.: 100.00%] [G loss: 1.045395]\n",
      "epoch:9 step:7177 [D loss: 0.009751, acc.: 100.00%] [G loss: 1.144447]\n",
      "epoch:9 step:7178 [D loss: 0.020851, acc.: 100.00%] [G loss: 1.174111]\n",
      "epoch:9 step:7179 [D loss: 0.052559, acc.: 100.00%] [G loss: 1.683067]\n",
      "epoch:9 step:7180 [D loss: 0.056209, acc.: 98.44%] [G loss: 3.328137]\n",
      "epoch:9 step:7181 [D loss: 0.094787, acc.: 96.09%] [G loss: 1.475694]\n",
      "epoch:9 step:7182 [D loss: 0.464020, acc.: 77.34%] [G loss: 1.062542]\n",
      "epoch:9 step:7183 [D loss: 0.059451, acc.: 98.44%] [G loss: 4.112170]\n",
      "epoch:9 step:7184 [D loss: 0.012568, acc.: 100.00%] [G loss: 1.885741]\n",
      "epoch:9 step:7185 [D loss: 0.142140, acc.: 93.75%] [G loss: 0.652700]\n",
      "epoch:9 step:7186 [D loss: 0.222736, acc.: 89.84%] [G loss: 6.609965]\n",
      "epoch:9 step:7187 [D loss: 1.586355, acc.: 50.00%] [G loss: 5.740328]\n",
      "epoch:9 step:7188 [D loss: 0.049411, acc.: 100.00%] [G loss: 3.882882]\n",
      "epoch:9 step:7189 [D loss: 0.459973, acc.: 76.56%] [G loss: 3.985987]\n",
      "epoch:9 step:7190 [D loss: 0.017134, acc.: 100.00%] [G loss: 4.764805]\n",
      "epoch:9 step:7191 [D loss: 0.020375, acc.: 100.00%] [G loss: 6.182283]\n",
      "epoch:9 step:7192 [D loss: 0.066919, acc.: 99.22%] [G loss: 3.556283]\n",
      "epoch:9 step:7193 [D loss: 0.048091, acc.: 99.22%] [G loss: 4.374786]\n",
      "epoch:9 step:7194 [D loss: 0.106862, acc.: 96.09%] [G loss: 5.148218]\n",
      "epoch:9 step:7195 [D loss: 0.031947, acc.: 100.00%] [G loss: 5.050417]\n",
      "epoch:9 step:7196 [D loss: 0.105440, acc.: 97.66%] [G loss: 3.125793]\n",
      "epoch:9 step:7197 [D loss: 0.058374, acc.: 99.22%] [G loss: 3.380434]\n",
      "epoch:9 step:7198 [D loss: 0.024902, acc.: 100.00%] [G loss: 4.096461]\n",
      "epoch:9 step:7199 [D loss: 0.026013, acc.: 99.22%] [G loss: 3.666611]\n",
      "epoch:9 step:7200 [D loss: 0.085257, acc.: 98.44%] [G loss: 5.059974]\n",
      "epoch:9 step:7201 [D loss: 0.092657, acc.: 96.09%] [G loss: 2.248999]\n",
      "epoch:9 step:7202 [D loss: 0.122224, acc.: 96.88%] [G loss: 4.837594]\n",
      "epoch:9 step:7203 [D loss: 0.244470, acc.: 89.84%] [G loss: 3.176270]\n",
      "epoch:9 step:7204 [D loss: 0.013086, acc.: 100.00%] [G loss: 3.891396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7205 [D loss: 0.128028, acc.: 96.88%] [G loss: 2.327698]\n",
      "epoch:9 step:7206 [D loss: 0.017370, acc.: 100.00%] [G loss: 1.648893]\n",
      "epoch:9 step:7207 [D loss: 0.151293, acc.: 92.97%] [G loss: 4.615666]\n",
      "epoch:9 step:7208 [D loss: 0.887594, acc.: 60.94%] [G loss: 1.749378]\n",
      "epoch:9 step:7209 [D loss: 0.045434, acc.: 99.22%] [G loss: 0.930192]\n",
      "epoch:9 step:7210 [D loss: 0.002043, acc.: 100.00%] [G loss: 1.947555]\n",
      "epoch:9 step:7211 [D loss: 0.124374, acc.: 97.66%] [G loss: 3.304823]\n",
      "epoch:9 step:7212 [D loss: 0.069448, acc.: 98.44%] [G loss: 3.738743]\n",
      "epoch:9 step:7213 [D loss: 0.017312, acc.: 99.22%] [G loss: 2.928126]\n",
      "epoch:9 step:7214 [D loss: 0.575283, acc.: 73.44%] [G loss: 1.516166]\n",
      "epoch:9 step:7215 [D loss: 0.015383, acc.: 100.00%] [G loss: 1.755273]\n",
      "epoch:9 step:7216 [D loss: 0.094853, acc.: 96.09%] [G loss: 1.983405]\n",
      "epoch:9 step:7217 [D loss: 0.093879, acc.: 97.66%] [G loss: 4.267378]\n",
      "epoch:9 step:7218 [D loss: 0.055276, acc.: 99.22%] [G loss: 4.104261]\n",
      "epoch:9 step:7219 [D loss: 0.049561, acc.: 99.22%] [G loss: 2.874926]\n",
      "epoch:9 step:7220 [D loss: 0.085356, acc.: 98.44%] [G loss: 2.331145]\n",
      "epoch:9 step:7221 [D loss: 0.039479, acc.: 99.22%] [G loss: 4.677661]\n",
      "epoch:9 step:7222 [D loss: 0.147889, acc.: 94.53%] [G loss: 3.163404]\n",
      "epoch:9 step:7223 [D loss: 0.144319, acc.: 94.53%] [G loss: 6.518896]\n",
      "epoch:9 step:7224 [D loss: 0.824300, acc.: 59.38%] [G loss: 7.915591]\n",
      "epoch:9 step:7225 [D loss: 0.595252, acc.: 73.44%] [G loss: 5.821215]\n",
      "epoch:9 step:7226 [D loss: 0.087675, acc.: 96.09%] [G loss: 4.344298]\n",
      "epoch:9 step:7227 [D loss: 0.017790, acc.: 100.00%] [G loss: 4.305102]\n",
      "epoch:9 step:7228 [D loss: 0.101307, acc.: 96.88%] [G loss: 4.248647]\n",
      "epoch:9 step:7229 [D loss: 0.014976, acc.: 100.00%] [G loss: 3.065040]\n",
      "epoch:9 step:7230 [D loss: 0.147127, acc.: 95.31%] [G loss: 5.536370]\n",
      "epoch:9 step:7231 [D loss: 0.292403, acc.: 86.72%] [G loss: 2.248733]\n",
      "epoch:9 step:7232 [D loss: 0.021488, acc.: 100.00%] [G loss: 1.666397]\n",
      "epoch:9 step:7233 [D loss: 0.003364, acc.: 100.00%] [G loss: 0.384525]\n",
      "epoch:9 step:7234 [D loss: 0.143755, acc.: 93.75%] [G loss: 4.089546]\n",
      "epoch:9 step:7235 [D loss: 0.280161, acc.: 88.28%] [G loss: 1.111805]\n",
      "epoch:9 step:7236 [D loss: 0.107110, acc.: 95.31%] [G loss: 1.832103]\n",
      "epoch:9 step:7237 [D loss: 0.015636, acc.: 100.00%] [G loss: 1.582638]\n",
      "epoch:9 step:7238 [D loss: 0.344248, acc.: 85.94%] [G loss: 1.083498]\n",
      "epoch:9 step:7239 [D loss: 0.018463, acc.: 100.00%] [G loss: 2.512701]\n",
      "epoch:9 step:7240 [D loss: 0.009479, acc.: 100.00%] [G loss: 3.333687]\n",
      "epoch:9 step:7241 [D loss: 0.037798, acc.: 100.00%] [G loss: 1.141335]\n",
      "epoch:9 step:7242 [D loss: 0.013719, acc.: 100.00%] [G loss: 0.574907]\n",
      "epoch:9 step:7243 [D loss: 3.085254, acc.: 34.38%] [G loss: 7.400017]\n",
      "epoch:9 step:7244 [D loss: 1.923421, acc.: 50.78%] [G loss: 5.536950]\n",
      "epoch:9 step:7245 [D loss: 0.599496, acc.: 72.66%] [G loss: 2.382874]\n",
      "epoch:9 step:7246 [D loss: 0.555788, acc.: 71.88%] [G loss: 4.459483]\n",
      "epoch:9 step:7247 [D loss: 0.160815, acc.: 93.75%] [G loss: 5.197105]\n",
      "epoch:9 step:7248 [D loss: 0.217428, acc.: 92.97%] [G loss: 3.733554]\n",
      "epoch:9 step:7249 [D loss: 0.198714, acc.: 96.09%] [G loss: 2.384456]\n",
      "epoch:9 step:7250 [D loss: 0.205120, acc.: 91.41%] [G loss: 4.251062]\n",
      "epoch:9 step:7251 [D loss: 0.104909, acc.: 98.44%] [G loss: 3.971373]\n",
      "epoch:9 step:7252 [D loss: 0.542245, acc.: 72.66%] [G loss: 2.914286]\n",
      "epoch:9 step:7253 [D loss: 0.115708, acc.: 98.44%] [G loss: 3.193422]\n",
      "epoch:9 step:7254 [D loss: 0.118804, acc.: 98.44%] [G loss: 1.754263]\n",
      "epoch:9 step:7255 [D loss: 0.089537, acc.: 97.66%] [G loss: 3.297729]\n",
      "epoch:9 step:7256 [D loss: 0.093320, acc.: 97.66%] [G loss: 2.691268]\n",
      "epoch:9 step:7257 [D loss: 0.086466, acc.: 99.22%] [G loss: 2.919757]\n",
      "epoch:9 step:7258 [D loss: 0.250965, acc.: 89.84%] [G loss: 3.289759]\n",
      "epoch:9 step:7259 [D loss: 0.062418, acc.: 98.44%] [G loss: 5.158285]\n",
      "epoch:9 step:7260 [D loss: 0.037334, acc.: 100.00%] [G loss: 3.742577]\n",
      "epoch:9 step:7261 [D loss: 0.204797, acc.: 95.31%] [G loss: 0.966425]\n",
      "epoch:9 step:7262 [D loss: 0.128005, acc.: 96.09%] [G loss: 3.487447]\n",
      "epoch:9 step:7263 [D loss: 0.034439, acc.: 100.00%] [G loss: 3.749907]\n",
      "epoch:9 step:7264 [D loss: 0.022933, acc.: 100.00%] [G loss: 3.580050]\n",
      "epoch:9 step:7265 [D loss: 0.142595, acc.: 95.31%] [G loss: 1.193808]\n",
      "epoch:9 step:7266 [D loss: 0.105241, acc.: 98.44%] [G loss: 3.999688]\n",
      "epoch:9 step:7267 [D loss: 0.190644, acc.: 93.75%] [G loss: 3.470593]\n",
      "epoch:9 step:7268 [D loss: 0.042695, acc.: 100.00%] [G loss: 3.214659]\n",
      "epoch:9 step:7269 [D loss: 0.957531, acc.: 46.88%] [G loss: 6.918834]\n",
      "epoch:9 step:7270 [D loss: 0.622761, acc.: 67.97%] [G loss: 4.838223]\n",
      "epoch:9 step:7271 [D loss: 0.048462, acc.: 100.00%] [G loss: 3.070395]\n",
      "epoch:9 step:7272 [D loss: 0.077063, acc.: 97.66%] [G loss: 2.717457]\n",
      "epoch:9 step:7273 [D loss: 0.043272, acc.: 100.00%] [G loss: 3.743852]\n",
      "epoch:9 step:7274 [D loss: 0.169242, acc.: 94.53%] [G loss: 3.274681]\n",
      "epoch:9 step:7275 [D loss: 0.106341, acc.: 96.88%] [G loss: 3.377444]\n",
      "epoch:9 step:7276 [D loss: 0.221471, acc.: 90.62%] [G loss: 1.697116]\n",
      "epoch:9 step:7277 [D loss: 0.117628, acc.: 97.66%] [G loss: 2.827133]\n",
      "epoch:9 step:7278 [D loss: 0.019409, acc.: 100.00%] [G loss: 3.521848]\n",
      "epoch:9 step:7279 [D loss: 0.207018, acc.: 92.19%] [G loss: 0.505037]\n",
      "epoch:9 step:7280 [D loss: 0.272711, acc.: 87.50%] [G loss: 5.092346]\n",
      "epoch:9 step:7281 [D loss: 0.373792, acc.: 77.34%] [G loss: 2.156677]\n",
      "epoch:9 step:7282 [D loss: 0.121358, acc.: 92.97%] [G loss: 4.185667]\n",
      "epoch:9 step:7283 [D loss: 0.036786, acc.: 98.44%] [G loss: 3.174645]\n",
      "epoch:9 step:7284 [D loss: 0.047950, acc.: 97.66%] [G loss: 2.299953]\n",
      "epoch:9 step:7285 [D loss: 0.044138, acc.: 100.00%] [G loss: 2.158890]\n",
      "epoch:9 step:7286 [D loss: 0.626527, acc.: 69.53%] [G loss: 6.794279]\n",
      "epoch:9 step:7287 [D loss: 1.486164, acc.: 50.78%] [G loss: 3.368022]\n",
      "epoch:9 step:7288 [D loss: 0.204366, acc.: 89.84%] [G loss: 4.454019]\n",
      "epoch:9 step:7289 [D loss: 0.070122, acc.: 100.00%] [G loss: 3.829392]\n",
      "epoch:9 step:7290 [D loss: 0.055026, acc.: 100.00%] [G loss: 4.727034]\n",
      "epoch:9 step:7291 [D loss: 0.066131, acc.: 99.22%] [G loss: 4.052442]\n",
      "epoch:9 step:7292 [D loss: 0.064319, acc.: 99.22%] [G loss: 4.161068]\n",
      "epoch:9 step:7293 [D loss: 0.215180, acc.: 93.75%] [G loss: 4.427242]\n",
      "epoch:9 step:7294 [D loss: 0.228758, acc.: 89.84%] [G loss: 2.186305]\n",
      "epoch:9 step:7295 [D loss: 0.259044, acc.: 89.06%] [G loss: 4.024884]\n",
      "epoch:9 step:7296 [D loss: 0.338600, acc.: 82.03%] [G loss: 2.399377]\n",
      "epoch:9 step:7297 [D loss: 0.085028, acc.: 97.66%] [G loss: 1.998809]\n",
      "epoch:9 step:7298 [D loss: 0.028197, acc.: 100.00%] [G loss: 2.275385]\n",
      "epoch:9 step:7299 [D loss: 0.054045, acc.: 100.00%] [G loss: 2.189276]\n",
      "epoch:9 step:7300 [D loss: 0.021163, acc.: 100.00%] [G loss: 1.652663]\n",
      "epoch:9 step:7301 [D loss: 0.144504, acc.: 93.75%] [G loss: 2.066020]\n",
      "epoch:9 step:7302 [D loss: 0.128044, acc.: 95.31%] [G loss: 2.531224]\n",
      "epoch:9 step:7303 [D loss: 0.057507, acc.: 98.44%] [G loss: 0.997781]\n",
      "epoch:9 step:7304 [D loss: 0.051207, acc.: 99.22%] [G loss: 0.766024]\n",
      "epoch:9 step:7305 [D loss: 0.104987, acc.: 98.44%] [G loss: 0.950880]\n",
      "epoch:9 step:7306 [D loss: 0.358160, acc.: 84.38%] [G loss: 2.227897]\n",
      "epoch:9 step:7307 [D loss: 0.028103, acc.: 100.00%] [G loss: 3.607525]\n",
      "epoch:9 step:7308 [D loss: 0.044429, acc.: 99.22%] [G loss: 3.089801]\n",
      "epoch:9 step:7309 [D loss: 0.036482, acc.: 100.00%] [G loss: 1.951406]\n",
      "epoch:9 step:7310 [D loss: 0.105797, acc.: 98.44%] [G loss: 3.133827]\n",
      "epoch:9 step:7311 [D loss: 0.151756, acc.: 95.31%] [G loss: 2.465477]\n",
      "epoch:9 step:7312 [D loss: 0.075583, acc.: 98.44%] [G loss: 2.971570]\n",
      "epoch:9 step:7313 [D loss: 0.069779, acc.: 96.88%] [G loss: 3.873917]\n",
      "epoch:9 step:7314 [D loss: 0.106175, acc.: 99.22%] [G loss: 3.683430]\n",
      "epoch:9 step:7315 [D loss: 1.073780, acc.: 40.62%] [G loss: 4.103260]\n",
      "epoch:9 step:7316 [D loss: 0.101679, acc.: 96.09%] [G loss: 6.228420]\n",
      "epoch:9 step:7317 [D loss: 0.061122, acc.: 97.66%] [G loss: 5.456991]\n",
      "epoch:9 step:7318 [D loss: 0.039415, acc.: 99.22%] [G loss: 3.705337]\n",
      "epoch:9 step:7319 [D loss: 0.017008, acc.: 100.00%] [G loss: 2.024658]\n",
      "epoch:9 step:7320 [D loss: 0.104172, acc.: 99.22%] [G loss: 5.560674]\n",
      "epoch:9 step:7321 [D loss: 0.048689, acc.: 100.00%] [G loss: 4.148335]\n",
      "epoch:9 step:7322 [D loss: 0.047331, acc.: 99.22%] [G loss: 2.687529]\n",
      "epoch:9 step:7323 [D loss: 0.076603, acc.: 100.00%] [G loss: 3.085683]\n",
      "epoch:9 step:7324 [D loss: 0.378485, acc.: 83.59%] [G loss: 5.177419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7325 [D loss: 0.397368, acc.: 78.12%] [G loss: 3.090562]\n",
      "epoch:9 step:7326 [D loss: 0.053412, acc.: 99.22%] [G loss: 2.871868]\n",
      "epoch:9 step:7327 [D loss: 0.030555, acc.: 100.00%] [G loss: 3.238137]\n",
      "epoch:9 step:7328 [D loss: 0.048684, acc.: 99.22%] [G loss: 2.726849]\n",
      "epoch:9 step:7329 [D loss: 0.327619, acc.: 82.03%] [G loss: 5.543772]\n",
      "epoch:9 step:7330 [D loss: 0.714881, acc.: 65.62%] [G loss: 1.923146]\n",
      "epoch:9 step:7331 [D loss: 0.310988, acc.: 85.16%] [G loss: 5.487417]\n",
      "epoch:9 step:7332 [D loss: 0.064083, acc.: 98.44%] [G loss: 7.233952]\n",
      "epoch:9 step:7333 [D loss: 0.030155, acc.: 99.22%] [G loss: 6.040393]\n",
      "epoch:9 step:7334 [D loss: 0.012448, acc.: 100.00%] [G loss: 4.171445]\n",
      "epoch:9 step:7335 [D loss: 0.034324, acc.: 100.00%] [G loss: 3.722544]\n",
      "epoch:9 step:7336 [D loss: 0.058248, acc.: 98.44%] [G loss: 3.742628]\n",
      "epoch:9 step:7337 [D loss: 0.059187, acc.: 98.44%] [G loss: 4.080418]\n",
      "epoch:9 step:7338 [D loss: 0.122387, acc.: 96.88%] [G loss: 3.239707]\n",
      "epoch:9 step:7339 [D loss: 0.245045, acc.: 92.19%] [G loss: 3.717695]\n",
      "epoch:9 step:7340 [D loss: 0.017790, acc.: 100.00%] [G loss: 4.766035]\n",
      "epoch:9 step:7341 [D loss: 0.078109, acc.: 99.22%] [G loss: 1.730548]\n",
      "epoch:9 step:7342 [D loss: 0.091123, acc.: 97.66%] [G loss: 4.102720]\n",
      "epoch:9 step:7343 [D loss: 0.027415, acc.: 100.00%] [G loss: 5.399260]\n",
      "epoch:9 step:7344 [D loss: 1.949547, acc.: 24.22%] [G loss: 7.812421]\n",
      "epoch:9 step:7345 [D loss: 0.997887, acc.: 57.03%] [G loss: 4.528366]\n",
      "epoch:9 step:7346 [D loss: 0.156297, acc.: 95.31%] [G loss: 3.579640]\n",
      "epoch:9 step:7347 [D loss: 0.103258, acc.: 96.88%] [G loss: 4.206808]\n",
      "epoch:9 step:7348 [D loss: 0.033695, acc.: 100.00%] [G loss: 3.196030]\n",
      "epoch:9 step:7349 [D loss: 0.179125, acc.: 92.97%] [G loss: 3.442953]\n",
      "epoch:9 step:7350 [D loss: 0.134281, acc.: 96.09%] [G loss: 2.455528]\n",
      "epoch:9 step:7351 [D loss: 0.107342, acc.: 97.66%] [G loss: 3.918082]\n",
      "epoch:9 step:7352 [D loss: 0.506423, acc.: 78.12%] [G loss: 2.527950]\n",
      "epoch:9 step:7353 [D loss: 0.055193, acc.: 100.00%] [G loss: 3.979559]\n",
      "epoch:9 step:7354 [D loss: 0.099717, acc.: 97.66%] [G loss: 2.556237]\n",
      "epoch:9 step:7355 [D loss: 0.052248, acc.: 99.22%] [G loss: 3.918641]\n",
      "epoch:9 step:7356 [D loss: 0.054135, acc.: 100.00%] [G loss: 1.928148]\n",
      "epoch:9 step:7357 [D loss: 0.085462, acc.: 98.44%] [G loss: 4.663909]\n",
      "epoch:9 step:7358 [D loss: 0.082102, acc.: 99.22%] [G loss: 1.834953]\n",
      "epoch:9 step:7359 [D loss: 0.174468, acc.: 95.31%] [G loss: 3.546193]\n",
      "epoch:9 step:7360 [D loss: 0.208190, acc.: 93.75%] [G loss: 3.098456]\n",
      "epoch:9 step:7361 [D loss: 0.079177, acc.: 97.66%] [G loss: 3.827256]\n",
      "epoch:9 step:7362 [D loss: 0.079101, acc.: 98.44%] [G loss: 5.382666]\n",
      "epoch:9 step:7363 [D loss: 0.153653, acc.: 96.09%] [G loss: 4.690557]\n",
      "epoch:9 step:7364 [D loss: 0.036581, acc.: 100.00%] [G loss: 4.248022]\n",
      "epoch:9 step:7365 [D loss: 0.312463, acc.: 85.94%] [G loss: 5.985403]\n",
      "epoch:9 step:7366 [D loss: 0.223517, acc.: 90.62%] [G loss: 4.211920]\n",
      "epoch:9 step:7367 [D loss: 0.078900, acc.: 97.66%] [G loss: 3.958927]\n",
      "epoch:9 step:7368 [D loss: 0.049990, acc.: 100.00%] [G loss: 4.482183]\n",
      "epoch:9 step:7369 [D loss: 0.505551, acc.: 76.56%] [G loss: 7.349682]\n",
      "epoch:9 step:7370 [D loss: 0.415017, acc.: 77.34%] [G loss: 4.755477]\n",
      "epoch:9 step:7371 [D loss: 0.039078, acc.: 100.00%] [G loss: 4.235497]\n",
      "epoch:9 step:7372 [D loss: 0.099464, acc.: 97.66%] [G loss: 4.638412]\n",
      "epoch:9 step:7373 [D loss: 0.018572, acc.: 100.00%] [G loss: 4.435359]\n",
      "epoch:9 step:7374 [D loss: 0.031145, acc.: 99.22%] [G loss: 2.386829]\n",
      "epoch:9 step:7375 [D loss: 0.302717, acc.: 90.62%] [G loss: 2.215393]\n",
      "epoch:9 step:7376 [D loss: 0.021012, acc.: 100.00%] [G loss: 5.340965]\n",
      "epoch:9 step:7377 [D loss: 0.059573, acc.: 100.00%] [G loss: 5.593125]\n",
      "epoch:9 step:7378 [D loss: 0.036602, acc.: 100.00%] [G loss: 4.647976]\n",
      "epoch:9 step:7379 [D loss: 0.324571, acc.: 83.59%] [G loss: 5.714573]\n",
      "epoch:9 step:7380 [D loss: 0.177361, acc.: 92.19%] [G loss: 3.297091]\n",
      "epoch:9 step:7381 [D loss: 0.052477, acc.: 99.22%] [G loss: 2.979196]\n",
      "epoch:9 step:7382 [D loss: 0.064543, acc.: 99.22%] [G loss: 4.606087]\n",
      "epoch:9 step:7383 [D loss: 0.099494, acc.: 98.44%] [G loss: 2.092088]\n",
      "epoch:9 step:7384 [D loss: 0.133130, acc.: 96.88%] [G loss: 5.050326]\n",
      "epoch:9 step:7385 [D loss: 0.086485, acc.: 99.22%] [G loss: 4.885150]\n",
      "epoch:9 step:7386 [D loss: 0.039510, acc.: 100.00%] [G loss: 4.716930]\n",
      "epoch:9 step:7387 [D loss: 0.211913, acc.: 94.53%] [G loss: 2.032112]\n",
      "epoch:9 step:7388 [D loss: 0.226265, acc.: 89.06%] [G loss: 6.070593]\n",
      "epoch:9 step:7389 [D loss: 0.150269, acc.: 95.31%] [G loss: 6.019201]\n",
      "epoch:9 step:7390 [D loss: 0.133600, acc.: 96.09%] [G loss: 4.252398]\n",
      "epoch:9 step:7391 [D loss: 0.253336, acc.: 86.72%] [G loss: 7.323590]\n",
      "epoch:9 step:7392 [D loss: 0.136493, acc.: 93.75%] [G loss: 5.895704]\n",
      "epoch:9 step:7393 [D loss: 0.399969, acc.: 82.03%] [G loss: 1.763814]\n",
      "epoch:9 step:7394 [D loss: 0.293979, acc.: 83.59%] [G loss: 7.285635]\n",
      "epoch:9 step:7395 [D loss: 0.068943, acc.: 98.44%] [G loss: 7.511515]\n",
      "epoch:9 step:7396 [D loss: 0.511628, acc.: 74.22%] [G loss: 2.068547]\n",
      "epoch:9 step:7397 [D loss: 1.488073, acc.: 53.12%] [G loss: 7.677551]\n",
      "epoch:9 step:7398 [D loss: 0.914043, acc.: 58.59%] [G loss: 4.569880]\n",
      "epoch:9 step:7399 [D loss: 0.303507, acc.: 89.06%] [G loss: 5.855747]\n",
      "epoch:9 step:7400 [D loss: 0.059597, acc.: 97.66%] [G loss: 6.274578]\n",
      "epoch:9 step:7401 [D loss: 0.188165, acc.: 92.19%] [G loss: 3.975298]\n",
      "epoch:9 step:7402 [D loss: 0.347375, acc.: 82.03%] [G loss: 6.299497]\n",
      "epoch:9 step:7403 [D loss: 0.421204, acc.: 80.47%] [G loss: 4.257102]\n",
      "epoch:9 step:7404 [D loss: 0.046912, acc.: 99.22%] [G loss: 3.327748]\n",
      "epoch:9 step:7405 [D loss: 0.073729, acc.: 99.22%] [G loss: 4.027947]\n",
      "epoch:9 step:7406 [D loss: 0.057897, acc.: 99.22%] [G loss: 3.366956]\n",
      "epoch:9 step:7407 [D loss: 0.110115, acc.: 96.09%] [G loss: 1.954570]\n",
      "epoch:9 step:7408 [D loss: 0.057790, acc.: 100.00%] [G loss: 2.673689]\n",
      "epoch:9 step:7409 [D loss: 0.058829, acc.: 100.00%] [G loss: 3.858058]\n",
      "epoch:9 step:7410 [D loss: 0.073401, acc.: 99.22%] [G loss: 0.886478]\n",
      "epoch:9 step:7411 [D loss: 0.401976, acc.: 82.03%] [G loss: 4.574424]\n",
      "epoch:9 step:7412 [D loss: 0.292131, acc.: 85.16%] [G loss: 1.788775]\n",
      "epoch:9 step:7413 [D loss: 0.311869, acc.: 89.84%] [G loss: 2.024957]\n",
      "epoch:9 step:7414 [D loss: 0.252458, acc.: 91.41%] [G loss: 4.892723]\n",
      "epoch:9 step:7415 [D loss: 0.403148, acc.: 81.25%] [G loss: 3.210662]\n",
      "epoch:9 step:7416 [D loss: 0.086070, acc.: 98.44%] [G loss: 1.492928]\n",
      "epoch:9 step:7417 [D loss: 0.065672, acc.: 100.00%] [G loss: 3.322435]\n",
      "epoch:9 step:7418 [D loss: 0.102093, acc.: 96.88%] [G loss: 3.898204]\n",
      "epoch:9 step:7419 [D loss: 0.047898, acc.: 99.22%] [G loss: 4.593923]\n",
      "epoch:9 step:7420 [D loss: 0.558273, acc.: 69.53%] [G loss: 4.902869]\n",
      "epoch:9 step:7421 [D loss: 0.072049, acc.: 98.44%] [G loss: 6.046062]\n",
      "epoch:9 step:7422 [D loss: 0.257828, acc.: 89.06%] [G loss: 2.800891]\n",
      "epoch:9 step:7423 [D loss: 0.117343, acc.: 96.88%] [G loss: 3.045404]\n",
      "epoch:9 step:7424 [D loss: 0.170629, acc.: 95.31%] [G loss: 3.221091]\n",
      "epoch:9 step:7425 [D loss: 0.065595, acc.: 99.22%] [G loss: 3.138340]\n",
      "epoch:9 step:7426 [D loss: 0.124471, acc.: 95.31%] [G loss: 2.950551]\n",
      "epoch:9 step:7427 [D loss: 0.042340, acc.: 99.22%] [G loss: 4.910647]\n",
      "epoch:9 step:7428 [D loss: 0.131871, acc.: 97.66%] [G loss: 3.689353]\n",
      "epoch:9 step:7429 [D loss: 0.125211, acc.: 96.88%] [G loss: 5.534432]\n",
      "epoch:9 step:7430 [D loss: 0.587502, acc.: 74.22%] [G loss: 5.381835]\n",
      "epoch:9 step:7431 [D loss: 0.121740, acc.: 95.31%] [G loss: 3.754839]\n",
      "epoch:9 step:7432 [D loss: 0.133162, acc.: 98.44%] [G loss: 1.453445]\n",
      "epoch:9 step:7433 [D loss: 0.040352, acc.: 100.00%] [G loss: 4.914897]\n",
      "epoch:9 step:7434 [D loss: 0.109546, acc.: 95.31%] [G loss: 1.399944]\n",
      "epoch:9 step:7435 [D loss: 0.062543, acc.: 98.44%] [G loss: 3.312979]\n",
      "epoch:9 step:7436 [D loss: 0.015447, acc.: 100.00%] [G loss: 2.794912]\n",
      "epoch:9 step:7437 [D loss: 0.022298, acc.: 100.00%] [G loss: 2.324793]\n",
      "epoch:9 step:7438 [D loss: 0.136924, acc.: 96.09%] [G loss: 1.592062]\n",
      "epoch:9 step:7439 [D loss: 0.053428, acc.: 99.22%] [G loss: 4.661741]\n",
      "epoch:9 step:7440 [D loss: 0.773735, acc.: 58.59%] [G loss: 7.231627]\n",
      "epoch:9 step:7441 [D loss: 0.218404, acc.: 89.06%] [G loss: 6.333700]\n",
      "epoch:9 step:7442 [D loss: 0.123506, acc.: 95.31%] [G loss: 4.572012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7443 [D loss: 0.025521, acc.: 99.22%] [G loss: 2.080318]\n",
      "epoch:9 step:7444 [D loss: 0.021451, acc.: 100.00%] [G loss: 1.778768]\n",
      "epoch:9 step:7445 [D loss: 0.042003, acc.: 100.00%] [G loss: 2.523234]\n",
      "epoch:9 step:7446 [D loss: 0.022704, acc.: 100.00%] [G loss: 4.600153]\n",
      "epoch:9 step:7447 [D loss: 0.097986, acc.: 96.88%] [G loss: 2.185138]\n",
      "epoch:9 step:7448 [D loss: 0.035538, acc.: 100.00%] [G loss: 3.806995]\n",
      "epoch:9 step:7449 [D loss: 0.031060, acc.: 100.00%] [G loss: 0.955516]\n",
      "epoch:9 step:7450 [D loss: 0.055439, acc.: 99.22%] [G loss: 2.141052]\n",
      "epoch:9 step:7451 [D loss: 0.025445, acc.: 100.00%] [G loss: 2.578373]\n",
      "epoch:9 step:7452 [D loss: 0.047498, acc.: 100.00%] [G loss: 3.810430]\n",
      "epoch:9 step:7453 [D loss: 0.601050, acc.: 68.75%] [G loss: 6.572641]\n",
      "epoch:9 step:7454 [D loss: 0.065014, acc.: 98.44%] [G loss: 6.865518]\n",
      "epoch:9 step:7455 [D loss: 0.590403, acc.: 75.78%] [G loss: 0.721505]\n",
      "epoch:9 step:7456 [D loss: 1.359875, acc.: 59.38%] [G loss: 9.683945]\n",
      "epoch:9 step:7457 [D loss: 0.506234, acc.: 75.00%] [G loss: 7.956625]\n",
      "epoch:9 step:7458 [D loss: 0.320530, acc.: 85.16%] [G loss: 2.733103]\n",
      "epoch:9 step:7459 [D loss: 0.245815, acc.: 89.84%] [G loss: 3.693235]\n",
      "epoch:9 step:7460 [D loss: 0.053123, acc.: 97.66%] [G loss: 5.502985]\n",
      "epoch:9 step:7461 [D loss: 0.036235, acc.: 99.22%] [G loss: 3.230402]\n",
      "epoch:9 step:7462 [D loss: 0.053606, acc.: 99.22%] [G loss: 3.046285]\n",
      "epoch:9 step:7463 [D loss: 0.650990, acc.: 61.72%] [G loss: 8.719411]\n",
      "epoch:9 step:7464 [D loss: 0.987004, acc.: 60.16%] [G loss: 3.176439]\n",
      "epoch:9 step:7465 [D loss: 0.130438, acc.: 95.31%] [G loss: 4.109362]\n",
      "epoch:9 step:7466 [D loss: 0.065822, acc.: 99.22%] [G loss: 3.180209]\n",
      "epoch:9 step:7467 [D loss: 0.081428, acc.: 96.88%] [G loss: 3.719321]\n",
      "epoch:9 step:7468 [D loss: 0.100933, acc.: 97.66%] [G loss: 3.920669]\n",
      "epoch:9 step:7469 [D loss: 0.036161, acc.: 100.00%] [G loss: 3.145969]\n",
      "epoch:9 step:7470 [D loss: 0.451732, acc.: 75.00%] [G loss: 7.443464]\n",
      "epoch:9 step:7471 [D loss: 0.844607, acc.: 60.94%] [G loss: 3.786684]\n",
      "epoch:9 step:7472 [D loss: 0.222453, acc.: 93.75%] [G loss: 2.164884]\n",
      "epoch:9 step:7473 [D loss: 0.130048, acc.: 97.66%] [G loss: 4.916787]\n",
      "epoch:9 step:7474 [D loss: 0.017505, acc.: 100.00%] [G loss: 4.946044]\n",
      "epoch:9 step:7475 [D loss: 0.023509, acc.: 100.00%] [G loss: 3.924479]\n",
      "epoch:9 step:7476 [D loss: 0.150934, acc.: 95.31%] [G loss: 4.311155]\n",
      "epoch:9 step:7477 [D loss: 0.206402, acc.: 92.97%] [G loss: 1.364779]\n",
      "epoch:9 step:7478 [D loss: 0.041728, acc.: 99.22%] [G loss: 2.526360]\n",
      "epoch:9 step:7479 [D loss: 0.089657, acc.: 97.66%] [G loss: 1.320121]\n",
      "epoch:9 step:7480 [D loss: 0.172732, acc.: 96.09%] [G loss: 3.151892]\n",
      "epoch:9 step:7481 [D loss: 0.064872, acc.: 98.44%] [G loss: 3.994456]\n",
      "epoch:9 step:7482 [D loss: 0.336918, acc.: 86.72%] [G loss: 0.147196]\n",
      "epoch:9 step:7483 [D loss: 0.241925, acc.: 92.97%] [G loss: 2.907745]\n",
      "epoch:9 step:7484 [D loss: 0.062412, acc.: 98.44%] [G loss: 4.535759]\n",
      "epoch:9 step:7485 [D loss: 0.159287, acc.: 92.19%] [G loss: 1.256692]\n",
      "epoch:9 step:7486 [D loss: 0.467925, acc.: 78.91%] [G loss: 6.857198]\n",
      "epoch:9 step:7487 [D loss: 0.276376, acc.: 88.28%] [G loss: 6.741481]\n",
      "epoch:9 step:7488 [D loss: 0.172437, acc.: 91.41%] [G loss: 3.422700]\n",
      "epoch:9 step:7489 [D loss: 0.192634, acc.: 92.19%] [G loss: 2.319726]\n",
      "epoch:9 step:7490 [D loss: 0.061241, acc.: 98.44%] [G loss: 3.906635]\n",
      "epoch:9 step:7491 [D loss: 0.166524, acc.: 96.88%] [G loss: 3.261937]\n",
      "epoch:9 step:7492 [D loss: 0.129832, acc.: 96.88%] [G loss: 6.436476]\n",
      "epoch:9 step:7493 [D loss: 0.124694, acc.: 96.88%] [G loss: 4.687258]\n",
      "epoch:9 step:7494 [D loss: 0.058615, acc.: 99.22%] [G loss: 3.903175]\n",
      "epoch:9 step:7495 [D loss: 0.094785, acc.: 99.22%] [G loss: 3.114719]\n",
      "epoch:9 step:7496 [D loss: 0.101610, acc.: 97.66%] [G loss: 2.359714]\n",
      "epoch:9 step:7497 [D loss: 0.031790, acc.: 100.00%] [G loss: 2.334643]\n",
      "epoch:9 step:7498 [D loss: 0.257391, acc.: 85.94%] [G loss: 5.685264]\n",
      "epoch:9 step:7499 [D loss: 0.187887, acc.: 92.19%] [G loss: 5.784126]\n",
      "epoch:9 step:7500 [D loss: 0.089318, acc.: 97.66%] [G loss: 3.054971]\n",
      "epoch:9 step:7501 [D loss: 0.006941, acc.: 100.00%] [G loss: 2.581507]\n",
      "epoch:9 step:7502 [D loss: 0.096473, acc.: 97.66%] [G loss: 2.956164]\n",
      "epoch:9 step:7503 [D loss: 0.045003, acc.: 99.22%] [G loss: 2.953187]\n",
      "epoch:9 step:7504 [D loss: 0.180092, acc.: 93.75%] [G loss: 4.292052]\n",
      "epoch:9 step:7505 [D loss: 0.059791, acc.: 97.66%] [G loss: 2.972262]\n",
      "epoch:9 step:7506 [D loss: 0.152240, acc.: 96.09%] [G loss: 3.642116]\n",
      "epoch:9 step:7507 [D loss: 0.106622, acc.: 96.09%] [G loss: 1.520205]\n",
      "epoch:9 step:7508 [D loss: 0.082496, acc.: 100.00%] [G loss: 1.949502]\n",
      "epoch:9 step:7509 [D loss: 0.032344, acc.: 99.22%] [G loss: 4.821042]\n",
      "epoch:9 step:7510 [D loss: 0.219144, acc.: 90.62%] [G loss: 6.513516]\n",
      "epoch:9 step:7511 [D loss: 0.161961, acc.: 93.75%] [G loss: 3.634123]\n",
      "epoch:9 step:7512 [D loss: 0.147132, acc.: 95.31%] [G loss: 4.706997]\n",
      "epoch:9 step:7513 [D loss: 0.027398, acc.: 99.22%] [G loss: 6.502326]\n",
      "epoch:9 step:7514 [D loss: 0.034942, acc.: 100.00%] [G loss: 1.960489]\n",
      "epoch:9 step:7515 [D loss: 0.102564, acc.: 97.66%] [G loss: 5.264057]\n",
      "epoch:9 step:7516 [D loss: 0.131725, acc.: 96.88%] [G loss: 5.047540]\n",
      "epoch:9 step:7517 [D loss: 0.488943, acc.: 73.44%] [G loss: 8.412584]\n",
      "epoch:9 step:7518 [D loss: 0.832424, acc.: 60.94%] [G loss: 1.821405]\n",
      "epoch:9 step:7519 [D loss: 0.011243, acc.: 100.00%] [G loss: 1.072170]\n",
      "epoch:9 step:7520 [D loss: 0.107310, acc.: 96.09%] [G loss: 1.143341]\n",
      "epoch:9 step:7521 [D loss: 0.004784, acc.: 100.00%] [G loss: 2.431898]\n",
      "epoch:9 step:7522 [D loss: 0.022836, acc.: 99.22%] [G loss: 1.288745]\n",
      "epoch:9 step:7523 [D loss: 0.198363, acc.: 91.41%] [G loss: 4.425878]\n",
      "epoch:9 step:7524 [D loss: 0.051853, acc.: 98.44%] [G loss: 5.666952]\n",
      "epoch:9 step:7525 [D loss: 0.205848, acc.: 92.19%] [G loss: 1.756029]\n",
      "epoch:9 step:7526 [D loss: 0.006408, acc.: 100.00%] [G loss: 0.767750]\n",
      "epoch:9 step:7527 [D loss: 0.035089, acc.: 99.22%] [G loss: 1.603739]\n",
      "epoch:9 step:7528 [D loss: 0.141757, acc.: 96.09%] [G loss: 0.540992]\n",
      "epoch:9 step:7529 [D loss: 0.142340, acc.: 91.41%] [G loss: 5.519781]\n",
      "epoch:9 step:7530 [D loss: 0.782274, acc.: 63.28%] [G loss: 3.113938]\n",
      "epoch:9 step:7531 [D loss: 0.043037, acc.: 99.22%] [G loss: 6.134717]\n",
      "epoch:9 step:7532 [D loss: 0.059257, acc.: 97.66%] [G loss: 4.812460]\n",
      "epoch:9 step:7533 [D loss: 0.034819, acc.: 99.22%] [G loss: 4.782481]\n",
      "epoch:9 step:7534 [D loss: 0.094978, acc.: 96.88%] [G loss: 5.635497]\n",
      "epoch:9 step:7535 [D loss: 0.017621, acc.: 100.00%] [G loss: 3.395161]\n",
      "epoch:9 step:7536 [D loss: 0.074794, acc.: 98.44%] [G loss: 3.801715]\n",
      "epoch:9 step:7537 [D loss: 0.107780, acc.: 95.31%] [G loss: 1.343160]\n",
      "epoch:9 step:7538 [D loss: 0.021208, acc.: 100.00%] [G loss: 0.281067]\n",
      "epoch:9 step:7539 [D loss: 0.145889, acc.: 97.66%] [G loss: 2.559169]\n",
      "epoch:9 step:7540 [D loss: 0.047108, acc.: 100.00%] [G loss: 4.373437]\n",
      "epoch:9 step:7541 [D loss: 0.068183, acc.: 97.66%] [G loss: 1.625399]\n",
      "epoch:9 step:7542 [D loss: 0.057918, acc.: 99.22%] [G loss: 0.871279]\n",
      "epoch:9 step:7543 [D loss: 0.029690, acc.: 100.00%] [G loss: 0.261734]\n",
      "epoch:9 step:7544 [D loss: 0.012761, acc.: 100.00%] [G loss: 0.749095]\n",
      "epoch:9 step:7545 [D loss: 0.026675, acc.: 100.00%] [G loss: 1.673953]\n",
      "epoch:9 step:7546 [D loss: 0.396387, acc.: 75.78%] [G loss: 8.900341]\n",
      "epoch:9 step:7547 [D loss: 2.026453, acc.: 51.56%] [G loss: 2.810312]\n",
      "epoch:9 step:7548 [D loss: 0.407358, acc.: 83.59%] [G loss: 5.807051]\n",
      "epoch:9 step:7549 [D loss: 0.131543, acc.: 92.19%] [G loss: 6.087961]\n",
      "epoch:9 step:7550 [D loss: 0.063119, acc.: 99.22%] [G loss: 4.632507]\n",
      "epoch:9 step:7551 [D loss: 0.104978, acc.: 94.53%] [G loss: 5.549769]\n",
      "epoch:9 step:7552 [D loss: 0.048057, acc.: 100.00%] [G loss: 5.460655]\n",
      "epoch:9 step:7553 [D loss: 0.077854, acc.: 99.22%] [G loss: 3.145607]\n",
      "epoch:9 step:7554 [D loss: 0.123554, acc.: 95.31%] [G loss: 4.392063]\n",
      "epoch:9 step:7555 [D loss: 0.123726, acc.: 96.88%] [G loss: 3.260461]\n",
      "epoch:9 step:7556 [D loss: 0.058398, acc.: 98.44%] [G loss: 2.737539]\n",
      "epoch:9 step:7557 [D loss: 0.076446, acc.: 98.44%] [G loss: 3.989576]\n",
      "epoch:9 step:7558 [D loss: 0.086634, acc.: 99.22%] [G loss: 3.519178]\n",
      "epoch:9 step:7559 [D loss: 0.102324, acc.: 97.66%] [G loss: 3.998703]\n",
      "epoch:9 step:7560 [D loss: 0.031485, acc.: 99.22%] [G loss: 4.262210]\n",
      "epoch:9 step:7561 [D loss: 0.242004, acc.: 92.97%] [G loss: 4.009698]\n",
      "epoch:9 step:7562 [D loss: 0.024703, acc.: 100.00%] [G loss: 4.340246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7563 [D loss: 0.029487, acc.: 100.00%] [G loss: 4.606100]\n",
      "epoch:9 step:7564 [D loss: 0.045635, acc.: 100.00%] [G loss: 3.203271]\n",
      "epoch:9 step:7565 [D loss: 0.090874, acc.: 98.44%] [G loss: 3.976813]\n",
      "epoch:9 step:7566 [D loss: 0.080207, acc.: 99.22%] [G loss: 3.811182]\n",
      "epoch:9 step:7567 [D loss: 0.032938, acc.: 100.00%] [G loss: 4.128255]\n",
      "epoch:9 step:7568 [D loss: 0.016684, acc.: 100.00%] [G loss: 3.757809]\n",
      "epoch:9 step:7569 [D loss: 0.040929, acc.: 100.00%] [G loss: 4.002501]\n",
      "epoch:9 step:7570 [D loss: 0.058157, acc.: 99.22%] [G loss: 4.443049]\n",
      "epoch:9 step:7571 [D loss: 0.996219, acc.: 49.22%] [G loss: 8.626659]\n",
      "epoch:9 step:7572 [D loss: 1.346744, acc.: 50.78%] [G loss: 4.544832]\n",
      "epoch:9 step:7573 [D loss: 0.119919, acc.: 95.31%] [G loss: 3.586791]\n",
      "epoch:9 step:7574 [D loss: 0.086300, acc.: 96.09%] [G loss: 2.759369]\n",
      "epoch:9 step:7575 [D loss: 0.106693, acc.: 96.09%] [G loss: 4.595206]\n",
      "epoch:9 step:7576 [D loss: 0.128239, acc.: 93.75%] [G loss: 3.779724]\n",
      "epoch:9 step:7577 [D loss: 0.084401, acc.: 98.44%] [G loss: 2.628513]\n",
      "epoch:9 step:7578 [D loss: 0.093297, acc.: 97.66%] [G loss: 3.073894]\n",
      "epoch:9 step:7579 [D loss: 0.340763, acc.: 81.25%] [G loss: 3.785056]\n",
      "epoch:9 step:7580 [D loss: 0.019479, acc.: 100.00%] [G loss: 3.024656]\n",
      "epoch:9 step:7581 [D loss: 0.092680, acc.: 100.00%] [G loss: 1.627705]\n",
      "epoch:9 step:7582 [D loss: 0.129592, acc.: 96.88%] [G loss: 3.037149]\n",
      "epoch:9 step:7583 [D loss: 0.075267, acc.: 99.22%] [G loss: 2.372165]\n",
      "epoch:9 step:7584 [D loss: 0.125580, acc.: 96.88%] [G loss: 3.892400]\n",
      "epoch:9 step:7585 [D loss: 0.071762, acc.: 98.44%] [G loss: 3.817571]\n",
      "epoch:9 step:7586 [D loss: 0.398797, acc.: 82.03%] [G loss: 5.391721]\n",
      "epoch:9 step:7587 [D loss: 0.387131, acc.: 81.25%] [G loss: 3.771457]\n",
      "epoch:9 step:7588 [D loss: 0.058527, acc.: 100.00%] [G loss: 2.367307]\n",
      "epoch:9 step:7589 [D loss: 0.039530, acc.: 100.00%] [G loss: 3.044250]\n",
      "epoch:9 step:7590 [D loss: 0.043928, acc.: 99.22%] [G loss: 2.353402]\n",
      "epoch:9 step:7591 [D loss: 0.306050, acc.: 87.50%] [G loss: 1.878699]\n",
      "epoch:9 step:7592 [D loss: 0.677917, acc.: 66.41%] [G loss: 8.084881]\n",
      "epoch:9 step:7593 [D loss: 1.407512, acc.: 53.12%] [G loss: 2.825859]\n",
      "epoch:9 step:7594 [D loss: 0.142876, acc.: 95.31%] [G loss: 3.164912]\n",
      "epoch:9 step:7595 [D loss: 0.194618, acc.: 92.19%] [G loss: 4.973339]\n",
      "epoch:9 step:7596 [D loss: 0.052457, acc.: 99.22%] [G loss: 4.178867]\n",
      "epoch:9 step:7597 [D loss: 0.152285, acc.: 95.31%] [G loss: 1.716497]\n",
      "epoch:9 step:7598 [D loss: 0.620568, acc.: 69.53%] [G loss: 7.683124]\n",
      "epoch:9 step:7599 [D loss: 0.406489, acc.: 76.56%] [G loss: 5.118811]\n",
      "epoch:9 step:7600 [D loss: 0.101830, acc.: 99.22%] [G loss: 2.819574]\n",
      "epoch:9 step:7601 [D loss: 0.019198, acc.: 100.00%] [G loss: 2.988858]\n",
      "epoch:9 step:7602 [D loss: 0.041069, acc.: 100.00%] [G loss: 3.322947]\n",
      "epoch:9 step:7603 [D loss: 0.024870, acc.: 100.00%] [G loss: 1.799933]\n",
      "epoch:9 step:7604 [D loss: 0.322431, acc.: 89.84%] [G loss: 5.225218]\n",
      "epoch:9 step:7605 [D loss: 0.090445, acc.: 97.66%] [G loss: 4.812003]\n",
      "epoch:9 step:7606 [D loss: 0.208541, acc.: 89.84%] [G loss: 0.618686]\n",
      "epoch:9 step:7607 [D loss: 0.354986, acc.: 78.91%] [G loss: 3.535091]\n",
      "epoch:9 step:7608 [D loss: 0.154323, acc.: 93.75%] [G loss: 4.589081]\n",
      "epoch:9 step:7609 [D loss: 0.171114, acc.: 92.97%] [G loss: 1.283119]\n",
      "epoch:9 step:7610 [D loss: 0.153821, acc.: 95.31%] [G loss: 2.773685]\n",
      "epoch:9 step:7611 [D loss: 0.018599, acc.: 100.00%] [G loss: 3.188379]\n",
      "epoch:9 step:7612 [D loss: 0.051474, acc.: 97.66%] [G loss: 2.038595]\n",
      "epoch:9 step:7613 [D loss: 0.072842, acc.: 100.00%] [G loss: 1.694317]\n",
      "epoch:9 step:7614 [D loss: 0.008901, acc.: 100.00%] [G loss: 1.103873]\n",
      "epoch:9 step:7615 [D loss: 0.029135, acc.: 100.00%] [G loss: 1.001488]\n",
      "epoch:9 step:7616 [D loss: 0.044072, acc.: 100.00%] [G loss: 1.227314]\n",
      "epoch:9 step:7617 [D loss: 0.172805, acc.: 93.75%] [G loss: 2.991527]\n",
      "epoch:9 step:7618 [D loss: 0.206329, acc.: 94.53%] [G loss: 2.049704]\n",
      "epoch:9 step:7619 [D loss: 0.090051, acc.: 98.44%] [G loss: 2.528542]\n",
      "epoch:9 step:7620 [D loss: 0.335221, acc.: 85.16%] [G loss: 1.479072]\n",
      "epoch:9 step:7621 [D loss: 0.068589, acc.: 98.44%] [G loss: 3.742513]\n",
      "epoch:9 step:7622 [D loss: 0.190001, acc.: 92.97%] [G loss: 2.122341]\n",
      "epoch:9 step:7623 [D loss: 0.084403, acc.: 98.44%] [G loss: 5.046435]\n",
      "epoch:9 step:7624 [D loss: 0.011494, acc.: 100.00%] [G loss: 4.113001]\n",
      "epoch:9 step:7625 [D loss: 0.074383, acc.: 99.22%] [G loss: 2.798470]\n",
      "epoch:9 step:7626 [D loss: 0.274441, acc.: 87.50%] [G loss: 7.087812]\n",
      "epoch:9 step:7627 [D loss: 0.277807, acc.: 85.16%] [G loss: 5.067796]\n",
      "epoch:9 step:7628 [D loss: 0.124066, acc.: 96.88%] [G loss: 2.720572]\n",
      "epoch:9 step:7629 [D loss: 0.048273, acc.: 100.00%] [G loss: 2.060361]\n",
      "epoch:9 step:7630 [D loss: 0.161003, acc.: 93.75%] [G loss: 5.078349]\n",
      "epoch:9 step:7631 [D loss: 0.040847, acc.: 98.44%] [G loss: 5.726506]\n",
      "epoch:9 step:7632 [D loss: 0.124427, acc.: 95.31%] [G loss: 2.558855]\n",
      "epoch:9 step:7633 [D loss: 0.107149, acc.: 98.44%] [G loss: 2.893216]\n",
      "epoch:9 step:7634 [D loss: 0.081894, acc.: 96.09%] [G loss: 4.508186]\n",
      "epoch:9 step:7635 [D loss: 0.048469, acc.: 100.00%] [G loss: 5.166710]\n",
      "epoch:9 step:7636 [D loss: 0.480998, acc.: 78.91%] [G loss: 4.865449]\n",
      "epoch:9 step:7637 [D loss: 0.071742, acc.: 97.66%] [G loss: 6.086399]\n",
      "epoch:9 step:7638 [D loss: 0.217142, acc.: 89.84%] [G loss: 2.423862]\n",
      "epoch:9 step:7639 [D loss: 0.268517, acc.: 89.84%] [G loss: 6.229033]\n",
      "epoch:9 step:7640 [D loss: 0.046274, acc.: 99.22%] [G loss: 5.920548]\n",
      "epoch:9 step:7641 [D loss: 0.400404, acc.: 79.69%] [G loss: 4.569807]\n",
      "epoch:9 step:7642 [D loss: 0.200469, acc.: 91.41%] [G loss: 3.488323]\n",
      "epoch:9 step:7643 [D loss: 0.058411, acc.: 99.22%] [G loss: 4.285272]\n",
      "epoch:9 step:7644 [D loss: 0.043162, acc.: 99.22%] [G loss: 3.972140]\n",
      "epoch:9 step:7645 [D loss: 0.279713, acc.: 86.72%] [G loss: 0.293452]\n",
      "epoch:9 step:7646 [D loss: 0.409831, acc.: 79.69%] [G loss: 6.408329]\n",
      "epoch:9 step:7647 [D loss: 0.319451, acc.: 83.59%] [G loss: 5.924165]\n",
      "epoch:9 step:7648 [D loss: 0.493617, acc.: 78.12%] [G loss: 1.509352]\n",
      "epoch:9 step:7649 [D loss: 0.363262, acc.: 86.72%] [G loss: 3.916486]\n",
      "epoch:9 step:7650 [D loss: 0.011420, acc.: 100.00%] [G loss: 5.601142]\n",
      "epoch:9 step:7651 [D loss: 0.184636, acc.: 93.75%] [G loss: 4.979939]\n",
      "epoch:9 step:7652 [D loss: 0.416706, acc.: 80.47%] [G loss: 6.417892]\n",
      "epoch:9 step:7653 [D loss: 0.093777, acc.: 97.66%] [G loss: 5.490072]\n",
      "epoch:9 step:7654 [D loss: 0.039197, acc.: 100.00%] [G loss: 4.922416]\n",
      "epoch:9 step:7655 [D loss: 0.050346, acc.: 100.00%] [G loss: 3.541307]\n",
      "epoch:9 step:7656 [D loss: 0.045608, acc.: 100.00%] [G loss: 2.811428]\n",
      "epoch:9 step:7657 [D loss: 0.044869, acc.: 98.44%] [G loss: 4.035141]\n",
      "epoch:9 step:7658 [D loss: 0.067258, acc.: 99.22%] [G loss: 5.017677]\n",
      "epoch:9 step:7659 [D loss: 0.030724, acc.: 99.22%] [G loss: 3.973109]\n",
      "epoch:9 step:7660 [D loss: 0.309012, acc.: 88.28%] [G loss: 4.245727]\n",
      "epoch:9 step:7661 [D loss: 0.189223, acc.: 93.75%] [G loss: 3.296801]\n",
      "epoch:9 step:7662 [D loss: 0.065674, acc.: 98.44%] [G loss: 4.933634]\n",
      "epoch:9 step:7663 [D loss: 0.040406, acc.: 98.44%] [G loss: 3.524877]\n",
      "epoch:9 step:7664 [D loss: 1.148536, acc.: 45.31%] [G loss: 8.593666]\n",
      "epoch:9 step:7665 [D loss: 1.556399, acc.: 52.34%] [G loss: 3.491098]\n",
      "epoch:9 step:7666 [D loss: 0.770751, acc.: 66.41%] [G loss: 3.576339]\n",
      "epoch:9 step:7667 [D loss: 0.175122, acc.: 91.41%] [G loss: 5.682472]\n",
      "epoch:9 step:7668 [D loss: 0.650930, acc.: 68.75%] [G loss: 1.987237]\n",
      "epoch:9 step:7669 [D loss: 0.070909, acc.: 99.22%] [G loss: 3.640616]\n",
      "epoch:9 step:7670 [D loss: 0.087313, acc.: 97.66%] [G loss: 3.279413]\n",
      "epoch:9 step:7671 [D loss: 0.157943, acc.: 93.75%] [G loss: 2.283179]\n",
      "epoch:9 step:7672 [D loss: 0.067662, acc.: 98.44%] [G loss: 3.391925]\n",
      "epoch:9 step:7673 [D loss: 0.062877, acc.: 99.22%] [G loss: 1.998097]\n",
      "epoch:9 step:7674 [D loss: 0.206768, acc.: 92.97%] [G loss: 3.464273]\n",
      "epoch:9 step:7675 [D loss: 0.086686, acc.: 96.88%] [G loss: 2.861013]\n",
      "epoch:9 step:7676 [D loss: 0.062648, acc.: 99.22%] [G loss: 3.558454]\n",
      "epoch:9 step:7677 [D loss: 0.121493, acc.: 99.22%] [G loss: 4.567109]\n",
      "epoch:9 step:7678 [D loss: 0.197569, acc.: 91.41%] [G loss: 1.984714]\n",
      "epoch:9 step:7679 [D loss: 0.133391, acc.: 92.97%] [G loss: 3.413513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7680 [D loss: 0.009755, acc.: 100.00%] [G loss: 4.906864]\n",
      "epoch:9 step:7681 [D loss: 0.167330, acc.: 92.97%] [G loss: 3.211733]\n",
      "epoch:9 step:7682 [D loss: 0.154041, acc.: 93.75%] [G loss: 4.092751]\n",
      "epoch:9 step:7683 [D loss: 0.073528, acc.: 98.44%] [G loss: 4.304367]\n",
      "epoch:9 step:7684 [D loss: 0.048626, acc.: 99.22%] [G loss: 4.369180]\n",
      "epoch:9 step:7685 [D loss: 0.037695, acc.: 100.00%] [G loss: 4.310158]\n",
      "epoch:9 step:7686 [D loss: 0.023948, acc.: 100.00%] [G loss: 3.037840]\n",
      "epoch:9 step:7687 [D loss: 0.156658, acc.: 96.09%] [G loss: 3.790991]\n",
      "epoch:9 step:7688 [D loss: 0.102816, acc.: 96.88%] [G loss: 1.005656]\n",
      "epoch:9 step:7689 [D loss: 0.082981, acc.: 98.44%] [G loss: 4.112712]\n",
      "epoch:9 step:7690 [D loss: 0.132175, acc.: 95.31%] [G loss: 3.764288]\n",
      "epoch:9 step:7691 [D loss: 0.038889, acc.: 99.22%] [G loss: 2.599519]\n",
      "epoch:9 step:7692 [D loss: 0.036687, acc.: 99.22%] [G loss: 4.360166]\n",
      "epoch:9 step:7693 [D loss: 0.041373, acc.: 100.00%] [G loss: 1.238856]\n",
      "epoch:9 step:7694 [D loss: 0.056532, acc.: 98.44%] [G loss: 1.200152]\n",
      "epoch:9 step:7695 [D loss: 0.245264, acc.: 88.28%] [G loss: 5.632020]\n",
      "epoch:9 step:7696 [D loss: 0.594436, acc.: 69.53%] [G loss: 0.872280]\n",
      "epoch:9 step:7697 [D loss: 0.745724, acc.: 65.62%] [G loss: 8.223362]\n",
      "epoch:9 step:7698 [D loss: 1.472326, acc.: 50.78%] [G loss: 4.758067]\n",
      "epoch:9 step:7699 [D loss: 0.085914, acc.: 98.44%] [G loss: 2.125717]\n",
      "epoch:9 step:7700 [D loss: 0.161622, acc.: 96.09%] [G loss: 3.590693]\n",
      "epoch:9 step:7701 [D loss: 0.078661, acc.: 99.22%] [G loss: 4.975437]\n",
      "epoch:9 step:7702 [D loss: 0.160969, acc.: 95.31%] [G loss: 3.031329]\n",
      "epoch:9 step:7703 [D loss: 0.274250, acc.: 88.28%] [G loss: 3.753443]\n",
      "epoch:9 step:7704 [D loss: 0.131056, acc.: 95.31%] [G loss: 4.776556]\n",
      "epoch:9 step:7705 [D loss: 0.165021, acc.: 92.97%] [G loss: 1.631256]\n",
      "epoch:9 step:7706 [D loss: 0.120597, acc.: 97.66%] [G loss: 3.108397]\n",
      "epoch:9 step:7707 [D loss: 0.151454, acc.: 95.31%] [G loss: 3.855793]\n",
      "epoch:9 step:7708 [D loss: 0.055870, acc.: 99.22%] [G loss: 1.445406]\n",
      "epoch:9 step:7709 [D loss: 0.157981, acc.: 95.31%] [G loss: 3.122724]\n",
      "epoch:9 step:7710 [D loss: 0.063531, acc.: 99.22%] [G loss: 2.406848]\n",
      "epoch:9 step:7711 [D loss: 0.152937, acc.: 96.09%] [G loss: 1.661364]\n",
      "epoch:9 step:7712 [D loss: 0.103974, acc.: 98.44%] [G loss: 2.639377]\n",
      "epoch:9 step:7713 [D loss: 0.072054, acc.: 98.44%] [G loss: 2.709275]\n",
      "epoch:9 step:7714 [D loss: 0.297512, acc.: 89.06%] [G loss: 2.948012]\n",
      "epoch:9 step:7715 [D loss: 0.084566, acc.: 97.66%] [G loss: 2.111852]\n",
      "epoch:9 step:7716 [D loss: 0.093754, acc.: 98.44%] [G loss: 2.699953]\n",
      "epoch:9 step:7717 [D loss: 0.052336, acc.: 98.44%] [G loss: 2.283235]\n",
      "epoch:9 step:7718 [D loss: 0.085279, acc.: 100.00%] [G loss: 1.351077]\n",
      "epoch:9 step:7719 [D loss: 0.032970, acc.: 100.00%] [G loss: 3.275709]\n",
      "epoch:9 step:7720 [D loss: 0.047934, acc.: 99.22%] [G loss: 1.374682]\n",
      "epoch:9 step:7721 [D loss: 0.629753, acc.: 69.53%] [G loss: 7.328134]\n",
      "epoch:9 step:7722 [D loss: 0.264843, acc.: 89.06%] [G loss: 5.465991]\n",
      "epoch:9 step:7723 [D loss: 0.038553, acc.: 99.22%] [G loss: 5.709651]\n",
      "epoch:9 step:7724 [D loss: 0.009615, acc.: 100.00%] [G loss: 3.565212]\n",
      "epoch:9 step:7725 [D loss: 0.133330, acc.: 95.31%] [G loss: 5.667605]\n",
      "epoch:9 step:7726 [D loss: 0.017139, acc.: 99.22%] [G loss: 6.855514]\n",
      "epoch:9 step:7727 [D loss: 0.037352, acc.: 100.00%] [G loss: 4.810973]\n",
      "epoch:9 step:7728 [D loss: 0.049169, acc.: 99.22%] [G loss: 3.694520]\n",
      "epoch:9 step:7729 [D loss: 0.081235, acc.: 98.44%] [G loss: 4.204075]\n",
      "epoch:9 step:7730 [D loss: 0.056065, acc.: 100.00%] [G loss: 5.391547]\n",
      "epoch:9 step:7731 [D loss: 0.054836, acc.: 99.22%] [G loss: 4.727168]\n",
      "epoch:9 step:7732 [D loss: 0.045546, acc.: 99.22%] [G loss: 3.348817]\n",
      "epoch:9 step:7733 [D loss: 0.254927, acc.: 89.84%] [G loss: 7.337246]\n",
      "epoch:9 step:7734 [D loss: 1.847366, acc.: 18.75%] [G loss: 8.044088]\n",
      "epoch:9 step:7735 [D loss: 0.051295, acc.: 99.22%] [G loss: 9.546457]\n",
      "epoch:9 step:7736 [D loss: 1.098259, acc.: 48.44%] [G loss: 2.927601]\n",
      "epoch:9 step:7737 [D loss: 0.208098, acc.: 89.84%] [G loss: 6.106061]\n",
      "epoch:9 step:7738 [D loss: 0.196458, acc.: 89.84%] [G loss: 4.619931]\n",
      "epoch:9 step:7739 [D loss: 0.097052, acc.: 96.09%] [G loss: 3.882320]\n",
      "epoch:9 step:7740 [D loss: 0.016081, acc.: 100.00%] [G loss: 4.135609]\n",
      "epoch:9 step:7741 [D loss: 0.077665, acc.: 98.44%] [G loss: 4.437202]\n",
      "epoch:9 step:7742 [D loss: 0.426126, acc.: 79.69%] [G loss: 4.081252]\n",
      "epoch:9 step:7743 [D loss: 0.073168, acc.: 98.44%] [G loss: 4.330907]\n",
      "epoch:9 step:7744 [D loss: 0.236758, acc.: 88.28%] [G loss: 2.504119]\n",
      "epoch:9 step:7745 [D loss: 0.147735, acc.: 94.53%] [G loss: 4.910222]\n",
      "epoch:9 step:7746 [D loss: 0.030058, acc.: 99.22%] [G loss: 5.494029]\n",
      "epoch:9 step:7747 [D loss: 0.250926, acc.: 90.62%] [G loss: 1.484963]\n",
      "epoch:9 step:7748 [D loss: 0.346102, acc.: 78.91%] [G loss: 5.732807]\n",
      "epoch:9 step:7749 [D loss: 0.047442, acc.: 97.66%] [G loss: 6.594862]\n",
      "epoch:9 step:7750 [D loss: 0.530322, acc.: 75.78%] [G loss: 2.058390]\n",
      "epoch:9 step:7751 [D loss: 0.600217, acc.: 70.31%] [G loss: 6.997061]\n",
      "epoch:9 step:7752 [D loss: 0.156929, acc.: 92.97%] [G loss: 7.317665]\n",
      "epoch:9 step:7753 [D loss: 0.096518, acc.: 96.88%] [G loss: 5.313643]\n",
      "epoch:9 step:7754 [D loss: 0.120756, acc.: 95.31%] [G loss: 3.095269]\n",
      "epoch:9 step:7755 [D loss: 0.094152, acc.: 96.88%] [G loss: 4.463461]\n",
      "epoch:9 step:7756 [D loss: 0.048993, acc.: 97.66%] [G loss: 2.914041]\n",
      "epoch:9 step:7757 [D loss: 0.071116, acc.: 98.44%] [G loss: 3.619604]\n",
      "epoch:9 step:7758 [D loss: 0.045770, acc.: 100.00%] [G loss: 3.874677]\n",
      "epoch:9 step:7759 [D loss: 0.148072, acc.: 96.09%] [G loss: 3.989787]\n",
      "epoch:9 step:7760 [D loss: 0.107667, acc.: 98.44%] [G loss: 4.301578]\n",
      "epoch:9 step:7761 [D loss: 0.084025, acc.: 98.44%] [G loss: 3.736313]\n",
      "epoch:9 step:7762 [D loss: 0.108083, acc.: 97.66%] [G loss: 4.300116]\n",
      "epoch:9 step:7763 [D loss: 0.036985, acc.: 100.00%] [G loss: 3.724900]\n",
      "epoch:9 step:7764 [D loss: 0.689507, acc.: 64.06%] [G loss: 4.170521]\n",
      "epoch:9 step:7765 [D loss: 0.061585, acc.: 100.00%] [G loss: 4.652523]\n",
      "epoch:9 step:7766 [D loss: 0.471947, acc.: 78.12%] [G loss: 0.612717]\n",
      "epoch:9 step:7767 [D loss: 0.169228, acc.: 94.53%] [G loss: 2.159986]\n",
      "epoch:9 step:7768 [D loss: 0.062497, acc.: 98.44%] [G loss: 2.031116]\n",
      "epoch:9 step:7769 [D loss: 0.315317, acc.: 86.72%] [G loss: 3.916639]\n",
      "epoch:9 step:7770 [D loss: 0.113425, acc.: 95.31%] [G loss: 2.737347]\n",
      "epoch:9 step:7771 [D loss: 0.125098, acc.: 96.09%] [G loss: 2.532064]\n",
      "epoch:9 step:7772 [D loss: 0.040949, acc.: 99.22%] [G loss: 2.558261]\n",
      "epoch:9 step:7773 [D loss: 0.046106, acc.: 98.44%] [G loss: 0.931512]\n",
      "epoch:9 step:7774 [D loss: 0.422708, acc.: 78.12%] [G loss: 6.380724]\n",
      "epoch:9 step:7775 [D loss: 1.350259, acc.: 55.47%] [G loss: 3.859441]\n",
      "epoch:9 step:7776 [D loss: 0.158281, acc.: 93.75%] [G loss: 3.570920]\n",
      "epoch:9 step:7777 [D loss: 0.025116, acc.: 100.00%] [G loss: 4.275164]\n",
      "epoch:9 step:7778 [D loss: 0.076696, acc.: 98.44%] [G loss: 3.026412]\n",
      "epoch:9 step:7779 [D loss: 0.063675, acc.: 98.44%] [G loss: 2.162613]\n",
      "epoch:9 step:7780 [D loss: 0.081135, acc.: 99.22%] [G loss: 3.813965]\n",
      "epoch:9 step:7781 [D loss: 0.031343, acc.: 100.00%] [G loss: 3.508479]\n",
      "epoch:9 step:7782 [D loss: 0.072353, acc.: 99.22%] [G loss: 3.291318]\n",
      "epoch:9 step:7783 [D loss: 0.257694, acc.: 91.41%] [G loss: 3.140176]\n",
      "epoch:9 step:7784 [D loss: 0.017245, acc.: 100.00%] [G loss: 2.999515]\n",
      "epoch:9 step:7785 [D loss: 0.080764, acc.: 98.44%] [G loss: 3.018144]\n",
      "epoch:9 step:7786 [D loss: 0.134913, acc.: 96.88%] [G loss: 4.228932]\n",
      "epoch:9 step:7787 [D loss: 0.058122, acc.: 99.22%] [G loss: 2.166189]\n",
      "epoch:9 step:7788 [D loss: 0.141390, acc.: 97.66%] [G loss: 1.714130]\n",
      "epoch:9 step:7789 [D loss: 0.214115, acc.: 91.41%] [G loss: 5.554037]\n",
      "epoch:9 step:7790 [D loss: 0.079082, acc.: 98.44%] [G loss: 3.365479]\n",
      "epoch:9 step:7791 [D loss: 0.268450, acc.: 88.28%] [G loss: 2.813579]\n",
      "epoch:9 step:7792 [D loss: 0.036835, acc.: 99.22%] [G loss: 0.620915]\n",
      "epoch:9 step:7793 [D loss: 0.006858, acc.: 100.00%] [G loss: 2.879244]\n",
      "epoch:9 step:7794 [D loss: 0.112829, acc.: 96.88%] [G loss: 4.898412]\n",
      "epoch:9 step:7795 [D loss: 0.092498, acc.: 96.88%] [G loss: 2.026084]\n",
      "epoch:9 step:7796 [D loss: 0.076026, acc.: 97.66%] [G loss: 3.437241]\n",
      "epoch:9 step:7797 [D loss: 0.087307, acc.: 97.66%] [G loss: 4.032574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7798 [D loss: 0.068598, acc.: 99.22%] [G loss: 3.261950]\n",
      "epoch:9 step:7799 [D loss: 0.055462, acc.: 97.66%] [G loss: 1.988393]\n",
      "epoch:9 step:7800 [D loss: 0.637174, acc.: 67.19%] [G loss: 8.469196]\n",
      "epoch:9 step:7801 [D loss: 2.457054, acc.: 50.78%] [G loss: 2.439535]\n",
      "epoch:9 step:7802 [D loss: 0.308448, acc.: 87.50%] [G loss: 1.571299]\n",
      "epoch:9 step:7803 [D loss: 0.052569, acc.: 100.00%] [G loss: 2.991168]\n",
      "epoch:9 step:7804 [D loss: 0.144555, acc.: 96.09%] [G loss: 2.261334]\n",
      "epoch:9 step:7805 [D loss: 0.054878, acc.: 100.00%] [G loss: 2.460845]\n",
      "epoch:9 step:7806 [D loss: 0.089996, acc.: 97.66%] [G loss: 1.072409]\n",
      "epoch:9 step:7807 [D loss: 0.099923, acc.: 99.22%] [G loss: 1.832061]\n",
      "epoch:9 step:7808 [D loss: 0.457177, acc.: 83.59%] [G loss: 3.036446]\n",
      "epoch:9 step:7809 [D loss: 0.315820, acc.: 82.81%] [G loss: 0.788520]\n",
      "epoch:9 step:7810 [D loss: 0.579934, acc.: 74.22%] [G loss: 4.886768]\n",
      "epoch:10 step:7811 [D loss: 0.134139, acc.: 95.31%] [G loss: 7.285615]\n",
      "epoch:10 step:7812 [D loss: 0.428575, acc.: 80.47%] [G loss: 2.396689]\n",
      "epoch:10 step:7813 [D loss: 0.107272, acc.: 96.88%] [G loss: 3.161814]\n",
      "epoch:10 step:7814 [D loss: 0.098507, acc.: 96.88%] [G loss: 4.053877]\n",
      "epoch:10 step:7815 [D loss: 0.053912, acc.: 99.22%] [G loss: 1.189221]\n",
      "epoch:10 step:7816 [D loss: 0.019806, acc.: 100.00%] [G loss: 2.151665]\n",
      "epoch:10 step:7817 [D loss: 0.079069, acc.: 99.22%] [G loss: 4.910687]\n",
      "epoch:10 step:7818 [D loss: 0.200488, acc.: 92.97%] [G loss: 3.484969]\n",
      "epoch:10 step:7819 [D loss: 0.200548, acc.: 92.97%] [G loss: 4.236052]\n",
      "epoch:10 step:7820 [D loss: 0.090480, acc.: 96.09%] [G loss: 4.889305]\n",
      "epoch:10 step:7821 [D loss: 0.011448, acc.: 100.00%] [G loss: 0.464516]\n",
      "epoch:10 step:7822 [D loss: 0.024563, acc.: 100.00%] [G loss: 0.144192]\n",
      "epoch:10 step:7823 [D loss: 0.018566, acc.: 100.00%] [G loss: 0.609678]\n",
      "epoch:10 step:7824 [D loss: 0.125936, acc.: 96.88%] [G loss: 0.670983]\n",
      "epoch:10 step:7825 [D loss: 0.036094, acc.: 100.00%] [G loss: 3.795608]\n",
      "epoch:10 step:7826 [D loss: 0.150140, acc.: 93.75%] [G loss: 1.176296]\n",
      "epoch:10 step:7827 [D loss: 0.110322, acc.: 96.88%] [G loss: 0.698185]\n",
      "epoch:10 step:7828 [D loss: 0.014272, acc.: 100.00%] [G loss: 2.500781]\n",
      "epoch:10 step:7829 [D loss: 0.023508, acc.: 100.00%] [G loss: 2.313682]\n",
      "epoch:10 step:7830 [D loss: 0.029305, acc.: 99.22%] [G loss: 0.517694]\n",
      "epoch:10 step:7831 [D loss: 0.099212, acc.: 99.22%] [G loss: 0.646804]\n",
      "epoch:10 step:7832 [D loss: 0.042267, acc.: 100.00%] [G loss: 0.776972]\n",
      "epoch:10 step:7833 [D loss: 0.041191, acc.: 100.00%] [G loss: 1.595665]\n",
      "epoch:10 step:7834 [D loss: 0.054187, acc.: 99.22%] [G loss: 1.555421]\n",
      "epoch:10 step:7835 [D loss: 0.070600, acc.: 99.22%] [G loss: 1.180585]\n",
      "epoch:10 step:7836 [D loss: 0.022012, acc.: 100.00%] [G loss: 0.962677]\n",
      "epoch:10 step:7837 [D loss: 0.395211, acc.: 76.56%] [G loss: 7.142067]\n",
      "epoch:10 step:7838 [D loss: 0.694088, acc.: 66.41%] [G loss: 4.084943]\n",
      "epoch:10 step:7839 [D loss: 0.153585, acc.: 91.41%] [G loss: 4.597106]\n",
      "epoch:10 step:7840 [D loss: 0.002939, acc.: 100.00%] [G loss: 6.555840]\n",
      "epoch:10 step:7841 [D loss: 0.003390, acc.: 100.00%] [G loss: 6.360054]\n",
      "epoch:10 step:7842 [D loss: 0.012363, acc.: 100.00%] [G loss: 5.447414]\n",
      "epoch:10 step:7843 [D loss: 0.022919, acc.: 100.00%] [G loss: 3.616903]\n",
      "epoch:10 step:7844 [D loss: 0.008781, acc.: 100.00%] [G loss: 3.913326]\n",
      "epoch:10 step:7845 [D loss: 0.022018, acc.: 100.00%] [G loss: 3.678870]\n",
      "epoch:10 step:7846 [D loss: 0.078172, acc.: 100.00%] [G loss: 1.893922]\n",
      "epoch:10 step:7847 [D loss: 0.008993, acc.: 100.00%] [G loss: 2.862617]\n",
      "epoch:10 step:7848 [D loss: 0.092512, acc.: 98.44%] [G loss: 0.515563]\n",
      "epoch:10 step:7849 [D loss: 0.302026, acc.: 88.28%] [G loss: 5.667986]\n",
      "epoch:10 step:7850 [D loss: 0.209424, acc.: 91.41%] [G loss: 2.077845]\n",
      "epoch:10 step:7851 [D loss: 0.040676, acc.: 100.00%] [G loss: 3.672479]\n",
      "epoch:10 step:7852 [D loss: 0.066642, acc.: 97.66%] [G loss: 1.590966]\n",
      "epoch:10 step:7853 [D loss: 0.184327, acc.: 90.62%] [G loss: 5.337430]\n",
      "epoch:10 step:7854 [D loss: 0.255142, acc.: 88.28%] [G loss: 0.644527]\n",
      "epoch:10 step:7855 [D loss: 0.530023, acc.: 71.09%] [G loss: 8.407354]\n",
      "epoch:10 step:7856 [D loss: 2.048138, acc.: 51.56%] [G loss: 4.045154]\n",
      "epoch:10 step:7857 [D loss: 0.085819, acc.: 97.66%] [G loss: 2.547639]\n",
      "epoch:10 step:7858 [D loss: 0.186680, acc.: 92.97%] [G loss: 3.869429]\n",
      "epoch:10 step:7859 [D loss: 0.143832, acc.: 95.31%] [G loss: 3.964429]\n",
      "epoch:10 step:7860 [D loss: 0.232297, acc.: 89.06%] [G loss: 2.423242]\n",
      "epoch:10 step:7861 [D loss: 0.223422, acc.: 89.06%] [G loss: 4.405777]\n",
      "epoch:10 step:7862 [D loss: 0.114346, acc.: 95.31%] [G loss: 5.102637]\n",
      "epoch:10 step:7863 [D loss: 0.066705, acc.: 98.44%] [G loss: 3.638234]\n",
      "epoch:10 step:7864 [D loss: 0.108721, acc.: 97.66%] [G loss: 3.359566]\n",
      "epoch:10 step:7865 [D loss: 0.128990, acc.: 93.75%] [G loss: 4.552051]\n",
      "epoch:10 step:7866 [D loss: 0.069917, acc.: 99.22%] [G loss: 4.112642]\n",
      "epoch:10 step:7867 [D loss: 0.047420, acc.: 100.00%] [G loss: 3.110605]\n",
      "epoch:10 step:7868 [D loss: 0.033180, acc.: 99.22%] [G loss: 2.618074]\n",
      "epoch:10 step:7869 [D loss: 0.091253, acc.: 97.66%] [G loss: 2.980758]\n",
      "epoch:10 step:7870 [D loss: 0.080591, acc.: 97.66%] [G loss: 3.569094]\n",
      "epoch:10 step:7871 [D loss: 0.388148, acc.: 82.03%] [G loss: 5.697661]\n",
      "epoch:10 step:7872 [D loss: 0.230093, acc.: 84.38%] [G loss: 3.309615]\n",
      "epoch:10 step:7873 [D loss: 0.074454, acc.: 97.66%] [G loss: 2.579585]\n",
      "epoch:10 step:7874 [D loss: 0.037491, acc.: 100.00%] [G loss: 1.415271]\n",
      "epoch:10 step:7875 [D loss: 0.506964, acc.: 76.56%] [G loss: 4.766120]\n",
      "epoch:10 step:7876 [D loss: 0.114327, acc.: 95.31%] [G loss: 4.024289]\n",
      "epoch:10 step:7877 [D loss: 0.134664, acc.: 96.09%] [G loss: 2.333121]\n",
      "epoch:10 step:7878 [D loss: 0.048477, acc.: 100.00%] [G loss: 2.337605]\n",
      "epoch:10 step:7879 [D loss: 0.037036, acc.: 100.00%] [G loss: 2.401726]\n",
      "epoch:10 step:7880 [D loss: 0.026027, acc.: 100.00%] [G loss: 2.892422]\n",
      "epoch:10 step:7881 [D loss: 0.056427, acc.: 98.44%] [G loss: 2.629369]\n",
      "epoch:10 step:7882 [D loss: 0.070640, acc.: 99.22%] [G loss: 4.028862]\n",
      "epoch:10 step:7883 [D loss: 0.253498, acc.: 90.62%] [G loss: 3.297220]\n",
      "epoch:10 step:7884 [D loss: 0.050857, acc.: 100.00%] [G loss: 3.589704]\n",
      "epoch:10 step:7885 [D loss: 0.029749, acc.: 99.22%] [G loss: 3.265117]\n",
      "epoch:10 step:7886 [D loss: 0.027737, acc.: 100.00%] [G loss: 2.010002]\n",
      "epoch:10 step:7887 [D loss: 0.244188, acc.: 88.28%] [G loss: 4.851008]\n",
      "epoch:10 step:7888 [D loss: 0.125514, acc.: 93.75%] [G loss: 4.701785]\n",
      "epoch:10 step:7889 [D loss: 0.037889, acc.: 99.22%] [G loss: 3.144755]\n",
      "epoch:10 step:7890 [D loss: 0.034690, acc.: 100.00%] [G loss: 3.476913]\n",
      "epoch:10 step:7891 [D loss: 0.142816, acc.: 95.31%] [G loss: 5.683621]\n",
      "epoch:10 step:7892 [D loss: 0.038001, acc.: 100.00%] [G loss: 6.464309]\n",
      "epoch:10 step:7893 [D loss: 0.202325, acc.: 90.62%] [G loss: 1.736162]\n",
      "epoch:10 step:7894 [D loss: 0.086778, acc.: 96.88%] [G loss: 3.074713]\n",
      "epoch:10 step:7895 [D loss: 0.051811, acc.: 99.22%] [G loss: 4.144533]\n",
      "epoch:10 step:7896 [D loss: 0.032421, acc.: 99.22%] [G loss: 4.190814]\n",
      "epoch:10 step:7897 [D loss: 0.010133, acc.: 100.00%] [G loss: 3.720873]\n",
      "epoch:10 step:7898 [D loss: 0.123990, acc.: 96.88%] [G loss: 3.034471]\n",
      "epoch:10 step:7899 [D loss: 0.050085, acc.: 100.00%] [G loss: 4.589302]\n",
      "epoch:10 step:7900 [D loss: 0.874374, acc.: 54.69%] [G loss: 6.794877]\n",
      "epoch:10 step:7901 [D loss: 0.373776, acc.: 83.59%] [G loss: 4.912220]\n",
      "epoch:10 step:7902 [D loss: 0.013412, acc.: 100.00%] [G loss: 3.663198]\n",
      "epoch:10 step:7903 [D loss: 0.081522, acc.: 97.66%] [G loss: 4.741230]\n",
      "epoch:10 step:7904 [D loss: 0.057518, acc.: 98.44%] [G loss: 5.455315]\n",
      "epoch:10 step:7905 [D loss: 0.155687, acc.: 96.09%] [G loss: 4.394798]\n",
      "epoch:10 step:7906 [D loss: 0.047371, acc.: 100.00%] [G loss: 3.827219]\n",
      "epoch:10 step:7907 [D loss: 0.085622, acc.: 97.66%] [G loss: 3.868638]\n",
      "epoch:10 step:7908 [D loss: 0.035004, acc.: 100.00%] [G loss: 4.917521]\n",
      "epoch:10 step:7909 [D loss: 0.090207, acc.: 98.44%] [G loss: 3.638460]\n",
      "epoch:10 step:7910 [D loss: 0.093252, acc.: 97.66%] [G loss: 4.816940]\n",
      "epoch:10 step:7911 [D loss: 0.015609, acc.: 100.00%] [G loss: 2.454179]\n",
      "epoch:10 step:7912 [D loss: 0.111338, acc.: 96.09%] [G loss: 2.723133]\n",
      "epoch:10 step:7913 [D loss: 0.223825, acc.: 88.28%] [G loss: 3.488595]\n",
      "epoch:10 step:7914 [D loss: 0.783887, acc.: 60.16%] [G loss: 2.872078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7915 [D loss: 0.050024, acc.: 100.00%] [G loss: 6.953447]\n",
      "epoch:10 step:7916 [D loss: 0.010842, acc.: 100.00%] [G loss: 6.084110]\n",
      "epoch:10 step:7917 [D loss: 0.311204, acc.: 91.41%] [G loss: 6.125204]\n",
      "epoch:10 step:7918 [D loss: 0.423011, acc.: 78.91%] [G loss: 1.042204]\n",
      "epoch:10 step:7919 [D loss: 0.455479, acc.: 76.56%] [G loss: 5.799919]\n",
      "epoch:10 step:7920 [D loss: 0.840041, acc.: 64.84%] [G loss: 4.510848]\n",
      "epoch:10 step:7921 [D loss: 0.125842, acc.: 95.31%] [G loss: 3.317880]\n",
      "epoch:10 step:7922 [D loss: 0.016897, acc.: 100.00%] [G loss: 3.401408]\n",
      "epoch:10 step:7923 [D loss: 0.029553, acc.: 99.22%] [G loss: 3.165312]\n",
      "epoch:10 step:7924 [D loss: 0.159514, acc.: 96.09%] [G loss: 2.585016]\n",
      "epoch:10 step:7925 [D loss: 0.058123, acc.: 100.00%] [G loss: 5.506344]\n",
      "epoch:10 step:7926 [D loss: 0.191981, acc.: 91.41%] [G loss: 2.901748]\n",
      "epoch:10 step:7927 [D loss: 0.318359, acc.: 86.72%] [G loss: 7.174462]\n",
      "epoch:10 step:7928 [D loss: 0.275241, acc.: 83.59%] [G loss: 5.303630]\n",
      "epoch:10 step:7929 [D loss: 0.041859, acc.: 100.00%] [G loss: 4.157204]\n",
      "epoch:10 step:7930 [D loss: 0.055881, acc.: 98.44%] [G loss: 5.640623]\n",
      "epoch:10 step:7931 [D loss: 0.089471, acc.: 98.44%] [G loss: 5.692339]\n",
      "epoch:10 step:7932 [D loss: 0.085167, acc.: 95.31%] [G loss: 5.206237]\n",
      "epoch:10 step:7933 [D loss: 0.051048, acc.: 100.00%] [G loss: 2.814918]\n",
      "epoch:10 step:7934 [D loss: 0.030836, acc.: 100.00%] [G loss: 4.289881]\n",
      "epoch:10 step:7935 [D loss: 0.044500, acc.: 99.22%] [G loss: 3.602522]\n",
      "epoch:10 step:7936 [D loss: 0.020957, acc.: 100.00%] [G loss: 3.238560]\n",
      "epoch:10 step:7937 [D loss: 0.097847, acc.: 97.66%] [G loss: 5.766764]\n",
      "epoch:10 step:7938 [D loss: 0.173840, acc.: 92.97%] [G loss: 2.839467]\n",
      "epoch:10 step:7939 [D loss: 0.152539, acc.: 92.97%] [G loss: 5.015049]\n",
      "epoch:10 step:7940 [D loss: 0.031730, acc.: 100.00%] [G loss: 6.160072]\n",
      "epoch:10 step:7941 [D loss: 0.019965, acc.: 99.22%] [G loss: 4.850351]\n",
      "epoch:10 step:7942 [D loss: 0.820785, acc.: 58.59%] [G loss: 6.545523]\n",
      "epoch:10 step:7943 [D loss: 0.067690, acc.: 96.09%] [G loss: 9.285633]\n",
      "epoch:10 step:7944 [D loss: 0.382733, acc.: 85.94%] [G loss: 4.780007]\n",
      "epoch:10 step:7945 [D loss: 0.514211, acc.: 77.34%] [G loss: 5.063854]\n",
      "epoch:10 step:7946 [D loss: 0.046937, acc.: 98.44%] [G loss: 7.696669]\n",
      "epoch:10 step:7947 [D loss: 0.289741, acc.: 85.94%] [G loss: 3.857190]\n",
      "epoch:10 step:7948 [D loss: 0.013382, acc.: 100.00%] [G loss: 5.009706]\n",
      "epoch:10 step:7949 [D loss: 0.113404, acc.: 96.88%] [G loss: 5.928741]\n",
      "epoch:10 step:7950 [D loss: 0.008605, acc.: 100.00%] [G loss: 4.302465]\n",
      "epoch:10 step:7951 [D loss: 0.026203, acc.: 100.00%] [G loss: 5.949440]\n",
      "epoch:10 step:7952 [D loss: 0.101865, acc.: 97.66%] [G loss: 5.124542]\n",
      "epoch:10 step:7953 [D loss: 0.042205, acc.: 97.66%] [G loss: 4.810529]\n",
      "epoch:10 step:7954 [D loss: 0.095084, acc.: 97.66%] [G loss: 4.225795]\n",
      "epoch:10 step:7955 [D loss: 0.049425, acc.: 98.44%] [G loss: 2.995824]\n",
      "epoch:10 step:7956 [D loss: 0.045813, acc.: 98.44%] [G loss: 1.409062]\n",
      "epoch:10 step:7957 [D loss: 0.119395, acc.: 96.88%] [G loss: 5.544189]\n",
      "epoch:10 step:7958 [D loss: 0.114673, acc.: 96.09%] [G loss: 4.619174]\n",
      "epoch:10 step:7959 [D loss: 0.037726, acc.: 99.22%] [G loss: 5.306173]\n",
      "epoch:10 step:7960 [D loss: 0.016112, acc.: 100.00%] [G loss: 3.963270]\n",
      "epoch:10 step:7961 [D loss: 0.063181, acc.: 99.22%] [G loss: 3.439377]\n",
      "epoch:10 step:7962 [D loss: 0.066092, acc.: 98.44%] [G loss: 3.560374]\n",
      "epoch:10 step:7963 [D loss: 0.632331, acc.: 68.75%] [G loss: 6.442599]\n",
      "epoch:10 step:7964 [D loss: 1.144332, acc.: 57.03%] [G loss: 2.476379]\n",
      "epoch:10 step:7965 [D loss: 0.113385, acc.: 96.09%] [G loss: 2.469938]\n",
      "epoch:10 step:7966 [D loss: 0.006458, acc.: 100.00%] [G loss: 4.630561]\n",
      "epoch:10 step:7967 [D loss: 0.083673, acc.: 97.66%] [G loss: 1.314317]\n",
      "epoch:10 step:7968 [D loss: 0.066141, acc.: 98.44%] [G loss: 2.737114]\n",
      "epoch:10 step:7969 [D loss: 0.033660, acc.: 99.22%] [G loss: 1.597117]\n",
      "epoch:10 step:7970 [D loss: 0.048461, acc.: 98.44%] [G loss: 4.007369]\n",
      "epoch:10 step:7971 [D loss: 0.223769, acc.: 89.06%] [G loss: 0.003871]\n",
      "epoch:10 step:7972 [D loss: 0.673555, acc.: 68.75%] [G loss: 6.976974]\n",
      "epoch:10 step:7973 [D loss: 1.234042, acc.: 55.47%] [G loss: 2.039051]\n",
      "epoch:10 step:7974 [D loss: 0.068639, acc.: 98.44%] [G loss: 1.085051]\n",
      "epoch:10 step:7975 [D loss: 0.021087, acc.: 100.00%] [G loss: 1.271225]\n",
      "epoch:10 step:7976 [D loss: 0.041415, acc.: 99.22%] [G loss: 1.382747]\n",
      "epoch:10 step:7977 [D loss: 0.036455, acc.: 99.22%] [G loss: 2.220783]\n",
      "epoch:10 step:7978 [D loss: 0.090424, acc.: 96.88%] [G loss: 1.998695]\n",
      "epoch:10 step:7979 [D loss: 0.014327, acc.: 100.00%] [G loss: 3.604664]\n",
      "epoch:10 step:7980 [D loss: 0.152595, acc.: 94.53%] [G loss: 2.802516]\n",
      "epoch:10 step:7981 [D loss: 0.374477, acc.: 82.81%] [G loss: 6.991176]\n",
      "epoch:10 step:7982 [D loss: 1.111173, acc.: 53.12%] [G loss: 4.036423]\n",
      "epoch:10 step:7983 [D loss: 0.231211, acc.: 92.19%] [G loss: 4.482853]\n",
      "epoch:10 step:7984 [D loss: 0.016072, acc.: 100.00%] [G loss: 5.780241]\n",
      "epoch:10 step:7985 [D loss: 0.063574, acc.: 96.88%] [G loss: 4.124276]\n",
      "epoch:10 step:7986 [D loss: 0.096113, acc.: 97.66%] [G loss: 3.427336]\n",
      "epoch:10 step:7987 [D loss: 0.027021, acc.: 100.00%] [G loss: 3.205104]\n",
      "epoch:10 step:7988 [D loss: 0.108231, acc.: 97.66%] [G loss: 2.170308]\n",
      "epoch:10 step:7989 [D loss: 0.138499, acc.: 96.09%] [G loss: 4.367183]\n",
      "epoch:10 step:7990 [D loss: 0.065989, acc.: 98.44%] [G loss: 4.155063]\n",
      "epoch:10 step:7991 [D loss: 0.029204, acc.: 100.00%] [G loss: 3.660790]\n",
      "epoch:10 step:7992 [D loss: 0.057033, acc.: 100.00%] [G loss: 2.859073]\n",
      "epoch:10 step:7993 [D loss: 0.034959, acc.: 100.00%] [G loss: 3.853311]\n",
      "epoch:10 step:7994 [D loss: 0.037781, acc.: 100.00%] [G loss: 3.119099]\n",
      "epoch:10 step:7995 [D loss: 0.043183, acc.: 100.00%] [G loss: 3.686390]\n",
      "epoch:10 step:7996 [D loss: 0.032584, acc.: 100.00%] [G loss: 2.974845]\n",
      "epoch:10 step:7997 [D loss: 0.595090, acc.: 64.84%] [G loss: 7.640859]\n",
      "epoch:10 step:7998 [D loss: 0.528298, acc.: 75.78%] [G loss: 3.678475]\n",
      "epoch:10 step:7999 [D loss: 0.047264, acc.: 99.22%] [G loss: 2.241448]\n",
      "epoch:10 step:8000 [D loss: 0.011043, acc.: 100.00%] [G loss: 1.848784]\n",
      "epoch:10 step:8001 [D loss: 0.009778, acc.: 100.00%] [G loss: 1.847481]\n",
      "epoch:10 step:8002 [D loss: 0.102861, acc.: 96.88%] [G loss: 3.120672]\n",
      "epoch:10 step:8003 [D loss: 0.003823, acc.: 100.00%] [G loss: 4.028160]\n",
      "epoch:10 step:8004 [D loss: 0.003332, acc.: 100.00%] [G loss: 3.120812]\n",
      "epoch:10 step:8005 [D loss: 0.012034, acc.: 100.00%] [G loss: 3.001210]\n",
      "epoch:10 step:8006 [D loss: 0.038582, acc.: 99.22%] [G loss: 2.961381]\n",
      "epoch:10 step:8007 [D loss: 0.018248, acc.: 100.00%] [G loss: 1.792212]\n",
      "epoch:10 step:8008 [D loss: 0.014576, acc.: 100.00%] [G loss: 2.111327]\n",
      "epoch:10 step:8009 [D loss: 0.043293, acc.: 99.22%] [G loss: 1.804729]\n",
      "epoch:10 step:8010 [D loss: 0.037780, acc.: 100.00%] [G loss: 0.564803]\n",
      "epoch:10 step:8011 [D loss: 0.247049, acc.: 90.62%] [G loss: 5.200508]\n",
      "epoch:10 step:8012 [D loss: 0.813855, acc.: 58.59%] [G loss: 1.463885]\n",
      "epoch:10 step:8013 [D loss: 0.019917, acc.: 100.00%] [G loss: 2.081421]\n",
      "epoch:10 step:8014 [D loss: 0.021703, acc.: 100.00%] [G loss: 2.090896]\n",
      "epoch:10 step:8015 [D loss: 0.087691, acc.: 96.88%] [G loss: 2.938790]\n",
      "epoch:10 step:8016 [D loss: 0.011589, acc.: 100.00%] [G loss: 3.484418]\n",
      "epoch:10 step:8017 [D loss: 0.046753, acc.: 98.44%] [G loss: 2.953983]\n",
      "epoch:10 step:8018 [D loss: 0.270953, acc.: 91.41%] [G loss: 3.890144]\n",
      "epoch:10 step:8019 [D loss: 0.129498, acc.: 98.44%] [G loss: 2.283070]\n",
      "epoch:10 step:8020 [D loss: 0.021221, acc.: 100.00%] [G loss: 4.545373]\n",
      "epoch:10 step:8021 [D loss: 0.022162, acc.: 100.00%] [G loss: 2.000613]\n",
      "epoch:10 step:8022 [D loss: 0.037365, acc.: 100.00%] [G loss: 4.503729]\n",
      "epoch:10 step:8023 [D loss: 0.088677, acc.: 97.66%] [G loss: 1.036655]\n",
      "epoch:10 step:8024 [D loss: 3.068139, acc.: 16.41%] [G loss: 7.823659]\n",
      "epoch:10 step:8025 [D loss: 0.873728, acc.: 60.16%] [G loss: 4.625028]\n",
      "epoch:10 step:8026 [D loss: 0.047425, acc.: 98.44%] [G loss: 2.318019]\n",
      "epoch:10 step:8027 [D loss: 0.133530, acc.: 95.31%] [G loss: 2.415537]\n",
      "epoch:10 step:8028 [D loss: 0.154655, acc.: 96.88%] [G loss: 1.293349]\n",
      "epoch:10 step:8029 [D loss: 0.096998, acc.: 97.66%] [G loss: 3.207205]\n",
      "epoch:10 step:8030 [D loss: 0.054028, acc.: 99.22%] [G loss: 0.491799]\n",
      "epoch:10 step:8031 [D loss: 0.156619, acc.: 95.31%] [G loss: 1.647011]\n",
      "epoch:10 step:8032 [D loss: 0.085752, acc.: 98.44%] [G loss: 2.115705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8033 [D loss: 0.360484, acc.: 87.50%] [G loss: 3.358329]\n",
      "epoch:10 step:8034 [D loss: 0.625439, acc.: 59.38%] [G loss: 2.453143]\n",
      "epoch:10 step:8035 [D loss: 0.016304, acc.: 100.00%] [G loss: 3.306551]\n",
      "epoch:10 step:8036 [D loss: 0.057531, acc.: 100.00%] [G loss: 4.383715]\n",
      "epoch:10 step:8037 [D loss: 0.970809, acc.: 49.22%] [G loss: 6.125958]\n",
      "epoch:10 step:8038 [D loss: 0.580168, acc.: 68.75%] [G loss: 5.555636]\n",
      "epoch:10 step:8039 [D loss: 0.167181, acc.: 96.09%] [G loss: 2.773176]\n",
      "epoch:10 step:8040 [D loss: 0.120039, acc.: 96.88%] [G loss: 1.961504]\n",
      "epoch:10 step:8041 [D loss: 0.044676, acc.: 99.22%] [G loss: 2.965318]\n",
      "epoch:10 step:8042 [D loss: 0.925166, acc.: 55.47%] [G loss: 5.159582]\n",
      "epoch:10 step:8043 [D loss: 0.107292, acc.: 97.66%] [G loss: 6.813960]\n",
      "epoch:10 step:8044 [D loss: 0.699649, acc.: 64.06%] [G loss: 1.541505]\n",
      "epoch:10 step:8045 [D loss: 0.280918, acc.: 87.50%] [G loss: 3.787148]\n",
      "epoch:10 step:8046 [D loss: 0.038874, acc.: 100.00%] [G loss: 3.401068]\n",
      "epoch:10 step:8047 [D loss: 0.085180, acc.: 98.44%] [G loss: 2.549861]\n",
      "epoch:10 step:8048 [D loss: 0.083481, acc.: 99.22%] [G loss: 4.064896]\n",
      "epoch:10 step:8049 [D loss: 0.073522, acc.: 98.44%] [G loss: 2.949587]\n",
      "epoch:10 step:8050 [D loss: 0.207074, acc.: 93.75%] [G loss: 1.785285]\n",
      "epoch:10 step:8051 [D loss: 0.126508, acc.: 96.09%] [G loss: 2.109360]\n",
      "epoch:10 step:8052 [D loss: 0.083159, acc.: 100.00%] [G loss: 2.455056]\n",
      "epoch:10 step:8053 [D loss: 0.179413, acc.: 93.75%] [G loss: 2.428816]\n",
      "epoch:10 step:8054 [D loss: 0.065965, acc.: 98.44%] [G loss: 2.911695]\n",
      "epoch:10 step:8055 [D loss: 0.229712, acc.: 91.41%] [G loss: 3.216743]\n",
      "epoch:10 step:8056 [D loss: 0.245641, acc.: 89.84%] [G loss: 1.340133]\n",
      "epoch:10 step:8057 [D loss: 0.527145, acc.: 76.56%] [G loss: 6.244810]\n",
      "epoch:10 step:8058 [D loss: 0.216421, acc.: 89.84%] [G loss: 5.253715]\n",
      "epoch:10 step:8059 [D loss: 0.145822, acc.: 93.75%] [G loss: 3.213908]\n",
      "epoch:10 step:8060 [D loss: 0.087692, acc.: 96.88%] [G loss: 1.900861]\n",
      "epoch:10 step:8061 [D loss: 0.019749, acc.: 100.00%] [G loss: 2.592261]\n",
      "epoch:10 step:8062 [D loss: 0.061630, acc.: 100.00%] [G loss: 2.175763]\n",
      "epoch:10 step:8063 [D loss: 0.030441, acc.: 100.00%] [G loss: 1.964062]\n",
      "epoch:10 step:8064 [D loss: 0.214048, acc.: 93.75%] [G loss: 2.287292]\n",
      "epoch:10 step:8065 [D loss: 0.117313, acc.: 97.66%] [G loss: 3.530548]\n",
      "epoch:10 step:8066 [D loss: 0.110945, acc.: 98.44%] [G loss: 2.262460]\n",
      "epoch:10 step:8067 [D loss: 0.224394, acc.: 92.97%] [G loss: 3.896485]\n",
      "epoch:10 step:8068 [D loss: 0.112135, acc.: 97.66%] [G loss: 3.167443]\n",
      "epoch:10 step:8069 [D loss: 0.079773, acc.: 98.44%] [G loss: 5.125561]\n",
      "epoch:10 step:8070 [D loss: 0.088878, acc.: 98.44%] [G loss: 3.285153]\n",
      "epoch:10 step:8071 [D loss: 0.332547, acc.: 84.38%] [G loss: 5.706635]\n",
      "epoch:10 step:8072 [D loss: 0.168146, acc.: 91.41%] [G loss: 5.805334]\n",
      "epoch:10 step:8073 [D loss: 0.040458, acc.: 98.44%] [G loss: 3.790136]\n",
      "epoch:10 step:8074 [D loss: 0.092100, acc.: 96.88%] [G loss: 1.552538]\n",
      "epoch:10 step:8075 [D loss: 0.077351, acc.: 98.44%] [G loss: 1.363298]\n",
      "epoch:10 step:8076 [D loss: 0.033510, acc.: 100.00%] [G loss: 3.539318]\n",
      "epoch:10 step:8077 [D loss: 0.211015, acc.: 90.62%] [G loss: 0.704171]\n",
      "epoch:10 step:8078 [D loss: 0.015995, acc.: 99.22%] [G loss: 1.017169]\n",
      "epoch:10 step:8079 [D loss: 0.007935, acc.: 100.00%] [G loss: 1.590268]\n",
      "epoch:10 step:8080 [D loss: 0.029721, acc.: 100.00%] [G loss: 0.985803]\n",
      "epoch:10 step:8081 [D loss: 0.045926, acc.: 99.22%] [G loss: 1.123657]\n",
      "epoch:10 step:8082 [D loss: 0.055283, acc.: 99.22%] [G loss: 0.843447]\n",
      "epoch:10 step:8083 [D loss: 0.057967, acc.: 99.22%] [G loss: 1.573934]\n",
      "epoch:10 step:8084 [D loss: 0.032941, acc.: 100.00%] [G loss: 0.851177]\n",
      "epoch:10 step:8085 [D loss: 0.100686, acc.: 96.88%] [G loss: 4.443880]\n",
      "epoch:10 step:8086 [D loss: 0.224308, acc.: 92.97%] [G loss: 3.210149]\n",
      "epoch:10 step:8087 [D loss: 0.304305, acc.: 85.16%] [G loss: 0.933044]\n",
      "epoch:10 step:8088 [D loss: 0.029382, acc.: 99.22%] [G loss: 2.395483]\n",
      "epoch:10 step:8089 [D loss: 0.032758, acc.: 99.22%] [G loss: 3.724986]\n",
      "epoch:10 step:8090 [D loss: 0.007563, acc.: 100.00%] [G loss: 3.332209]\n",
      "epoch:10 step:8091 [D loss: 0.014000, acc.: 100.00%] [G loss: 2.757491]\n",
      "epoch:10 step:8092 [D loss: 0.065185, acc.: 98.44%] [G loss: 4.648275]\n",
      "epoch:10 step:8093 [D loss: 0.013291, acc.: 100.00%] [G loss: 3.630391]\n",
      "epoch:10 step:8094 [D loss: 0.048295, acc.: 98.44%] [G loss: 3.583290]\n",
      "epoch:10 step:8095 [D loss: 0.354616, acc.: 80.47%] [G loss: 7.934622]\n",
      "epoch:10 step:8096 [D loss: 1.641393, acc.: 39.06%] [G loss: 5.246243]\n",
      "epoch:10 step:8097 [D loss: 0.033531, acc.: 100.00%] [G loss: 6.015481]\n",
      "epoch:10 step:8098 [D loss: 0.025290, acc.: 99.22%] [G loss: 5.582222]\n",
      "epoch:10 step:8099 [D loss: 0.021127, acc.: 100.00%] [G loss: 5.452087]\n",
      "epoch:10 step:8100 [D loss: 0.024959, acc.: 100.00%] [G loss: 3.494394]\n",
      "epoch:10 step:8101 [D loss: 0.146145, acc.: 97.66%] [G loss: 3.794821]\n",
      "epoch:10 step:8102 [D loss: 0.028118, acc.: 99.22%] [G loss: 4.136731]\n",
      "epoch:10 step:8103 [D loss: 0.040555, acc.: 100.00%] [G loss: 3.456881]\n",
      "epoch:10 step:8104 [D loss: 0.129664, acc.: 96.88%] [G loss: 4.922588]\n",
      "epoch:10 step:8105 [D loss: 0.263833, acc.: 89.06%] [G loss: 3.808915]\n",
      "epoch:10 step:8106 [D loss: 0.114304, acc.: 96.88%] [G loss: 5.357925]\n",
      "epoch:10 step:8107 [D loss: 0.048929, acc.: 100.00%] [G loss: 4.760050]\n",
      "epoch:10 step:8108 [D loss: 0.146744, acc.: 95.31%] [G loss: 3.342493]\n",
      "epoch:10 step:8109 [D loss: 0.112328, acc.: 97.66%] [G loss: 2.321133]\n",
      "epoch:10 step:8110 [D loss: 0.020975, acc.: 100.00%] [G loss: 5.651433]\n",
      "epoch:10 step:8111 [D loss: 0.026134, acc.: 99.22%] [G loss: 2.037364]\n",
      "epoch:10 step:8112 [D loss: 0.291958, acc.: 86.72%] [G loss: 5.734147]\n",
      "epoch:10 step:8113 [D loss: 0.134682, acc.: 93.75%] [G loss: 5.402552]\n",
      "epoch:10 step:8114 [D loss: 0.034621, acc.: 100.00%] [G loss: 4.644125]\n",
      "epoch:10 step:8115 [D loss: 0.066656, acc.: 99.22%] [G loss: 4.238907]\n",
      "epoch:10 step:8116 [D loss: 0.057239, acc.: 97.66%] [G loss: 5.288739]\n",
      "epoch:10 step:8117 [D loss: 0.089365, acc.: 99.22%] [G loss: 3.475143]\n",
      "epoch:10 step:8118 [D loss: 0.064844, acc.: 98.44%] [G loss: 4.112104]\n",
      "epoch:10 step:8119 [D loss: 0.119533, acc.: 97.66%] [G loss: 4.071185]\n",
      "epoch:10 step:8120 [D loss: 0.062856, acc.: 98.44%] [G loss: 2.874355]\n",
      "epoch:10 step:8121 [D loss: 0.022336, acc.: 100.00%] [G loss: 4.885230]\n",
      "epoch:10 step:8122 [D loss: 0.111130, acc.: 96.09%] [G loss: 4.960286]\n",
      "epoch:10 step:8123 [D loss: 0.035120, acc.: 100.00%] [G loss: 5.880014]\n",
      "epoch:10 step:8124 [D loss: 0.048628, acc.: 99.22%] [G loss: 4.663071]\n",
      "epoch:10 step:8125 [D loss: 0.133949, acc.: 96.09%] [G loss: 3.712821]\n",
      "epoch:10 step:8126 [D loss: 0.071315, acc.: 99.22%] [G loss: 5.457389]\n",
      "epoch:10 step:8127 [D loss: 0.044180, acc.: 98.44%] [G loss: 5.120818]\n",
      "epoch:10 step:8128 [D loss: 0.010161, acc.: 100.00%] [G loss: 1.956565]\n",
      "epoch:10 step:8129 [D loss: 0.042849, acc.: 100.00%] [G loss: 5.106975]\n",
      "epoch:10 step:8130 [D loss: 5.215781, acc.: 16.41%] [G loss: 8.688688]\n",
      "epoch:10 step:8131 [D loss: 1.935149, acc.: 50.00%] [G loss: 4.465794]\n",
      "epoch:10 step:8132 [D loss: 0.214443, acc.: 91.41%] [G loss: 2.822861]\n",
      "epoch:10 step:8133 [D loss: 0.062685, acc.: 100.00%] [G loss: 1.242254]\n",
      "epoch:10 step:8134 [D loss: 0.218715, acc.: 94.53%] [G loss: 3.298852]\n",
      "epoch:10 step:8135 [D loss: 0.044434, acc.: 100.00%] [G loss: 3.526276]\n",
      "epoch:10 step:8136 [D loss: 0.090499, acc.: 99.22%] [G loss: 4.264375]\n",
      "epoch:10 step:8137 [D loss: 0.124468, acc.: 96.88%] [G loss: 1.429731]\n",
      "epoch:10 step:8138 [D loss: 0.105378, acc.: 99.22%] [G loss: 3.326887]\n",
      "epoch:10 step:8139 [D loss: 0.079409, acc.: 98.44%] [G loss: 3.534601]\n",
      "epoch:10 step:8140 [D loss: 0.105686, acc.: 96.88%] [G loss: 3.798658]\n",
      "epoch:10 step:8141 [D loss: 0.132205, acc.: 96.88%] [G loss: 3.208075]\n",
      "epoch:10 step:8142 [D loss: 0.125784, acc.: 97.66%] [G loss: 2.799834]\n",
      "epoch:10 step:8143 [D loss: 0.087116, acc.: 98.44%] [G loss: 4.545533]\n",
      "epoch:10 step:8144 [D loss: 0.088550, acc.: 98.44%] [G loss: 4.123777]\n",
      "epoch:10 step:8145 [D loss: 0.059770, acc.: 100.00%] [G loss: 4.720491]\n",
      "epoch:10 step:8146 [D loss: 0.601909, acc.: 67.19%] [G loss: 4.783966]\n",
      "epoch:10 step:8147 [D loss: 0.115210, acc.: 96.88%] [G loss: 4.581249]\n",
      "epoch:10 step:8148 [D loss: 0.345719, acc.: 87.50%] [G loss: 4.217281]\n",
      "epoch:10 step:8149 [D loss: 0.103079, acc.: 97.66%] [G loss: 2.852340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8150 [D loss: 0.272740, acc.: 89.84%] [G loss: 1.745949]\n",
      "epoch:10 step:8151 [D loss: 0.150540, acc.: 93.75%] [G loss: 5.408221]\n",
      "epoch:10 step:8152 [D loss: 0.129648, acc.: 96.09%] [G loss: 4.449187]\n",
      "epoch:10 step:8153 [D loss: 0.079585, acc.: 96.88%] [G loss: 2.722653]\n",
      "epoch:10 step:8154 [D loss: 0.056108, acc.: 98.44%] [G loss: 2.615289]\n",
      "epoch:10 step:8155 [D loss: 0.096933, acc.: 98.44%] [G loss: 2.673168]\n",
      "epoch:10 step:8156 [D loss: 0.065107, acc.: 98.44%] [G loss: 2.988474]\n",
      "epoch:10 step:8157 [D loss: 0.063506, acc.: 99.22%] [G loss: 4.013731]\n",
      "epoch:10 step:8158 [D loss: 0.223444, acc.: 92.97%] [G loss: 3.508035]\n",
      "epoch:10 step:8159 [D loss: 0.022472, acc.: 100.00%] [G loss: 3.034691]\n",
      "epoch:10 step:8160 [D loss: 0.379115, acc.: 82.81%] [G loss: 1.092972]\n",
      "epoch:10 step:8161 [D loss: 0.315107, acc.: 85.94%] [G loss: 6.110214]\n",
      "epoch:10 step:8162 [D loss: 0.450420, acc.: 75.00%] [G loss: 4.684476]\n",
      "epoch:10 step:8163 [D loss: 0.077336, acc.: 96.88%] [G loss: 2.673329]\n",
      "epoch:10 step:8164 [D loss: 0.166373, acc.: 93.75%] [G loss: 4.414257]\n",
      "epoch:10 step:8165 [D loss: 0.117649, acc.: 92.97%] [G loss: 3.088641]\n",
      "epoch:10 step:8166 [D loss: 0.039943, acc.: 100.00%] [G loss: 1.960148]\n",
      "epoch:10 step:8167 [D loss: 0.159582, acc.: 94.53%] [G loss: 3.030796]\n",
      "epoch:10 step:8168 [D loss: 0.025892, acc.: 100.00%] [G loss: 4.514876]\n",
      "epoch:10 step:8169 [D loss: 0.045932, acc.: 100.00%] [G loss: 2.415275]\n",
      "epoch:10 step:8170 [D loss: 0.048017, acc.: 98.44%] [G loss: 2.260254]\n",
      "epoch:10 step:8171 [D loss: 0.867217, acc.: 56.25%] [G loss: 5.170000]\n",
      "epoch:10 step:8172 [D loss: 0.116438, acc.: 95.31%] [G loss: 5.776451]\n",
      "epoch:10 step:8173 [D loss: 0.238012, acc.: 92.97%] [G loss: 3.428193]\n",
      "epoch:10 step:8174 [D loss: 0.096246, acc.: 97.66%] [G loss: 3.614711]\n",
      "epoch:10 step:8175 [D loss: 0.012195, acc.: 100.00%] [G loss: 4.143941]\n",
      "epoch:10 step:8176 [D loss: 0.055711, acc.: 100.00%] [G loss: 3.714907]\n",
      "epoch:10 step:8177 [D loss: 0.084072, acc.: 97.66%] [G loss: 5.010045]\n",
      "epoch:10 step:8178 [D loss: 0.013542, acc.: 100.00%] [G loss: 5.431199]\n",
      "epoch:10 step:8179 [D loss: 0.038453, acc.: 100.00%] [G loss: 2.996046]\n",
      "epoch:10 step:8180 [D loss: 0.100374, acc.: 96.09%] [G loss: 2.885521]\n",
      "epoch:10 step:8181 [D loss: 0.060151, acc.: 100.00%] [G loss: 4.055917]\n",
      "epoch:10 step:8182 [D loss: 0.029334, acc.: 99.22%] [G loss: 4.756391]\n",
      "epoch:10 step:8183 [D loss: 0.035052, acc.: 100.00%] [G loss: 2.753226]\n",
      "epoch:10 step:8184 [D loss: 0.146580, acc.: 96.88%] [G loss: 4.799710]\n",
      "epoch:10 step:8185 [D loss: 0.302172, acc.: 87.50%] [G loss: 1.918203]\n",
      "epoch:10 step:8186 [D loss: 0.049973, acc.: 99.22%] [G loss: 3.204258]\n",
      "epoch:10 step:8187 [D loss: 0.026791, acc.: 100.00%] [G loss: 3.822192]\n",
      "epoch:10 step:8188 [D loss: 0.057669, acc.: 100.00%] [G loss: 2.012653]\n",
      "epoch:10 step:8189 [D loss: 0.104440, acc.: 99.22%] [G loss: 4.765275]\n",
      "epoch:10 step:8190 [D loss: 0.031112, acc.: 100.00%] [G loss: 4.505951]\n",
      "epoch:10 step:8191 [D loss: 0.036511, acc.: 100.00%] [G loss: 2.899351]\n",
      "epoch:10 step:8192 [D loss: 0.125949, acc.: 98.44%] [G loss: 4.050710]\n",
      "epoch:10 step:8193 [D loss: 0.067886, acc.: 98.44%] [G loss: 2.928872]\n",
      "epoch:10 step:8194 [D loss: 0.074961, acc.: 98.44%] [G loss: 3.221522]\n",
      "epoch:10 step:8195 [D loss: 0.094810, acc.: 97.66%] [G loss: 2.041663]\n",
      "epoch:10 step:8196 [D loss: 0.099180, acc.: 98.44%] [G loss: 4.046673]\n",
      "epoch:10 step:8197 [D loss: 0.025679, acc.: 100.00%] [G loss: 4.869896]\n",
      "epoch:10 step:8198 [D loss: 0.017633, acc.: 100.00%] [G loss: 4.295477]\n",
      "epoch:10 step:8199 [D loss: 0.052518, acc.: 99.22%] [G loss: 2.855508]\n",
      "epoch:10 step:8200 [D loss: 0.237300, acc.: 87.50%] [G loss: 5.042605]\n",
      "epoch:10 step:8201 [D loss: 0.012754, acc.: 100.00%] [G loss: 5.116755]\n",
      "epoch:10 step:8202 [D loss: 0.143846, acc.: 95.31%] [G loss: 1.695430]\n",
      "epoch:10 step:8203 [D loss: 0.121459, acc.: 94.53%] [G loss: 5.744120]\n",
      "epoch:10 step:8204 [D loss: 0.011360, acc.: 100.00%] [G loss: 6.429672]\n",
      "epoch:10 step:8205 [D loss: 0.463131, acc.: 79.69%] [G loss: 6.979130]\n",
      "epoch:10 step:8206 [D loss: 0.198916, acc.: 90.62%] [G loss: 4.585457]\n",
      "epoch:10 step:8207 [D loss: 0.084462, acc.: 96.88%] [G loss: 5.576682]\n",
      "epoch:10 step:8208 [D loss: 0.008262, acc.: 100.00%] [G loss: 6.173334]\n",
      "epoch:10 step:8209 [D loss: 0.024615, acc.: 100.00%] [G loss: 5.919297]\n",
      "epoch:10 step:8210 [D loss: 0.025539, acc.: 100.00%] [G loss: 4.865096]\n",
      "epoch:10 step:8211 [D loss: 0.036981, acc.: 100.00%] [G loss: 4.284349]\n",
      "epoch:10 step:8212 [D loss: 0.055418, acc.: 98.44%] [G loss: 3.037807]\n",
      "epoch:10 step:8213 [D loss: 0.238755, acc.: 90.62%] [G loss: 1.120668]\n",
      "epoch:10 step:8214 [D loss: 0.042918, acc.: 100.00%] [G loss: 6.697448]\n",
      "epoch:10 step:8215 [D loss: 0.031601, acc.: 100.00%] [G loss: 6.633204]\n",
      "epoch:10 step:8216 [D loss: 0.006190, acc.: 100.00%] [G loss: 5.898768]\n",
      "epoch:10 step:8217 [D loss: 0.026179, acc.: 100.00%] [G loss: 2.085334]\n",
      "epoch:10 step:8218 [D loss: 0.010451, acc.: 100.00%] [G loss: 3.653268]\n",
      "epoch:10 step:8219 [D loss: 0.019856, acc.: 100.00%] [G loss: 2.101342]\n",
      "epoch:10 step:8220 [D loss: 0.017151, acc.: 100.00%] [G loss: 4.210231]\n",
      "epoch:10 step:8221 [D loss: 0.028396, acc.: 100.00%] [G loss: 0.265433]\n",
      "epoch:10 step:8222 [D loss: 1.018122, acc.: 58.59%] [G loss: 10.954645]\n",
      "epoch:10 step:8223 [D loss: 3.963748, acc.: 50.00%] [G loss: 7.385612]\n",
      "epoch:10 step:8224 [D loss: 1.400415, acc.: 39.84%] [G loss: 0.816710]\n",
      "epoch:10 step:8225 [D loss: 0.069837, acc.: 100.00%] [G loss: 4.147206]\n",
      "epoch:10 step:8226 [D loss: 0.032283, acc.: 100.00%] [G loss: 2.110394]\n",
      "epoch:10 step:8227 [D loss: 0.052978, acc.: 100.00%] [G loss: 2.203165]\n",
      "epoch:10 step:8228 [D loss: 0.041302, acc.: 100.00%] [G loss: 3.478986]\n",
      "epoch:10 step:8229 [D loss: 0.043729, acc.: 100.00%] [G loss: 1.764674]\n",
      "epoch:10 step:8230 [D loss: 0.097256, acc.: 98.44%] [G loss: 3.319643]\n",
      "epoch:10 step:8231 [D loss: 0.020726, acc.: 100.00%] [G loss: 1.570239]\n",
      "epoch:10 step:8232 [D loss: 0.029788, acc.: 100.00%] [G loss: 1.892887]\n",
      "epoch:10 step:8233 [D loss: 0.049443, acc.: 99.22%] [G loss: 1.616016]\n",
      "epoch:10 step:8234 [D loss: 0.129252, acc.: 96.88%] [G loss: 0.919122]\n",
      "epoch:10 step:8235 [D loss: 0.044287, acc.: 100.00%] [G loss: 0.509624]\n",
      "epoch:10 step:8236 [D loss: 0.046293, acc.: 100.00%] [G loss: 1.575587]\n",
      "epoch:10 step:8237 [D loss: 0.019517, acc.: 100.00%] [G loss: 4.159917]\n",
      "epoch:10 step:8238 [D loss: 0.086148, acc.: 96.88%] [G loss: 0.641488]\n",
      "epoch:10 step:8239 [D loss: 0.094184, acc.: 97.66%] [G loss: 1.727008]\n",
      "epoch:10 step:8240 [D loss: 0.180631, acc.: 95.31%] [G loss: 0.472028]\n",
      "epoch:10 step:8241 [D loss: 0.106321, acc.: 98.44%] [G loss: 0.622533]\n",
      "epoch:10 step:8242 [D loss: 0.054315, acc.: 99.22%] [G loss: 1.998202]\n",
      "epoch:10 step:8243 [D loss: 0.056293, acc.: 98.44%] [G loss: 3.052254]\n",
      "epoch:10 step:8244 [D loss: 0.025369, acc.: 100.00%] [G loss: 2.030200]\n",
      "epoch:10 step:8245 [D loss: 0.013258, acc.: 100.00%] [G loss: 2.409448]\n",
      "epoch:10 step:8246 [D loss: 0.538236, acc.: 70.31%] [G loss: 5.798261]\n",
      "epoch:10 step:8247 [D loss: 0.595881, acc.: 66.41%] [G loss: 3.931919]\n",
      "epoch:10 step:8248 [D loss: 0.030060, acc.: 100.00%] [G loss: 3.037563]\n",
      "epoch:10 step:8249 [D loss: 0.029457, acc.: 100.00%] [G loss: 4.153059]\n",
      "epoch:10 step:8250 [D loss: 0.030426, acc.: 100.00%] [G loss: 4.225791]\n",
      "epoch:10 step:8251 [D loss: 0.080023, acc.: 98.44%] [G loss: 3.448685]\n",
      "epoch:10 step:8252 [D loss: 0.055019, acc.: 100.00%] [G loss: 3.856722]\n",
      "epoch:10 step:8253 [D loss: 0.027588, acc.: 100.00%] [G loss: 4.586347]\n",
      "epoch:10 step:8254 [D loss: 0.398131, acc.: 85.16%] [G loss: 4.826881]\n",
      "epoch:10 step:8255 [D loss: 0.006956, acc.: 100.00%] [G loss: 5.878688]\n",
      "epoch:10 step:8256 [D loss: 0.017319, acc.: 100.00%] [G loss: 5.471646]\n",
      "epoch:10 step:8257 [D loss: 0.048052, acc.: 98.44%] [G loss: 4.754197]\n",
      "epoch:10 step:8258 [D loss: 0.022727, acc.: 100.00%] [G loss: 3.215705]\n",
      "epoch:10 step:8259 [D loss: 0.010691, acc.: 100.00%] [G loss: 3.598546]\n",
      "epoch:10 step:8260 [D loss: 0.021649, acc.: 100.00%] [G loss: 3.725554]\n",
      "epoch:10 step:8261 [D loss: 0.013906, acc.: 100.00%] [G loss: 3.939244]\n",
      "epoch:10 step:8262 [D loss: 0.023511, acc.: 100.00%] [G loss: 3.209610]\n",
      "epoch:10 step:8263 [D loss: 0.090158, acc.: 95.31%] [G loss: 1.769335]\n",
      "epoch:10 step:8264 [D loss: 0.070398, acc.: 99.22%] [G loss: 1.622982]\n",
      "epoch:10 step:8265 [D loss: 0.043218, acc.: 99.22%] [G loss: 2.837830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8266 [D loss: 0.015746, acc.: 100.00%] [G loss: 4.105398]\n",
      "epoch:10 step:8267 [D loss: 0.006911, acc.: 100.00%] [G loss: 2.054727]\n",
      "epoch:10 step:8268 [D loss: 0.018800, acc.: 100.00%] [G loss: 2.139854]\n",
      "epoch:10 step:8269 [D loss: 0.137805, acc.: 96.09%] [G loss: 0.336735]\n",
      "epoch:10 step:8270 [D loss: 0.166947, acc.: 95.31%] [G loss: 1.958049]\n",
      "epoch:10 step:8271 [D loss: 0.020129, acc.: 100.00%] [G loss: 3.481030]\n",
      "epoch:10 step:8272 [D loss: 0.023636, acc.: 100.00%] [G loss: 1.861350]\n",
      "epoch:10 step:8273 [D loss: 0.007186, acc.: 100.00%] [G loss: 1.262736]\n",
      "epoch:10 step:8274 [D loss: 0.024584, acc.: 100.00%] [G loss: 0.139384]\n",
      "epoch:10 step:8275 [D loss: 0.010408, acc.: 100.00%] [G loss: 2.496503]\n",
      "epoch:10 step:8276 [D loss: 0.084508, acc.: 98.44%] [G loss: 0.580681]\n",
      "epoch:10 step:8277 [D loss: 0.068367, acc.: 99.22%] [G loss: 2.290716]\n",
      "epoch:10 step:8278 [D loss: 0.651008, acc.: 67.97%] [G loss: 6.917238]\n",
      "epoch:10 step:8279 [D loss: 0.472713, acc.: 75.78%] [G loss: 5.164883]\n",
      "epoch:10 step:8280 [D loss: 0.027393, acc.: 98.44%] [G loss: 0.898365]\n",
      "epoch:10 step:8281 [D loss: 0.115180, acc.: 95.31%] [G loss: 3.510621]\n",
      "epoch:10 step:8282 [D loss: 0.005212, acc.: 100.00%] [G loss: 5.663032]\n",
      "epoch:10 step:8283 [D loss: 0.016476, acc.: 100.00%] [G loss: 5.314597]\n",
      "epoch:10 step:8284 [D loss: 0.621918, acc.: 65.62%] [G loss: 3.900075]\n",
      "epoch:10 step:8285 [D loss: 0.034895, acc.: 99.22%] [G loss: 5.494816]\n",
      "epoch:10 step:8286 [D loss: 0.155504, acc.: 92.97%] [G loss: 3.009255]\n",
      "epoch:10 step:8287 [D loss: 0.178187, acc.: 92.19%] [G loss: 5.166860]\n",
      "epoch:10 step:8288 [D loss: 0.048659, acc.: 98.44%] [G loss: 5.594404]\n",
      "epoch:10 step:8289 [D loss: 0.103194, acc.: 98.44%] [G loss: 4.605310]\n",
      "epoch:10 step:8290 [D loss: 0.053301, acc.: 99.22%] [G loss: 4.894183]\n",
      "epoch:10 step:8291 [D loss: 0.026538, acc.: 99.22%] [G loss: 4.890059]\n",
      "epoch:10 step:8292 [D loss: 0.123640, acc.: 96.09%] [G loss: 3.945064]\n",
      "epoch:10 step:8293 [D loss: 0.048582, acc.: 99.22%] [G loss: 3.425501]\n",
      "epoch:10 step:8294 [D loss: 0.035315, acc.: 100.00%] [G loss: 3.897427]\n",
      "epoch:10 step:8295 [D loss: 0.064672, acc.: 98.44%] [G loss: 2.244622]\n",
      "epoch:10 step:8296 [D loss: 0.090101, acc.: 100.00%] [G loss: 2.668518]\n",
      "epoch:10 step:8297 [D loss: 0.042031, acc.: 100.00%] [G loss: 4.067996]\n",
      "epoch:10 step:8298 [D loss: 0.049320, acc.: 100.00%] [G loss: 2.924457]\n",
      "epoch:10 step:8299 [D loss: 0.069722, acc.: 98.44%] [G loss: 3.691138]\n",
      "epoch:10 step:8300 [D loss: 0.107286, acc.: 97.66%] [G loss: 2.941290]\n",
      "epoch:10 step:8301 [D loss: 0.200115, acc.: 92.97%] [G loss: 4.242421]\n",
      "epoch:10 step:8302 [D loss: 0.189969, acc.: 92.19%] [G loss: 3.401023]\n",
      "epoch:10 step:8303 [D loss: 0.098107, acc.: 98.44%] [G loss: 4.604798]\n",
      "epoch:10 step:8304 [D loss: 0.008813, acc.: 100.00%] [G loss: 4.949294]\n",
      "epoch:10 step:8305 [D loss: 0.100435, acc.: 95.31%] [G loss: 3.025454]\n",
      "epoch:10 step:8306 [D loss: 0.022218, acc.: 100.00%] [G loss: 2.313036]\n",
      "epoch:10 step:8307 [D loss: 0.173690, acc.: 92.19%] [G loss: 4.378335]\n",
      "epoch:10 step:8308 [D loss: 0.863834, acc.: 59.38%] [G loss: 3.059506]\n",
      "epoch:10 step:8309 [D loss: 0.045023, acc.: 100.00%] [G loss: 1.558599]\n",
      "epoch:10 step:8310 [D loss: 0.035121, acc.: 100.00%] [G loss: 3.899283]\n",
      "epoch:10 step:8311 [D loss: 0.082476, acc.: 99.22%] [G loss: 4.999068]\n",
      "epoch:10 step:8312 [D loss: 0.125610, acc.: 97.66%] [G loss: 2.468615]\n",
      "epoch:10 step:8313 [D loss: 0.319468, acc.: 86.72%] [G loss: 6.283008]\n",
      "epoch:10 step:8314 [D loss: 0.295487, acc.: 85.16%] [G loss: 2.196611]\n",
      "epoch:10 step:8315 [D loss: 0.004860, acc.: 100.00%] [G loss: 2.410064]\n",
      "epoch:10 step:8316 [D loss: 0.076855, acc.: 97.66%] [G loss: 2.694714]\n",
      "epoch:10 step:8317 [D loss: 0.010350, acc.: 100.00%] [G loss: 3.944821]\n",
      "epoch:10 step:8318 [D loss: 0.025893, acc.: 100.00%] [G loss: 3.849496]\n",
      "epoch:10 step:8319 [D loss: 0.093997, acc.: 97.66%] [G loss: 3.838088]\n",
      "epoch:10 step:8320 [D loss: 0.023201, acc.: 100.00%] [G loss: 2.816307]\n",
      "epoch:10 step:8321 [D loss: 0.027448, acc.: 100.00%] [G loss: 1.864362]\n",
      "epoch:10 step:8322 [D loss: 0.059479, acc.: 100.00%] [G loss: 2.347403]\n",
      "epoch:10 step:8323 [D loss: 0.095491, acc.: 97.66%] [G loss: 1.657211]\n",
      "epoch:10 step:8324 [D loss: 0.334790, acc.: 83.59%] [G loss: 7.391960]\n",
      "epoch:10 step:8325 [D loss: 0.274845, acc.: 85.16%] [G loss: 5.743727]\n",
      "epoch:10 step:8326 [D loss: 0.044239, acc.: 99.22%] [G loss: 1.294372]\n",
      "epoch:10 step:8327 [D loss: 0.021500, acc.: 100.00%] [G loss: 2.568422]\n",
      "epoch:10 step:8328 [D loss: 0.071370, acc.: 98.44%] [G loss: 7.391046]\n",
      "epoch:10 step:8329 [D loss: 0.073044, acc.: 98.44%] [G loss: 6.365175]\n",
      "epoch:10 step:8330 [D loss: 0.015264, acc.: 100.00%] [G loss: 6.482100]\n",
      "epoch:10 step:8331 [D loss: 0.121272, acc.: 96.88%] [G loss: 3.607831]\n",
      "epoch:10 step:8332 [D loss: 0.031683, acc.: 100.00%] [G loss: 5.116283]\n",
      "epoch:10 step:8333 [D loss: 0.081854, acc.: 99.22%] [G loss: 6.906149]\n",
      "epoch:10 step:8334 [D loss: 1.429277, acc.: 42.19%] [G loss: 7.981109]\n",
      "epoch:10 step:8335 [D loss: 1.441812, acc.: 52.34%] [G loss: 3.473576]\n",
      "epoch:10 step:8336 [D loss: 1.028305, acc.: 64.06%] [G loss: 7.083732]\n",
      "epoch:10 step:8337 [D loss: 0.085076, acc.: 95.31%] [G loss: 7.841094]\n",
      "epoch:10 step:8338 [D loss: 0.840058, acc.: 64.84%] [G loss: 2.911903]\n",
      "epoch:10 step:8339 [D loss: 0.293574, acc.: 85.16%] [G loss: 4.962668]\n",
      "epoch:10 step:8340 [D loss: 0.010361, acc.: 100.00%] [G loss: 6.322179]\n",
      "epoch:10 step:8341 [D loss: 0.070955, acc.: 97.66%] [G loss: 3.960853]\n",
      "epoch:10 step:8342 [D loss: 0.080367, acc.: 99.22%] [G loss: 4.300502]\n",
      "epoch:10 step:8343 [D loss: 0.064464, acc.: 99.22%] [G loss: 3.973466]\n",
      "epoch:10 step:8344 [D loss: 0.037138, acc.: 100.00%] [G loss: 3.573055]\n",
      "epoch:10 step:8345 [D loss: 0.045790, acc.: 99.22%] [G loss: 3.395874]\n",
      "epoch:10 step:8346 [D loss: 0.065429, acc.: 99.22%] [G loss: 2.186600]\n",
      "epoch:10 step:8347 [D loss: 0.248952, acc.: 90.62%] [G loss: 4.725237]\n",
      "epoch:10 step:8348 [D loss: 0.072191, acc.: 97.66%] [G loss: 5.075918]\n",
      "epoch:10 step:8349 [D loss: 0.278687, acc.: 90.62%] [G loss: 3.276572]\n",
      "epoch:10 step:8350 [D loss: 0.053166, acc.: 98.44%] [G loss: 3.443007]\n",
      "epoch:10 step:8351 [D loss: 0.040259, acc.: 100.00%] [G loss: 2.448979]\n",
      "epoch:10 step:8352 [D loss: 0.159516, acc.: 94.53%] [G loss: 0.587411]\n",
      "epoch:10 step:8353 [D loss: 0.390118, acc.: 78.91%] [G loss: 6.892732]\n",
      "epoch:10 step:8354 [D loss: 0.592522, acc.: 67.19%] [G loss: 4.261258]\n",
      "epoch:10 step:8355 [D loss: 0.341623, acc.: 84.38%] [G loss: 4.513430]\n",
      "epoch:10 step:8356 [D loss: 0.168640, acc.: 94.53%] [G loss: 5.154886]\n",
      "epoch:10 step:8357 [D loss: 0.091586, acc.: 95.31%] [G loss: 3.109446]\n",
      "epoch:10 step:8358 [D loss: 0.020312, acc.: 100.00%] [G loss: 1.634069]\n",
      "epoch:10 step:8359 [D loss: 0.159894, acc.: 94.53%] [G loss: 4.352550]\n",
      "epoch:10 step:8360 [D loss: 0.096835, acc.: 96.88%] [G loss: 2.527440]\n",
      "epoch:10 step:8361 [D loss: 0.016981, acc.: 100.00%] [G loss: 3.543337]\n",
      "epoch:10 step:8362 [D loss: 0.103413, acc.: 95.31%] [G loss: 0.708162]\n",
      "epoch:10 step:8363 [D loss: 0.118771, acc.: 97.66%] [G loss: 1.393496]\n",
      "epoch:10 step:8364 [D loss: 0.016722, acc.: 100.00%] [G loss: 2.480705]\n",
      "epoch:10 step:8365 [D loss: 0.050348, acc.: 99.22%] [G loss: 2.192843]\n",
      "epoch:10 step:8366 [D loss: 0.043947, acc.: 99.22%] [G loss: 2.636887]\n",
      "epoch:10 step:8367 [D loss: 0.145101, acc.: 96.88%] [G loss: 1.930606]\n",
      "epoch:10 step:8368 [D loss: 0.032198, acc.: 99.22%] [G loss: 1.404097]\n",
      "epoch:10 step:8369 [D loss: 0.039670, acc.: 100.00%] [G loss: 0.507929]\n",
      "epoch:10 step:8370 [D loss: 0.102876, acc.: 99.22%] [G loss: 2.157464]\n",
      "epoch:10 step:8371 [D loss: 0.064870, acc.: 98.44%] [G loss: 1.277100]\n",
      "epoch:10 step:8372 [D loss: 0.157392, acc.: 93.75%] [G loss: 0.299862]\n",
      "epoch:10 step:8373 [D loss: 0.704223, acc.: 67.97%] [G loss: 6.249194]\n",
      "epoch:10 step:8374 [D loss: 0.268296, acc.: 85.94%] [G loss: 6.330843]\n",
      "epoch:10 step:8375 [D loss: 0.539612, acc.: 74.22%] [G loss: 3.428660]\n",
      "epoch:10 step:8376 [D loss: 0.090281, acc.: 97.66%] [G loss: 4.485761]\n",
      "epoch:10 step:8377 [D loss: 0.024321, acc.: 100.00%] [G loss: 4.937087]\n",
      "epoch:10 step:8378 [D loss: 0.035452, acc.: 99.22%] [G loss: 4.907940]\n",
      "epoch:10 step:8379 [D loss: 0.594266, acc.: 67.19%] [G loss: 7.400098]\n",
      "epoch:10 step:8380 [D loss: 0.137729, acc.: 92.19%] [G loss: 8.171729]\n",
      "epoch:10 step:8381 [D loss: 0.176140, acc.: 91.41%] [G loss: 4.347867]\n",
      "epoch:10 step:8382 [D loss: 0.176227, acc.: 93.75%] [G loss: 4.702104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8383 [D loss: 0.022245, acc.: 100.00%] [G loss: 5.045867]\n",
      "epoch:10 step:8384 [D loss: 0.084805, acc.: 97.66%] [G loss: 3.640019]\n",
      "epoch:10 step:8385 [D loss: 0.163551, acc.: 94.53%] [G loss: 5.825602]\n",
      "epoch:10 step:8386 [D loss: 0.043450, acc.: 100.00%] [G loss: 4.017079]\n",
      "epoch:10 step:8387 [D loss: 0.070087, acc.: 99.22%] [G loss: 1.607097]\n",
      "epoch:10 step:8388 [D loss: 0.025829, acc.: 100.00%] [G loss: 3.684268]\n",
      "epoch:10 step:8389 [D loss: 0.032223, acc.: 100.00%] [G loss: 2.621718]\n",
      "epoch:10 step:8390 [D loss: 0.114699, acc.: 97.66%] [G loss: 1.934204]\n",
      "epoch:10 step:8391 [D loss: 0.050701, acc.: 99.22%] [G loss: 0.713032]\n",
      "epoch:10 step:8392 [D loss: 0.024537, acc.: 100.00%] [G loss: 2.811541]\n",
      "epoch:10 step:8393 [D loss: 0.049405, acc.: 100.00%] [G loss: 0.855716]\n",
      "epoch:10 step:8394 [D loss: 0.315540, acc.: 82.81%] [G loss: 8.006245]\n",
      "epoch:10 step:8395 [D loss: 0.943981, acc.: 57.03%] [G loss: 2.217755]\n",
      "epoch:10 step:8396 [D loss: 0.012787, acc.: 100.00%] [G loss: 0.054363]\n",
      "epoch:10 step:8397 [D loss: 0.804407, acc.: 67.19%] [G loss: 8.347263]\n",
      "epoch:10 step:8398 [D loss: 0.437631, acc.: 79.69%] [G loss: 5.742985]\n",
      "epoch:10 step:8399 [D loss: 0.154561, acc.: 91.41%] [G loss: 1.349601]\n",
      "epoch:10 step:8400 [D loss: 0.005548, acc.: 100.00%] [G loss: 0.275538]\n",
      "epoch:10 step:8401 [D loss: 0.047196, acc.: 100.00%] [G loss: 0.233357]\n",
      "epoch:10 step:8402 [D loss: 0.139536, acc.: 92.97%] [G loss: 3.045442]\n",
      "epoch:10 step:8403 [D loss: 0.189817, acc.: 91.41%] [G loss: 3.613507]\n",
      "epoch:10 step:8404 [D loss: 0.020662, acc.: 100.00%] [G loss: 1.166748]\n",
      "epoch:10 step:8405 [D loss: 0.025320, acc.: 100.00%] [G loss: 0.602975]\n",
      "epoch:10 step:8406 [D loss: 0.053828, acc.: 99.22%] [G loss: 4.785801]\n",
      "epoch:10 step:8407 [D loss: 0.015500, acc.: 100.00%] [G loss: 5.356301]\n",
      "epoch:10 step:8408 [D loss: 0.070491, acc.: 100.00%] [G loss: 1.893867]\n",
      "epoch:10 step:8409 [D loss: 0.208316, acc.: 90.62%] [G loss: 5.333395]\n",
      "epoch:10 step:8410 [D loss: 1.274733, acc.: 53.91%] [G loss: 0.575280]\n",
      "epoch:10 step:8411 [D loss: 0.578595, acc.: 67.97%] [G loss: 3.986790]\n",
      "epoch:10 step:8412 [D loss: 0.421697, acc.: 79.69%] [G loss: 3.677236]\n",
      "epoch:10 step:8413 [D loss: 0.166631, acc.: 91.41%] [G loss: 3.739771]\n",
      "epoch:10 step:8414 [D loss: 0.226202, acc.: 91.41%] [G loss: 2.363070]\n",
      "epoch:10 step:8415 [D loss: 0.214515, acc.: 92.19%] [G loss: 3.295108]\n",
      "epoch:10 step:8416 [D loss: 0.072677, acc.: 98.44%] [G loss: 4.271931]\n",
      "epoch:10 step:8417 [D loss: 0.248648, acc.: 89.84%] [G loss: 2.174261]\n",
      "epoch:10 step:8418 [D loss: 0.067868, acc.: 98.44%] [G loss: 3.029986]\n",
      "epoch:10 step:8419 [D loss: 0.219162, acc.: 92.19%] [G loss: 3.070112]\n",
      "epoch:10 step:8420 [D loss: 0.059459, acc.: 97.66%] [G loss: 2.824727]\n",
      "epoch:10 step:8421 [D loss: 0.038809, acc.: 98.44%] [G loss: 1.857782]\n",
      "epoch:10 step:8422 [D loss: 0.062348, acc.: 99.22%] [G loss: 2.531935]\n",
      "epoch:10 step:8423 [D loss: 0.094659, acc.: 98.44%] [G loss: 1.375304]\n",
      "epoch:10 step:8424 [D loss: 0.024661, acc.: 100.00%] [G loss: 0.657143]\n",
      "epoch:10 step:8425 [D loss: 0.058464, acc.: 100.00%] [G loss: 0.900164]\n",
      "epoch:10 step:8426 [D loss: 0.149864, acc.: 96.88%] [G loss: 0.861124]\n",
      "epoch:10 step:8427 [D loss: 0.010844, acc.: 100.00%] [G loss: 0.994838]\n",
      "epoch:10 step:8428 [D loss: 0.048134, acc.: 100.00%] [G loss: 1.423809]\n",
      "epoch:10 step:8429 [D loss: 0.063733, acc.: 99.22%] [G loss: 1.228461]\n",
      "epoch:10 step:8430 [D loss: 0.056204, acc.: 99.22%] [G loss: 2.124407]\n",
      "epoch:10 step:8431 [D loss: 0.124127, acc.: 96.88%] [G loss: 2.335676]\n",
      "epoch:10 step:8432 [D loss: 0.101117, acc.: 97.66%] [G loss: 0.485333]\n",
      "epoch:10 step:8433 [D loss: 0.887483, acc.: 68.75%] [G loss: 9.692777]\n",
      "epoch:10 step:8434 [D loss: 2.740247, acc.: 50.00%] [G loss: 3.694523]\n",
      "epoch:10 step:8435 [D loss: 0.481557, acc.: 78.12%] [G loss: 3.498500]\n",
      "epoch:10 step:8436 [D loss: 0.153640, acc.: 94.53%] [G loss: 3.330334]\n",
      "epoch:10 step:8437 [D loss: 0.130663, acc.: 96.09%] [G loss: 1.719423]\n",
      "epoch:10 step:8438 [D loss: 0.213370, acc.: 94.53%] [G loss: 3.401433]\n",
      "epoch:10 step:8439 [D loss: 0.231368, acc.: 90.62%] [G loss: 3.139308]\n",
      "epoch:10 step:8440 [D loss: 0.107287, acc.: 96.88%] [G loss: 3.438597]\n",
      "epoch:10 step:8441 [D loss: 0.094134, acc.: 99.22%] [G loss: 1.421985]\n",
      "epoch:10 step:8442 [D loss: 0.232906, acc.: 92.97%] [G loss: 3.353678]\n",
      "epoch:10 step:8443 [D loss: 0.106106, acc.: 96.88%] [G loss: 2.899182]\n",
      "epoch:10 step:8444 [D loss: 0.326545, acc.: 89.84%] [G loss: 3.680788]\n",
      "epoch:10 step:8445 [D loss: 0.355926, acc.: 82.03%] [G loss: 3.489655]\n",
      "epoch:10 step:8446 [D loss: 0.312180, acc.: 87.50%] [G loss: 4.761695]\n",
      "epoch:10 step:8447 [D loss: 0.499365, acc.: 71.09%] [G loss: 2.844965]\n",
      "epoch:10 step:8448 [D loss: 0.321454, acc.: 83.59%] [G loss: 4.842321]\n",
      "epoch:10 step:8449 [D loss: 0.385701, acc.: 78.91%] [G loss: 3.267267]\n",
      "epoch:10 step:8450 [D loss: 0.140828, acc.: 95.31%] [G loss: 3.404673]\n",
      "epoch:10 step:8451 [D loss: 0.019083, acc.: 100.00%] [G loss: 4.796391]\n",
      "epoch:10 step:8452 [D loss: 0.228983, acc.: 92.19%] [G loss: 3.468404]\n",
      "epoch:10 step:8453 [D loss: 0.051503, acc.: 100.00%] [G loss: 2.555191]\n",
      "epoch:10 step:8454 [D loss: 0.048860, acc.: 97.66%] [G loss: 3.537522]\n",
      "epoch:10 step:8455 [D loss: 0.056876, acc.: 100.00%] [G loss: 2.894754]\n",
      "epoch:10 step:8456 [D loss: 0.116986, acc.: 97.66%] [G loss: 1.639752]\n",
      "epoch:10 step:8457 [D loss: 0.179854, acc.: 92.19%] [G loss: 2.249860]\n",
      "epoch:10 step:8458 [D loss: 0.061002, acc.: 99.22%] [G loss: 2.854990]\n",
      "epoch:10 step:8459 [D loss: 0.165068, acc.: 94.53%] [G loss: 1.037966]\n",
      "epoch:10 step:8460 [D loss: 0.190867, acc.: 92.19%] [G loss: 2.101187]\n",
      "epoch:10 step:8461 [D loss: 0.041984, acc.: 99.22%] [G loss: 3.406330]\n",
      "epoch:10 step:8462 [D loss: 0.163776, acc.: 93.75%] [G loss: 4.116257]\n",
      "epoch:10 step:8463 [D loss: 0.201376, acc.: 90.62%] [G loss: 3.737643]\n",
      "epoch:10 step:8464 [D loss: 0.101293, acc.: 97.66%] [G loss: 3.455340]\n",
      "epoch:10 step:8465 [D loss: 0.052895, acc.: 100.00%] [G loss: 4.375224]\n",
      "epoch:10 step:8466 [D loss: 0.082502, acc.: 99.22%] [G loss: 2.978239]\n",
      "epoch:10 step:8467 [D loss: 0.224307, acc.: 88.28%] [G loss: 5.662423]\n",
      "epoch:10 step:8468 [D loss: 0.302663, acc.: 86.72%] [G loss: 4.712593]\n",
      "epoch:10 step:8469 [D loss: 0.060761, acc.: 98.44%] [G loss: 1.977157]\n",
      "epoch:10 step:8470 [D loss: 0.048590, acc.: 100.00%] [G loss: 3.795496]\n",
      "epoch:10 step:8471 [D loss: 0.065948, acc.: 97.66%] [G loss: 3.117244]\n",
      "epoch:10 step:8472 [D loss: 0.078648, acc.: 98.44%] [G loss: 4.669105]\n",
      "epoch:10 step:8473 [D loss: 0.035648, acc.: 100.00%] [G loss: 4.679851]\n",
      "epoch:10 step:8474 [D loss: 0.168738, acc.: 96.88%] [G loss: 4.447849]\n",
      "epoch:10 step:8475 [D loss: 0.175101, acc.: 93.75%] [G loss: 4.486758]\n",
      "epoch:10 step:8476 [D loss: 0.022883, acc.: 100.00%] [G loss: 4.576362]\n",
      "epoch:10 step:8477 [D loss: 0.021773, acc.: 100.00%] [G loss: 3.749084]\n",
      "epoch:10 step:8478 [D loss: 0.138913, acc.: 93.75%] [G loss: 4.082400]\n",
      "epoch:10 step:8479 [D loss: 0.019359, acc.: 100.00%] [G loss: 4.201244]\n",
      "epoch:10 step:8480 [D loss: 0.056525, acc.: 100.00%] [G loss: 3.903308]\n",
      "epoch:10 step:8481 [D loss: 0.034766, acc.: 100.00%] [G loss: 4.301167]\n",
      "epoch:10 step:8482 [D loss: 0.079179, acc.: 97.66%] [G loss: 1.837639]\n",
      "epoch:10 step:8483 [D loss: 0.077931, acc.: 98.44%] [G loss: 3.531364]\n",
      "epoch:10 step:8484 [D loss: 0.015833, acc.: 100.00%] [G loss: 3.151247]\n",
      "epoch:10 step:8485 [D loss: 0.060754, acc.: 100.00%] [G loss: 2.837521]\n",
      "epoch:10 step:8486 [D loss: 0.025476, acc.: 100.00%] [G loss: 2.469102]\n",
      "epoch:10 step:8487 [D loss: 0.029480, acc.: 100.00%] [G loss: 3.705476]\n",
      "epoch:10 step:8488 [D loss: 0.146527, acc.: 94.53%] [G loss: 0.499252]\n",
      "epoch:10 step:8489 [D loss: 0.166312, acc.: 91.41%] [G loss: 4.963490]\n",
      "epoch:10 step:8490 [D loss: 0.019165, acc.: 100.00%] [G loss: 5.466746]\n",
      "epoch:10 step:8491 [D loss: 1.218213, acc.: 39.84%] [G loss: 6.336612]\n",
      "epoch:10 step:8492 [D loss: 0.108484, acc.: 93.75%] [G loss: 7.961324]\n",
      "epoch:10 step:8493 [D loss: 0.349218, acc.: 82.81%] [G loss: 0.755663]\n",
      "epoch:10 step:8494 [D loss: 0.209379, acc.: 91.41%] [G loss: 4.487223]\n",
      "epoch:10 step:8495 [D loss: 0.007468, acc.: 100.00%] [G loss: 5.296268]\n",
      "epoch:10 step:8496 [D loss: 0.027394, acc.: 99.22%] [G loss: 3.683075]\n",
      "epoch:10 step:8497 [D loss: 0.522802, acc.: 73.44%] [G loss: 5.005595]\n",
      "epoch:10 step:8498 [D loss: 0.025384, acc.: 100.00%] [G loss: 6.013603]\n",
      "epoch:10 step:8499 [D loss: 0.107245, acc.: 96.88%] [G loss: 4.544520]\n",
      "epoch:10 step:8500 [D loss: 0.160416, acc.: 95.31%] [G loss: 2.645044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8501 [D loss: 0.076536, acc.: 98.44%] [G loss: 3.886918]\n",
      "epoch:10 step:8502 [D loss: 0.048640, acc.: 99.22%] [G loss: 3.231932]\n",
      "epoch:10 step:8503 [D loss: 0.171145, acc.: 95.31%] [G loss: 5.120032]\n",
      "epoch:10 step:8504 [D loss: 0.035729, acc.: 100.00%] [G loss: 6.034137]\n",
      "epoch:10 step:8505 [D loss: 0.131316, acc.: 93.75%] [G loss: 2.059134]\n",
      "epoch:10 step:8506 [D loss: 0.176379, acc.: 91.41%] [G loss: 4.753694]\n",
      "epoch:10 step:8507 [D loss: 0.086856, acc.: 98.44%] [G loss: 5.010120]\n",
      "epoch:10 step:8508 [D loss: 0.044510, acc.: 100.00%] [G loss: 4.089484]\n",
      "epoch:10 step:8509 [D loss: 0.276516, acc.: 89.84%] [G loss: 5.853985]\n",
      "epoch:10 step:8510 [D loss: 0.174296, acc.: 91.41%] [G loss: 4.618335]\n",
      "epoch:10 step:8511 [D loss: 0.021135, acc.: 100.00%] [G loss: 3.897812]\n",
      "epoch:10 step:8512 [D loss: 0.040231, acc.: 100.00%] [G loss: 4.253846]\n",
      "epoch:10 step:8513 [D loss: 0.023988, acc.: 100.00%] [G loss: 2.812870]\n",
      "epoch:10 step:8514 [D loss: 0.019322, acc.: 100.00%] [G loss: 3.966330]\n",
      "epoch:10 step:8515 [D loss: 0.169141, acc.: 92.97%] [G loss: 5.681840]\n",
      "epoch:10 step:8516 [D loss: 0.053132, acc.: 99.22%] [G loss: 4.633258]\n",
      "epoch:10 step:8517 [D loss: 0.195944, acc.: 92.19%] [G loss: 2.623322]\n",
      "epoch:10 step:8518 [D loss: 0.728932, acc.: 66.41%] [G loss: 6.929477]\n",
      "epoch:10 step:8519 [D loss: 1.155659, acc.: 58.59%] [G loss: 3.573522]\n",
      "epoch:10 step:8520 [D loss: 0.072737, acc.: 97.66%] [G loss: 1.733578]\n",
      "epoch:10 step:8521 [D loss: 0.167846, acc.: 93.75%] [G loss: 4.430038]\n",
      "epoch:10 step:8522 [D loss: 0.021024, acc.: 100.00%] [G loss: 2.509901]\n",
      "epoch:10 step:8523 [D loss: 0.267226, acc.: 88.28%] [G loss: 2.113534]\n",
      "epoch:10 step:8524 [D loss: 0.126999, acc.: 94.53%] [G loss: 1.735055]\n",
      "epoch:10 step:8525 [D loss: 0.259135, acc.: 88.28%] [G loss: 1.145695]\n",
      "epoch:10 step:8526 [D loss: 0.041620, acc.: 100.00%] [G loss: 3.792714]\n",
      "epoch:10 step:8527 [D loss: 0.032269, acc.: 100.00%] [G loss: 2.756408]\n",
      "epoch:10 step:8528 [D loss: 0.025366, acc.: 100.00%] [G loss: 2.823423]\n",
      "epoch:10 step:8529 [D loss: 0.124666, acc.: 96.88%] [G loss: 2.558905]\n",
      "epoch:10 step:8530 [D loss: 0.010796, acc.: 100.00%] [G loss: 3.764587]\n",
      "epoch:10 step:8531 [D loss: 0.131251, acc.: 96.09%] [G loss: 3.614696]\n",
      "epoch:10 step:8532 [D loss: 0.038815, acc.: 100.00%] [G loss: 2.692804]\n",
      "epoch:10 step:8533 [D loss: 0.075982, acc.: 99.22%] [G loss: 1.669547]\n",
      "epoch:10 step:8534 [D loss: 0.035775, acc.: 99.22%] [G loss: 2.109178]\n",
      "epoch:10 step:8535 [D loss: 0.188252, acc.: 95.31%] [G loss: 1.606581]\n",
      "epoch:10 step:8536 [D loss: 0.010773, acc.: 100.00%] [G loss: 3.106618]\n",
      "epoch:10 step:8537 [D loss: 0.081318, acc.: 97.66%] [G loss: 1.016466]\n",
      "epoch:10 step:8538 [D loss: 0.040463, acc.: 99.22%] [G loss: 1.851672]\n",
      "epoch:10 step:8539 [D loss: 0.026433, acc.: 100.00%] [G loss: 2.262872]\n",
      "epoch:10 step:8540 [D loss: 0.003760, acc.: 100.00%] [G loss: 2.450139]\n",
      "epoch:10 step:8541 [D loss: 0.016890, acc.: 100.00%] [G loss: 2.935729]\n",
      "epoch:10 step:8542 [D loss: 0.033530, acc.: 100.00%] [G loss: 1.208423]\n",
      "epoch:10 step:8543 [D loss: 0.058443, acc.: 98.44%] [G loss: 2.534916]\n",
      "epoch:10 step:8544 [D loss: 0.011114, acc.: 100.00%] [G loss: 2.555856]\n",
      "epoch:10 step:8545 [D loss: 0.228148, acc.: 89.06%] [G loss: 2.349701]\n",
      "epoch:10 step:8546 [D loss: 0.040598, acc.: 100.00%] [G loss: 4.453033]\n",
      "epoch:10 step:8547 [D loss: 0.016149, acc.: 100.00%] [G loss: 2.771730]\n",
      "epoch:10 step:8548 [D loss: 0.026585, acc.: 100.00%] [G loss: 3.230131]\n",
      "epoch:10 step:8549 [D loss: 0.063606, acc.: 99.22%] [G loss: 1.615308]\n",
      "epoch:10 step:8550 [D loss: 0.387766, acc.: 82.03%] [G loss: 5.594028]\n",
      "epoch:10 step:8551 [D loss: 0.291399, acc.: 85.94%] [G loss: 4.454180]\n",
      "epoch:10 step:8552 [D loss: 0.143943, acc.: 95.31%] [G loss: 3.866176]\n",
      "epoch:10 step:8553 [D loss: 0.106661, acc.: 95.31%] [G loss: 2.091118]\n",
      "epoch:10 step:8554 [D loss: 0.030157, acc.: 100.00%] [G loss: 1.465293]\n",
      "epoch:10 step:8555 [D loss: 0.057943, acc.: 99.22%] [G loss: 2.354902]\n",
      "epoch:10 step:8556 [D loss: 0.017036, acc.: 100.00%] [G loss: 5.230958]\n",
      "epoch:10 step:8557 [D loss: 0.296725, acc.: 89.06%] [G loss: 4.104234]\n",
      "epoch:10 step:8558 [D loss: 0.110111, acc.: 96.09%] [G loss: 6.104861]\n",
      "epoch:10 step:8559 [D loss: 0.097506, acc.: 97.66%] [G loss: 3.047312]\n",
      "epoch:10 step:8560 [D loss: 0.158585, acc.: 93.75%] [G loss: 5.350239]\n",
      "epoch:10 step:8561 [D loss: 0.034158, acc.: 98.44%] [G loss: 3.905264]\n",
      "epoch:10 step:8562 [D loss: 0.067195, acc.: 98.44%] [G loss: 3.781263]\n",
      "epoch:10 step:8563 [D loss: 0.031029, acc.: 99.22%] [G loss: 4.444160]\n",
      "epoch:10 step:8564 [D loss: 0.212809, acc.: 94.53%] [G loss: 4.369981]\n",
      "epoch:10 step:8565 [D loss: 0.012335, acc.: 100.00%] [G loss: 6.140702]\n",
      "epoch:10 step:8566 [D loss: 0.509080, acc.: 76.56%] [G loss: 1.916424]\n",
      "epoch:10 step:8567 [D loss: 0.356961, acc.: 82.81%] [G loss: 6.011376]\n",
      "epoch:10 step:8568 [D loss: 0.628771, acc.: 72.66%] [G loss: 1.383006]\n",
      "epoch:10 step:8569 [D loss: 0.067957, acc.: 98.44%] [G loss: 2.024654]\n",
      "epoch:10 step:8570 [D loss: 0.036638, acc.: 99.22%] [G loss: 2.736933]\n",
      "epoch:10 step:8571 [D loss: 0.024869, acc.: 100.00%] [G loss: 3.571339]\n",
      "epoch:10 step:8572 [D loss: 0.100208, acc.: 97.66%] [G loss: 2.292495]\n",
      "epoch:10 step:8573 [D loss: 0.008108, acc.: 100.00%] [G loss: 1.564154]\n",
      "epoch:10 step:8574 [D loss: 0.048244, acc.: 99.22%] [G loss: 1.000971]\n",
      "epoch:10 step:8575 [D loss: 0.024053, acc.: 100.00%] [G loss: 2.082209]\n",
      "epoch:10 step:8576 [D loss: 0.028285, acc.: 100.00%] [G loss: 1.744936]\n",
      "epoch:10 step:8577 [D loss: 0.018813, acc.: 100.00%] [G loss: 2.253621]\n",
      "epoch:10 step:8578 [D loss: 0.016870, acc.: 100.00%] [G loss: 2.405411]\n",
      "epoch:10 step:8579 [D loss: 0.308353, acc.: 84.38%] [G loss: 7.620246]\n",
      "epoch:10 step:8580 [D loss: 0.471916, acc.: 75.00%] [G loss: 4.573589]\n",
      "epoch:10 step:8581 [D loss: 0.059297, acc.: 98.44%] [G loss: 5.404838]\n",
      "epoch:10 step:8582 [D loss: 0.017991, acc.: 100.00%] [G loss: 5.406236]\n",
      "epoch:10 step:8583 [D loss: 0.006520, acc.: 100.00%] [G loss: 5.756482]\n",
      "epoch:10 step:8584 [D loss: 0.011279, acc.: 100.00%] [G loss: 5.615414]\n",
      "epoch:10 step:8585 [D loss: 0.014554, acc.: 100.00%] [G loss: 3.645926]\n",
      "epoch:10 step:8586 [D loss: 0.006242, acc.: 100.00%] [G loss: 0.775606]\n",
      "epoch:10 step:8587 [D loss: 0.054995, acc.: 99.22%] [G loss: 0.147531]\n",
      "epoch:10 step:8588 [D loss: 0.054222, acc.: 99.22%] [G loss: 1.100330]\n",
      "epoch:10 step:8589 [D loss: 0.177026, acc.: 94.53%] [G loss: 4.052741]\n",
      "epoch:10 step:8590 [D loss: 0.024430, acc.: 100.00%] [G loss: 5.743142]\n",
      "epoch:10 step:8591 [D loss: 0.004487, acc.: 100.00%] [G loss: 5.248463]\n",
      "epoch:11 step:8592 [D loss: 0.041998, acc.: 100.00%] [G loss: 6.479451]\n",
      "epoch:11 step:8593 [D loss: 0.007383, acc.: 100.00%] [G loss: 4.459361]\n",
      "epoch:11 step:8594 [D loss: 0.048761, acc.: 99.22%] [G loss: 0.018829]\n",
      "epoch:11 step:8595 [D loss: 0.283318, acc.: 87.50%] [G loss: 8.883281]\n",
      "epoch:11 step:8596 [D loss: 1.287577, acc.: 46.09%] [G loss: 2.361368]\n",
      "epoch:11 step:8597 [D loss: 0.010174, acc.: 100.00%] [G loss: 7.296753]\n",
      "epoch:11 step:8598 [D loss: 0.396066, acc.: 79.69%] [G loss: 4.113074]\n",
      "epoch:11 step:8599 [D loss: 0.003693, acc.: 100.00%] [G loss: 5.181103]\n",
      "epoch:11 step:8600 [D loss: 0.035622, acc.: 99.22%] [G loss: 3.035273]\n",
      "epoch:11 step:8601 [D loss: 0.022467, acc.: 100.00%] [G loss: 5.136432]\n",
      "epoch:11 step:8602 [D loss: 0.079683, acc.: 100.00%] [G loss: 6.101332]\n",
      "epoch:11 step:8603 [D loss: 0.432732, acc.: 82.81%] [G loss: 1.346487]\n",
      "epoch:11 step:8604 [D loss: 0.047005, acc.: 99.22%] [G loss: 4.054069]\n",
      "epoch:11 step:8605 [D loss: 0.253738, acc.: 89.84%] [G loss: 4.386754]\n",
      "epoch:11 step:8606 [D loss: 0.010253, acc.: 100.00%] [G loss: 4.423297]\n",
      "epoch:11 step:8607 [D loss: 1.995417, acc.: 32.81%] [G loss: 8.621419]\n",
      "epoch:11 step:8608 [D loss: 0.931436, acc.: 58.59%] [G loss: 5.621309]\n",
      "epoch:11 step:8609 [D loss: 0.065532, acc.: 98.44%] [G loss: 4.259212]\n",
      "epoch:11 step:8610 [D loss: 0.016587, acc.: 100.00%] [G loss: 2.540493]\n",
      "epoch:11 step:8611 [D loss: 0.037453, acc.: 99.22%] [G loss: 2.639125]\n",
      "epoch:11 step:8612 [D loss: 0.103625, acc.: 97.66%] [G loss: 3.092753]\n",
      "epoch:11 step:8613 [D loss: 0.170946, acc.: 94.53%] [G loss: 2.561478]\n",
      "epoch:11 step:8614 [D loss: 0.366662, acc.: 85.16%] [G loss: 5.274258]\n",
      "epoch:11 step:8615 [D loss: 0.049781, acc.: 98.44%] [G loss: 4.967678]\n",
      "epoch:11 step:8616 [D loss: 0.040013, acc.: 100.00%] [G loss: 3.979172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8617 [D loss: 0.034433, acc.: 99.22%] [G loss: 1.787260]\n",
      "epoch:11 step:8618 [D loss: 0.210825, acc.: 91.41%] [G loss: 5.188783]\n",
      "epoch:11 step:8619 [D loss: 0.284615, acc.: 86.72%] [G loss: 3.677887]\n",
      "epoch:11 step:8620 [D loss: 0.084962, acc.: 97.66%] [G loss: 1.786294]\n",
      "epoch:11 step:8621 [D loss: 0.058993, acc.: 97.66%] [G loss: 3.519822]\n",
      "epoch:11 step:8622 [D loss: 0.034080, acc.: 100.00%] [G loss: 3.507991]\n",
      "epoch:11 step:8623 [D loss: 0.032750, acc.: 100.00%] [G loss: 2.902846]\n",
      "epoch:11 step:8624 [D loss: 0.033931, acc.: 99.22%] [G loss: 3.152891]\n",
      "epoch:11 step:8625 [D loss: 0.077615, acc.: 97.66%] [G loss: 5.254759]\n",
      "epoch:11 step:8626 [D loss: 0.142677, acc.: 98.44%] [G loss: 2.777123]\n",
      "epoch:11 step:8627 [D loss: 0.085175, acc.: 98.44%] [G loss: 2.548669]\n",
      "epoch:11 step:8628 [D loss: 0.026859, acc.: 100.00%] [G loss: 4.612539]\n",
      "epoch:11 step:8629 [D loss: 0.230828, acc.: 90.62%] [G loss: 1.756586]\n",
      "epoch:11 step:8630 [D loss: 0.022402, acc.: 100.00%] [G loss: 2.448703]\n",
      "epoch:11 step:8631 [D loss: 0.237619, acc.: 89.84%] [G loss: 6.505699]\n",
      "epoch:11 step:8632 [D loss: 0.249267, acc.: 89.06%] [G loss: 4.358672]\n",
      "epoch:11 step:8633 [D loss: 0.041444, acc.: 99.22%] [G loss: 2.273227]\n",
      "epoch:11 step:8634 [D loss: 0.037246, acc.: 100.00%] [G loss: 4.108655]\n",
      "epoch:11 step:8635 [D loss: 0.072714, acc.: 99.22%] [G loss: 5.178030]\n",
      "epoch:11 step:8636 [D loss: 0.119544, acc.: 97.66%] [G loss: 3.391273]\n",
      "epoch:11 step:8637 [D loss: 0.494362, acc.: 71.09%] [G loss: 8.288817]\n",
      "epoch:11 step:8638 [D loss: 1.701593, acc.: 50.78%] [G loss: 2.028205]\n",
      "epoch:11 step:8639 [D loss: 0.258566, acc.: 87.50%] [G loss: 3.354649]\n",
      "epoch:11 step:8640 [D loss: 0.012728, acc.: 100.00%] [G loss: 5.448416]\n",
      "epoch:11 step:8641 [D loss: 0.196609, acc.: 91.41%] [G loss: 2.529369]\n",
      "epoch:11 step:8642 [D loss: 0.080500, acc.: 97.66%] [G loss: 2.014057]\n",
      "epoch:11 step:8643 [D loss: 0.108512, acc.: 97.66%] [G loss: 5.769811]\n",
      "epoch:11 step:8644 [D loss: 0.053810, acc.: 99.22%] [G loss: 4.778668]\n",
      "epoch:11 step:8645 [D loss: 0.080471, acc.: 98.44%] [G loss: 3.460657]\n",
      "epoch:11 step:8646 [D loss: 0.042874, acc.: 100.00%] [G loss: 3.514037]\n",
      "epoch:11 step:8647 [D loss: 0.096255, acc.: 97.66%] [G loss: 4.566196]\n",
      "epoch:11 step:8648 [D loss: 0.043584, acc.: 99.22%] [G loss: 3.723343]\n",
      "epoch:11 step:8649 [D loss: 0.354539, acc.: 85.16%] [G loss: 5.816092]\n",
      "epoch:11 step:8650 [D loss: 0.804626, acc.: 62.50%] [G loss: 3.674621]\n",
      "epoch:11 step:8651 [D loss: 0.003945, acc.: 100.00%] [G loss: 3.469721]\n",
      "epoch:11 step:8652 [D loss: 0.041524, acc.: 99.22%] [G loss: 3.216014]\n",
      "epoch:11 step:8653 [D loss: 0.025711, acc.: 100.00%] [G loss: 2.501848]\n",
      "epoch:11 step:8654 [D loss: 0.035186, acc.: 100.00%] [G loss: 3.136908]\n",
      "epoch:11 step:8655 [D loss: 0.083039, acc.: 97.66%] [G loss: 4.670599]\n",
      "epoch:11 step:8656 [D loss: 0.886770, acc.: 57.81%] [G loss: 3.784344]\n",
      "epoch:11 step:8657 [D loss: 0.006528, acc.: 100.00%] [G loss: 6.496818]\n",
      "epoch:11 step:8658 [D loss: 0.023064, acc.: 100.00%] [G loss: 5.356693]\n",
      "epoch:11 step:8659 [D loss: 0.033973, acc.: 98.44%] [G loss: 1.576685]\n",
      "epoch:11 step:8660 [D loss: 0.035403, acc.: 100.00%] [G loss: 2.549870]\n",
      "epoch:11 step:8661 [D loss: 0.049810, acc.: 99.22%] [G loss: 0.613212]\n",
      "epoch:11 step:8662 [D loss: 0.076317, acc.: 99.22%] [G loss: 6.373488]\n",
      "epoch:11 step:8663 [D loss: 0.093767, acc.: 97.66%] [G loss: 6.343256]\n",
      "epoch:11 step:8664 [D loss: 0.298323, acc.: 87.50%] [G loss: 6.130396]\n",
      "epoch:11 step:8665 [D loss: 0.014918, acc.: 100.00%] [G loss: 2.075448]\n",
      "epoch:11 step:8666 [D loss: 0.206397, acc.: 92.97%] [G loss: 1.951412]\n",
      "epoch:11 step:8667 [D loss: 0.089057, acc.: 95.31%] [G loss: 4.943661]\n",
      "epoch:11 step:8668 [D loss: 0.491681, acc.: 77.34%] [G loss: 5.217683]\n",
      "epoch:11 step:8669 [D loss: 0.140873, acc.: 93.75%] [G loss: 3.059374]\n",
      "epoch:11 step:8670 [D loss: 0.055623, acc.: 99.22%] [G loss: 1.871638]\n",
      "epoch:11 step:8671 [D loss: 0.231584, acc.: 89.06%] [G loss: 4.860387]\n",
      "epoch:11 step:8672 [D loss: 0.379636, acc.: 78.12%] [G loss: 2.765625]\n",
      "epoch:11 step:8673 [D loss: 0.452904, acc.: 77.34%] [G loss: 7.402478]\n",
      "epoch:11 step:8674 [D loss: 0.918929, acc.: 62.50%] [G loss: 3.817421]\n",
      "epoch:11 step:8675 [D loss: 0.075900, acc.: 98.44%] [G loss: 3.928554]\n",
      "epoch:11 step:8676 [D loss: 0.072785, acc.: 98.44%] [G loss: 4.691163]\n",
      "epoch:11 step:8677 [D loss: 0.034044, acc.: 100.00%] [G loss: 3.684101]\n",
      "epoch:11 step:8678 [D loss: 0.051896, acc.: 99.22%] [G loss: 4.031770]\n",
      "epoch:11 step:8679 [D loss: 0.172291, acc.: 94.53%] [G loss: 3.912610]\n",
      "epoch:11 step:8680 [D loss: 0.130379, acc.: 95.31%] [G loss: 5.190488]\n",
      "epoch:11 step:8681 [D loss: 0.933825, acc.: 57.03%] [G loss: 1.672330]\n",
      "epoch:11 step:8682 [D loss: 0.017976, acc.: 100.00%] [G loss: 4.911244]\n",
      "epoch:11 step:8683 [D loss: 0.024775, acc.: 100.00%] [G loss: 3.381092]\n",
      "epoch:11 step:8684 [D loss: 0.038131, acc.: 100.00%] [G loss: 3.658983]\n",
      "epoch:11 step:8685 [D loss: 0.332406, acc.: 86.72%] [G loss: 4.911431]\n",
      "epoch:11 step:8686 [D loss: 0.052358, acc.: 99.22%] [G loss: 2.270087]\n",
      "epoch:11 step:8687 [D loss: 0.212830, acc.: 90.62%] [G loss: 3.234540]\n",
      "epoch:11 step:8688 [D loss: 0.275796, acc.: 85.94%] [G loss: 4.091928]\n",
      "epoch:11 step:8689 [D loss: 0.145974, acc.: 93.75%] [G loss: 5.209969]\n",
      "epoch:11 step:8690 [D loss: 0.149941, acc.: 92.97%] [G loss: 0.640633]\n",
      "epoch:11 step:8691 [D loss: 0.080699, acc.: 98.44%] [G loss: 3.302389]\n",
      "epoch:11 step:8692 [D loss: 0.017171, acc.: 99.22%] [G loss: 1.472134]\n",
      "epoch:11 step:8693 [D loss: 0.011552, acc.: 100.00%] [G loss: 4.585593]\n",
      "epoch:11 step:8694 [D loss: 0.040828, acc.: 98.44%] [G loss: 3.466441]\n",
      "epoch:11 step:8695 [D loss: 0.065206, acc.: 99.22%] [G loss: 4.010096]\n",
      "epoch:11 step:8696 [D loss: 0.325482, acc.: 88.28%] [G loss: 4.152798]\n",
      "epoch:11 step:8697 [D loss: 0.023344, acc.: 100.00%] [G loss: 3.702598]\n",
      "epoch:11 step:8698 [D loss: 0.547780, acc.: 75.00%] [G loss: 3.477547]\n",
      "epoch:11 step:8699 [D loss: 0.008041, acc.: 100.00%] [G loss: 2.401807]\n",
      "epoch:11 step:8700 [D loss: 0.046356, acc.: 99.22%] [G loss: 2.445659]\n",
      "epoch:11 step:8701 [D loss: 0.121349, acc.: 96.88%] [G loss: 1.725464]\n",
      "epoch:11 step:8702 [D loss: 0.071553, acc.: 99.22%] [G loss: 2.113118]\n",
      "epoch:11 step:8703 [D loss: 0.011535, acc.: 100.00%] [G loss: 2.036130]\n",
      "epoch:11 step:8704 [D loss: 0.027410, acc.: 100.00%] [G loss: 2.877131]\n",
      "epoch:11 step:8705 [D loss: 0.112625, acc.: 96.88%] [G loss: 4.017157]\n",
      "epoch:11 step:8706 [D loss: 0.011980, acc.: 100.00%] [G loss: 3.939190]\n",
      "epoch:11 step:8707 [D loss: 0.174562, acc.: 96.09%] [G loss: 3.579770]\n",
      "epoch:11 step:8708 [D loss: 0.058808, acc.: 100.00%] [G loss: 2.283315]\n",
      "epoch:11 step:8709 [D loss: 0.023821, acc.: 100.00%] [G loss: 1.727386]\n",
      "epoch:11 step:8710 [D loss: 0.055899, acc.: 99.22%] [G loss: 1.066720]\n",
      "epoch:11 step:8711 [D loss: 0.022500, acc.: 100.00%] [G loss: 3.656729]\n",
      "epoch:11 step:8712 [D loss: 0.073645, acc.: 99.22%] [G loss: 1.306484]\n",
      "epoch:11 step:8713 [D loss: 0.109048, acc.: 96.09%] [G loss: 4.901823]\n",
      "epoch:11 step:8714 [D loss: 0.135123, acc.: 96.88%] [G loss: 5.710015]\n",
      "epoch:11 step:8715 [D loss: 0.044817, acc.: 98.44%] [G loss: 3.638790]\n",
      "epoch:11 step:8716 [D loss: 0.051477, acc.: 98.44%] [G loss: 4.011550]\n",
      "epoch:11 step:8717 [D loss: 0.134235, acc.: 96.88%] [G loss: 3.306954]\n",
      "epoch:11 step:8718 [D loss: 0.012107, acc.: 100.00%] [G loss: 3.696404]\n",
      "epoch:11 step:8719 [D loss: 0.109811, acc.: 96.88%] [G loss: 2.294086]\n",
      "epoch:11 step:8720 [D loss: 0.092320, acc.: 97.66%] [G loss: 3.533137]\n",
      "epoch:11 step:8721 [D loss: 0.049679, acc.: 98.44%] [G loss: 3.920784]\n",
      "epoch:11 step:8722 [D loss: 0.016383, acc.: 100.00%] [G loss: 5.242712]\n",
      "epoch:11 step:8723 [D loss: 0.120256, acc.: 96.88%] [G loss: 5.468791]\n",
      "epoch:11 step:8724 [D loss: 0.037803, acc.: 100.00%] [G loss: 4.245453]\n",
      "epoch:11 step:8725 [D loss: 0.035088, acc.: 100.00%] [G loss: 2.046097]\n",
      "epoch:11 step:8726 [D loss: 0.034824, acc.: 100.00%] [G loss: 2.394336]\n",
      "epoch:11 step:8727 [D loss: 0.036508, acc.: 100.00%] [G loss: 1.662825]\n",
      "epoch:11 step:8728 [D loss: 0.016833, acc.: 100.00%] [G loss: 1.674948]\n",
      "epoch:11 step:8729 [D loss: 0.006462, acc.: 100.00%] [G loss: 3.717740]\n",
      "epoch:11 step:8730 [D loss: 0.079600, acc.: 99.22%] [G loss: 2.567245]\n",
      "epoch:11 step:8731 [D loss: 0.107021, acc.: 96.88%] [G loss: 5.617397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8732 [D loss: 0.100590, acc.: 96.88%] [G loss: 4.001097]\n",
      "epoch:11 step:8733 [D loss: 0.020294, acc.: 100.00%] [G loss: 2.935797]\n",
      "epoch:11 step:8734 [D loss: 0.029271, acc.: 100.00%] [G loss: 0.822505]\n",
      "epoch:11 step:8735 [D loss: 0.103289, acc.: 96.88%] [G loss: 5.266441]\n",
      "epoch:11 step:8736 [D loss: 0.299660, acc.: 85.94%] [G loss: 0.780677]\n",
      "epoch:11 step:8737 [D loss: 0.750564, acc.: 64.84%] [G loss: 9.369688]\n",
      "epoch:11 step:8738 [D loss: 2.784642, acc.: 49.22%] [G loss: 1.992536]\n",
      "epoch:11 step:8739 [D loss: 0.362707, acc.: 82.81%] [G loss: 5.529360]\n",
      "epoch:11 step:8740 [D loss: 0.354711, acc.: 83.59%] [G loss: 2.104449]\n",
      "epoch:11 step:8741 [D loss: 0.083541, acc.: 96.88%] [G loss: 3.032574]\n",
      "epoch:11 step:8742 [D loss: 0.013021, acc.: 100.00%] [G loss: 4.698970]\n",
      "epoch:11 step:8743 [D loss: 0.037437, acc.: 99.22%] [G loss: 3.098391]\n",
      "epoch:11 step:8744 [D loss: 0.491820, acc.: 75.78%] [G loss: 5.979643]\n",
      "epoch:11 step:8745 [D loss: 0.494478, acc.: 75.00%] [G loss: 5.086800]\n",
      "epoch:11 step:8746 [D loss: 0.069418, acc.: 99.22%] [G loss: 3.894715]\n",
      "epoch:11 step:8747 [D loss: 0.033518, acc.: 100.00%] [G loss: 3.641531]\n",
      "epoch:11 step:8748 [D loss: 0.014546, acc.: 100.00%] [G loss: 4.144989]\n",
      "epoch:11 step:8749 [D loss: 0.102655, acc.: 96.88%] [G loss: 3.202444]\n",
      "epoch:11 step:8750 [D loss: 0.010569, acc.: 100.00%] [G loss: 3.574258]\n",
      "epoch:11 step:8751 [D loss: 0.081970, acc.: 99.22%] [G loss: 4.202743]\n",
      "epoch:11 step:8752 [D loss: 0.114204, acc.: 97.66%] [G loss: 4.126263]\n",
      "epoch:11 step:8753 [D loss: 0.047641, acc.: 99.22%] [G loss: 3.909600]\n",
      "epoch:11 step:8754 [D loss: 0.040662, acc.: 99.22%] [G loss: 3.514857]\n",
      "epoch:11 step:8755 [D loss: 1.351260, acc.: 47.66%] [G loss: 7.849226]\n",
      "epoch:11 step:8756 [D loss: 1.693906, acc.: 50.78%] [G loss: 2.808520]\n",
      "epoch:11 step:8757 [D loss: 0.263682, acc.: 89.06%] [G loss: 3.625462]\n",
      "epoch:11 step:8758 [D loss: 0.049606, acc.: 100.00%] [G loss: 2.900305]\n",
      "epoch:11 step:8759 [D loss: 0.088684, acc.: 96.88%] [G loss: 4.173173]\n",
      "epoch:11 step:8760 [D loss: 0.054003, acc.: 99.22%] [G loss: 2.347958]\n",
      "epoch:11 step:8761 [D loss: 0.130504, acc.: 98.44%] [G loss: 0.841421]\n",
      "epoch:11 step:8762 [D loss: 0.102831, acc.: 97.66%] [G loss: 2.236454]\n",
      "epoch:11 step:8763 [D loss: 0.027326, acc.: 100.00%] [G loss: 3.911195]\n",
      "epoch:11 step:8764 [D loss: 0.097511, acc.: 99.22%] [G loss: 4.782564]\n",
      "epoch:11 step:8765 [D loss: 0.523247, acc.: 71.88%] [G loss: 7.190183]\n",
      "epoch:11 step:8766 [D loss: 0.595980, acc.: 68.75%] [G loss: 4.556067]\n",
      "epoch:11 step:8767 [D loss: 1.021211, acc.: 47.66%] [G loss: 4.286458]\n",
      "epoch:11 step:8768 [D loss: 0.007921, acc.: 100.00%] [G loss: 3.006221]\n",
      "epoch:11 step:8769 [D loss: 0.283974, acc.: 84.38%] [G loss: 3.199059]\n",
      "epoch:11 step:8770 [D loss: 0.220704, acc.: 92.19%] [G loss: 2.085480]\n",
      "epoch:11 step:8771 [D loss: 0.032915, acc.: 100.00%] [G loss: 5.137424]\n",
      "epoch:11 step:8772 [D loss: 0.070844, acc.: 99.22%] [G loss: 5.607841]\n",
      "epoch:11 step:8773 [D loss: 0.037142, acc.: 100.00%] [G loss: 4.527419]\n",
      "epoch:11 step:8774 [D loss: 0.123188, acc.: 97.66%] [G loss: 3.853507]\n",
      "epoch:11 step:8775 [D loss: 0.043402, acc.: 100.00%] [G loss: 3.888481]\n",
      "epoch:11 step:8776 [D loss: 0.022159, acc.: 100.00%] [G loss: 1.926507]\n",
      "epoch:11 step:8777 [D loss: 0.041479, acc.: 100.00%] [G loss: 4.834317]\n",
      "epoch:11 step:8778 [D loss: 0.267405, acc.: 91.41%] [G loss: 4.475865]\n",
      "epoch:11 step:8779 [D loss: 0.084257, acc.: 99.22%] [G loss: 2.940354]\n",
      "epoch:11 step:8780 [D loss: 0.183569, acc.: 94.53%] [G loss: 2.709847]\n",
      "epoch:11 step:8781 [D loss: 0.430371, acc.: 79.69%] [G loss: 6.272847]\n",
      "epoch:11 step:8782 [D loss: 0.418455, acc.: 76.56%] [G loss: 5.077426]\n",
      "epoch:11 step:8783 [D loss: 0.059533, acc.: 100.00%] [G loss: 1.941971]\n",
      "epoch:11 step:8784 [D loss: 0.069553, acc.: 99.22%] [G loss: 1.333844]\n",
      "epoch:11 step:8785 [D loss: 0.026151, acc.: 100.00%] [G loss: 4.280355]\n",
      "epoch:11 step:8786 [D loss: 0.054421, acc.: 100.00%] [G loss: 1.260701]\n",
      "epoch:11 step:8787 [D loss: 0.154608, acc.: 97.66%] [G loss: 4.552783]\n",
      "epoch:11 step:8788 [D loss: 0.045280, acc.: 99.22%] [G loss: 4.772034]\n",
      "epoch:11 step:8789 [D loss: 0.054520, acc.: 100.00%] [G loss: 4.059114]\n",
      "epoch:11 step:8790 [D loss: 0.113257, acc.: 97.66%] [G loss: 3.149134]\n",
      "epoch:11 step:8791 [D loss: 0.048079, acc.: 99.22%] [G loss: 2.087376]\n",
      "epoch:11 step:8792 [D loss: 0.122373, acc.: 97.66%] [G loss: 4.884239]\n",
      "epoch:11 step:8793 [D loss: 0.171831, acc.: 94.53%] [G loss: 4.165045]\n",
      "epoch:11 step:8794 [D loss: 0.218189, acc.: 91.41%] [G loss: 0.879246]\n",
      "epoch:11 step:8795 [D loss: 0.301385, acc.: 83.59%] [G loss: 6.469549]\n",
      "epoch:11 step:8796 [D loss: 0.537836, acc.: 68.75%] [G loss: 3.676661]\n",
      "epoch:11 step:8797 [D loss: 0.064183, acc.: 100.00%] [G loss: 4.127497]\n",
      "epoch:11 step:8798 [D loss: 0.030044, acc.: 99.22%] [G loss: 3.052590]\n",
      "epoch:11 step:8799 [D loss: 0.023436, acc.: 100.00%] [G loss: 4.216974]\n",
      "epoch:11 step:8800 [D loss: 0.199581, acc.: 96.88%] [G loss: 5.328095]\n",
      "epoch:11 step:8801 [D loss: 0.043179, acc.: 98.44%] [G loss: 4.419838]\n",
      "epoch:11 step:8802 [D loss: 0.012786, acc.: 100.00%] [G loss: 2.180404]\n",
      "epoch:11 step:8803 [D loss: 0.046679, acc.: 99.22%] [G loss: 2.867470]\n",
      "epoch:11 step:8804 [D loss: 0.061648, acc.: 98.44%] [G loss: 2.665005]\n",
      "epoch:11 step:8805 [D loss: 0.291036, acc.: 90.62%] [G loss: 3.992593]\n",
      "epoch:11 step:8806 [D loss: 0.024259, acc.: 100.00%] [G loss: 5.144424]\n",
      "epoch:11 step:8807 [D loss: 0.035704, acc.: 99.22%] [G loss: 4.206203]\n",
      "epoch:11 step:8808 [D loss: 0.079934, acc.: 98.44%] [G loss: 1.948874]\n",
      "epoch:11 step:8809 [D loss: 0.515219, acc.: 77.34%] [G loss: 1.287445]\n",
      "epoch:11 step:8810 [D loss: 0.125690, acc.: 92.97%] [G loss: 2.688025]\n",
      "epoch:11 step:8811 [D loss: 0.029830, acc.: 99.22%] [G loss: 3.769338]\n",
      "epoch:11 step:8812 [D loss: 0.042086, acc.: 99.22%] [G loss: 3.687156]\n",
      "epoch:11 step:8813 [D loss: 0.252749, acc.: 89.84%] [G loss: 3.396548]\n",
      "epoch:11 step:8814 [D loss: 0.044391, acc.: 100.00%] [G loss: 4.389566]\n",
      "epoch:11 step:8815 [D loss: 0.048564, acc.: 100.00%] [G loss: 2.976964]\n",
      "epoch:11 step:8816 [D loss: 0.019842, acc.: 100.00%] [G loss: 3.426771]\n",
      "epoch:11 step:8817 [D loss: 0.071795, acc.: 98.44%] [G loss: 1.795640]\n",
      "epoch:11 step:8818 [D loss: 0.034405, acc.: 99.22%] [G loss: 3.518067]\n",
      "epoch:11 step:8819 [D loss: 0.024893, acc.: 100.00%] [G loss: 3.028507]\n",
      "epoch:11 step:8820 [D loss: 1.914713, acc.: 35.16%] [G loss: 9.551178]\n",
      "epoch:11 step:8821 [D loss: 3.141386, acc.: 50.00%] [G loss: 4.726372]\n",
      "epoch:11 step:8822 [D loss: 0.504549, acc.: 72.66%] [G loss: 1.892484]\n",
      "epoch:11 step:8823 [D loss: 0.659478, acc.: 69.53%] [G loss: 3.727114]\n",
      "epoch:11 step:8824 [D loss: 0.061122, acc.: 96.88%] [G loss: 6.511277]\n",
      "epoch:11 step:8825 [D loss: 0.661646, acc.: 64.84%] [G loss: 4.421307]\n",
      "epoch:11 step:8826 [D loss: 0.047724, acc.: 100.00%] [G loss: 3.733404]\n",
      "epoch:11 step:8827 [D loss: 0.033883, acc.: 99.22%] [G loss: 2.049060]\n",
      "epoch:11 step:8828 [D loss: 0.067067, acc.: 99.22%] [G loss: 2.529567]\n",
      "epoch:11 step:8829 [D loss: 0.045929, acc.: 100.00%] [G loss: 2.862626]\n",
      "epoch:11 step:8830 [D loss: 0.037910, acc.: 100.00%] [G loss: 2.596164]\n",
      "epoch:11 step:8831 [D loss: 0.155710, acc.: 97.66%] [G loss: 1.690001]\n",
      "epoch:11 step:8832 [D loss: 0.276112, acc.: 88.28%] [G loss: 4.673821]\n",
      "epoch:11 step:8833 [D loss: 0.361198, acc.: 80.47%] [G loss: 3.690267]\n",
      "epoch:11 step:8834 [D loss: 0.066659, acc.: 99.22%] [G loss: 1.802165]\n",
      "epoch:11 step:8835 [D loss: 0.066806, acc.: 99.22%] [G loss: 2.174917]\n",
      "epoch:11 step:8836 [D loss: 0.035533, acc.: 99.22%] [G loss: 3.603515]\n",
      "epoch:11 step:8837 [D loss: 0.126345, acc.: 96.88%] [G loss: 3.010485]\n",
      "epoch:11 step:8838 [D loss: 0.340493, acc.: 81.25%] [G loss: 5.255853]\n",
      "epoch:11 step:8839 [D loss: 0.146243, acc.: 93.75%] [G loss: 2.919796]\n",
      "epoch:11 step:8840 [D loss: 0.179736, acc.: 92.19%] [G loss: 2.764332]\n",
      "epoch:11 step:8841 [D loss: 0.357799, acc.: 84.38%] [G loss: 5.156388]\n",
      "epoch:11 step:8842 [D loss: 0.135186, acc.: 93.75%] [G loss: 6.014246]\n",
      "epoch:11 step:8843 [D loss: 0.242190, acc.: 89.06%] [G loss: 2.157470]\n",
      "epoch:11 step:8844 [D loss: 0.088399, acc.: 98.44%] [G loss: 0.351771]\n",
      "epoch:11 step:8845 [D loss: 0.028816, acc.: 100.00%] [G loss: 3.568820]\n",
      "epoch:11 step:8846 [D loss: 0.032678, acc.: 100.00%] [G loss: 3.433877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8847 [D loss: 0.053912, acc.: 100.00%] [G loss: 1.269415]\n",
      "epoch:11 step:8848 [D loss: 0.111153, acc.: 96.09%] [G loss: 3.776484]\n",
      "epoch:11 step:8849 [D loss: 0.055403, acc.: 100.00%] [G loss: 1.937822]\n",
      "epoch:11 step:8850 [D loss: 0.082172, acc.: 99.22%] [G loss: 2.180993]\n",
      "epoch:11 step:8851 [D loss: 0.039281, acc.: 100.00%] [G loss: 3.364120]\n",
      "epoch:11 step:8852 [D loss: 0.308060, acc.: 90.62%] [G loss: 1.930554]\n",
      "epoch:11 step:8853 [D loss: 0.013891, acc.: 100.00%] [G loss: 6.156935]\n",
      "epoch:11 step:8854 [D loss: 0.235540, acc.: 90.62%] [G loss: 0.439229]\n",
      "epoch:11 step:8855 [D loss: 0.202556, acc.: 94.53%] [G loss: 4.552563]\n",
      "epoch:11 step:8856 [D loss: 0.183955, acc.: 92.19%] [G loss: 1.176040]\n",
      "epoch:11 step:8857 [D loss: 0.034126, acc.: 100.00%] [G loss: 0.908209]\n",
      "epoch:11 step:8858 [D loss: 0.035057, acc.: 100.00%] [G loss: 2.850200]\n",
      "epoch:11 step:8859 [D loss: 0.030247, acc.: 100.00%] [G loss: 1.619372]\n",
      "epoch:11 step:8860 [D loss: 0.020966, acc.: 100.00%] [G loss: 1.183522]\n",
      "epoch:11 step:8861 [D loss: 0.063937, acc.: 100.00%] [G loss: 2.723244]\n",
      "epoch:11 step:8862 [D loss: 0.122803, acc.: 96.88%] [G loss: 2.522852]\n",
      "epoch:11 step:8863 [D loss: 0.039955, acc.: 99.22%] [G loss: 2.899949]\n",
      "epoch:11 step:8864 [D loss: 0.130850, acc.: 96.88%] [G loss: 0.873929]\n",
      "epoch:11 step:8865 [D loss: 0.055678, acc.: 98.44%] [G loss: 3.076255]\n",
      "epoch:11 step:8866 [D loss: 0.042986, acc.: 100.00%] [G loss: 2.611939]\n",
      "epoch:11 step:8867 [D loss: 0.053208, acc.: 98.44%] [G loss: 1.976226]\n",
      "epoch:11 step:8868 [D loss: 0.114940, acc.: 98.44%] [G loss: 5.024465]\n",
      "epoch:11 step:8869 [D loss: 0.017417, acc.: 100.00%] [G loss: 6.631155]\n",
      "epoch:11 step:8870 [D loss: 0.799923, acc.: 53.91%] [G loss: 5.536550]\n",
      "epoch:11 step:8871 [D loss: 0.076519, acc.: 97.66%] [G loss: 6.744809]\n",
      "epoch:11 step:8872 [D loss: 0.021989, acc.: 100.00%] [G loss: 5.182161]\n",
      "epoch:11 step:8873 [D loss: 0.012053, acc.: 100.00%] [G loss: 2.876632]\n",
      "epoch:11 step:8874 [D loss: 0.027273, acc.: 100.00%] [G loss: 1.807466]\n",
      "epoch:11 step:8875 [D loss: 0.015414, acc.: 100.00%] [G loss: 4.567872]\n",
      "epoch:11 step:8876 [D loss: 0.029401, acc.: 100.00%] [G loss: 5.076675]\n",
      "epoch:11 step:8877 [D loss: 1.142324, acc.: 52.34%] [G loss: 7.453144]\n",
      "epoch:11 step:8878 [D loss: 1.802844, acc.: 50.78%] [G loss: 3.876848]\n",
      "epoch:11 step:8879 [D loss: 0.314946, acc.: 85.94%] [G loss: 2.389903]\n",
      "epoch:11 step:8880 [D loss: 0.122777, acc.: 96.09%] [G loss: 3.953798]\n",
      "epoch:11 step:8881 [D loss: 0.021578, acc.: 100.00%] [G loss: 4.007409]\n",
      "epoch:11 step:8882 [D loss: 0.281207, acc.: 90.62%] [G loss: 2.957872]\n",
      "epoch:11 step:8883 [D loss: 0.073730, acc.: 99.22%] [G loss: 0.676178]\n",
      "epoch:11 step:8884 [D loss: 0.027775, acc.: 100.00%] [G loss: 3.191647]\n",
      "epoch:11 step:8885 [D loss: 0.019387, acc.: 100.00%] [G loss: 4.067524]\n",
      "epoch:11 step:8886 [D loss: 0.067409, acc.: 99.22%] [G loss: 0.293806]\n",
      "epoch:11 step:8887 [D loss: 0.293237, acc.: 84.38%] [G loss: 5.505061]\n",
      "epoch:11 step:8888 [D loss: 0.177658, acc.: 94.53%] [G loss: 5.890000]\n",
      "epoch:11 step:8889 [D loss: 0.104223, acc.: 97.66%] [G loss: 4.742219]\n",
      "epoch:11 step:8890 [D loss: 0.115803, acc.: 96.09%] [G loss: 2.028363]\n",
      "epoch:11 step:8891 [D loss: 0.346730, acc.: 81.25%] [G loss: 5.768552]\n",
      "epoch:11 step:8892 [D loss: 0.068881, acc.: 99.22%] [G loss: 3.922708]\n",
      "epoch:11 step:8893 [D loss: 0.187267, acc.: 91.41%] [G loss: 4.970747]\n",
      "epoch:11 step:8894 [D loss: 0.092674, acc.: 97.66%] [G loss: 4.388726]\n",
      "epoch:11 step:8895 [D loss: 0.013394, acc.: 100.00%] [G loss: 4.525924]\n",
      "epoch:11 step:8896 [D loss: 0.069901, acc.: 98.44%] [G loss: 3.173257]\n",
      "epoch:11 step:8897 [D loss: 0.006282, acc.: 100.00%] [G loss: 3.073147]\n",
      "epoch:11 step:8898 [D loss: 0.150214, acc.: 93.75%] [G loss: 4.012630]\n",
      "epoch:11 step:8899 [D loss: 0.010814, acc.: 100.00%] [G loss: 5.181377]\n",
      "epoch:11 step:8900 [D loss: 0.878000, acc.: 55.47%] [G loss: 4.662672]\n",
      "epoch:11 step:8901 [D loss: 0.005914, acc.: 100.00%] [G loss: 5.704687]\n",
      "epoch:11 step:8902 [D loss: 0.025419, acc.: 100.00%] [G loss: 5.416982]\n",
      "epoch:11 step:8903 [D loss: 0.130416, acc.: 96.09%] [G loss: 3.840860]\n",
      "epoch:11 step:8904 [D loss: 0.071564, acc.: 96.88%] [G loss: 3.651769]\n",
      "epoch:11 step:8905 [D loss: 0.046446, acc.: 100.00%] [G loss: 4.098059]\n",
      "epoch:11 step:8906 [D loss: 0.168732, acc.: 96.09%] [G loss: 3.394298]\n",
      "epoch:11 step:8907 [D loss: 0.092792, acc.: 97.66%] [G loss: 3.545218]\n",
      "epoch:11 step:8908 [D loss: 0.023377, acc.: 100.00%] [G loss: 3.824238]\n",
      "epoch:11 step:8909 [D loss: 0.007379, acc.: 100.00%] [G loss: 4.130765]\n",
      "epoch:11 step:8910 [D loss: 0.014182, acc.: 100.00%] [G loss: 4.366276]\n",
      "epoch:11 step:8911 [D loss: 0.168326, acc.: 92.97%] [G loss: 0.132948]\n",
      "epoch:11 step:8912 [D loss: 0.163433, acc.: 93.75%] [G loss: 5.098146]\n",
      "epoch:11 step:8913 [D loss: 0.033158, acc.: 99.22%] [G loss: 6.323506]\n",
      "epoch:11 step:8914 [D loss: 0.052697, acc.: 99.22%] [G loss: 5.665825]\n",
      "epoch:11 step:8915 [D loss: 0.042210, acc.: 99.22%] [G loss: 0.700409]\n",
      "epoch:11 step:8916 [D loss: 0.064994, acc.: 100.00%] [G loss: 5.061928]\n",
      "epoch:11 step:8917 [D loss: 0.025566, acc.: 100.00%] [G loss: 5.251065]\n",
      "epoch:11 step:8918 [D loss: 0.009101, acc.: 100.00%] [G loss: 5.030646]\n",
      "epoch:11 step:8919 [D loss: 0.009056, acc.: 100.00%] [G loss: 4.830021]\n",
      "epoch:11 step:8920 [D loss: 0.045900, acc.: 100.00%] [G loss: 0.522717]\n",
      "epoch:11 step:8921 [D loss: 0.039150, acc.: 99.22%] [G loss: 4.094447]\n",
      "epoch:11 step:8922 [D loss: 0.064552, acc.: 100.00%] [G loss: 5.112397]\n",
      "epoch:11 step:8923 [D loss: 0.052323, acc.: 100.00%] [G loss: 2.342103]\n",
      "epoch:11 step:8924 [D loss: 0.022427, acc.: 100.00%] [G loss: 4.466469]\n",
      "epoch:11 step:8925 [D loss: 0.125109, acc.: 96.88%] [G loss: 5.395876]\n",
      "epoch:11 step:8926 [D loss: 0.143019, acc.: 96.09%] [G loss: 4.304608]\n",
      "epoch:11 step:8927 [D loss: 0.050120, acc.: 100.00%] [G loss: 0.390235]\n",
      "epoch:11 step:8928 [D loss: 0.064929, acc.: 98.44%] [G loss: 5.239555]\n",
      "epoch:11 step:8929 [D loss: 0.048673, acc.: 100.00%] [G loss: 2.408518]\n",
      "epoch:11 step:8930 [D loss: 0.031533, acc.: 100.00%] [G loss: 1.856174]\n",
      "epoch:11 step:8931 [D loss: 0.202815, acc.: 92.97%] [G loss: 1.034563]\n",
      "epoch:11 step:8932 [D loss: 0.095366, acc.: 98.44%] [G loss: 2.320060]\n",
      "epoch:11 step:8933 [D loss: 0.036982, acc.: 100.00%] [G loss: 5.109146]\n",
      "epoch:11 step:8934 [D loss: 2.005290, acc.: 30.47%] [G loss: 9.367903]\n",
      "epoch:11 step:8935 [D loss: 0.753694, acc.: 64.84%] [G loss: 6.542560]\n",
      "epoch:11 step:8936 [D loss: 0.023384, acc.: 100.00%] [G loss: 5.943104]\n",
      "epoch:11 step:8937 [D loss: 0.017677, acc.: 100.00%] [G loss: 5.347621]\n",
      "epoch:11 step:8938 [D loss: 0.024302, acc.: 100.00%] [G loss: 4.939697]\n",
      "epoch:11 step:8939 [D loss: 0.037742, acc.: 98.44%] [G loss: 2.258853]\n",
      "epoch:11 step:8940 [D loss: 0.032884, acc.: 100.00%] [G loss: 3.458253]\n",
      "epoch:11 step:8941 [D loss: 0.083549, acc.: 99.22%] [G loss: 3.690423]\n",
      "epoch:11 step:8942 [D loss: 0.028383, acc.: 100.00%] [G loss: 2.204992]\n",
      "epoch:11 step:8943 [D loss: 0.080319, acc.: 99.22%] [G loss: 2.607948]\n",
      "epoch:11 step:8944 [D loss: 0.437946, acc.: 82.81%] [G loss: 3.402120]\n",
      "epoch:11 step:8945 [D loss: 0.055144, acc.: 99.22%] [G loss: 4.099798]\n",
      "epoch:11 step:8946 [D loss: 0.025725, acc.: 100.00%] [G loss: 4.469388]\n",
      "epoch:11 step:8947 [D loss: 0.028218, acc.: 100.00%] [G loss: 3.527572]\n",
      "epoch:11 step:8948 [D loss: 0.043018, acc.: 100.00%] [G loss: 2.363068]\n",
      "epoch:11 step:8949 [D loss: 0.102578, acc.: 99.22%] [G loss: 2.911470]\n",
      "epoch:11 step:8950 [D loss: 0.066527, acc.: 98.44%] [G loss: 4.141281]\n",
      "epoch:11 step:8951 [D loss: 0.057567, acc.: 99.22%] [G loss: 2.815859]\n",
      "epoch:11 step:8952 [D loss: 0.199491, acc.: 92.97%] [G loss: 1.668146]\n",
      "epoch:11 step:8953 [D loss: 0.148516, acc.: 96.09%] [G loss: 5.414482]\n",
      "epoch:11 step:8954 [D loss: 0.221996, acc.: 92.19%] [G loss: 1.393420]\n",
      "epoch:11 step:8955 [D loss: 0.147643, acc.: 96.88%] [G loss: 6.074339]\n",
      "epoch:11 step:8956 [D loss: 0.038980, acc.: 99.22%] [G loss: 5.216118]\n",
      "epoch:11 step:8957 [D loss: 0.245803, acc.: 87.50%] [G loss: 1.535868]\n",
      "epoch:11 step:8958 [D loss: 0.038025, acc.: 100.00%] [G loss: 3.323247]\n",
      "epoch:11 step:8959 [D loss: 0.030672, acc.: 100.00%] [G loss: 2.966959]\n",
      "epoch:11 step:8960 [D loss: 0.020941, acc.: 100.00%] [G loss: 2.016357]\n",
      "epoch:11 step:8961 [D loss: 0.114767, acc.: 96.88%] [G loss: 3.519065]\n",
      "epoch:11 step:8962 [D loss: 0.094609, acc.: 96.88%] [G loss: 4.911921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8963 [D loss: 0.084629, acc.: 96.09%] [G loss: 3.433800]\n",
      "epoch:11 step:8964 [D loss: 0.434444, acc.: 82.03%] [G loss: 4.171849]\n",
      "epoch:11 step:8965 [D loss: 0.079366, acc.: 97.66%] [G loss: 4.556170]\n",
      "epoch:11 step:8966 [D loss: 0.075493, acc.: 98.44%] [G loss: 1.319141]\n",
      "epoch:11 step:8967 [D loss: 0.176170, acc.: 93.75%] [G loss: 5.798475]\n",
      "epoch:11 step:8968 [D loss: 0.023157, acc.: 100.00%] [G loss: 6.715668]\n",
      "epoch:11 step:8969 [D loss: 0.111554, acc.: 96.09%] [G loss: 3.945144]\n",
      "epoch:11 step:8970 [D loss: 0.018335, acc.: 100.00%] [G loss: 3.577320]\n",
      "epoch:11 step:8971 [D loss: 0.026600, acc.: 100.00%] [G loss: 3.678742]\n",
      "epoch:11 step:8972 [D loss: 0.053632, acc.: 99.22%] [G loss: 4.326210]\n",
      "epoch:11 step:8973 [D loss: 0.321889, acc.: 87.50%] [G loss: 4.945532]\n",
      "epoch:11 step:8974 [D loss: 0.005058, acc.: 100.00%] [G loss: 5.797943]\n",
      "epoch:11 step:8975 [D loss: 0.055256, acc.: 98.44%] [G loss: 5.134482]\n",
      "epoch:11 step:8976 [D loss: 0.024553, acc.: 100.00%] [G loss: 4.101894]\n",
      "epoch:11 step:8977 [D loss: 0.261711, acc.: 87.50%] [G loss: 6.411836]\n",
      "epoch:11 step:8978 [D loss: 0.370951, acc.: 79.69%] [G loss: 1.271974]\n",
      "epoch:11 step:8979 [D loss: 0.227030, acc.: 87.50%] [G loss: 5.944524]\n",
      "epoch:11 step:8980 [D loss: 0.187877, acc.: 91.41%] [G loss: 4.465340]\n",
      "epoch:11 step:8981 [D loss: 0.022274, acc.: 100.00%] [G loss: 3.480841]\n",
      "epoch:11 step:8982 [D loss: 0.011235, acc.: 100.00%] [G loss: 3.975132]\n",
      "epoch:11 step:8983 [D loss: 0.111122, acc.: 96.88%] [G loss: 5.072457]\n",
      "epoch:11 step:8984 [D loss: 0.396685, acc.: 78.91%] [G loss: 5.146267]\n",
      "epoch:11 step:8985 [D loss: 0.003726, acc.: 100.00%] [G loss: 4.581211]\n",
      "epoch:11 step:8986 [D loss: 0.136073, acc.: 96.88%] [G loss: 2.366653]\n",
      "epoch:11 step:8987 [D loss: 0.131657, acc.: 96.09%] [G loss: 4.217892]\n",
      "epoch:11 step:8988 [D loss: 0.024642, acc.: 100.00%] [G loss: 6.711483]\n",
      "epoch:11 step:8989 [D loss: 0.150156, acc.: 96.09%] [G loss: 4.239478]\n",
      "epoch:11 step:8990 [D loss: 0.301547, acc.: 83.59%] [G loss: 6.473217]\n",
      "epoch:11 step:8991 [D loss: 0.059684, acc.: 97.66%] [G loss: 8.841145]\n",
      "epoch:11 step:8992 [D loss: 1.097153, acc.: 57.03%] [G loss: 0.786673]\n",
      "epoch:11 step:8993 [D loss: 0.698029, acc.: 70.31%] [G loss: 7.234462]\n",
      "epoch:11 step:8994 [D loss: 0.410391, acc.: 79.69%] [G loss: 5.850958]\n",
      "epoch:11 step:8995 [D loss: 0.020482, acc.: 99.22%] [G loss: 2.576132]\n",
      "epoch:11 step:8996 [D loss: 0.075467, acc.: 98.44%] [G loss: 0.911184]\n",
      "epoch:11 step:8997 [D loss: 0.013179, acc.: 100.00%] [G loss: 3.787704]\n",
      "epoch:11 step:8998 [D loss: 0.077389, acc.: 96.88%] [G loss: 0.519315]\n",
      "epoch:11 step:8999 [D loss: 0.075222, acc.: 97.66%] [G loss: 4.243784]\n",
      "epoch:11 step:9000 [D loss: 0.072643, acc.: 97.66%] [G loss: 1.496843]\n",
      "epoch:11 step:9001 [D loss: 0.020299, acc.: 100.00%] [G loss: 4.318639]\n",
      "epoch:11 step:9002 [D loss: 0.066671, acc.: 98.44%] [G loss: 3.594661]\n",
      "epoch:11 step:9003 [D loss: 0.169544, acc.: 92.97%] [G loss: 2.159196]\n",
      "epoch:11 step:9004 [D loss: 0.112959, acc.: 96.88%] [G loss: 3.427824]\n",
      "epoch:11 step:9005 [D loss: 0.032426, acc.: 99.22%] [G loss: 2.965043]\n",
      "epoch:11 step:9006 [D loss: 0.076385, acc.: 96.09%] [G loss: 2.300212]\n",
      "epoch:11 step:9007 [D loss: 0.026777, acc.: 100.00%] [G loss: 2.689032]\n",
      "epoch:11 step:9008 [D loss: 0.471153, acc.: 76.56%] [G loss: 4.131605]\n",
      "epoch:11 step:9009 [D loss: 0.023207, acc.: 100.00%] [G loss: 5.562186]\n",
      "epoch:11 step:9010 [D loss: 0.179561, acc.: 90.62%] [G loss: 2.043116]\n",
      "epoch:11 step:9011 [D loss: 0.176056, acc.: 93.75%] [G loss: 4.498637]\n",
      "epoch:11 step:9012 [D loss: 0.033885, acc.: 100.00%] [G loss: 5.962914]\n",
      "epoch:11 step:9013 [D loss: 0.106162, acc.: 96.09%] [G loss: 4.529554]\n",
      "epoch:11 step:9014 [D loss: 0.008926, acc.: 100.00%] [G loss: 3.239257]\n",
      "epoch:11 step:9015 [D loss: 0.039518, acc.: 99.22%] [G loss: 2.605679]\n",
      "epoch:11 step:9016 [D loss: 0.016345, acc.: 100.00%] [G loss: 3.184862]\n",
      "epoch:11 step:9017 [D loss: 0.005186, acc.: 100.00%] [G loss: 3.466915]\n",
      "epoch:11 step:9018 [D loss: 0.012051, acc.: 100.00%] [G loss: 2.540237]\n",
      "epoch:11 step:9019 [D loss: 0.043063, acc.: 99.22%] [G loss: 3.495282]\n",
      "epoch:11 step:9020 [D loss: 0.021011, acc.: 100.00%] [G loss: 4.003908]\n",
      "epoch:11 step:9021 [D loss: 0.023911, acc.: 100.00%] [G loss: 3.908439]\n",
      "epoch:11 step:9022 [D loss: 0.122119, acc.: 98.44%] [G loss: 1.974052]\n",
      "epoch:11 step:9023 [D loss: 0.040804, acc.: 99.22%] [G loss: 3.733667]\n",
      "epoch:11 step:9024 [D loss: 0.002715, acc.: 100.00%] [G loss: 2.797600]\n",
      "epoch:11 step:9025 [D loss: 0.060954, acc.: 99.22%] [G loss: 2.456233]\n",
      "epoch:11 step:9026 [D loss: 0.005246, acc.: 100.00%] [G loss: 3.424652]\n",
      "epoch:11 step:9027 [D loss: 0.058991, acc.: 98.44%] [G loss: 1.258911]\n",
      "epoch:11 step:9028 [D loss: 0.159844, acc.: 93.75%] [G loss: 2.235131]\n",
      "epoch:11 step:9029 [D loss: 0.019898, acc.: 100.00%] [G loss: 0.719833]\n",
      "epoch:11 step:9030 [D loss: 0.030208, acc.: 99.22%] [G loss: 1.634031]\n",
      "epoch:11 step:9031 [D loss: 0.021259, acc.: 100.00%] [G loss: 1.940125]\n",
      "epoch:11 step:9032 [D loss: 0.045017, acc.: 100.00%] [G loss: 4.178307]\n",
      "epoch:11 step:9033 [D loss: 0.245575, acc.: 87.50%] [G loss: 7.452184]\n",
      "epoch:11 step:9034 [D loss: 1.182692, acc.: 51.56%] [G loss: 1.361956]\n",
      "epoch:11 step:9035 [D loss: 1.068168, acc.: 58.59%] [G loss: 8.238008]\n",
      "epoch:11 step:9036 [D loss: 1.464646, acc.: 54.69%] [G loss: 5.666853]\n",
      "epoch:11 step:9037 [D loss: 0.230562, acc.: 89.06%] [G loss: 3.598774]\n",
      "epoch:11 step:9038 [D loss: 0.078637, acc.: 99.22%] [G loss: 2.427965]\n",
      "epoch:11 step:9039 [D loss: 0.141103, acc.: 95.31%] [G loss: 4.010139]\n",
      "epoch:11 step:9040 [D loss: 0.035796, acc.: 100.00%] [G loss: 4.363304]\n",
      "epoch:11 step:9041 [D loss: 0.031114, acc.: 100.00%] [G loss: 3.203987]\n",
      "epoch:11 step:9042 [D loss: 0.128136, acc.: 97.66%] [G loss: 2.653359]\n",
      "epoch:11 step:9043 [D loss: 0.097357, acc.: 98.44%] [G loss: 1.539925]\n",
      "epoch:11 step:9044 [D loss: 0.046964, acc.: 100.00%] [G loss: 3.404130]\n",
      "epoch:11 step:9045 [D loss: 0.019454, acc.: 100.00%] [G loss: 2.786040]\n",
      "epoch:11 step:9046 [D loss: 0.074853, acc.: 100.00%] [G loss: 2.571525]\n",
      "epoch:11 step:9047 [D loss: 0.060399, acc.: 99.22%] [G loss: 1.254253]\n",
      "epoch:11 step:9048 [D loss: 0.050241, acc.: 99.22%] [G loss: 0.614353]\n",
      "epoch:11 step:9049 [D loss: 0.059640, acc.: 97.66%] [G loss: 3.448687]\n",
      "epoch:11 step:9050 [D loss: 0.180870, acc.: 91.41%] [G loss: 0.111434]\n",
      "epoch:11 step:9051 [D loss: 0.181889, acc.: 96.09%] [G loss: 2.865130]\n",
      "epoch:11 step:9052 [D loss: 0.009862, acc.: 100.00%] [G loss: 3.022713]\n",
      "epoch:11 step:9053 [D loss: 0.109382, acc.: 97.66%] [G loss: 1.453864]\n",
      "epoch:11 step:9054 [D loss: 0.182281, acc.: 95.31%] [G loss: 1.146652]\n",
      "epoch:11 step:9055 [D loss: 0.036149, acc.: 99.22%] [G loss: 0.194421]\n",
      "epoch:11 step:9056 [D loss: 0.546245, acc.: 71.88%] [G loss: 6.641326]\n",
      "epoch:11 step:9057 [D loss: 0.636198, acc.: 65.62%] [G loss: 3.547405]\n",
      "epoch:11 step:9058 [D loss: 0.092684, acc.: 96.09%] [G loss: 4.226941]\n",
      "epoch:11 step:9059 [D loss: 0.009761, acc.: 100.00%] [G loss: 4.660967]\n",
      "epoch:11 step:9060 [D loss: 0.009898, acc.: 100.00%] [G loss: 5.255833]\n",
      "epoch:11 step:9061 [D loss: 0.037206, acc.: 99.22%] [G loss: 4.177732]\n",
      "epoch:11 step:9062 [D loss: 0.023214, acc.: 100.00%] [G loss: 3.998576]\n",
      "epoch:11 step:9063 [D loss: 0.021898, acc.: 100.00%] [G loss: 3.818694]\n",
      "epoch:11 step:9064 [D loss: 0.009319, acc.: 100.00%] [G loss: 4.050511]\n",
      "epoch:11 step:9065 [D loss: 0.175042, acc.: 94.53%] [G loss: 3.268084]\n",
      "epoch:11 step:9066 [D loss: 0.053348, acc.: 99.22%] [G loss: 4.763513]\n",
      "epoch:11 step:9067 [D loss: 0.018336, acc.: 100.00%] [G loss: 4.520183]\n",
      "epoch:11 step:9068 [D loss: 0.012597, acc.: 100.00%] [G loss: 0.405134]\n",
      "epoch:11 step:9069 [D loss: 0.131684, acc.: 96.09%] [G loss: 4.003483]\n",
      "epoch:11 step:9070 [D loss: 0.266286, acc.: 86.72%] [G loss: 0.943671]\n",
      "epoch:11 step:9071 [D loss: 0.036040, acc.: 100.00%] [G loss: 5.930799]\n",
      "epoch:11 step:9072 [D loss: 0.016040, acc.: 100.00%] [G loss: 4.739015]\n",
      "epoch:11 step:9073 [D loss: 0.122252, acc.: 96.88%] [G loss: 5.128449]\n",
      "epoch:11 step:9074 [D loss: 0.024672, acc.: 99.22%] [G loss: 5.365043]\n",
      "epoch:11 step:9075 [D loss: 0.011499, acc.: 100.00%] [G loss: 4.520734]\n",
      "epoch:11 step:9076 [D loss: 0.018640, acc.: 100.00%] [G loss: 1.735675]\n",
      "epoch:11 step:9077 [D loss: 0.052105, acc.: 100.00%] [G loss: 4.996367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9078 [D loss: 0.030993, acc.: 100.00%] [G loss: 3.529676]\n",
      "epoch:11 step:9079 [D loss: 0.057172, acc.: 98.44%] [G loss: 1.496196]\n",
      "epoch:11 step:9080 [D loss: 0.051951, acc.: 99.22%] [G loss: 1.099040]\n",
      "epoch:11 step:9081 [D loss: 0.102418, acc.: 96.88%] [G loss: 3.144124]\n",
      "epoch:11 step:9082 [D loss: 0.278518, acc.: 89.06%] [G loss: 5.335770]\n",
      "epoch:11 step:9083 [D loss: 0.207095, acc.: 94.53%] [G loss: 3.589436]\n",
      "epoch:11 step:9084 [D loss: 0.106489, acc.: 96.09%] [G loss: 4.960373]\n",
      "epoch:11 step:9085 [D loss: 0.008695, acc.: 100.00%] [G loss: 5.700221]\n",
      "epoch:11 step:9086 [D loss: 0.044357, acc.: 98.44%] [G loss: 4.611184]\n",
      "epoch:11 step:9087 [D loss: 0.035414, acc.: 99.22%] [G loss: 4.113064]\n",
      "epoch:11 step:9088 [D loss: 0.007207, acc.: 100.00%] [G loss: 2.979321]\n",
      "epoch:11 step:9089 [D loss: 0.110952, acc.: 96.09%] [G loss: 4.923127]\n",
      "epoch:11 step:9090 [D loss: 0.114888, acc.: 94.53%] [G loss: 2.072011]\n",
      "epoch:11 step:9091 [D loss: 0.184727, acc.: 92.19%] [G loss: 6.820467]\n",
      "epoch:11 step:9092 [D loss: 0.602069, acc.: 71.09%] [G loss: 3.839926]\n",
      "epoch:11 step:9093 [D loss: 0.011230, acc.: 100.00%] [G loss: 6.684451]\n",
      "epoch:11 step:9094 [D loss: 0.005791, acc.: 100.00%] [G loss: 1.095996]\n",
      "epoch:11 step:9095 [D loss: 0.010891, acc.: 100.00%] [G loss: 2.548378]\n",
      "epoch:11 step:9096 [D loss: 0.006783, acc.: 100.00%] [G loss: 0.329373]\n",
      "epoch:11 step:9097 [D loss: 0.013864, acc.: 100.00%] [G loss: 5.473924]\n",
      "epoch:11 step:9098 [D loss: 0.080478, acc.: 99.22%] [G loss: 0.031406]\n",
      "epoch:11 step:9099 [D loss: 0.053116, acc.: 99.22%] [G loss: 0.129309]\n",
      "epoch:11 step:9100 [D loss: 0.051713, acc.: 99.22%] [G loss: 5.154196]\n",
      "epoch:11 step:9101 [D loss: 0.232675, acc.: 91.41%] [G loss: 2.406793]\n",
      "epoch:11 step:9102 [D loss: 0.244195, acc.: 86.72%] [G loss: 4.526459]\n",
      "epoch:11 step:9103 [D loss: 0.136277, acc.: 93.75%] [G loss: 5.814171]\n",
      "epoch:11 step:9104 [D loss: 1.021616, acc.: 58.59%] [G loss: 7.317873]\n",
      "epoch:11 step:9105 [D loss: 0.062915, acc.: 98.44%] [G loss: 8.223146]\n",
      "epoch:11 step:9106 [D loss: 0.474390, acc.: 80.47%] [G loss: 1.578597]\n",
      "epoch:11 step:9107 [D loss: 0.046008, acc.: 99.22%] [G loss: 3.443171]\n",
      "epoch:11 step:9108 [D loss: 0.102990, acc.: 96.88%] [G loss: 2.933559]\n",
      "epoch:11 step:9109 [D loss: 0.004649, acc.: 100.00%] [G loss: 6.256692]\n",
      "epoch:11 step:9110 [D loss: 0.066328, acc.: 97.66%] [G loss: 3.570166]\n",
      "epoch:11 step:9111 [D loss: 0.034249, acc.: 100.00%] [G loss: 4.755271]\n",
      "epoch:11 step:9112 [D loss: 0.117777, acc.: 96.09%] [G loss: 0.600923]\n",
      "epoch:11 step:9113 [D loss: 0.629521, acc.: 67.97%] [G loss: 8.196688]\n",
      "epoch:11 step:9114 [D loss: 2.574368, acc.: 50.00%] [G loss: 1.794593]\n",
      "epoch:11 step:9115 [D loss: 0.369563, acc.: 86.72%] [G loss: 1.298834]\n",
      "epoch:11 step:9116 [D loss: 0.037594, acc.: 99.22%] [G loss: 4.228849]\n",
      "epoch:11 step:9117 [D loss: 0.167001, acc.: 92.97%] [G loss: 1.126602]\n",
      "epoch:11 step:9118 [D loss: 0.038282, acc.: 100.00%] [G loss: 1.659651]\n",
      "epoch:11 step:9119 [D loss: 0.183106, acc.: 92.97%] [G loss: 2.599729]\n",
      "epoch:11 step:9120 [D loss: 0.041931, acc.: 100.00%] [G loss: 3.552491]\n",
      "epoch:11 step:9121 [D loss: 0.025870, acc.: 100.00%] [G loss: 2.890249]\n",
      "epoch:11 step:9122 [D loss: 0.059579, acc.: 98.44%] [G loss: 2.512207]\n",
      "epoch:11 step:9123 [D loss: 0.042923, acc.: 100.00%] [G loss: 1.603026]\n",
      "epoch:11 step:9124 [D loss: 0.064253, acc.: 100.00%] [G loss: 3.153944]\n",
      "epoch:11 step:9125 [D loss: 0.042890, acc.: 100.00%] [G loss: 3.042750]\n",
      "epoch:11 step:9126 [D loss: 0.018824, acc.: 100.00%] [G loss: 2.252079]\n",
      "epoch:11 step:9127 [D loss: 0.117232, acc.: 95.31%] [G loss: 1.832683]\n",
      "epoch:11 step:9128 [D loss: 0.042464, acc.: 100.00%] [G loss: 1.521317]\n",
      "epoch:11 step:9129 [D loss: 0.038941, acc.: 100.00%] [G loss: 2.775165]\n",
      "epoch:11 step:9130 [D loss: 0.007181, acc.: 100.00%] [G loss: 2.683603]\n",
      "epoch:11 step:9131 [D loss: 0.036557, acc.: 99.22%] [G loss: 0.518731]\n",
      "epoch:11 step:9132 [D loss: 0.020912, acc.: 100.00%] [G loss: 1.461170]\n",
      "epoch:11 step:9133 [D loss: 0.038473, acc.: 100.00%] [G loss: 3.216475]\n",
      "epoch:11 step:9134 [D loss: 0.079485, acc.: 99.22%] [G loss: 4.038665]\n",
      "epoch:11 step:9135 [D loss: 0.046486, acc.: 97.66%] [G loss: 3.904220]\n",
      "epoch:11 step:9136 [D loss: 0.087505, acc.: 98.44%] [G loss: 0.134584]\n",
      "epoch:11 step:9137 [D loss: 0.118652, acc.: 97.66%] [G loss: 4.940787]\n",
      "epoch:11 step:9138 [D loss: 0.328003, acc.: 83.59%] [G loss: 0.005495]\n",
      "epoch:11 step:9139 [D loss: 0.845748, acc.: 64.06%] [G loss: 8.263708]\n",
      "epoch:11 step:9140 [D loss: 1.807189, acc.: 51.56%] [G loss: 2.904531]\n",
      "epoch:11 step:9141 [D loss: 0.089915, acc.: 97.66%] [G loss: 3.243414]\n",
      "epoch:11 step:9142 [D loss: 0.025115, acc.: 100.00%] [G loss: 4.010729]\n",
      "epoch:11 step:9143 [D loss: 0.113607, acc.: 96.88%] [G loss: 4.560561]\n",
      "epoch:11 step:9144 [D loss: 0.115531, acc.: 96.09%] [G loss: 3.543218]\n",
      "epoch:11 step:9145 [D loss: 0.032166, acc.: 100.00%] [G loss: 3.727724]\n",
      "epoch:11 step:9146 [D loss: 0.035559, acc.: 99.22%] [G loss: 4.050707]\n",
      "epoch:11 step:9147 [D loss: 0.032786, acc.: 100.00%] [G loss: 1.962864]\n",
      "epoch:11 step:9148 [D loss: 0.019058, acc.: 100.00%] [G loss: 2.927037]\n",
      "epoch:11 step:9149 [D loss: 0.105883, acc.: 98.44%] [G loss: 4.522805]\n",
      "epoch:11 step:9150 [D loss: 0.031115, acc.: 100.00%] [G loss: 3.210048]\n",
      "epoch:11 step:9151 [D loss: 0.043197, acc.: 99.22%] [G loss: 2.610338]\n",
      "epoch:11 step:9152 [D loss: 0.137435, acc.: 96.09%] [G loss: 2.044218]\n",
      "epoch:11 step:9153 [D loss: 0.075892, acc.: 99.22%] [G loss: 3.319327]\n",
      "epoch:11 step:9154 [D loss: 0.007149, acc.: 100.00%] [G loss: 1.167980]\n",
      "epoch:11 step:9155 [D loss: 0.030803, acc.: 100.00%] [G loss: 2.289520]\n",
      "epoch:11 step:9156 [D loss: 0.136505, acc.: 97.66%] [G loss: 3.101069]\n",
      "epoch:11 step:9157 [D loss: 0.374150, acc.: 83.59%] [G loss: 0.797559]\n",
      "epoch:11 step:9158 [D loss: 0.086731, acc.: 97.66%] [G loss: 3.122946]\n",
      "epoch:11 step:9159 [D loss: 0.004947, acc.: 100.00%] [G loss: 3.085583]\n",
      "epoch:11 step:9160 [D loss: 0.005615, acc.: 100.00%] [G loss: 2.069441]\n",
      "epoch:11 step:9161 [D loss: 0.003369, acc.: 100.00%] [G loss: 1.678363]\n",
      "epoch:11 step:9162 [D loss: 0.038598, acc.: 100.00%] [G loss: 2.756195]\n",
      "epoch:11 step:9163 [D loss: 0.027372, acc.: 100.00%] [G loss: 2.293564]\n",
      "epoch:11 step:9164 [D loss: 0.069598, acc.: 100.00%] [G loss: 0.357330]\n",
      "epoch:11 step:9165 [D loss: 0.044905, acc.: 99.22%] [G loss: 0.640811]\n",
      "epoch:11 step:9166 [D loss: 0.404247, acc.: 75.78%] [G loss: 6.078568]\n",
      "epoch:11 step:9167 [D loss: 0.991218, acc.: 57.81%] [G loss: 2.197526]\n",
      "epoch:11 step:9168 [D loss: 0.242696, acc.: 88.28%] [G loss: 4.188530]\n",
      "epoch:11 step:9169 [D loss: 0.035364, acc.: 99.22%] [G loss: 5.901133]\n",
      "epoch:11 step:9170 [D loss: 0.051887, acc.: 98.44%] [G loss: 3.723654]\n",
      "epoch:11 step:9171 [D loss: 0.105858, acc.: 98.44%] [G loss: 2.916070]\n",
      "epoch:11 step:9172 [D loss: 0.078906, acc.: 100.00%] [G loss: 3.594495]\n",
      "epoch:11 step:9173 [D loss: 0.019693, acc.: 100.00%] [G loss: 3.886985]\n",
      "epoch:11 step:9174 [D loss: 0.054844, acc.: 99.22%] [G loss: 3.936239]\n",
      "epoch:11 step:9175 [D loss: 0.020833, acc.: 100.00%] [G loss: 2.399606]\n",
      "epoch:11 step:9176 [D loss: 0.023048, acc.: 100.00%] [G loss: 3.211277]\n",
      "epoch:11 step:9177 [D loss: 0.061883, acc.: 99.22%] [G loss: 3.845578]\n",
      "epoch:11 step:9178 [D loss: 0.051302, acc.: 100.00%] [G loss: 3.919347]\n",
      "epoch:11 step:9179 [D loss: 0.068637, acc.: 100.00%] [G loss: 3.856453]\n",
      "epoch:11 step:9180 [D loss: 0.076153, acc.: 99.22%] [G loss: 4.221451]\n",
      "epoch:11 step:9181 [D loss: 0.037176, acc.: 100.00%] [G loss: 3.825896]\n",
      "epoch:11 step:9182 [D loss: 0.036347, acc.: 99.22%] [G loss: 4.079929]\n",
      "epoch:11 step:9183 [D loss: 0.038352, acc.: 99.22%] [G loss: 2.654484]\n",
      "epoch:11 step:9184 [D loss: 0.052813, acc.: 100.00%] [G loss: 2.515896]\n",
      "epoch:11 step:9185 [D loss: 0.114536, acc.: 98.44%] [G loss: 5.870716]\n",
      "epoch:11 step:9186 [D loss: 0.044341, acc.: 99.22%] [G loss: 2.740592]\n",
      "epoch:11 step:9187 [D loss: 0.077626, acc.: 97.66%] [G loss: 1.428744]\n",
      "epoch:11 step:9188 [D loss: 0.034612, acc.: 100.00%] [G loss: 4.755465]\n",
      "epoch:11 step:9189 [D loss: 0.031408, acc.: 99.22%] [G loss: 4.928061]\n",
      "epoch:11 step:9190 [D loss: 0.202451, acc.: 93.75%] [G loss: 4.764655]\n",
      "epoch:11 step:9191 [D loss: 0.111044, acc.: 96.09%] [G loss: 3.156244]\n",
      "epoch:11 step:9192 [D loss: 0.063748, acc.: 99.22%] [G loss: 2.660176]\n",
      "epoch:11 step:9193 [D loss: 0.033489, acc.: 99.22%] [G loss: 4.274672]\n",
      "epoch:11 step:9194 [D loss: 0.018736, acc.: 100.00%] [G loss: 5.475823]\n",
      "epoch:11 step:9195 [D loss: 0.152704, acc.: 92.97%] [G loss: 0.637757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9196 [D loss: 0.031374, acc.: 100.00%] [G loss: 1.023983]\n",
      "epoch:11 step:9197 [D loss: 0.768654, acc.: 62.50%] [G loss: 10.363156]\n",
      "epoch:11 step:9198 [D loss: 3.035906, acc.: 50.00%] [G loss: 6.146897]\n",
      "epoch:11 step:9199 [D loss: 0.871569, acc.: 54.69%] [G loss: 0.454430]\n",
      "epoch:11 step:9200 [D loss: 0.043028, acc.: 99.22%] [G loss: 3.688506]\n",
      "epoch:11 step:9201 [D loss: 0.182814, acc.: 93.75%] [G loss: 4.070114]\n",
      "epoch:11 step:9202 [D loss: 0.123711, acc.: 98.44%] [G loss: 3.503891]\n",
      "epoch:11 step:9203 [D loss: 0.079030, acc.: 99.22%] [G loss: 1.977710]\n",
      "epoch:11 step:9204 [D loss: 0.049457, acc.: 99.22%] [G loss: 2.998116]\n",
      "epoch:11 step:9205 [D loss: 0.227739, acc.: 93.75%] [G loss: 0.319278]\n",
      "epoch:11 step:9206 [D loss: 0.055301, acc.: 100.00%] [G loss: 3.955651]\n",
      "epoch:11 step:9207 [D loss: 0.698154, acc.: 65.62%] [G loss: 2.311710]\n",
      "epoch:11 step:9208 [D loss: 0.491777, acc.: 75.00%] [G loss: 0.950099]\n",
      "epoch:11 step:9209 [D loss: 0.139215, acc.: 97.66%] [G loss: 0.336532]\n",
      "epoch:11 step:9210 [D loss: 0.078738, acc.: 99.22%] [G loss: 1.151264]\n",
      "epoch:11 step:9211 [D loss: 0.048457, acc.: 100.00%] [G loss: 1.762240]\n",
      "epoch:11 step:9212 [D loss: 0.024427, acc.: 100.00%] [G loss: 0.994622]\n",
      "epoch:11 step:9213 [D loss: 0.044491, acc.: 99.22%] [G loss: 5.243022]\n",
      "epoch:11 step:9214 [D loss: 0.042369, acc.: 98.44%] [G loss: 1.963432]\n",
      "epoch:11 step:9215 [D loss: 0.013038, acc.: 100.00%] [G loss: 4.103270]\n",
      "epoch:11 step:9216 [D loss: 0.025302, acc.: 100.00%] [G loss: 3.995035]\n",
      "epoch:11 step:9217 [D loss: 0.062785, acc.: 98.44%] [G loss: 3.721720]\n",
      "epoch:11 step:9218 [D loss: 0.243776, acc.: 89.84%] [G loss: 0.311995]\n",
      "epoch:11 step:9219 [D loss: 0.018290, acc.: 100.00%] [G loss: 0.250878]\n",
      "epoch:11 step:9220 [D loss: 0.106521, acc.: 99.22%] [G loss: 0.486169]\n",
      "epoch:11 step:9221 [D loss: 0.007819, acc.: 100.00%] [G loss: 3.645676]\n",
      "epoch:11 step:9222 [D loss: 0.180186, acc.: 95.31%] [G loss: 0.317909]\n",
      "epoch:11 step:9223 [D loss: 0.013121, acc.: 100.00%] [G loss: 0.113698]\n",
      "epoch:11 step:9224 [D loss: 0.044627, acc.: 100.00%] [G loss: 0.700936]\n",
      "epoch:11 step:9225 [D loss: 0.004642, acc.: 100.00%] [G loss: 4.625660]\n",
      "epoch:11 step:9226 [D loss: 0.039473, acc.: 100.00%] [G loss: 3.227820]\n",
      "epoch:11 step:9227 [D loss: 0.030404, acc.: 99.22%] [G loss: 0.343874]\n",
      "epoch:11 step:9228 [D loss: 0.052190, acc.: 100.00%] [G loss: 0.455428]\n",
      "epoch:11 step:9229 [D loss: 0.126215, acc.: 94.53%] [G loss: 1.078679]\n",
      "epoch:11 step:9230 [D loss: 0.019492, acc.: 100.00%] [G loss: 4.287841]\n",
      "epoch:11 step:9231 [D loss: 0.031532, acc.: 99.22%] [G loss: 3.531186]\n",
      "epoch:11 step:9232 [D loss: 0.045953, acc.: 99.22%] [G loss: 1.110423]\n",
      "epoch:11 step:9233 [D loss: 0.492764, acc.: 74.22%] [G loss: 2.118868]\n",
      "epoch:11 step:9234 [D loss: 0.450585, acc.: 72.66%] [G loss: 1.392932]\n",
      "epoch:11 step:9235 [D loss: 0.206209, acc.: 89.84%] [G loss: 2.211474]\n",
      "epoch:11 step:9236 [D loss: 0.009563, acc.: 100.00%] [G loss: 4.333227]\n",
      "epoch:11 step:9237 [D loss: 0.521464, acc.: 75.00%] [G loss: 0.430428]\n",
      "epoch:11 step:9238 [D loss: 0.497494, acc.: 76.56%] [G loss: 5.076337]\n",
      "epoch:11 step:9239 [D loss: 0.095045, acc.: 98.44%] [G loss: 5.905108]\n",
      "epoch:11 step:9240 [D loss: 0.420097, acc.: 76.56%] [G loss: 4.585145]\n",
      "epoch:11 step:9241 [D loss: 0.031875, acc.: 99.22%] [G loss: 2.681312]\n",
      "epoch:11 step:9242 [D loss: 0.164075, acc.: 94.53%] [G loss: 4.825381]\n",
      "epoch:11 step:9243 [D loss: 0.003750, acc.: 100.00%] [G loss: 5.165028]\n",
      "epoch:11 step:9244 [D loss: 0.040018, acc.: 99.22%] [G loss: 4.710082]\n",
      "epoch:11 step:9245 [D loss: 0.046006, acc.: 100.00%] [G loss: 3.006261]\n",
      "epoch:11 step:9246 [D loss: 0.005870, acc.: 100.00%] [G loss: 3.170326]\n",
      "epoch:11 step:9247 [D loss: 0.035456, acc.: 100.00%] [G loss: 2.727713]\n",
      "epoch:11 step:9248 [D loss: 0.063726, acc.: 99.22%] [G loss: 3.251075]\n",
      "epoch:11 step:9249 [D loss: 0.087228, acc.: 99.22%] [G loss: 3.699770]\n",
      "epoch:11 step:9250 [D loss: 0.050580, acc.: 98.44%] [G loss: 2.599321]\n",
      "epoch:11 step:9251 [D loss: 0.140365, acc.: 97.66%] [G loss: 3.349106]\n",
      "epoch:11 step:9252 [D loss: 0.040654, acc.: 99.22%] [G loss: 1.926860]\n",
      "epoch:11 step:9253 [D loss: 0.030791, acc.: 100.00%] [G loss: 1.690687]\n",
      "epoch:11 step:9254 [D loss: 0.017924, acc.: 100.00%] [G loss: 4.332288]\n",
      "epoch:11 step:9255 [D loss: 0.018774, acc.: 100.00%] [G loss: 0.847093]\n",
      "epoch:11 step:9256 [D loss: 0.146660, acc.: 96.09%] [G loss: 5.749341]\n",
      "epoch:11 step:9257 [D loss: 0.036647, acc.: 99.22%] [G loss: 4.954021]\n",
      "epoch:11 step:9258 [D loss: 0.355454, acc.: 86.72%] [G loss: 3.032942]\n",
      "epoch:11 step:9259 [D loss: 0.113646, acc.: 96.09%] [G loss: 4.716036]\n",
      "epoch:11 step:9260 [D loss: 0.002551, acc.: 100.00%] [G loss: 4.624725]\n",
      "epoch:11 step:9261 [D loss: 0.015795, acc.: 100.00%] [G loss: 4.863601]\n",
      "epoch:11 step:9262 [D loss: 0.012959, acc.: 100.00%] [G loss: 4.663538]\n",
      "epoch:11 step:9263 [D loss: 0.009015, acc.: 100.00%] [G loss: 5.347078]\n",
      "epoch:11 step:9264 [D loss: 0.068446, acc.: 98.44%] [G loss: 3.553762]\n",
      "epoch:11 step:9265 [D loss: 0.032203, acc.: 100.00%] [G loss: 4.805417]\n",
      "epoch:11 step:9266 [D loss: 0.012633, acc.: 100.00%] [G loss: 5.139048]\n",
      "epoch:11 step:9267 [D loss: 0.007803, acc.: 100.00%] [G loss: 2.120368]\n",
      "epoch:11 step:9268 [D loss: 0.028517, acc.: 100.00%] [G loss: 4.627707]\n",
      "epoch:11 step:9269 [D loss: 0.021291, acc.: 100.00%] [G loss: 3.967576]\n",
      "epoch:11 step:9270 [D loss: 0.040966, acc.: 100.00%] [G loss: 4.164081]\n",
      "epoch:11 step:9271 [D loss: 0.021899, acc.: 100.00%] [G loss: 4.828603]\n",
      "epoch:11 step:9272 [D loss: 0.062072, acc.: 98.44%] [G loss: 4.006672]\n",
      "epoch:11 step:9273 [D loss: 0.010191, acc.: 100.00%] [G loss: 4.771698]\n",
      "epoch:11 step:9274 [D loss: 0.016612, acc.: 100.00%] [G loss: 2.015441]\n",
      "epoch:11 step:9275 [D loss: 0.019650, acc.: 100.00%] [G loss: 4.340172]\n",
      "epoch:11 step:9276 [D loss: 0.465942, acc.: 74.22%] [G loss: 5.390519]\n",
      "epoch:11 step:9277 [D loss: 0.690118, acc.: 67.97%] [G loss: 3.059828]\n",
      "epoch:11 step:9278 [D loss: 0.054288, acc.: 98.44%] [G loss: 2.978861]\n",
      "epoch:11 step:9279 [D loss: 0.038235, acc.: 99.22%] [G loss: 4.030872]\n",
      "epoch:11 step:9280 [D loss: 0.006316, acc.: 100.00%] [G loss: 0.737816]\n",
      "epoch:11 step:9281 [D loss: 0.006914, acc.: 100.00%] [G loss: 4.618730]\n",
      "epoch:11 step:9282 [D loss: 0.030355, acc.: 100.00%] [G loss: 3.758120]\n",
      "epoch:11 step:9283 [D loss: 0.008314, acc.: 100.00%] [G loss: 4.970225]\n",
      "epoch:11 step:9284 [D loss: 0.005795, acc.: 100.00%] [G loss: 4.864429]\n",
      "epoch:11 step:9285 [D loss: 0.007585, acc.: 100.00%] [G loss: 4.691272]\n",
      "epoch:11 step:9286 [D loss: 0.014744, acc.: 100.00%] [G loss: 3.999391]\n",
      "epoch:11 step:9287 [D loss: 0.005471, acc.: 100.00%] [G loss: 3.743651]\n",
      "epoch:11 step:9288 [D loss: 0.008058, acc.: 100.00%] [G loss: 3.405566]\n",
      "epoch:11 step:9289 [D loss: 0.017564, acc.: 100.00%] [G loss: 2.607957]\n",
      "epoch:11 step:9290 [D loss: 0.081582, acc.: 96.88%] [G loss: 5.664393]\n",
      "epoch:11 step:9291 [D loss: 0.042306, acc.: 100.00%] [G loss: 2.212103]\n",
      "epoch:11 step:9292 [D loss: 0.018908, acc.: 100.00%] [G loss: 3.952974]\n",
      "epoch:11 step:9293 [D loss: 0.009177, acc.: 100.00%] [G loss: 0.529501]\n",
      "epoch:11 step:9294 [D loss: 0.080732, acc.: 98.44%] [G loss: 4.836733]\n",
      "epoch:11 step:9295 [D loss: 0.328919, acc.: 85.94%] [G loss: 2.381156]\n",
      "epoch:11 step:9296 [D loss: 0.116296, acc.: 96.09%] [G loss: 3.950223]\n",
      "epoch:11 step:9297 [D loss: 0.001516, acc.: 100.00%] [G loss: 7.363277]\n",
      "epoch:11 step:9298 [D loss: 0.136952, acc.: 94.53%] [G loss: 3.701663]\n",
      "epoch:11 step:9299 [D loss: 0.090118, acc.: 96.09%] [G loss: 4.344131]\n",
      "epoch:11 step:9300 [D loss: 0.005471, acc.: 100.00%] [G loss: 4.837884]\n",
      "epoch:11 step:9301 [D loss: 0.017366, acc.: 100.00%] [G loss: 4.622341]\n",
      "epoch:11 step:9302 [D loss: 0.017332, acc.: 100.00%] [G loss: 2.497585]\n",
      "epoch:11 step:9303 [D loss: 0.077227, acc.: 96.88%] [G loss: 6.059136]\n",
      "epoch:11 step:9304 [D loss: 3.249533, acc.: 5.47%] [G loss: 7.912123]\n",
      "epoch:11 step:9305 [D loss: 1.535455, acc.: 50.78%] [G loss: 4.034359]\n",
      "epoch:11 step:9306 [D loss: 0.246414, acc.: 85.16%] [G loss: 3.082962]\n",
      "epoch:11 step:9307 [D loss: 0.109711, acc.: 95.31%] [G loss: 1.167588]\n",
      "epoch:11 step:9308 [D loss: 0.093553, acc.: 97.66%] [G loss: 4.127425]\n",
      "epoch:11 step:9309 [D loss: 0.020219, acc.: 100.00%] [G loss: 3.982589]\n",
      "epoch:11 step:9310 [D loss: 0.072393, acc.: 98.44%] [G loss: 3.074291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9311 [D loss: 0.078621, acc.: 99.22%] [G loss: 1.299480]\n",
      "epoch:11 step:9312 [D loss: 0.102756, acc.: 97.66%] [G loss: 1.758261]\n",
      "epoch:11 step:9313 [D loss: 0.047586, acc.: 99.22%] [G loss: 0.917361]\n",
      "epoch:11 step:9314 [D loss: 0.103273, acc.: 97.66%] [G loss: 1.401799]\n",
      "epoch:11 step:9315 [D loss: 0.140993, acc.: 97.66%] [G loss: 2.788283]\n",
      "epoch:11 step:9316 [D loss: 0.360468, acc.: 83.59%] [G loss: 0.829613]\n",
      "epoch:11 step:9317 [D loss: 0.095789, acc.: 99.22%] [G loss: 1.526487]\n",
      "epoch:11 step:9318 [D loss: 0.173541, acc.: 94.53%] [G loss: 2.267988]\n",
      "epoch:11 step:9319 [D loss: 0.260346, acc.: 88.28%] [G loss: 0.484177]\n",
      "epoch:11 step:9320 [D loss: 0.113961, acc.: 96.88%] [G loss: 0.302317]\n",
      "epoch:11 step:9321 [D loss: 0.194227, acc.: 92.19%] [G loss: 2.040751]\n",
      "epoch:11 step:9322 [D loss: 0.314028, acc.: 84.38%] [G loss: 0.419201]\n",
      "epoch:11 step:9323 [D loss: 0.151777, acc.: 94.53%] [G loss: 1.079504]\n",
      "epoch:11 step:9324 [D loss: 0.253637, acc.: 89.84%] [G loss: 1.491729]\n",
      "epoch:11 step:9325 [D loss: 0.151189, acc.: 94.53%] [G loss: 3.316361]\n",
      "epoch:11 step:9326 [D loss: 0.173388, acc.: 95.31%] [G loss: 2.688309]\n",
      "epoch:11 step:9327 [D loss: 0.081533, acc.: 98.44%] [G loss: 1.777021]\n",
      "epoch:11 step:9328 [D loss: 0.059785, acc.: 99.22%] [G loss: 3.477438]\n",
      "epoch:11 step:9329 [D loss: 0.098202, acc.: 98.44%] [G loss: 2.741654]\n",
      "epoch:11 step:9330 [D loss: 0.234226, acc.: 90.62%] [G loss: 2.280159]\n",
      "epoch:11 step:9331 [D loss: 0.515279, acc.: 78.91%] [G loss: 4.143949]\n",
      "epoch:11 step:9332 [D loss: 0.074409, acc.: 99.22%] [G loss: 4.577806]\n",
      "epoch:11 step:9333 [D loss: 0.249279, acc.: 89.84%] [G loss: 0.503315]\n",
      "epoch:11 step:9334 [D loss: 0.057434, acc.: 99.22%] [G loss: 0.858954]\n",
      "epoch:11 step:9335 [D loss: 0.042060, acc.: 100.00%] [G loss: 0.469489]\n",
      "epoch:11 step:9336 [D loss: 0.106559, acc.: 96.09%] [G loss: 0.371372]\n",
      "epoch:11 step:9337 [D loss: 0.063553, acc.: 98.44%] [G loss: 4.358253]\n",
      "epoch:11 step:9338 [D loss: 0.088744, acc.: 99.22%] [G loss: 1.262683]\n",
      "epoch:11 step:9339 [D loss: 0.444527, acc.: 78.12%] [G loss: 1.854797]\n",
      "epoch:11 step:9340 [D loss: 0.043345, acc.: 100.00%] [G loss: 2.028387]\n",
      "epoch:11 step:9341 [D loss: 0.128769, acc.: 94.53%] [G loss: 0.595175]\n",
      "epoch:11 step:9342 [D loss: 0.200039, acc.: 92.19%] [G loss: 1.353877]\n",
      "epoch:11 step:9343 [D loss: 0.003117, acc.: 100.00%] [G loss: 2.414480]\n",
      "epoch:11 step:9344 [D loss: 0.532107, acc.: 72.66%] [G loss: 0.002667]\n",
      "epoch:11 step:9345 [D loss: 0.847884, acc.: 68.75%] [G loss: 2.227103]\n",
      "epoch:11 step:9346 [D loss: 0.052132, acc.: 96.88%] [G loss: 7.916874]\n",
      "epoch:11 step:9347 [D loss: 0.778905, acc.: 65.62%] [G loss: 0.194903]\n",
      "epoch:11 step:9348 [D loss: 0.037666, acc.: 99.22%] [G loss: 0.509874]\n",
      "epoch:11 step:9349 [D loss: 0.013626, acc.: 100.00%] [G loss: 0.198178]\n",
      "epoch:11 step:9350 [D loss: 0.049126, acc.: 99.22%] [G loss: 0.073786]\n",
      "epoch:11 step:9351 [D loss: 0.004178, acc.: 100.00%] [G loss: 0.166059]\n",
      "epoch:11 step:9352 [D loss: 0.007836, acc.: 100.00%] [G loss: 1.758938]\n",
      "epoch:11 step:9353 [D loss: 0.183158, acc.: 91.41%] [G loss: 2.428529]\n",
      "epoch:11 step:9354 [D loss: 0.053556, acc.: 97.66%] [G loss: 1.632495]\n",
      "epoch:11 step:9355 [D loss: 0.046038, acc.: 99.22%] [G loss: 0.428786]\n",
      "epoch:11 step:9356 [D loss: 0.019913, acc.: 100.00%] [G loss: 0.373331]\n",
      "epoch:11 step:9357 [D loss: 0.031951, acc.: 99.22%] [G loss: 0.163762]\n",
      "epoch:11 step:9358 [D loss: 0.009680, acc.: 100.00%] [G loss: 1.019190]\n",
      "epoch:11 step:9359 [D loss: 0.069666, acc.: 98.44%] [G loss: 0.578183]\n",
      "epoch:11 step:9360 [D loss: 0.074640, acc.: 99.22%] [G loss: 1.032930]\n",
      "epoch:11 step:9361 [D loss: 0.273679, acc.: 89.06%] [G loss: 1.168511]\n",
      "epoch:11 step:9362 [D loss: 1.509346, acc.: 37.50%] [G loss: 3.807643]\n",
      "epoch:11 step:9363 [D loss: 0.070755, acc.: 96.88%] [G loss: 6.257339]\n",
      "epoch:11 step:9364 [D loss: 0.327365, acc.: 85.16%] [G loss: 2.942589]\n",
      "epoch:11 step:9365 [D loss: 0.072471, acc.: 99.22%] [G loss: 2.708890]\n",
      "epoch:11 step:9366 [D loss: 0.018744, acc.: 100.00%] [G loss: 2.734181]\n",
      "epoch:11 step:9367 [D loss: 0.041253, acc.: 99.22%] [G loss: 2.323336]\n",
      "epoch:11 step:9368 [D loss: 0.082513, acc.: 97.66%] [G loss: 2.999495]\n",
      "epoch:11 step:9369 [D loss: 0.080985, acc.: 97.66%] [G loss: 2.753467]\n",
      "epoch:11 step:9370 [D loss: 0.108500, acc.: 98.44%] [G loss: 2.813065]\n",
      "epoch:11 step:9371 [D loss: 0.075110, acc.: 99.22%] [G loss: 2.827529]\n",
      "epoch:11 step:9372 [D loss: 0.014458, acc.: 100.00%] [G loss: 3.154804]\n",
      "epoch:12 step:9373 [D loss: 0.295236, acc.: 89.06%] [G loss: 1.814672]\n",
      "epoch:12 step:9374 [D loss: 0.262583, acc.: 85.16%] [G loss: 4.236234]\n",
      "epoch:12 step:9375 [D loss: 0.068017, acc.: 98.44%] [G loss: 5.783262]\n",
      "epoch:12 step:9376 [D loss: 0.160202, acc.: 93.75%] [G loss: 4.539789]\n",
      "epoch:12 step:9377 [D loss: 0.023091, acc.: 100.00%] [G loss: 3.478940]\n",
      "epoch:12 step:9378 [D loss: 0.068718, acc.: 97.66%] [G loss: 3.421305]\n",
      "epoch:12 step:9379 [D loss: 0.046939, acc.: 100.00%] [G loss: 1.149113]\n",
      "epoch:12 step:9380 [D loss: 0.131683, acc.: 96.88%] [G loss: 5.836139]\n",
      "epoch:12 step:9381 [D loss: 0.057247, acc.: 99.22%] [G loss: 5.052128]\n",
      "epoch:12 step:9382 [D loss: 0.071111, acc.: 98.44%] [G loss: 4.805614]\n",
      "epoch:12 step:9383 [D loss: 0.011556, acc.: 100.00%] [G loss: 0.847857]\n",
      "epoch:12 step:9384 [D loss: 0.090782, acc.: 97.66%] [G loss: 4.061522]\n",
      "epoch:12 step:9385 [D loss: 0.020634, acc.: 100.00%] [G loss: 4.330910]\n",
      "epoch:12 step:9386 [D loss: 0.230235, acc.: 91.41%] [G loss: 3.966884]\n",
      "epoch:12 step:9387 [D loss: 0.060339, acc.: 97.66%] [G loss: 5.423121]\n",
      "epoch:12 step:9388 [D loss: 0.007732, acc.: 100.00%] [G loss: 5.035186]\n",
      "epoch:12 step:9389 [D loss: 0.012619, acc.: 100.00%] [G loss: 5.060409]\n",
      "epoch:12 step:9390 [D loss: 0.008263, acc.: 100.00%] [G loss: 4.818266]\n",
      "epoch:12 step:9391 [D loss: 0.010976, acc.: 100.00%] [G loss: 0.591486]\n",
      "epoch:12 step:9392 [D loss: 0.142614, acc.: 95.31%] [G loss: 2.244756]\n",
      "epoch:12 step:9393 [D loss: 0.075517, acc.: 97.66%] [G loss: 5.261099]\n",
      "epoch:12 step:9394 [D loss: 0.144185, acc.: 95.31%] [G loss: 4.860130]\n",
      "epoch:12 step:9395 [D loss: 0.098324, acc.: 97.66%] [G loss: 4.626688]\n",
      "epoch:12 step:9396 [D loss: 0.016245, acc.: 100.00%] [G loss: 5.034160]\n",
      "epoch:12 step:9397 [D loss: 0.051397, acc.: 100.00%] [G loss: 3.555990]\n",
      "epoch:12 step:9398 [D loss: 0.057828, acc.: 100.00%] [G loss: 2.801900]\n",
      "epoch:12 step:9399 [D loss: 0.034830, acc.: 98.44%] [G loss: 3.110187]\n",
      "epoch:12 step:9400 [D loss: 0.057804, acc.: 99.22%] [G loss: 2.665490]\n",
      "epoch:12 step:9401 [D loss: 0.023236, acc.: 100.00%] [G loss: 4.200224]\n",
      "epoch:12 step:9402 [D loss: 0.189253, acc.: 92.97%] [G loss: 1.771068]\n",
      "epoch:12 step:9403 [D loss: 0.012475, acc.: 100.00%] [G loss: 2.562090]\n",
      "epoch:12 step:9404 [D loss: 0.014661, acc.: 100.00%] [G loss: 4.807397]\n",
      "epoch:12 step:9405 [D loss: 0.528671, acc.: 78.91%] [G loss: 10.640714]\n",
      "epoch:12 step:9406 [D loss: 0.921681, acc.: 63.28%] [G loss: 2.653090]\n",
      "epoch:12 step:9407 [D loss: 0.207258, acc.: 90.62%] [G loss: 5.867749]\n",
      "epoch:12 step:9408 [D loss: 0.047583, acc.: 99.22%] [G loss: 4.837943]\n",
      "epoch:12 step:9409 [D loss: 0.014625, acc.: 100.00%] [G loss: 3.879880]\n",
      "epoch:12 step:9410 [D loss: 0.538589, acc.: 71.88%] [G loss: 4.153299]\n",
      "epoch:12 step:9411 [D loss: 0.004623, acc.: 100.00%] [G loss: 7.915573]\n",
      "epoch:12 step:9412 [D loss: 0.068042, acc.: 96.88%] [G loss: 4.904035]\n",
      "epoch:12 step:9413 [D loss: 0.027131, acc.: 100.00%] [G loss: 5.403698]\n",
      "epoch:12 step:9414 [D loss: 0.040160, acc.: 99.22%] [G loss: 3.045043]\n",
      "epoch:12 step:9415 [D loss: 0.069061, acc.: 100.00%] [G loss: 4.500089]\n",
      "epoch:12 step:9416 [D loss: 0.065776, acc.: 99.22%] [G loss: 6.335582]\n",
      "epoch:12 step:9417 [D loss: 0.144586, acc.: 93.75%] [G loss: 3.339197]\n",
      "epoch:12 step:9418 [D loss: 0.243047, acc.: 89.06%] [G loss: 6.063200]\n",
      "epoch:12 step:9419 [D loss: 0.136003, acc.: 96.09%] [G loss: 6.428020]\n",
      "epoch:12 step:9420 [D loss: 0.035195, acc.: 99.22%] [G loss: 1.810876]\n",
      "epoch:12 step:9421 [D loss: 0.028494, acc.: 100.00%] [G loss: 2.586804]\n",
      "epoch:12 step:9422 [D loss: 0.049684, acc.: 99.22%] [G loss: 0.979022]\n",
      "epoch:12 step:9423 [D loss: 0.059998, acc.: 100.00%] [G loss: 4.732834]\n",
      "epoch:12 step:9424 [D loss: 0.137292, acc.: 94.53%] [G loss: 1.495294]\n",
      "epoch:12 step:9425 [D loss: 0.005561, acc.: 100.00%] [G loss: 2.496480]\n",
      "epoch:12 step:9426 [D loss: 0.114885, acc.: 96.09%] [G loss: 0.261945]\n",
      "epoch:12 step:9427 [D loss: 0.021346, acc.: 100.00%] [G loss: 0.505383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9428 [D loss: 0.105369, acc.: 97.66%] [G loss: 0.615997]\n",
      "epoch:12 step:9429 [D loss: 0.011812, acc.: 100.00%] [G loss: 0.694735]\n",
      "epoch:12 step:9430 [D loss: 0.006606, acc.: 100.00%] [G loss: 1.237904]\n",
      "epoch:12 step:9431 [D loss: 0.021359, acc.: 100.00%] [G loss: 4.254389]\n",
      "epoch:12 step:9432 [D loss: 0.007409, acc.: 100.00%] [G loss: 1.205800]\n",
      "epoch:12 step:9433 [D loss: 0.040807, acc.: 99.22%] [G loss: 1.107613]\n",
      "epoch:12 step:9434 [D loss: 0.052685, acc.: 100.00%] [G loss: 2.960929]\n",
      "epoch:12 step:9435 [D loss: 0.057017, acc.: 99.22%] [G loss: 4.255429]\n",
      "epoch:12 step:9436 [D loss: 0.235711, acc.: 90.62%] [G loss: 7.602305]\n",
      "epoch:12 step:9437 [D loss: 2.139557, acc.: 18.75%] [G loss: 9.722820]\n",
      "epoch:12 step:9438 [D loss: 0.188418, acc.: 90.62%] [G loss: 8.632800]\n",
      "epoch:12 step:9439 [D loss: 0.110700, acc.: 95.31%] [G loss: 5.692939]\n",
      "epoch:12 step:9440 [D loss: 0.129099, acc.: 96.88%] [G loss: 6.503176]\n",
      "epoch:12 step:9441 [D loss: 0.019254, acc.: 100.00%] [G loss: 6.892799]\n",
      "epoch:12 step:9442 [D loss: 0.051689, acc.: 100.00%] [G loss: 4.794460]\n",
      "epoch:12 step:9443 [D loss: 0.047277, acc.: 98.44%] [G loss: 5.712728]\n",
      "epoch:12 step:9444 [D loss: 0.046612, acc.: 100.00%] [G loss: 4.660009]\n",
      "epoch:12 step:9445 [D loss: 0.225649, acc.: 90.62%] [G loss: 5.360292]\n",
      "epoch:12 step:9446 [D loss: 0.031580, acc.: 100.00%] [G loss: 5.585518]\n",
      "epoch:12 step:9447 [D loss: 0.048374, acc.: 100.00%] [G loss: 3.788180]\n",
      "epoch:12 step:9448 [D loss: 0.068014, acc.: 99.22%] [G loss: 3.373950]\n",
      "epoch:12 step:9449 [D loss: 0.182236, acc.: 93.75%] [G loss: 4.401264]\n",
      "epoch:12 step:9450 [D loss: 0.060158, acc.: 99.22%] [G loss: 5.138469]\n",
      "epoch:12 step:9451 [D loss: 0.035161, acc.: 100.00%] [G loss: 5.064283]\n",
      "epoch:12 step:9452 [D loss: 0.057011, acc.: 100.00%] [G loss: 3.204466]\n",
      "epoch:12 step:9453 [D loss: 0.033563, acc.: 100.00%] [G loss: 0.518823]\n",
      "epoch:12 step:9454 [D loss: 0.128304, acc.: 96.09%] [G loss: 6.130136]\n",
      "epoch:12 step:9455 [D loss: 0.030920, acc.: 99.22%] [G loss: 6.293116]\n",
      "epoch:12 step:9456 [D loss: 0.383368, acc.: 77.34%] [G loss: 1.323590]\n",
      "epoch:12 step:9457 [D loss: 0.808579, acc.: 65.62%] [G loss: 6.453565]\n",
      "epoch:12 step:9458 [D loss: 1.000132, acc.: 59.38%] [G loss: 5.373639]\n",
      "epoch:12 step:9459 [D loss: 0.014133, acc.: 100.00%] [G loss: 3.830547]\n",
      "epoch:12 step:9460 [D loss: 0.077452, acc.: 98.44%] [G loss: 4.447842]\n",
      "epoch:12 step:9461 [D loss: 0.011304, acc.: 100.00%] [G loss: 2.822083]\n",
      "epoch:12 step:9462 [D loss: 0.698926, acc.: 67.19%] [G loss: 2.137147]\n",
      "epoch:12 step:9463 [D loss: 0.007456, acc.: 100.00%] [G loss: 4.413615]\n",
      "epoch:12 step:9464 [D loss: 0.059790, acc.: 99.22%] [G loss: 4.183266]\n",
      "epoch:12 step:9465 [D loss: 0.023585, acc.: 100.00%] [G loss: 3.015869]\n",
      "epoch:12 step:9466 [D loss: 0.100187, acc.: 96.09%] [G loss: 4.070402]\n",
      "epoch:12 step:9467 [D loss: 0.146652, acc.: 94.53%] [G loss: 3.589165]\n",
      "epoch:12 step:9468 [D loss: 0.575052, acc.: 71.88%] [G loss: 8.106142]\n",
      "epoch:12 step:9469 [D loss: 1.054892, acc.: 55.47%] [G loss: 5.322452]\n",
      "epoch:12 step:9470 [D loss: 0.243582, acc.: 91.41%] [G loss: 6.937044]\n",
      "epoch:12 step:9471 [D loss: 0.102956, acc.: 97.66%] [G loss: 7.177521]\n",
      "epoch:12 step:9472 [D loss: 0.142032, acc.: 93.75%] [G loss: 4.251142]\n",
      "epoch:12 step:9473 [D loss: 0.108965, acc.: 95.31%] [G loss: 1.911897]\n",
      "epoch:12 step:9474 [D loss: 0.014622, acc.: 100.00%] [G loss: 3.188607]\n",
      "epoch:12 step:9475 [D loss: 0.112232, acc.: 96.88%] [G loss: 2.466369]\n",
      "epoch:12 step:9476 [D loss: 0.021323, acc.: 100.00%] [G loss: 4.545529]\n",
      "epoch:12 step:9477 [D loss: 0.139708, acc.: 96.09%] [G loss: 6.390675]\n",
      "epoch:12 step:9478 [D loss: 0.029067, acc.: 100.00%] [G loss: 6.112458]\n",
      "epoch:12 step:9479 [D loss: 0.402282, acc.: 77.34%] [G loss: 4.979032]\n",
      "epoch:12 step:9480 [D loss: 0.007885, acc.: 100.00%] [G loss: 5.006696]\n",
      "epoch:12 step:9481 [D loss: 0.009948, acc.: 100.00%] [G loss: 5.416202]\n",
      "epoch:12 step:9482 [D loss: 0.022401, acc.: 99.22%] [G loss: 4.039680]\n",
      "epoch:12 step:9483 [D loss: 0.025863, acc.: 100.00%] [G loss: 3.447134]\n",
      "epoch:12 step:9484 [D loss: 0.017671, acc.: 100.00%] [G loss: 2.787293]\n",
      "epoch:12 step:9485 [D loss: 0.108975, acc.: 96.88%] [G loss: 5.038253]\n",
      "epoch:12 step:9486 [D loss: 0.056359, acc.: 98.44%] [G loss: 2.870624]\n",
      "epoch:12 step:9487 [D loss: 0.020152, acc.: 100.00%] [G loss: 4.397017]\n",
      "epoch:12 step:9488 [D loss: 0.128067, acc.: 96.88%] [G loss: 1.203374]\n",
      "epoch:12 step:9489 [D loss: 0.075116, acc.: 99.22%] [G loss: 4.704192]\n",
      "epoch:12 step:9490 [D loss: 0.035451, acc.: 99.22%] [G loss: 4.553746]\n",
      "epoch:12 step:9491 [D loss: 0.060686, acc.: 99.22%] [G loss: 1.490579]\n",
      "epoch:12 step:9492 [D loss: 0.044331, acc.: 100.00%] [G loss: 4.061311]\n",
      "epoch:12 step:9493 [D loss: 0.030307, acc.: 99.22%] [G loss: 1.762735]\n",
      "epoch:12 step:9494 [D loss: 0.037646, acc.: 100.00%] [G loss: 3.562469]\n",
      "epoch:12 step:9495 [D loss: 0.152097, acc.: 92.97%] [G loss: 5.411759]\n",
      "epoch:12 step:9496 [D loss: 0.149837, acc.: 95.31%] [G loss: 2.955729]\n",
      "epoch:12 step:9497 [D loss: 0.156651, acc.: 96.09%] [G loss: 3.644937]\n",
      "epoch:12 step:9498 [D loss: 0.026886, acc.: 100.00%] [G loss: 4.341208]\n",
      "epoch:12 step:9499 [D loss: 0.062450, acc.: 98.44%] [G loss: 1.708801]\n",
      "epoch:12 step:9500 [D loss: 0.054136, acc.: 99.22%] [G loss: 4.447574]\n",
      "epoch:12 step:9501 [D loss: 0.073091, acc.: 98.44%] [G loss: 4.749721]\n",
      "epoch:12 step:9502 [D loss: 0.365055, acc.: 83.59%] [G loss: 6.541261]\n",
      "epoch:12 step:9503 [D loss: 0.003102, acc.: 100.00%] [G loss: 6.416239]\n",
      "epoch:12 step:9504 [D loss: 0.267032, acc.: 89.06%] [G loss: 4.354397]\n",
      "epoch:12 step:9505 [D loss: 0.134813, acc.: 93.75%] [G loss: 3.797798]\n",
      "epoch:12 step:9506 [D loss: 0.000814, acc.: 100.00%] [G loss: 3.159804]\n",
      "epoch:12 step:9507 [D loss: 0.004967, acc.: 100.00%] [G loss: 2.542546]\n",
      "epoch:12 step:9508 [D loss: 0.052873, acc.: 99.22%] [G loss: 2.171222]\n",
      "epoch:12 step:9509 [D loss: 0.005306, acc.: 100.00%] [G loss: 3.920879]\n",
      "epoch:12 step:9510 [D loss: 0.063713, acc.: 96.88%] [G loss: 3.656447]\n",
      "epoch:12 step:9511 [D loss: 0.162305, acc.: 92.97%] [G loss: 0.613566]\n",
      "epoch:12 step:9512 [D loss: 0.026826, acc.: 100.00%] [G loss: 1.115630]\n",
      "epoch:12 step:9513 [D loss: 0.224712, acc.: 91.41%] [G loss: 6.239448]\n",
      "epoch:12 step:9514 [D loss: 0.627475, acc.: 65.62%] [G loss: 3.952942]\n",
      "epoch:12 step:9515 [D loss: 0.025875, acc.: 98.44%] [G loss: 3.611373]\n",
      "epoch:12 step:9516 [D loss: 0.006692, acc.: 100.00%] [G loss: 1.502496]\n",
      "epoch:12 step:9517 [D loss: 0.009596, acc.: 100.00%] [G loss: 5.253477]\n",
      "epoch:12 step:9518 [D loss: 0.014010, acc.: 100.00%] [G loss: 2.188325]\n",
      "epoch:12 step:9519 [D loss: 0.023752, acc.: 100.00%] [G loss: 0.774909]\n",
      "epoch:12 step:9520 [D loss: 0.676416, acc.: 72.66%] [G loss: 9.486088]\n",
      "epoch:12 step:9521 [D loss: 2.778206, acc.: 50.00%] [G loss: 5.248785]\n",
      "epoch:12 step:9522 [D loss: 0.640928, acc.: 77.34%] [G loss: 0.912866]\n",
      "epoch:12 step:9523 [D loss: 0.492122, acc.: 76.56%] [G loss: 1.342115]\n",
      "epoch:12 step:9524 [D loss: 0.137946, acc.: 94.53%] [G loss: 5.257066]\n",
      "epoch:12 step:9525 [D loss: 0.156302, acc.: 92.97%] [G loss: 0.862548]\n",
      "epoch:12 step:9526 [D loss: 0.064584, acc.: 100.00%] [G loss: 2.763267]\n",
      "epoch:12 step:9527 [D loss: 0.179509, acc.: 93.75%] [G loss: 4.610416]\n",
      "epoch:12 step:9528 [D loss: 0.099325, acc.: 96.88%] [G loss: 3.774513]\n",
      "epoch:12 step:9529 [D loss: 0.077773, acc.: 97.66%] [G loss: 1.007225]\n",
      "epoch:12 step:9530 [D loss: 1.009847, acc.: 54.69%] [G loss: 5.058341]\n",
      "epoch:12 step:9531 [D loss: 1.781290, acc.: 50.00%] [G loss: 3.426914]\n",
      "epoch:12 step:9532 [D loss: 0.861484, acc.: 55.47%] [G loss: 3.520331]\n",
      "epoch:12 step:9533 [D loss: 0.143418, acc.: 97.66%] [G loss: 3.891692]\n",
      "epoch:12 step:9534 [D loss: 0.089932, acc.: 100.00%] [G loss: 4.205044]\n",
      "epoch:12 step:9535 [D loss: 0.170216, acc.: 96.09%] [G loss: 2.983119]\n",
      "epoch:12 step:9536 [D loss: 0.082111, acc.: 99.22%] [G loss: 2.582160]\n",
      "epoch:12 step:9537 [D loss: 0.095574, acc.: 100.00%] [G loss: 0.380173]\n",
      "epoch:12 step:9538 [D loss: 0.236655, acc.: 92.19%] [G loss: 3.932327]\n",
      "epoch:12 step:9539 [D loss: 0.235653, acc.: 85.16%] [G loss: 3.460168]\n",
      "epoch:12 step:9540 [D loss: 0.112721, acc.: 97.66%] [G loss: 2.480195]\n",
      "epoch:12 step:9541 [D loss: 0.173260, acc.: 94.53%] [G loss: 0.883625]\n",
      "epoch:12 step:9542 [D loss: 0.087617, acc.: 97.66%] [G loss: 1.122471]\n",
      "epoch:12 step:9543 [D loss: 0.108236, acc.: 99.22%] [G loss: 0.922689]\n",
      "epoch:12 step:9544 [D loss: 0.066517, acc.: 98.44%] [G loss: 0.398597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9545 [D loss: 0.248473, acc.: 93.75%] [G loss: 0.632086]\n",
      "epoch:12 step:9546 [D loss: 0.112265, acc.: 99.22%] [G loss: 4.240058]\n",
      "epoch:12 step:9547 [D loss: 0.089852, acc.: 98.44%] [G loss: 0.438447]\n",
      "epoch:12 step:9548 [D loss: 0.208679, acc.: 92.97%] [G loss: 3.055570]\n",
      "epoch:12 step:9549 [D loss: 0.056884, acc.: 100.00%] [G loss: 3.459126]\n",
      "epoch:12 step:9550 [D loss: 0.136333, acc.: 96.88%] [G loss: 3.139391]\n",
      "epoch:12 step:9551 [D loss: 0.076375, acc.: 96.88%] [G loss: 0.272991]\n",
      "epoch:12 step:9552 [D loss: 0.201047, acc.: 92.97%] [G loss: 3.914742]\n",
      "epoch:12 step:9553 [D loss: 0.125484, acc.: 97.66%] [G loss: 4.020498]\n",
      "epoch:12 step:9554 [D loss: 0.196142, acc.: 96.09%] [G loss: 0.820608]\n",
      "epoch:12 step:9555 [D loss: 0.057706, acc.: 99.22%] [G loss: 1.081291]\n",
      "epoch:12 step:9556 [D loss: 0.110433, acc.: 98.44%] [G loss: 3.222568]\n",
      "epoch:12 step:9557 [D loss: 0.099196, acc.: 97.66%] [G loss: 0.774693]\n",
      "epoch:12 step:9558 [D loss: 0.066325, acc.: 98.44%] [G loss: 0.799658]\n",
      "epoch:12 step:9559 [D loss: 0.082756, acc.: 99.22%] [G loss: 4.810937]\n",
      "epoch:12 step:9560 [D loss: 0.056721, acc.: 99.22%] [G loss: 0.491475]\n",
      "epoch:12 step:9561 [D loss: 0.066878, acc.: 99.22%] [G loss: 0.724191]\n",
      "epoch:12 step:9562 [D loss: 0.057383, acc.: 100.00%] [G loss: 0.929143]\n",
      "epoch:12 step:9563 [D loss: 0.051093, acc.: 100.00%] [G loss: 3.897274]\n",
      "epoch:12 step:9564 [D loss: 0.156001, acc.: 93.75%] [G loss: 6.499318]\n",
      "epoch:12 step:9565 [D loss: 0.331137, acc.: 83.59%] [G loss: 0.776259]\n",
      "epoch:12 step:9566 [D loss: 0.050776, acc.: 99.22%] [G loss: 4.069057]\n",
      "epoch:12 step:9567 [D loss: 0.012447, acc.: 100.00%] [G loss: 1.345851]\n",
      "epoch:12 step:9568 [D loss: 0.038400, acc.: 100.00%] [G loss: 0.901166]\n",
      "epoch:12 step:9569 [D loss: 0.014963, acc.: 100.00%] [G loss: 3.806930]\n",
      "epoch:12 step:9570 [D loss: 0.072493, acc.: 98.44%] [G loss: 4.018742]\n",
      "epoch:12 step:9571 [D loss: 0.014021, acc.: 100.00%] [G loss: 3.608706]\n",
      "epoch:12 step:9572 [D loss: 0.058983, acc.: 100.00%] [G loss: 1.686628]\n",
      "epoch:12 step:9573 [D loss: 0.057360, acc.: 100.00%] [G loss: 3.097598]\n",
      "epoch:12 step:9574 [D loss: 0.078352, acc.: 99.22%] [G loss: 3.559244]\n",
      "epoch:12 step:9575 [D loss: 0.130930, acc.: 96.88%] [G loss: 4.126785]\n",
      "epoch:12 step:9576 [D loss: 0.104558, acc.: 96.88%] [G loss: 1.890053]\n",
      "epoch:12 step:9577 [D loss: 0.069037, acc.: 99.22%] [G loss: 0.491724]\n",
      "epoch:12 step:9578 [D loss: 0.026680, acc.: 100.00%] [G loss: 0.444909]\n",
      "epoch:12 step:9579 [D loss: 0.010607, acc.: 100.00%] [G loss: 0.439585]\n",
      "epoch:12 step:9580 [D loss: 0.160118, acc.: 95.31%] [G loss: 1.925699]\n",
      "epoch:12 step:9581 [D loss: 0.441330, acc.: 75.78%] [G loss: 1.622524]\n",
      "epoch:12 step:9582 [D loss: 0.006942, acc.: 100.00%] [G loss: 1.349432]\n",
      "epoch:12 step:9583 [D loss: 0.002313, acc.: 100.00%] [G loss: 2.029119]\n",
      "epoch:12 step:9584 [D loss: 0.007663, acc.: 100.00%] [G loss: 3.577535]\n",
      "epoch:12 step:9585 [D loss: 0.113127, acc.: 96.09%] [G loss: 0.838273]\n",
      "epoch:12 step:9586 [D loss: 0.339386, acc.: 85.16%] [G loss: 0.125488]\n",
      "epoch:12 step:9587 [D loss: 0.002886, acc.: 100.00%] [G loss: 1.487256]\n",
      "epoch:12 step:9588 [D loss: 0.033478, acc.: 98.44%] [G loss: 2.489694]\n",
      "epoch:12 step:9589 [D loss: 0.002936, acc.: 100.00%] [G loss: 6.978949]\n",
      "epoch:12 step:9590 [D loss: 0.407012, acc.: 83.59%] [G loss: 6.783553]\n",
      "epoch:12 step:9591 [D loss: 0.034118, acc.: 99.22%] [G loss: 5.030214]\n",
      "epoch:12 step:9592 [D loss: 0.153702, acc.: 91.41%] [G loss: 1.302939]\n",
      "epoch:12 step:9593 [D loss: 0.214063, acc.: 91.41%] [G loss: 5.084629]\n",
      "epoch:12 step:9594 [D loss: 0.006220, acc.: 100.00%] [G loss: 7.533713]\n",
      "epoch:12 step:9595 [D loss: 0.120070, acc.: 93.75%] [G loss: 5.079032]\n",
      "epoch:12 step:9596 [D loss: 0.020401, acc.: 100.00%] [G loss: 3.192880]\n",
      "epoch:12 step:9597 [D loss: 0.008572, acc.: 100.00%] [G loss: 3.997089]\n",
      "epoch:12 step:9598 [D loss: 0.008480, acc.: 100.00%] [G loss: 1.864490]\n",
      "epoch:12 step:9599 [D loss: 0.024673, acc.: 100.00%] [G loss: 5.037208]\n",
      "epoch:12 step:9600 [D loss: 0.020853, acc.: 99.22%] [G loss: 5.530097]\n",
      "epoch:12 step:9601 [D loss: 0.098067, acc.: 99.22%] [G loss: 3.959920]\n",
      "epoch:12 step:9602 [D loss: 0.013210, acc.: 100.00%] [G loss: 3.038655]\n",
      "epoch:12 step:9603 [D loss: 0.010285, acc.: 100.00%] [G loss: 2.408181]\n",
      "epoch:12 step:9604 [D loss: 0.138176, acc.: 96.09%] [G loss: 5.673931]\n",
      "epoch:12 step:9605 [D loss: 0.071840, acc.: 96.88%] [G loss: 3.194974]\n",
      "epoch:12 step:9606 [D loss: 0.131329, acc.: 95.31%] [G loss: 2.039291]\n",
      "epoch:12 step:9607 [D loss: 0.258285, acc.: 86.72%] [G loss: 5.624094]\n",
      "epoch:12 step:9608 [D loss: 0.609433, acc.: 74.22%] [G loss: 1.489725]\n",
      "epoch:12 step:9609 [D loss: 0.062313, acc.: 97.66%] [G loss: 2.712032]\n",
      "epoch:12 step:9610 [D loss: 0.008519, acc.: 100.00%] [G loss: 3.778485]\n",
      "epoch:12 step:9611 [D loss: 0.027355, acc.: 100.00%] [G loss: 3.092391]\n",
      "epoch:12 step:9612 [D loss: 0.187450, acc.: 95.31%] [G loss: 5.852337]\n",
      "epoch:12 step:9613 [D loss: 0.007992, acc.: 100.00%] [G loss: 6.202345]\n",
      "epoch:12 step:9614 [D loss: 0.006061, acc.: 100.00%] [G loss: 6.508837]\n",
      "epoch:12 step:9615 [D loss: 0.060366, acc.: 98.44%] [G loss: 6.382231]\n",
      "epoch:12 step:9616 [D loss: 0.007617, acc.: 100.00%] [G loss: 6.805203]\n",
      "epoch:12 step:9617 [D loss: 0.011083, acc.: 100.00%] [G loss: 6.001745]\n",
      "epoch:12 step:9618 [D loss: 0.042000, acc.: 99.22%] [G loss: 4.294569]\n",
      "epoch:12 step:9619 [D loss: 0.080784, acc.: 98.44%] [G loss: 4.880706]\n",
      "epoch:12 step:9620 [D loss: 0.012100, acc.: 100.00%] [G loss: 5.582950]\n",
      "epoch:12 step:9621 [D loss: 0.013454, acc.: 100.00%] [G loss: 6.245194]\n",
      "epoch:12 step:9622 [D loss: 0.253858, acc.: 90.62%] [G loss: 6.994158]\n",
      "epoch:12 step:9623 [D loss: 0.001114, acc.: 100.00%] [G loss: 8.595413]\n",
      "epoch:12 step:9624 [D loss: 0.162525, acc.: 92.19%] [G loss: 5.691878]\n",
      "epoch:12 step:9625 [D loss: 0.008035, acc.: 100.00%] [G loss: 4.713453]\n",
      "epoch:12 step:9626 [D loss: 0.069559, acc.: 98.44%] [G loss: 5.820405]\n",
      "epoch:12 step:9627 [D loss: 0.000881, acc.: 100.00%] [G loss: 7.180446]\n",
      "epoch:12 step:9628 [D loss: 0.001112, acc.: 100.00%] [G loss: 6.973530]\n",
      "epoch:12 step:9629 [D loss: 0.009374, acc.: 100.00%] [G loss: 5.740480]\n",
      "epoch:12 step:9630 [D loss: 0.002831, acc.: 100.00%] [G loss: 5.912496]\n",
      "epoch:12 step:9631 [D loss: 0.004575, acc.: 100.00%] [G loss: 5.448162]\n",
      "epoch:12 step:9632 [D loss: 0.002393, acc.: 100.00%] [G loss: 5.784293]\n",
      "epoch:12 step:9633 [D loss: 0.005187, acc.: 100.00%] [G loss: 3.855515]\n",
      "epoch:12 step:9634 [D loss: 0.010415, acc.: 100.00%] [G loss: 3.442305]\n",
      "epoch:12 step:9635 [D loss: 0.020228, acc.: 100.00%] [G loss: 5.071413]\n",
      "epoch:12 step:9636 [D loss: 0.177618, acc.: 93.75%] [G loss: 5.771446]\n",
      "epoch:12 step:9637 [D loss: 0.015106, acc.: 99.22%] [G loss: 6.451456]\n",
      "epoch:12 step:9638 [D loss: 0.042589, acc.: 99.22%] [G loss: 6.726844]\n",
      "epoch:12 step:9639 [D loss: 0.137207, acc.: 96.09%] [G loss: 4.007470]\n",
      "epoch:12 step:9640 [D loss: 0.077551, acc.: 96.88%] [G loss: 5.251934]\n",
      "epoch:12 step:9641 [D loss: 0.006065, acc.: 100.00%] [G loss: 6.882918]\n",
      "epoch:12 step:9642 [D loss: 0.052402, acc.: 99.22%] [G loss: 6.032922]\n",
      "epoch:12 step:9643 [D loss: 0.024325, acc.: 99.22%] [G loss: 5.541773]\n",
      "epoch:12 step:9644 [D loss: 0.002951, acc.: 100.00%] [G loss: 4.838632]\n",
      "epoch:12 step:9645 [D loss: 0.012075, acc.: 100.00%] [G loss: 5.550122]\n",
      "epoch:12 step:9646 [D loss: 0.013390, acc.: 100.00%] [G loss: 0.134405]\n",
      "epoch:12 step:9647 [D loss: 0.009439, acc.: 100.00%] [G loss: 0.046975]\n",
      "epoch:12 step:9648 [D loss: 0.018258, acc.: 100.00%] [G loss: 5.685543]\n",
      "epoch:12 step:9649 [D loss: 0.293706, acc.: 83.59%] [G loss: 9.461253]\n",
      "epoch:12 step:9650 [D loss: 1.582728, acc.: 40.62%] [G loss: 7.123872]\n",
      "epoch:12 step:9651 [D loss: 0.003328, acc.: 100.00%] [G loss: 8.432796]\n",
      "epoch:12 step:9652 [D loss: 0.003238, acc.: 100.00%] [G loss: 7.535230]\n",
      "epoch:12 step:9653 [D loss: 0.003197, acc.: 100.00%] [G loss: 7.557537]\n",
      "epoch:12 step:9654 [D loss: 0.009797, acc.: 100.00%] [G loss: 7.260313]\n",
      "epoch:12 step:9655 [D loss: 0.004463, acc.: 100.00%] [G loss: 6.851718]\n",
      "epoch:12 step:9656 [D loss: 0.012200, acc.: 100.00%] [G loss: 6.240196]\n",
      "epoch:12 step:9657 [D loss: 0.013124, acc.: 100.00%] [G loss: 6.617623]\n",
      "epoch:12 step:9658 [D loss: 0.216709, acc.: 89.84%] [G loss: 0.372001]\n",
      "epoch:12 step:9659 [D loss: 0.004238, acc.: 100.00%] [G loss: 6.825919]\n",
      "epoch:12 step:9660 [D loss: 0.021940, acc.: 100.00%] [G loss: 6.914536]\n",
      "epoch:12 step:9661 [D loss: 0.033069, acc.: 99.22%] [G loss: 6.594709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9662 [D loss: 0.013549, acc.: 100.00%] [G loss: 5.283787]\n",
      "epoch:12 step:9663 [D loss: 0.028390, acc.: 99.22%] [G loss: 6.052294]\n",
      "epoch:12 step:9664 [D loss: 0.111805, acc.: 96.09%] [G loss: 0.045499]\n",
      "epoch:12 step:9665 [D loss: 0.105890, acc.: 96.88%] [G loss: 7.785645]\n",
      "epoch:12 step:9666 [D loss: 0.028517, acc.: 100.00%] [G loss: 7.125735]\n",
      "epoch:12 step:9667 [D loss: 1.189695, acc.: 45.31%] [G loss: 4.543621]\n",
      "epoch:12 step:9668 [D loss: 0.437816, acc.: 78.12%] [G loss: 3.357095]\n",
      "epoch:12 step:9669 [D loss: 0.028956, acc.: 100.00%] [G loss: 5.223164]\n",
      "epoch:12 step:9670 [D loss: 0.054464, acc.: 98.44%] [G loss: 3.282530]\n",
      "epoch:12 step:9671 [D loss: 0.051216, acc.: 99.22%] [G loss: 5.250929]\n",
      "epoch:12 step:9672 [D loss: 0.037965, acc.: 99.22%] [G loss: 6.118247]\n",
      "epoch:12 step:9673 [D loss: 0.014617, acc.: 100.00%] [G loss: 6.181559]\n",
      "epoch:12 step:9674 [D loss: 0.335986, acc.: 88.28%] [G loss: 1.198374]\n",
      "epoch:12 step:9675 [D loss: 0.071330, acc.: 97.66%] [G loss: 6.184619]\n",
      "epoch:12 step:9676 [D loss: 0.023009, acc.: 100.00%] [G loss: 5.179266]\n",
      "epoch:12 step:9677 [D loss: 0.143101, acc.: 92.97%] [G loss: 3.216544]\n",
      "epoch:12 step:9678 [D loss: 0.065338, acc.: 98.44%] [G loss: 5.793258]\n",
      "epoch:12 step:9679 [D loss: 0.012968, acc.: 100.00%] [G loss: 4.441093]\n",
      "epoch:12 step:9680 [D loss: 0.010057, acc.: 100.00%] [G loss: 3.579363]\n",
      "epoch:12 step:9681 [D loss: 0.145461, acc.: 96.09%] [G loss: 4.335146]\n",
      "epoch:12 step:9682 [D loss: 0.161824, acc.: 92.97%] [G loss: 5.608179]\n",
      "epoch:12 step:9683 [D loss: 0.008592, acc.: 100.00%] [G loss: 4.874830]\n",
      "epoch:12 step:9684 [D loss: 0.765208, acc.: 61.72%] [G loss: 5.216599]\n",
      "epoch:12 step:9685 [D loss: 0.006262, acc.: 100.00%] [G loss: 3.384522]\n",
      "epoch:12 step:9686 [D loss: 0.091027, acc.: 96.09%] [G loss: 3.403852]\n",
      "epoch:12 step:9687 [D loss: 0.183301, acc.: 93.75%] [G loss: 5.207603]\n",
      "epoch:12 step:9688 [D loss: 0.008334, acc.: 100.00%] [G loss: 2.393011]\n",
      "epoch:12 step:9689 [D loss: 0.010327, acc.: 100.00%] [G loss: 2.290446]\n",
      "epoch:12 step:9690 [D loss: 0.026884, acc.: 100.00%] [G loss: 3.966001]\n",
      "epoch:12 step:9691 [D loss: 0.008061, acc.: 100.00%] [G loss: 3.039776]\n",
      "epoch:12 step:9692 [D loss: 0.236123, acc.: 91.41%] [G loss: 1.883335]\n",
      "epoch:12 step:9693 [D loss: 0.017556, acc.: 100.00%] [G loss: 4.176975]\n",
      "epoch:12 step:9694 [D loss: 0.047201, acc.: 99.22%] [G loss: 2.688493]\n",
      "epoch:12 step:9695 [D loss: 0.005494, acc.: 100.00%] [G loss: 4.738850]\n",
      "epoch:12 step:9696 [D loss: 0.022736, acc.: 100.00%] [G loss: 4.169881]\n",
      "epoch:12 step:9697 [D loss: 0.022514, acc.: 100.00%] [G loss: 0.819642]\n",
      "epoch:12 step:9698 [D loss: 0.259247, acc.: 85.16%] [G loss: 5.434821]\n",
      "epoch:12 step:9699 [D loss: 3.247947, acc.: 9.38%] [G loss: 10.294149]\n",
      "epoch:12 step:9700 [D loss: 2.965401, acc.: 50.00%] [G loss: 5.323566]\n",
      "epoch:12 step:9701 [D loss: 0.782273, acc.: 66.41%] [G loss: 1.506934]\n",
      "epoch:12 step:9702 [D loss: 0.207764, acc.: 88.28%] [G loss: 2.290555]\n",
      "epoch:12 step:9703 [D loss: 0.068002, acc.: 98.44%] [G loss: 1.739658]\n",
      "epoch:12 step:9704 [D loss: 0.058093, acc.: 96.88%] [G loss: 4.545531]\n",
      "epoch:12 step:9705 [D loss: 0.031069, acc.: 100.00%] [G loss: 3.070896]\n",
      "epoch:12 step:9706 [D loss: 0.059412, acc.: 99.22%] [G loss: 3.740539]\n",
      "epoch:12 step:9707 [D loss: 0.136411, acc.: 96.09%] [G loss: 1.733795]\n",
      "epoch:12 step:9708 [D loss: 0.104393, acc.: 98.44%] [G loss: 3.277496]\n",
      "epoch:12 step:9709 [D loss: 0.187321, acc.: 96.88%] [G loss: 3.547243]\n",
      "epoch:12 step:9710 [D loss: 0.231551, acc.: 90.62%] [G loss: 2.664688]\n",
      "epoch:12 step:9711 [D loss: 0.133314, acc.: 98.44%] [G loss: 2.280312]\n",
      "epoch:12 step:9712 [D loss: 0.092140, acc.: 98.44%] [G loss: 2.825050]\n",
      "epoch:12 step:9713 [D loss: 0.082010, acc.: 98.44%] [G loss: 2.575977]\n",
      "epoch:12 step:9714 [D loss: 0.074821, acc.: 99.22%] [G loss: 1.705530]\n",
      "epoch:12 step:9715 [D loss: 0.102118, acc.: 97.66%] [G loss: 2.645541]\n",
      "epoch:12 step:9716 [D loss: 0.116979, acc.: 98.44%] [G loss: 3.601882]\n",
      "epoch:12 step:9717 [D loss: 0.198596, acc.: 92.19%] [G loss: 2.450599]\n",
      "epoch:12 step:9718 [D loss: 0.174529, acc.: 94.53%] [G loss: 4.448591]\n",
      "epoch:12 step:9719 [D loss: 0.027322, acc.: 100.00%] [G loss: 4.737700]\n",
      "epoch:12 step:9720 [D loss: 0.013537, acc.: 100.00%] [G loss: 3.988433]\n",
      "epoch:12 step:9721 [D loss: 0.028920, acc.: 99.22%] [G loss: 2.849536]\n",
      "epoch:12 step:9722 [D loss: 0.080792, acc.: 98.44%] [G loss: 4.875102]\n",
      "epoch:12 step:9723 [D loss: 0.047711, acc.: 99.22%] [G loss: 4.039075]\n",
      "epoch:12 step:9724 [D loss: 0.015787, acc.: 100.00%] [G loss: 0.362738]\n",
      "epoch:12 step:9725 [D loss: 0.095997, acc.: 96.88%] [G loss: 4.432580]\n",
      "epoch:12 step:9726 [D loss: 0.155697, acc.: 95.31%] [G loss: 4.727083]\n",
      "epoch:12 step:9727 [D loss: 0.108587, acc.: 94.53%] [G loss: 3.131029]\n",
      "epoch:12 step:9728 [D loss: 0.201397, acc.: 91.41%] [G loss: 5.908356]\n",
      "epoch:12 step:9729 [D loss: 0.263485, acc.: 85.16%] [G loss: 0.992425]\n",
      "epoch:12 step:9730 [D loss: 0.130459, acc.: 99.22%] [G loss: 4.888542]\n",
      "epoch:12 step:9731 [D loss: 0.009997, acc.: 100.00%] [G loss: 5.490572]\n",
      "epoch:12 step:9732 [D loss: 0.047867, acc.: 99.22%] [G loss: 3.197310]\n",
      "epoch:12 step:9733 [D loss: 0.040918, acc.: 99.22%] [G loss: 0.945870]\n",
      "epoch:12 step:9734 [D loss: 0.037349, acc.: 100.00%] [G loss: 1.065618]\n",
      "epoch:12 step:9735 [D loss: 0.108307, acc.: 97.66%] [G loss: 2.273901]\n",
      "epoch:12 step:9736 [D loss: 2.486309, acc.: 23.44%] [G loss: 7.499805]\n",
      "epoch:12 step:9737 [D loss: 1.956795, acc.: 51.56%] [G loss: 5.230564]\n",
      "epoch:12 step:9738 [D loss: 0.956917, acc.: 55.47%] [G loss: 2.231788]\n",
      "epoch:12 step:9739 [D loss: 0.181478, acc.: 94.53%] [G loss: 1.101966]\n",
      "epoch:12 step:9740 [D loss: 0.027963, acc.: 100.00%] [G loss: 1.213913]\n",
      "epoch:12 step:9741 [D loss: 0.047356, acc.: 99.22%] [G loss: 1.474623]\n",
      "epoch:12 step:9742 [D loss: 0.061520, acc.: 100.00%] [G loss: 1.670677]\n",
      "epoch:12 step:9743 [D loss: 0.024797, acc.: 100.00%] [G loss: 0.768462]\n",
      "epoch:12 step:9744 [D loss: 0.024816, acc.: 100.00%] [G loss: 0.588679]\n",
      "epoch:12 step:9745 [D loss: 0.099352, acc.: 99.22%] [G loss: 2.224497]\n",
      "epoch:12 step:9746 [D loss: 0.164057, acc.: 93.75%] [G loss: 0.989430]\n",
      "epoch:12 step:9747 [D loss: 0.179985, acc.: 95.31%] [G loss: 1.115743]\n",
      "epoch:12 step:9748 [D loss: 0.042507, acc.: 100.00%] [G loss: 1.415678]\n",
      "epoch:12 step:9749 [D loss: 0.058294, acc.: 100.00%] [G loss: 1.513143]\n",
      "epoch:12 step:9750 [D loss: 0.294724, acc.: 89.06%] [G loss: 0.919690]\n",
      "epoch:12 step:9751 [D loss: 0.009982, acc.: 100.00%] [G loss: 1.217663]\n",
      "epoch:12 step:9752 [D loss: 0.092725, acc.: 99.22%] [G loss: 1.118342]\n",
      "epoch:12 step:9753 [D loss: 0.218766, acc.: 95.31%] [G loss: 2.128433]\n",
      "epoch:12 step:9754 [D loss: 0.101968, acc.: 96.88%] [G loss: 1.625148]\n",
      "epoch:12 step:9755 [D loss: 0.055369, acc.: 99.22%] [G loss: 2.208112]\n",
      "epoch:12 step:9756 [D loss: 0.190973, acc.: 93.75%] [G loss: 3.282788]\n",
      "epoch:12 step:9757 [D loss: 0.238863, acc.: 89.06%] [G loss: 2.427949]\n",
      "epoch:12 step:9758 [D loss: 0.056745, acc.: 100.00%] [G loss: 1.754276]\n",
      "epoch:12 step:9759 [D loss: 0.042287, acc.: 99.22%] [G loss: 3.220670]\n",
      "epoch:12 step:9760 [D loss: 0.226881, acc.: 89.84%] [G loss: 3.229640]\n",
      "epoch:12 step:9761 [D loss: 0.013423, acc.: 100.00%] [G loss: 4.262527]\n",
      "epoch:12 step:9762 [D loss: 0.053226, acc.: 98.44%] [G loss: 4.373294]\n",
      "epoch:12 step:9763 [D loss: 0.032704, acc.: 100.00%] [G loss: 3.738136]\n",
      "epoch:12 step:9764 [D loss: 0.035030, acc.: 100.00%] [G loss: 3.463262]\n",
      "epoch:12 step:9765 [D loss: 0.029462, acc.: 99.22%] [G loss: 3.904240]\n",
      "epoch:12 step:9766 [D loss: 0.030285, acc.: 100.00%] [G loss: 3.040871]\n",
      "epoch:12 step:9767 [D loss: 0.040789, acc.: 100.00%] [G loss: 3.143464]\n",
      "epoch:12 step:9768 [D loss: 0.044151, acc.: 100.00%] [G loss: 3.164642]\n",
      "epoch:12 step:9769 [D loss: 0.018119, acc.: 100.00%] [G loss: 1.574018]\n",
      "epoch:12 step:9770 [D loss: 0.011463, acc.: 100.00%] [G loss: 1.946425]\n",
      "epoch:12 step:9771 [D loss: 0.056713, acc.: 99.22%] [G loss: 2.761907]\n",
      "epoch:12 step:9772 [D loss: 0.014461, acc.: 100.00%] [G loss: 1.191306]\n",
      "epoch:12 step:9773 [D loss: 0.209951, acc.: 92.97%] [G loss: 0.716393]\n",
      "epoch:12 step:9774 [D loss: 0.165744, acc.: 92.97%] [G loss: 3.162148]\n",
      "epoch:12 step:9775 [D loss: 0.093403, acc.: 98.44%] [G loss: 2.656340]\n",
      "epoch:12 step:9776 [D loss: 0.012151, acc.: 100.00%] [G loss: 3.752579]\n",
      "epoch:12 step:9777 [D loss: 0.055813, acc.: 97.66%] [G loss: 0.137223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9778 [D loss: 0.222909, acc.: 89.84%] [G loss: 5.436975]\n",
      "epoch:12 step:9779 [D loss: 0.029615, acc.: 100.00%] [G loss: 3.739678]\n",
      "epoch:12 step:9780 [D loss: 0.279780, acc.: 85.16%] [G loss: 3.296246]\n",
      "epoch:12 step:9781 [D loss: 0.195616, acc.: 90.62%] [G loss: 4.643573]\n",
      "epoch:12 step:9782 [D loss: 0.004223, acc.: 100.00%] [G loss: 5.789645]\n",
      "epoch:12 step:9783 [D loss: 0.294998, acc.: 82.81%] [G loss: 2.317437]\n",
      "epoch:12 step:9784 [D loss: 0.296343, acc.: 89.06%] [G loss: 4.375065]\n",
      "epoch:12 step:9785 [D loss: 0.011418, acc.: 100.00%] [G loss: 6.682790]\n",
      "epoch:12 step:9786 [D loss: 0.102430, acc.: 95.31%] [G loss: 4.164613]\n",
      "epoch:12 step:9787 [D loss: 0.028119, acc.: 100.00%] [G loss: 3.721685]\n",
      "epoch:12 step:9788 [D loss: 0.059238, acc.: 99.22%] [G loss: 3.113507]\n",
      "epoch:12 step:9789 [D loss: 0.047165, acc.: 99.22%] [G loss: 3.026018]\n",
      "epoch:12 step:9790 [D loss: 0.041900, acc.: 100.00%] [G loss: 2.598061]\n",
      "epoch:12 step:9791 [D loss: 0.081557, acc.: 97.66%] [G loss: 3.262154]\n",
      "epoch:12 step:9792 [D loss: 0.425753, acc.: 77.34%] [G loss: 1.807144]\n",
      "epoch:12 step:9793 [D loss: 0.020853, acc.: 100.00%] [G loss: 3.914470]\n",
      "epoch:12 step:9794 [D loss: 0.160506, acc.: 96.09%] [G loss: 2.988236]\n",
      "epoch:12 step:9795 [D loss: 0.013978, acc.: 100.00%] [G loss: 2.447464]\n",
      "epoch:12 step:9796 [D loss: 0.038702, acc.: 99.22%] [G loss: 1.695502]\n",
      "epoch:12 step:9797 [D loss: 0.019815, acc.: 100.00%] [G loss: 2.492027]\n",
      "epoch:12 step:9798 [D loss: 0.033521, acc.: 99.22%] [G loss: 1.138100]\n",
      "epoch:12 step:9799 [D loss: 0.092069, acc.: 97.66%] [G loss: 2.233011]\n",
      "epoch:12 step:9800 [D loss: 0.207568, acc.: 90.62%] [G loss: 1.993640]\n",
      "epoch:12 step:9801 [D loss: 0.115838, acc.: 95.31%] [G loss: 3.992150]\n",
      "epoch:12 step:9802 [D loss: 0.074360, acc.: 98.44%] [G loss: 1.782479]\n",
      "epoch:12 step:9803 [D loss: 0.093111, acc.: 99.22%] [G loss: 0.905482]\n",
      "epoch:12 step:9804 [D loss: 0.062977, acc.: 98.44%] [G loss: 1.793758]\n",
      "epoch:12 step:9805 [D loss: 0.013443, acc.: 100.00%] [G loss: 2.686261]\n",
      "epoch:12 step:9806 [D loss: 0.119415, acc.: 97.66%] [G loss: 2.222620]\n",
      "epoch:12 step:9807 [D loss: 0.053977, acc.: 100.00%] [G loss: 1.278466]\n",
      "epoch:12 step:9808 [D loss: 0.025341, acc.: 100.00%] [G loss: 2.697317]\n",
      "epoch:12 step:9809 [D loss: 0.063966, acc.: 100.00%] [G loss: 3.817745]\n",
      "epoch:12 step:9810 [D loss: 0.028784, acc.: 100.00%] [G loss: 1.704830]\n",
      "epoch:12 step:9811 [D loss: 0.025281, acc.: 99.22%] [G loss: 2.193130]\n",
      "epoch:12 step:9812 [D loss: 0.049458, acc.: 100.00%] [G loss: 4.102090]\n",
      "epoch:12 step:9813 [D loss: 0.213996, acc.: 92.19%] [G loss: 1.587721]\n",
      "epoch:12 step:9814 [D loss: 0.015583, acc.: 100.00%] [G loss: 1.598142]\n",
      "epoch:12 step:9815 [D loss: 0.012216, acc.: 100.00%] [G loss: 0.787649]\n",
      "epoch:12 step:9816 [D loss: 0.029327, acc.: 100.00%] [G loss: 2.050653]\n",
      "epoch:12 step:9817 [D loss: 0.030702, acc.: 100.00%] [G loss: 0.794698]\n",
      "epoch:12 step:9818 [D loss: 0.060980, acc.: 99.22%] [G loss: 3.016386]\n",
      "epoch:12 step:9819 [D loss: 0.148602, acc.: 94.53%] [G loss: 0.500552]\n",
      "epoch:12 step:9820 [D loss: 0.055303, acc.: 99.22%] [G loss: 0.187974]\n",
      "epoch:12 step:9821 [D loss: 0.010642, acc.: 100.00%] [G loss: 0.365398]\n",
      "epoch:12 step:9822 [D loss: 0.009852, acc.: 100.00%] [G loss: 1.416105]\n",
      "epoch:12 step:9823 [D loss: 0.012918, acc.: 100.00%] [G loss: 0.634094]\n",
      "epoch:12 step:9824 [D loss: 0.003522, acc.: 100.00%] [G loss: 0.719608]\n",
      "epoch:12 step:9825 [D loss: 0.084573, acc.: 99.22%] [G loss: 0.415676]\n",
      "epoch:12 step:9826 [D loss: 0.005348, acc.: 100.00%] [G loss: 0.598586]\n",
      "epoch:12 step:9827 [D loss: 0.009233, acc.: 100.00%] [G loss: 0.450647]\n",
      "epoch:12 step:9828 [D loss: 0.019905, acc.: 100.00%] [G loss: 2.111600]\n",
      "epoch:12 step:9829 [D loss: 0.006346, acc.: 100.00%] [G loss: 1.691988]\n",
      "epoch:12 step:9830 [D loss: 0.011434, acc.: 100.00%] [G loss: 2.921030]\n",
      "epoch:12 step:9831 [D loss: 0.752591, acc.: 64.84%] [G loss: 7.569266]\n",
      "epoch:12 step:9832 [D loss: 1.622002, acc.: 52.34%] [G loss: 1.028514]\n",
      "epoch:12 step:9833 [D loss: 0.600046, acc.: 72.66%] [G loss: 7.313632]\n",
      "epoch:12 step:9834 [D loss: 0.176069, acc.: 92.97%] [G loss: 7.201150]\n",
      "epoch:12 step:9835 [D loss: 0.094445, acc.: 96.09%] [G loss: 3.481786]\n",
      "epoch:12 step:9836 [D loss: 0.016386, acc.: 100.00%] [G loss: 4.435292]\n",
      "epoch:12 step:9837 [D loss: 0.052846, acc.: 99.22%] [G loss: 4.761044]\n",
      "epoch:12 step:9838 [D loss: 0.035783, acc.: 100.00%] [G loss: 4.972449]\n",
      "epoch:12 step:9839 [D loss: 0.028097, acc.: 100.00%] [G loss: 4.991239]\n",
      "epoch:12 step:9840 [D loss: 0.080408, acc.: 98.44%] [G loss: 3.863605]\n",
      "epoch:12 step:9841 [D loss: 0.062650, acc.: 98.44%] [G loss: 3.874219]\n",
      "epoch:12 step:9842 [D loss: 0.036108, acc.: 99.22%] [G loss: 3.876213]\n",
      "epoch:12 step:9843 [D loss: 0.226366, acc.: 91.41%] [G loss: 3.178794]\n",
      "epoch:12 step:9844 [D loss: 0.077509, acc.: 98.44%] [G loss: 4.352768]\n",
      "epoch:12 step:9845 [D loss: 0.010313, acc.: 100.00%] [G loss: 5.376476]\n",
      "epoch:12 step:9846 [D loss: 1.366183, acc.: 28.12%] [G loss: 5.132310]\n",
      "epoch:12 step:9847 [D loss: 0.279000, acc.: 87.50%] [G loss: 5.192828]\n",
      "epoch:12 step:9848 [D loss: 0.020665, acc.: 100.00%] [G loss: 4.296530]\n",
      "epoch:12 step:9849 [D loss: 0.013571, acc.: 100.00%] [G loss: 4.495409]\n",
      "epoch:12 step:9850 [D loss: 0.058253, acc.: 100.00%] [G loss: 3.303572]\n",
      "epoch:12 step:9851 [D loss: 0.046185, acc.: 100.00%] [G loss: 3.096436]\n",
      "epoch:12 step:9852 [D loss: 0.021594, acc.: 100.00%] [G loss: 3.109186]\n",
      "epoch:12 step:9853 [D loss: 0.137147, acc.: 96.09%] [G loss: 5.290798]\n",
      "epoch:12 step:9854 [D loss: 0.057669, acc.: 98.44%] [G loss: 5.401697]\n",
      "epoch:12 step:9855 [D loss: 0.250474, acc.: 90.62%] [G loss: 2.419984]\n",
      "epoch:12 step:9856 [D loss: 0.062233, acc.: 99.22%] [G loss: 3.585896]\n",
      "epoch:12 step:9857 [D loss: 0.034814, acc.: 100.00%] [G loss: 0.980560]\n",
      "epoch:12 step:9858 [D loss: 0.025882, acc.: 100.00%] [G loss: 1.555765]\n",
      "epoch:12 step:9859 [D loss: 0.086762, acc.: 98.44%] [G loss: 4.552126]\n",
      "epoch:12 step:9860 [D loss: 0.053755, acc.: 100.00%] [G loss: 5.852651]\n",
      "epoch:12 step:9861 [D loss: 0.036798, acc.: 100.00%] [G loss: 4.525770]\n",
      "epoch:12 step:9862 [D loss: 0.008585, acc.: 100.00%] [G loss: 5.056419]\n",
      "epoch:12 step:9863 [D loss: 0.083532, acc.: 98.44%] [G loss: 3.388252]\n",
      "epoch:12 step:9864 [D loss: 0.074730, acc.: 99.22%] [G loss: 3.710333]\n",
      "epoch:12 step:9865 [D loss: 0.039837, acc.: 99.22%] [G loss: 4.978047]\n",
      "epoch:12 step:9866 [D loss: 0.007673, acc.: 100.00%] [G loss: 3.748665]\n",
      "epoch:12 step:9867 [D loss: 0.028930, acc.: 100.00%] [G loss: 1.560485]\n",
      "epoch:12 step:9868 [D loss: 0.043295, acc.: 100.00%] [G loss: 3.500002]\n",
      "epoch:12 step:9869 [D loss: 0.096021, acc.: 99.22%] [G loss: 1.579221]\n",
      "epoch:12 step:9870 [D loss: 0.107619, acc.: 97.66%] [G loss: 4.305286]\n",
      "epoch:12 step:9871 [D loss: 0.099222, acc.: 99.22%] [G loss: 2.945410]\n",
      "epoch:12 step:9872 [D loss: 0.011788, acc.: 100.00%] [G loss: 1.500732]\n",
      "epoch:12 step:9873 [D loss: 0.374490, acc.: 85.16%] [G loss: 2.378741]\n",
      "epoch:12 step:9874 [D loss: 0.011441, acc.: 100.00%] [G loss: 3.723014]\n",
      "epoch:12 step:9875 [D loss: 0.131928, acc.: 96.09%] [G loss: 0.578449]\n",
      "epoch:12 step:9876 [D loss: 0.015585, acc.: 100.00%] [G loss: 0.640998]\n",
      "epoch:12 step:9877 [D loss: 0.051271, acc.: 99.22%] [G loss: 0.985350]\n",
      "epoch:12 step:9878 [D loss: 0.062454, acc.: 99.22%] [G loss: 4.487267]\n",
      "epoch:12 step:9879 [D loss: 0.352284, acc.: 82.03%] [G loss: 0.439690]\n",
      "epoch:12 step:9880 [D loss: 0.280875, acc.: 88.28%] [G loss: 2.564127]\n",
      "epoch:12 step:9881 [D loss: 1.909426, acc.: 48.44%] [G loss: 0.007074]\n",
      "epoch:12 step:9882 [D loss: 0.130623, acc.: 95.31%] [G loss: 4.556601]\n",
      "epoch:12 step:9883 [D loss: 0.055341, acc.: 98.44%] [G loss: 0.877577]\n",
      "epoch:12 step:9884 [D loss: 0.019052, acc.: 100.00%] [G loss: 0.418043]\n",
      "epoch:12 step:9885 [D loss: 0.040532, acc.: 100.00%] [G loss: 0.115102]\n",
      "epoch:12 step:9886 [D loss: 0.078849, acc.: 97.66%] [G loss: 0.076644]\n",
      "epoch:12 step:9887 [D loss: 0.010560, acc.: 100.00%] [G loss: 0.195303]\n",
      "epoch:12 step:9888 [D loss: 0.023497, acc.: 100.00%] [G loss: 0.053129]\n",
      "epoch:12 step:9889 [D loss: 0.022572, acc.: 100.00%] [G loss: 4.229325]\n",
      "epoch:12 step:9890 [D loss: 0.020221, acc.: 99.22%] [G loss: 0.085507]\n",
      "epoch:12 step:9891 [D loss: 0.171629, acc.: 94.53%] [G loss: 0.414439]\n",
      "epoch:12 step:9892 [D loss: 0.037617, acc.: 99.22%] [G loss: 1.261969]\n",
      "epoch:12 step:9893 [D loss: 0.055902, acc.: 98.44%] [G loss: 0.133156]\n",
      "epoch:12 step:9894 [D loss: 0.046731, acc.: 99.22%] [G loss: 0.691809]\n",
      "epoch:12 step:9895 [D loss: 0.164798, acc.: 93.75%] [G loss: 2.187871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9896 [D loss: 0.585198, acc.: 71.09%] [G loss: 0.347850]\n",
      "epoch:12 step:9897 [D loss: 0.500934, acc.: 79.69%] [G loss: 7.681822]\n",
      "epoch:12 step:9898 [D loss: 0.505414, acc.: 75.00%] [G loss: 4.108215]\n",
      "epoch:12 step:9899 [D loss: 0.060985, acc.: 99.22%] [G loss: 2.963521]\n",
      "epoch:12 step:9900 [D loss: 0.010652, acc.: 100.00%] [G loss: 1.016682]\n",
      "epoch:12 step:9901 [D loss: 0.027406, acc.: 100.00%] [G loss: 2.088715]\n",
      "epoch:12 step:9902 [D loss: 0.066280, acc.: 99.22%] [G loss: 4.267291]\n",
      "epoch:12 step:9903 [D loss: 0.046463, acc.: 99.22%] [G loss: 3.901525]\n",
      "epoch:12 step:9904 [D loss: 0.186231, acc.: 94.53%] [G loss: 3.618342]\n",
      "epoch:12 step:9905 [D loss: 0.437753, acc.: 81.25%] [G loss: 2.574841]\n",
      "epoch:12 step:9906 [D loss: 0.005450, acc.: 100.00%] [G loss: 4.605242]\n",
      "epoch:12 step:9907 [D loss: 0.037525, acc.: 98.44%] [G loss: 3.978451]\n",
      "epoch:12 step:9908 [D loss: 0.083280, acc.: 98.44%] [G loss: 0.742979]\n",
      "epoch:12 step:9909 [D loss: 0.065931, acc.: 99.22%] [G loss: 2.577328]\n",
      "epoch:12 step:9910 [D loss: 0.005961, acc.: 100.00%] [G loss: 4.187157]\n",
      "epoch:12 step:9911 [D loss: 0.006467, acc.: 100.00%] [G loss: 5.333758]\n",
      "epoch:12 step:9912 [D loss: 0.014171, acc.: 100.00%] [G loss: 2.302866]\n",
      "epoch:12 step:9913 [D loss: 0.014685, acc.: 100.00%] [G loss: 2.012405]\n",
      "epoch:12 step:9914 [D loss: 0.223942, acc.: 92.97%] [G loss: 3.815958]\n",
      "epoch:12 step:9915 [D loss: 0.008362, acc.: 100.00%] [G loss: 5.418219]\n",
      "epoch:12 step:9916 [D loss: 0.020726, acc.: 100.00%] [G loss: 5.498331]\n",
      "epoch:12 step:9917 [D loss: 0.037163, acc.: 99.22%] [G loss: 4.969739]\n",
      "epoch:12 step:9918 [D loss: 0.013538, acc.: 100.00%] [G loss: 3.472292]\n",
      "epoch:12 step:9919 [D loss: 0.028607, acc.: 100.00%] [G loss: 3.510100]\n",
      "epoch:12 step:9920 [D loss: 0.024589, acc.: 100.00%] [G loss: 1.514196]\n",
      "epoch:12 step:9921 [D loss: 0.031658, acc.: 99.22%] [G loss: 4.726479]\n",
      "epoch:12 step:9922 [D loss: 0.022022, acc.: 100.00%] [G loss: 5.620667]\n",
      "epoch:12 step:9923 [D loss: 0.018637, acc.: 100.00%] [G loss: 4.906959]\n",
      "epoch:12 step:9924 [D loss: 0.323598, acc.: 88.28%] [G loss: 3.551075]\n",
      "epoch:12 step:9925 [D loss: 0.152295, acc.: 93.75%] [G loss: 5.491423]\n",
      "epoch:12 step:9926 [D loss: 0.007561, acc.: 100.00%] [G loss: 5.024187]\n",
      "epoch:12 step:9927 [D loss: 0.007287, acc.: 100.00%] [G loss: 4.815160]\n",
      "epoch:12 step:9928 [D loss: 0.011198, acc.: 100.00%] [G loss: 5.679892]\n",
      "epoch:12 step:9929 [D loss: 0.008529, acc.: 100.00%] [G loss: 4.618503]\n",
      "epoch:12 step:9930 [D loss: 0.086119, acc.: 96.88%] [G loss: 4.442225]\n",
      "epoch:12 step:9931 [D loss: 0.011064, acc.: 100.00%] [G loss: 3.847903]\n",
      "epoch:12 step:9932 [D loss: 0.016003, acc.: 100.00%] [G loss: 0.261773]\n",
      "epoch:12 step:9933 [D loss: 0.037216, acc.: 100.00%] [G loss: 4.960106]\n",
      "epoch:12 step:9934 [D loss: 0.010921, acc.: 100.00%] [G loss: 5.476342]\n",
      "epoch:12 step:9935 [D loss: 0.010488, acc.: 100.00%] [G loss: 5.037775]\n",
      "epoch:12 step:9936 [D loss: 0.002462, acc.: 100.00%] [G loss: 4.855708]\n",
      "epoch:12 step:9937 [D loss: 0.030107, acc.: 99.22%] [G loss: 4.604028]\n",
      "epoch:12 step:9938 [D loss: 0.202909, acc.: 94.53%] [G loss: 6.444881]\n",
      "epoch:12 step:9939 [D loss: 0.021106, acc.: 99.22%] [G loss: 7.994453]\n",
      "epoch:12 step:9940 [D loss: 0.019183, acc.: 100.00%] [G loss: 6.263612]\n",
      "epoch:12 step:9941 [D loss: 0.069197, acc.: 98.44%] [G loss: 0.164386]\n",
      "epoch:12 step:9942 [D loss: 0.012893, acc.: 100.00%] [G loss: 0.823891]\n",
      "epoch:12 step:9943 [D loss: 0.040532, acc.: 99.22%] [G loss: 6.789455]\n",
      "epoch:12 step:9944 [D loss: 0.008430, acc.: 100.00%] [G loss: 0.208628]\n",
      "epoch:12 step:9945 [D loss: 0.249733, acc.: 87.50%] [G loss: 7.515338]\n",
      "epoch:12 step:9946 [D loss: 1.257486, acc.: 49.22%] [G loss: 4.003259]\n",
      "epoch:12 step:9947 [D loss: 0.451992, acc.: 76.56%] [G loss: 1.032738]\n",
      "epoch:12 step:9948 [D loss: 0.029735, acc.: 99.22%] [G loss: 0.433689]\n",
      "epoch:12 step:9949 [D loss: 0.225851, acc.: 89.06%] [G loss: 4.894692]\n",
      "epoch:12 step:9950 [D loss: 0.017619, acc.: 100.00%] [G loss: 4.797892]\n",
      "epoch:12 step:9951 [D loss: 0.122209, acc.: 95.31%] [G loss: 1.336704]\n",
      "epoch:12 step:9952 [D loss: 0.093795, acc.: 96.09%] [G loss: 2.615506]\n",
      "epoch:12 step:9953 [D loss: 0.017050, acc.: 100.00%] [G loss: 2.293936]\n",
      "epoch:12 step:9954 [D loss: 0.008877, acc.: 100.00%] [G loss: 2.083428]\n",
      "epoch:12 step:9955 [D loss: 0.016708, acc.: 100.00%] [G loss: 1.333750]\n",
      "epoch:12 step:9956 [D loss: 0.150919, acc.: 96.88%] [G loss: 4.456004]\n",
      "epoch:12 step:9957 [D loss: 0.007382, acc.: 100.00%] [G loss: 2.714395]\n",
      "epoch:12 step:9958 [D loss: 0.122207, acc.: 96.09%] [G loss: 0.117067]\n",
      "epoch:12 step:9959 [D loss: 1.083110, acc.: 58.59%] [G loss: 8.848886]\n",
      "epoch:12 step:9960 [D loss: 2.773442, acc.: 50.00%] [G loss: 6.550074]\n",
      "epoch:12 step:9961 [D loss: 0.406677, acc.: 78.12%] [G loss: 2.856410]\n",
      "epoch:12 step:9962 [D loss: 0.287874, acc.: 85.94%] [G loss: 3.611381]\n",
      "epoch:12 step:9963 [D loss: 0.009041, acc.: 100.00%] [G loss: 5.754028]\n",
      "epoch:12 step:9964 [D loss: 0.096842, acc.: 96.88%] [G loss: 3.734845]\n",
      "epoch:12 step:9965 [D loss: 0.025726, acc.: 100.00%] [G loss: 3.229748]\n",
      "epoch:12 step:9966 [D loss: 0.032897, acc.: 100.00%] [G loss: 2.432861]\n",
      "epoch:12 step:9967 [D loss: 0.029780, acc.: 99.22%] [G loss: 2.767439]\n",
      "epoch:12 step:9968 [D loss: 0.072722, acc.: 97.66%] [G loss: 4.032319]\n",
      "epoch:12 step:9969 [D loss: 0.030577, acc.: 100.00%] [G loss: 3.704212]\n",
      "epoch:12 step:9970 [D loss: 0.162169, acc.: 96.09%] [G loss: 3.517754]\n",
      "epoch:12 step:9971 [D loss: 0.150499, acc.: 92.97%] [G loss: 2.736856]\n",
      "epoch:12 step:9972 [D loss: 0.505460, acc.: 77.34%] [G loss: 5.922241]\n",
      "epoch:12 step:9973 [D loss: 0.852674, acc.: 57.03%] [G loss: 4.856150]\n",
      "epoch:12 step:9974 [D loss: 0.024482, acc.: 99.22%] [G loss: 5.091573]\n",
      "epoch:12 step:9975 [D loss: 0.008797, acc.: 100.00%] [G loss: 3.016313]\n",
      "epoch:12 step:9976 [D loss: 0.081374, acc.: 99.22%] [G loss: 3.897233]\n",
      "epoch:12 step:9977 [D loss: 0.016778, acc.: 100.00%] [G loss: 3.740358]\n",
      "epoch:12 step:9978 [D loss: 0.048116, acc.: 100.00%] [G loss: 4.452094]\n",
      "epoch:12 step:9979 [D loss: 0.012620, acc.: 100.00%] [G loss: 4.092829]\n",
      "epoch:12 step:9980 [D loss: 0.017861, acc.: 100.00%] [G loss: 3.538167]\n",
      "epoch:12 step:9981 [D loss: 0.064073, acc.: 100.00%] [G loss: 1.061773]\n",
      "epoch:12 step:9982 [D loss: 0.010967, acc.: 100.00%] [G loss: 2.678296]\n",
      "epoch:12 step:9983 [D loss: 0.039857, acc.: 100.00%] [G loss: 3.308732]\n",
      "epoch:12 step:9984 [D loss: 0.012504, acc.: 100.00%] [G loss: 0.417102]\n",
      "epoch:12 step:9985 [D loss: 0.141401, acc.: 96.88%] [G loss: 4.241403]\n",
      "epoch:12 step:9986 [D loss: 0.009882, acc.: 100.00%] [G loss: 4.884800]\n",
      "epoch:12 step:9987 [D loss: 0.021783, acc.: 100.00%] [G loss: 4.069535]\n",
      "epoch:12 step:9988 [D loss: 0.056398, acc.: 100.00%] [G loss: 3.580944]\n",
      "epoch:12 step:9989 [D loss: 0.027769, acc.: 100.00%] [G loss: 3.188934]\n",
      "epoch:12 step:9990 [D loss: 0.021980, acc.: 100.00%] [G loss: 2.744838]\n",
      "epoch:12 step:9991 [D loss: 0.085012, acc.: 99.22%] [G loss: 3.781976]\n",
      "epoch:12 step:9992 [D loss: 0.067235, acc.: 99.22%] [G loss: 4.214482]\n",
      "epoch:12 step:9993 [D loss: 0.036014, acc.: 100.00%] [G loss: 0.432818]\n",
      "epoch:12 step:9994 [D loss: 0.192859, acc.: 93.75%] [G loss: 1.983894]\n",
      "epoch:12 step:9995 [D loss: 0.199546, acc.: 91.41%] [G loss: 5.476020]\n",
      "epoch:12 step:9996 [D loss: 0.058308, acc.: 99.22%] [G loss: 0.791044]\n",
      "epoch:12 step:9997 [D loss: 0.070353, acc.: 98.44%] [G loss: 6.231207]\n",
      "epoch:12 step:9998 [D loss: 0.388641, acc.: 80.47%] [G loss: 5.789677]\n",
      "epoch:12 step:9999 [D loss: 0.144035, acc.: 95.31%] [G loss: 4.390430]\n",
      "epoch:12 step:10000 [D loss: 0.030969, acc.: 100.00%] [G loss: 4.319487]\n",
      "epoch:12 step:10001 [D loss: 0.029064, acc.: 100.00%] [G loss: 1.793372]\n",
      "epoch:12 step:10002 [D loss: 0.124760, acc.: 97.66%] [G loss: 4.557810]\n",
      "epoch:12 step:10003 [D loss: 0.156891, acc.: 94.53%] [G loss: 4.360500]\n",
      "epoch:12 step:10004 [D loss: 0.088530, acc.: 99.22%] [G loss: 3.586060]\n",
      "epoch:12 step:10005 [D loss: 0.031477, acc.: 100.00%] [G loss: 2.500931]\n",
      "epoch:12 step:10006 [D loss: 0.041254, acc.: 98.44%] [G loss: 1.567345]\n",
      "epoch:12 step:10007 [D loss: 0.051637, acc.: 99.22%] [G loss: 1.370956]\n",
      "epoch:12 step:10008 [D loss: 0.065341, acc.: 99.22%] [G loss: 2.469669]\n",
      "epoch:12 step:10009 [D loss: 0.009600, acc.: 100.00%] [G loss: 4.082970]\n",
      "epoch:12 step:10010 [D loss: 0.038924, acc.: 100.00%] [G loss: 1.987591]\n",
      "epoch:12 step:10011 [D loss: 0.053551, acc.: 99.22%] [G loss: 1.120419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10012 [D loss: 0.073731, acc.: 98.44%] [G loss: 1.096148]\n",
      "epoch:12 step:10013 [D loss: 0.030198, acc.: 100.00%] [G loss: 3.711623]\n",
      "epoch:12 step:10014 [D loss: 0.937760, acc.: 49.22%] [G loss: 5.846013]\n",
      "epoch:12 step:10015 [D loss: 1.687918, acc.: 50.00%] [G loss: 2.275139]\n",
      "epoch:12 step:10016 [D loss: 0.064848, acc.: 97.66%] [G loss: 2.625793]\n",
      "epoch:12 step:10017 [D loss: 0.077034, acc.: 97.66%] [G loss: 2.834118]\n",
      "epoch:12 step:10018 [D loss: 0.013223, acc.: 100.00%] [G loss: 4.763650]\n",
      "epoch:12 step:10019 [D loss: 0.191021, acc.: 94.53%] [G loss: 2.385514]\n",
      "epoch:12 step:10020 [D loss: 0.011966, acc.: 100.00%] [G loss: 1.518705]\n",
      "epoch:12 step:10021 [D loss: 0.008993, acc.: 100.00%] [G loss: 2.319417]\n",
      "epoch:12 step:10022 [D loss: 0.082223, acc.: 97.66%] [G loss: 3.536254]\n",
      "epoch:12 step:10023 [D loss: 0.034864, acc.: 100.00%] [G loss: 1.310313]\n",
      "epoch:12 step:10024 [D loss: 0.040103, acc.: 100.00%] [G loss: 0.544347]\n",
      "epoch:12 step:10025 [D loss: 0.022629, acc.: 100.00%] [G loss: 1.075810]\n",
      "epoch:12 step:10026 [D loss: 0.087710, acc.: 97.66%] [G loss: 3.378658]\n",
      "epoch:12 step:10027 [D loss: 0.016112, acc.: 100.00%] [G loss: 1.714938]\n",
      "epoch:12 step:10028 [D loss: 0.045751, acc.: 99.22%] [G loss: 0.999283]\n",
      "epoch:12 step:10029 [D loss: 0.030855, acc.: 100.00%] [G loss: 1.923644]\n",
      "epoch:12 step:10030 [D loss: 0.230291, acc.: 92.19%] [G loss: 4.552904]\n",
      "epoch:12 step:10031 [D loss: 0.402225, acc.: 76.56%] [G loss: 0.993384]\n",
      "epoch:12 step:10032 [D loss: 0.064624, acc.: 98.44%] [G loss: 0.916161]\n",
      "epoch:12 step:10033 [D loss: 0.016257, acc.: 100.00%] [G loss: 0.775777]\n",
      "epoch:12 step:10034 [D loss: 0.013143, acc.: 100.00%] [G loss: 1.354488]\n",
      "epoch:12 step:10035 [D loss: 0.025529, acc.: 100.00%] [G loss: 0.827783]\n",
      "epoch:12 step:10036 [D loss: 0.067769, acc.: 99.22%] [G loss: 3.197016]\n",
      "epoch:12 step:10037 [D loss: 0.195411, acc.: 94.53%] [G loss: 1.877174]\n",
      "epoch:12 step:10038 [D loss: 0.052292, acc.: 99.22%] [G loss: 2.718976]\n",
      "epoch:12 step:10039 [D loss: 0.025612, acc.: 100.00%] [G loss: 1.693080]\n",
      "epoch:12 step:10040 [D loss: 0.369453, acc.: 84.38%] [G loss: 4.586844]\n",
      "epoch:12 step:10041 [D loss: 0.058999, acc.: 98.44%] [G loss: 5.057457]\n",
      "epoch:12 step:10042 [D loss: 0.161099, acc.: 93.75%] [G loss: 1.088244]\n",
      "epoch:12 step:10043 [D loss: 0.032552, acc.: 99.22%] [G loss: 0.819773]\n",
      "epoch:12 step:10044 [D loss: 0.009094, acc.: 100.00%] [G loss: 0.681459]\n",
      "epoch:12 step:10045 [D loss: 0.006450, acc.: 100.00%] [G loss: 0.420680]\n",
      "epoch:12 step:10046 [D loss: 0.021829, acc.: 100.00%] [G loss: 2.226294]\n",
      "epoch:12 step:10047 [D loss: 0.021175, acc.: 100.00%] [G loss: 3.770291]\n",
      "epoch:12 step:10048 [D loss: 0.067600, acc.: 98.44%] [G loss: 2.638955]\n",
      "epoch:12 step:10049 [D loss: 0.009927, acc.: 100.00%] [G loss: 0.781626]\n",
      "epoch:12 step:10050 [D loss: 0.103112, acc.: 96.09%] [G loss: 0.757389]\n",
      "epoch:12 step:10051 [D loss: 0.043982, acc.: 99.22%] [G loss: 0.077418]\n",
      "epoch:12 step:10052 [D loss: 0.043989, acc.: 99.22%] [G loss: 1.048363]\n",
      "epoch:12 step:10053 [D loss: 0.043812, acc.: 100.00%] [G loss: 1.979063]\n",
      "epoch:12 step:10054 [D loss: 0.007274, acc.: 100.00%] [G loss: 3.107835]\n",
      "epoch:12 step:10055 [D loss: 0.020768, acc.: 99.22%] [G loss: 1.772048]\n",
      "epoch:12 step:10056 [D loss: 0.030835, acc.: 100.00%] [G loss: 1.269837]\n",
      "epoch:12 step:10057 [D loss: 0.357608, acc.: 85.94%] [G loss: 4.355721]\n",
      "epoch:12 step:10058 [D loss: 0.105098, acc.: 96.09%] [G loss: 6.474264]\n",
      "epoch:12 step:10059 [D loss: 1.346943, acc.: 40.62%] [G loss: 5.989320]\n",
      "epoch:12 step:10060 [D loss: 0.218585, acc.: 91.41%] [G loss: 3.017047]\n",
      "epoch:12 step:10061 [D loss: 0.007029, acc.: 100.00%] [G loss: 2.556141]\n",
      "epoch:12 step:10062 [D loss: 0.003530, acc.: 100.00%] [G loss: 1.104486]\n",
      "epoch:12 step:10063 [D loss: 0.020312, acc.: 100.00%] [G loss: 1.605255]\n",
      "epoch:12 step:10064 [D loss: 0.041712, acc.: 98.44%] [G loss: 2.531168]\n",
      "epoch:12 step:10065 [D loss: 0.226030, acc.: 88.28%] [G loss: 4.793349]\n",
      "epoch:12 step:10066 [D loss: 0.022234, acc.: 99.22%] [G loss: 5.710500]\n",
      "epoch:12 step:10067 [D loss: 0.040823, acc.: 98.44%] [G loss: 5.166434]\n",
      "epoch:12 step:10068 [D loss: 0.022446, acc.: 100.00%] [G loss: 4.006236]\n",
      "epoch:12 step:10069 [D loss: 0.012112, acc.: 100.00%] [G loss: 4.485130]\n",
      "epoch:12 step:10070 [D loss: 0.005364, acc.: 100.00%] [G loss: 1.483231]\n",
      "epoch:12 step:10071 [D loss: 0.093415, acc.: 96.88%] [G loss: 2.322088]\n",
      "epoch:12 step:10072 [D loss: 0.060304, acc.: 99.22%] [G loss: 3.746986]\n",
      "epoch:12 step:10073 [D loss: 0.042060, acc.: 99.22%] [G loss: 3.354178]\n",
      "epoch:12 step:10074 [D loss: 0.078390, acc.: 97.66%] [G loss: 0.612223]\n",
      "epoch:12 step:10075 [D loss: 0.019978, acc.: 100.00%] [G loss: 3.434715]\n",
      "epoch:12 step:10076 [D loss: 0.095353, acc.: 99.22%] [G loss: 3.706176]\n",
      "epoch:12 step:10077 [D loss: 0.169903, acc.: 94.53%] [G loss: 3.850575]\n",
      "epoch:12 step:10078 [D loss: 0.010941, acc.: 100.00%] [G loss: 3.579829]\n",
      "epoch:12 step:10079 [D loss: 0.121423, acc.: 93.75%] [G loss: 0.578383]\n",
      "epoch:12 step:10080 [D loss: 0.051299, acc.: 99.22%] [G loss: 3.644362]\n",
      "epoch:12 step:10081 [D loss: 0.007700, acc.: 100.00%] [G loss: 0.738008]\n",
      "epoch:12 step:10082 [D loss: 0.009045, acc.: 100.00%] [G loss: 0.538496]\n",
      "epoch:12 step:10083 [D loss: 0.018701, acc.: 100.00%] [G loss: 0.997274]\n",
      "epoch:12 step:10084 [D loss: 0.044777, acc.: 100.00%] [G loss: 1.048256]\n",
      "epoch:12 step:10085 [D loss: 0.198299, acc.: 92.97%] [G loss: 0.745399]\n",
      "epoch:12 step:10086 [D loss: 0.004709, acc.: 100.00%] [G loss: 0.232331]\n",
      "epoch:12 step:10087 [D loss: 0.211680, acc.: 92.19%] [G loss: 3.467080]\n",
      "epoch:12 step:10088 [D loss: 0.032481, acc.: 100.00%] [G loss: 7.880938]\n",
      "epoch:12 step:10089 [D loss: 0.101089, acc.: 96.09%] [G loss: 4.790772]\n",
      "epoch:12 step:10090 [D loss: 0.027201, acc.: 99.22%] [G loss: 4.110073]\n",
      "epoch:12 step:10091 [D loss: 0.039501, acc.: 99.22%] [G loss: 4.783768]\n",
      "epoch:12 step:10092 [D loss: 0.001947, acc.: 100.00%] [G loss: 3.607937]\n",
      "epoch:12 step:10093 [D loss: 0.004234, acc.: 100.00%] [G loss: 2.063026]\n",
      "epoch:12 step:10094 [D loss: 0.004465, acc.: 100.00%] [G loss: 3.218096]\n",
      "epoch:12 step:10095 [D loss: 0.007547, acc.: 100.00%] [G loss: 4.020633]\n",
      "epoch:12 step:10096 [D loss: 0.011962, acc.: 100.00%] [G loss: 0.159084]\n",
      "epoch:12 step:10097 [D loss: 0.619474, acc.: 71.09%] [G loss: 6.198744]\n",
      "epoch:12 step:10098 [D loss: 0.652482, acc.: 69.53%] [G loss: 4.337463]\n",
      "epoch:12 step:10099 [D loss: 0.015501, acc.: 100.00%] [G loss: 2.515498]\n",
      "epoch:12 step:10100 [D loss: 0.014592, acc.: 100.00%] [G loss: 0.428828]\n",
      "epoch:12 step:10101 [D loss: 0.011876, acc.: 100.00%] [G loss: 0.256480]\n",
      "epoch:12 step:10102 [D loss: 0.024096, acc.: 100.00%] [G loss: 0.055880]\n",
      "epoch:12 step:10103 [D loss: 0.048179, acc.: 99.22%] [G loss: 3.580034]\n",
      "epoch:12 step:10104 [D loss: 0.015483, acc.: 100.00%] [G loss: 4.106647]\n",
      "epoch:12 step:10105 [D loss: 0.006099, acc.: 100.00%] [G loss: 0.629316]\n",
      "epoch:12 step:10106 [D loss: 0.011990, acc.: 100.00%] [G loss: 2.859756]\n",
      "epoch:12 step:10107 [D loss: 0.080162, acc.: 99.22%] [G loss: 1.408550]\n",
      "epoch:12 step:10108 [D loss: 0.077535, acc.: 96.88%] [G loss: 1.423493]\n",
      "epoch:12 step:10109 [D loss: 0.040878, acc.: 96.88%] [G loss: 4.118098]\n",
      "epoch:12 step:10110 [D loss: 0.019630, acc.: 99.22%] [G loss: 1.655320]\n",
      "epoch:12 step:10111 [D loss: 0.050443, acc.: 99.22%] [G loss: 1.386959]\n",
      "epoch:12 step:10112 [D loss: 0.002893, acc.: 100.00%] [G loss: 0.517129]\n",
      "epoch:12 step:10113 [D loss: 0.057531, acc.: 98.44%] [G loss: 3.523403]\n",
      "epoch:12 step:10114 [D loss: 0.015478, acc.: 99.22%] [G loss: 3.270923]\n",
      "epoch:12 step:10115 [D loss: 0.116806, acc.: 96.09%] [G loss: 4.820499]\n",
      "epoch:12 step:10116 [D loss: 0.005815, acc.: 100.00%] [G loss: 2.174115]\n",
      "epoch:12 step:10117 [D loss: 0.077503, acc.: 96.88%] [G loss: 3.002787]\n",
      "epoch:12 step:10118 [D loss: 0.023594, acc.: 100.00%] [G loss: 4.378180]\n",
      "epoch:12 step:10119 [D loss: 0.168482, acc.: 94.53%] [G loss: 1.720140]\n",
      "epoch:12 step:10120 [D loss: 0.043620, acc.: 99.22%] [G loss: 4.841658]\n",
      "epoch:12 step:10121 [D loss: 0.017206, acc.: 100.00%] [G loss: 3.137860]\n",
      "epoch:12 step:10122 [D loss: 0.052215, acc.: 99.22%] [G loss: 3.303722]\n",
      "epoch:12 step:10123 [D loss: 0.014805, acc.: 100.00%] [G loss: 4.426741]\n",
      "epoch:12 step:10124 [D loss: 0.011959, acc.: 100.00%] [G loss: 3.024327]\n",
      "epoch:12 step:10125 [D loss: 0.143844, acc.: 96.09%] [G loss: 5.281790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10126 [D loss: 0.699744, acc.: 67.19%] [G loss: 6.444877]\n",
      "epoch:12 step:10127 [D loss: 0.739061, acc.: 67.97%] [G loss: 0.253150]\n",
      "epoch:12 step:10128 [D loss: 0.505072, acc.: 74.22%] [G loss: 8.396633]\n",
      "epoch:12 step:10129 [D loss: 0.106592, acc.: 93.75%] [G loss: 6.512590]\n",
      "epoch:12 step:10130 [D loss: 0.114889, acc.: 93.75%] [G loss: 5.024375]\n",
      "epoch:12 step:10131 [D loss: 0.041806, acc.: 99.22%] [G loss: 5.563444]\n",
      "epoch:12 step:10132 [D loss: 0.016896, acc.: 100.00%] [G loss: 6.896311]\n",
      "epoch:12 step:10133 [D loss: 0.070197, acc.: 97.66%] [G loss: 4.903524]\n",
      "epoch:12 step:10134 [D loss: 0.281717, acc.: 91.41%] [G loss: 3.395698]\n",
      "epoch:12 step:10135 [D loss: 0.052879, acc.: 99.22%] [G loss: 1.859263]\n",
      "epoch:12 step:10136 [D loss: 0.006433, acc.: 100.00%] [G loss: 0.673959]\n",
      "epoch:12 step:10137 [D loss: 0.008043, acc.: 100.00%] [G loss: 2.943897]\n",
      "epoch:12 step:10138 [D loss: 0.009212, acc.: 100.00%] [G loss: 1.683725]\n",
      "epoch:12 step:10139 [D loss: 0.068421, acc.: 99.22%] [G loss: 1.005070]\n",
      "epoch:12 step:10140 [D loss: 0.474930, acc.: 75.00%] [G loss: 6.400900]\n",
      "epoch:12 step:10141 [D loss: 0.012314, acc.: 99.22%] [G loss: 2.823814]\n",
      "epoch:12 step:10142 [D loss: 0.662801, acc.: 70.31%] [G loss: 1.645012]\n",
      "epoch:12 step:10143 [D loss: 0.828097, acc.: 71.88%] [G loss: 0.457821]\n",
      "epoch:12 step:10144 [D loss: 0.073120, acc.: 96.88%] [G loss: 8.243001]\n",
      "epoch:12 step:10145 [D loss: 0.244040, acc.: 88.28%] [G loss: 0.269476]\n",
      "epoch:12 step:10146 [D loss: 0.009155, acc.: 100.00%] [G loss: 5.245901]\n",
      "epoch:12 step:10147 [D loss: 0.015953, acc.: 100.00%] [G loss: 0.634917]\n",
      "epoch:12 step:10148 [D loss: 0.044921, acc.: 97.66%] [G loss: 4.187260]\n",
      "epoch:12 step:10149 [D loss: 0.005958, acc.: 100.00%] [G loss: 3.070126]\n",
      "epoch:12 step:10150 [D loss: 0.051775, acc.: 98.44%] [G loss: 0.952934]\n",
      "epoch:12 step:10151 [D loss: 0.120232, acc.: 97.66%] [G loss: 0.085864]\n",
      "epoch:12 step:10152 [D loss: 0.014938, acc.: 100.00%] [G loss: 4.061038]\n",
      "epoch:12 step:10153 [D loss: 0.020527, acc.: 100.00%] [G loss: 0.207748]\n",
      "epoch:13 step:10154 [D loss: 0.415667, acc.: 85.16%] [G loss: 0.001999]\n",
      "epoch:13 step:10155 [D loss: 0.044115, acc.: 99.22%] [G loss: 0.011990]\n",
      "epoch:13 step:10156 [D loss: 1.008853, acc.: 59.38%] [G loss: 8.419304]\n",
      "epoch:13 step:10157 [D loss: 3.026965, acc.: 50.00%] [G loss: 5.015282]\n",
      "epoch:13 step:10158 [D loss: 1.195590, acc.: 39.06%] [G loss: 3.057122]\n",
      "epoch:13 step:10159 [D loss: 0.204855, acc.: 94.53%] [G loss: 1.882795]\n",
      "epoch:13 step:10160 [D loss: 0.266143, acc.: 89.06%] [G loss: 1.712054]\n",
      "epoch:13 step:10161 [D loss: 0.243621, acc.: 91.41%] [G loss: 4.279939]\n",
      "epoch:13 step:10162 [D loss: 0.299241, acc.: 86.72%] [G loss: 1.809172]\n",
      "epoch:13 step:10163 [D loss: 0.096576, acc.: 100.00%] [G loss: 1.240155]\n",
      "epoch:13 step:10164 [D loss: 0.089919, acc.: 100.00%] [G loss: 1.896579]\n",
      "epoch:13 step:10165 [D loss: 0.337007, acc.: 84.38%] [G loss: 0.731726]\n",
      "epoch:13 step:10166 [D loss: 0.213618, acc.: 92.97%] [G loss: 1.806873]\n",
      "epoch:13 step:10167 [D loss: 0.175056, acc.: 92.97%] [G loss: 3.355418]\n",
      "epoch:13 step:10168 [D loss: 0.229699, acc.: 95.31%] [G loss: 2.012989]\n",
      "epoch:13 step:10169 [D loss: 0.135474, acc.: 92.97%] [G loss: 2.930470]\n",
      "epoch:13 step:10170 [D loss: 0.300464, acc.: 89.06%] [G loss: 3.488788]\n",
      "epoch:13 step:10171 [D loss: 0.069786, acc.: 99.22%] [G loss: 5.210073]\n",
      "epoch:13 step:10172 [D loss: 0.504021, acc.: 77.34%] [G loss: 0.180245]\n",
      "epoch:13 step:10173 [D loss: 0.880147, acc.: 55.47%] [G loss: 6.121655]\n",
      "epoch:13 step:10174 [D loss: 0.679529, acc.: 66.41%] [G loss: 3.267624]\n",
      "epoch:13 step:10175 [D loss: 0.297776, acc.: 87.50%] [G loss: 2.072704]\n",
      "epoch:13 step:10176 [D loss: 0.478683, acc.: 74.22%] [G loss: 4.334958]\n",
      "epoch:13 step:10177 [D loss: 0.067728, acc.: 98.44%] [G loss: 4.184374]\n",
      "epoch:13 step:10178 [D loss: 0.065272, acc.: 99.22%] [G loss: 4.090945]\n",
      "epoch:13 step:10179 [D loss: 0.085500, acc.: 98.44%] [G loss: 2.680858]\n",
      "epoch:13 step:10180 [D loss: 0.024674, acc.: 100.00%] [G loss: 0.363988]\n",
      "epoch:13 step:10181 [D loss: 0.185258, acc.: 91.41%] [G loss: 4.233941]\n",
      "epoch:13 step:10182 [D loss: 0.040241, acc.: 100.00%] [G loss: 2.915121]\n",
      "epoch:13 step:10183 [D loss: 0.244389, acc.: 89.84%] [G loss: 3.341082]\n",
      "epoch:13 step:10184 [D loss: 0.120035, acc.: 97.66%] [G loss: 1.389889]\n",
      "epoch:13 step:10185 [D loss: 0.058264, acc.: 98.44%] [G loss: 3.482512]\n",
      "epoch:13 step:10186 [D loss: 0.178045, acc.: 94.53%] [G loss: 2.137268]\n",
      "epoch:13 step:10187 [D loss: 0.103505, acc.: 97.66%] [G loss: 2.706455]\n",
      "epoch:13 step:10188 [D loss: 0.029374, acc.: 100.00%] [G loss: 3.867032]\n",
      "epoch:13 step:10189 [D loss: 0.210968, acc.: 92.19%] [G loss: 0.075084]\n",
      "epoch:13 step:10190 [D loss: 0.159118, acc.: 94.53%] [G loss: 3.270340]\n",
      "epoch:13 step:10191 [D loss: 0.142470, acc.: 95.31%] [G loss: 0.965310]\n",
      "epoch:13 step:10192 [D loss: 0.068258, acc.: 100.00%] [G loss: 3.031033]\n",
      "epoch:13 step:10193 [D loss: 0.023208, acc.: 100.00%] [G loss: 0.368066]\n",
      "epoch:13 step:10194 [D loss: 0.061378, acc.: 100.00%] [G loss: 0.632965]\n",
      "epoch:13 step:10195 [D loss: 0.105743, acc.: 99.22%] [G loss: 2.631847]\n",
      "epoch:13 step:10196 [D loss: 0.060876, acc.: 99.22%] [G loss: 4.920318]\n",
      "epoch:13 step:10197 [D loss: 0.206937, acc.: 92.19%] [G loss: 1.682469]\n",
      "epoch:13 step:10198 [D loss: 0.089377, acc.: 96.09%] [G loss: 1.852844]\n",
      "epoch:13 step:10199 [D loss: 0.023603, acc.: 100.00%] [G loss: 1.817014]\n",
      "epoch:13 step:10200 [D loss: 0.020650, acc.: 100.00%] [G loss: 1.411337]\n",
      "epoch:13 step:10201 [D loss: 0.037615, acc.: 99.22%] [G loss: 0.930309]\n",
      "epoch:13 step:10202 [D loss: 0.013160, acc.: 100.00%] [G loss: 2.644809]\n",
      "epoch:13 step:10203 [D loss: 0.128306, acc.: 98.44%] [G loss: 1.986345]\n",
      "epoch:13 step:10204 [D loss: 0.027473, acc.: 98.44%] [G loss: 2.092148]\n",
      "epoch:13 step:10205 [D loss: 0.024551, acc.: 100.00%] [G loss: 2.348741]\n",
      "epoch:13 step:10206 [D loss: 0.024553, acc.: 100.00%] [G loss: 2.961540]\n",
      "epoch:13 step:10207 [D loss: 0.020842, acc.: 100.00%] [G loss: 2.504109]\n",
      "epoch:13 step:10208 [D loss: 0.011564, acc.: 100.00%] [G loss: 2.302132]\n",
      "epoch:13 step:10209 [D loss: 0.083574, acc.: 96.88%] [G loss: 0.207808]\n",
      "epoch:13 step:10210 [D loss: 0.028431, acc.: 100.00%] [G loss: 3.164927]\n",
      "epoch:13 step:10211 [D loss: 0.010522, acc.: 100.00%] [G loss: 3.069185]\n",
      "epoch:13 step:10212 [D loss: 0.026592, acc.: 100.00%] [G loss: 3.474611]\n",
      "epoch:13 step:10213 [D loss: 0.033867, acc.: 99.22%] [G loss: 1.571310]\n",
      "epoch:13 step:10214 [D loss: 0.222774, acc.: 89.06%] [G loss: 4.098751]\n",
      "epoch:13 step:10215 [D loss: 0.053825, acc.: 98.44%] [G loss: 5.379243]\n",
      "epoch:13 step:10216 [D loss: 0.262547, acc.: 90.62%] [G loss: 3.099186]\n",
      "epoch:13 step:10217 [D loss: 0.021762, acc.: 100.00%] [G loss: 3.289153]\n",
      "epoch:13 step:10218 [D loss: 0.163212, acc.: 92.19%] [G loss: 1.972708]\n",
      "epoch:13 step:10219 [D loss: 0.052865, acc.: 99.22%] [G loss: 0.248700]\n",
      "epoch:13 step:10220 [D loss: 0.016187, acc.: 99.22%] [G loss: 2.389852]\n",
      "epoch:13 step:10221 [D loss: 0.033707, acc.: 100.00%] [G loss: 3.557709]\n",
      "epoch:13 step:10222 [D loss: 0.012991, acc.: 100.00%] [G loss: 3.132183]\n",
      "epoch:13 step:10223 [D loss: 0.007118, acc.: 100.00%] [G loss: 3.089926]\n",
      "epoch:13 step:10224 [D loss: 0.007965, acc.: 100.00%] [G loss: 4.235917]\n",
      "epoch:13 step:10225 [D loss: 0.042381, acc.: 100.00%] [G loss: 2.348565]\n",
      "epoch:13 step:10226 [D loss: 0.018239, acc.: 100.00%] [G loss: 2.920495]\n",
      "epoch:13 step:10227 [D loss: 0.045449, acc.: 99.22%] [G loss: 3.501705]\n",
      "epoch:13 step:10228 [D loss: 0.045140, acc.: 100.00%] [G loss: 4.224802]\n",
      "epoch:13 step:10229 [D loss: 0.055037, acc.: 98.44%] [G loss: 3.748724]\n",
      "epoch:13 step:10230 [D loss: 0.035682, acc.: 98.44%] [G loss: 0.016584]\n",
      "epoch:13 step:10231 [D loss: 1.642989, acc.: 53.91%] [G loss: 10.290107]\n",
      "epoch:13 step:10232 [D loss: 3.895208, acc.: 50.00%] [G loss: 4.296038]\n",
      "epoch:13 step:10233 [D loss: 1.796564, acc.: 50.00%] [G loss: 2.787471]\n",
      "epoch:13 step:10234 [D loss: 0.534523, acc.: 78.12%] [G loss: 0.362412]\n",
      "epoch:13 step:10235 [D loss: 0.142431, acc.: 97.66%] [G loss: 0.255018]\n",
      "epoch:13 step:10236 [D loss: 0.436539, acc.: 78.91%] [G loss: 0.546087]\n",
      "epoch:13 step:10237 [D loss: 0.171573, acc.: 96.88%] [G loss: 1.137291]\n",
      "epoch:13 step:10238 [D loss: 0.104281, acc.: 100.00%] [G loss: 3.602179]\n",
      "epoch:13 step:10239 [D loss: 0.059255, acc.: 100.00%] [G loss: 0.527517]\n",
      "epoch:13 step:10240 [D loss: 0.114736, acc.: 100.00%] [G loss: 0.424907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10241 [D loss: 0.109544, acc.: 100.00%] [G loss: 3.636561]\n",
      "epoch:13 step:10242 [D loss: 0.063183, acc.: 100.00%] [G loss: 0.198687]\n",
      "epoch:13 step:10243 [D loss: 0.150032, acc.: 98.44%] [G loss: 0.210447]\n",
      "epoch:13 step:10244 [D loss: 0.075858, acc.: 100.00%] [G loss: 0.210657]\n",
      "epoch:13 step:10245 [D loss: 0.045143, acc.: 100.00%] [G loss: 0.221904]\n",
      "epoch:13 step:10246 [D loss: 0.107209, acc.: 97.66%] [G loss: 0.064300]\n",
      "epoch:13 step:10247 [D loss: 0.062374, acc.: 100.00%] [G loss: 0.092635]\n",
      "epoch:13 step:10248 [D loss: 0.044579, acc.: 100.00%] [G loss: 3.452266]\n",
      "epoch:13 step:10249 [D loss: 0.111738, acc.: 100.00%] [G loss: 0.988275]\n",
      "epoch:13 step:10250 [D loss: 0.092702, acc.: 99.22%] [G loss: 3.583357]\n",
      "epoch:13 step:10251 [D loss: 0.066852, acc.: 99.22%] [G loss: 0.107312]\n",
      "epoch:13 step:10252 [D loss: 0.205141, acc.: 96.09%] [G loss: 1.427045]\n",
      "epoch:13 step:10253 [D loss: 0.151560, acc.: 97.66%] [G loss: 0.339506]\n",
      "epoch:13 step:10254 [D loss: 0.196217, acc.: 91.41%] [G loss: 0.119349]\n",
      "epoch:13 step:10255 [D loss: 0.677930, acc.: 63.28%] [G loss: 2.178578]\n",
      "epoch:13 step:10256 [D loss: 1.182291, acc.: 52.34%] [G loss: 1.278378]\n",
      "epoch:13 step:10257 [D loss: 0.594210, acc.: 67.19%] [G loss: 0.055365]\n",
      "epoch:13 step:10258 [D loss: 1.056966, acc.: 57.03%] [G loss: 2.089863]\n",
      "epoch:13 step:10259 [D loss: 0.210291, acc.: 90.62%] [G loss: 3.101668]\n",
      "epoch:13 step:10260 [D loss: 0.826797, acc.: 53.91%] [G loss: 1.544292]\n",
      "epoch:13 step:10261 [D loss: 0.770384, acc.: 57.81%] [G loss: 2.745412]\n",
      "epoch:13 step:10262 [D loss: 0.075269, acc.: 98.44%] [G loss: 2.529216]\n",
      "epoch:13 step:10263 [D loss: 0.110384, acc.: 99.22%] [G loss: 0.676688]\n",
      "epoch:13 step:10264 [D loss: 0.165140, acc.: 96.88%] [G loss: 0.707372]\n",
      "epoch:13 step:10265 [D loss: 0.053938, acc.: 100.00%] [G loss: 0.680937]\n",
      "epoch:13 step:10266 [D loss: 0.312637, acc.: 89.06%] [G loss: 1.305750]\n",
      "epoch:13 step:10267 [D loss: 0.310464, acc.: 85.16%] [G loss: 0.558177]\n",
      "epoch:13 step:10268 [D loss: 0.119349, acc.: 98.44%] [G loss: 2.190012]\n",
      "epoch:13 step:10269 [D loss: 0.125960, acc.: 97.66%] [G loss: 0.155269]\n",
      "epoch:13 step:10270 [D loss: 0.090612, acc.: 99.22%] [G loss: 0.229813]\n",
      "epoch:13 step:10271 [D loss: 0.097037, acc.: 98.44%] [G loss: 0.539109]\n",
      "epoch:13 step:10272 [D loss: 0.185368, acc.: 97.66%] [G loss: 0.500283]\n",
      "epoch:13 step:10273 [D loss: 0.134647, acc.: 98.44%] [G loss: 0.524785]\n",
      "epoch:13 step:10274 [D loss: 0.145158, acc.: 95.31%] [G loss: 0.737202]\n",
      "epoch:13 step:10275 [D loss: 0.295523, acc.: 89.84%] [G loss: 3.694405]\n",
      "epoch:13 step:10276 [D loss: 0.224766, acc.: 90.62%] [G loss: 3.289828]\n",
      "epoch:13 step:10277 [D loss: 0.237127, acc.: 92.19%] [G loss: 0.857756]\n",
      "epoch:13 step:10278 [D loss: 0.257224, acc.: 90.62%] [G loss: 1.339687]\n",
      "epoch:13 step:10279 [D loss: 0.454057, acc.: 78.12%] [G loss: 1.982582]\n",
      "epoch:13 step:10280 [D loss: 0.271798, acc.: 93.75%] [G loss: 2.284126]\n",
      "epoch:13 step:10281 [D loss: 0.176217, acc.: 96.88%] [G loss: 1.741211]\n",
      "epoch:13 step:10282 [D loss: 0.213838, acc.: 89.84%] [G loss: 1.319623]\n",
      "epoch:13 step:10283 [D loss: 0.168475, acc.: 94.53%] [G loss: 2.426315]\n",
      "epoch:13 step:10284 [D loss: 0.370656, acc.: 83.59%] [G loss: 1.334931]\n",
      "epoch:13 step:10285 [D loss: 0.241439, acc.: 91.41%] [G loss: 1.673764]\n",
      "epoch:13 step:10286 [D loss: 0.071834, acc.: 99.22%] [G loss: 1.223892]\n",
      "epoch:13 step:10287 [D loss: 0.246459, acc.: 91.41%] [G loss: 0.584560]\n",
      "epoch:13 step:10288 [D loss: 0.128350, acc.: 98.44%] [G loss: 2.076942]\n",
      "epoch:13 step:10289 [D loss: 0.107104, acc.: 99.22%] [G loss: 1.341946]\n",
      "epoch:13 step:10290 [D loss: 0.128028, acc.: 96.88%] [G loss: 0.759539]\n",
      "epoch:13 step:10291 [D loss: 0.449710, acc.: 78.91%] [G loss: 4.748079]\n",
      "epoch:13 step:10292 [D loss: 0.162858, acc.: 89.06%] [G loss: 1.831460]\n",
      "epoch:13 step:10293 [D loss: 0.164199, acc.: 93.75%] [G loss: 0.546067]\n",
      "epoch:13 step:10294 [D loss: 0.162373, acc.: 96.09%] [G loss: 3.756490]\n",
      "epoch:13 step:10295 [D loss: 0.161707, acc.: 96.88%] [G loss: 2.068476]\n",
      "epoch:13 step:10296 [D loss: 0.116706, acc.: 96.09%] [G loss: 1.461173]\n",
      "epoch:13 step:10297 [D loss: 0.229399, acc.: 89.84%] [G loss: 3.271301]\n",
      "epoch:13 step:10298 [D loss: 0.169789, acc.: 95.31%] [G loss: 3.475181]\n",
      "epoch:13 step:10299 [D loss: 0.282307, acc.: 88.28%] [G loss: 4.719980]\n",
      "epoch:13 step:10300 [D loss: 0.128362, acc.: 96.09%] [G loss: 1.476030]\n",
      "epoch:13 step:10301 [D loss: 0.321941, acc.: 85.16%] [G loss: 2.870006]\n",
      "epoch:13 step:10302 [D loss: 0.130369, acc.: 96.88%] [G loss: 4.229904]\n",
      "epoch:13 step:10303 [D loss: 2.353057, acc.: 17.97%] [G loss: 4.332014]\n",
      "epoch:13 step:10304 [D loss: 0.608530, acc.: 74.22%] [G loss: 3.260419]\n",
      "epoch:13 step:10305 [D loss: 0.288254, acc.: 87.50%] [G loss: 2.513628]\n",
      "epoch:13 step:10306 [D loss: 0.120755, acc.: 96.09%] [G loss: 2.409044]\n",
      "epoch:13 step:10307 [D loss: 0.247898, acc.: 91.41%] [G loss: 1.713115]\n",
      "epoch:13 step:10308 [D loss: 0.160667, acc.: 92.97%] [G loss: 1.185954]\n",
      "epoch:13 step:10309 [D loss: 0.436001, acc.: 78.91%] [G loss: 0.308722]\n",
      "epoch:13 step:10310 [D loss: 0.173324, acc.: 94.53%] [G loss: 0.506539]\n",
      "epoch:13 step:10311 [D loss: 0.180433, acc.: 92.97%] [G loss: 3.171650]\n",
      "epoch:13 step:10312 [D loss: 0.053583, acc.: 100.00%] [G loss: 3.594337]\n",
      "epoch:13 step:10313 [D loss: 0.125058, acc.: 97.66%] [G loss: 0.190217]\n",
      "epoch:13 step:10314 [D loss: 0.223452, acc.: 89.84%] [G loss: 1.542365]\n",
      "epoch:13 step:10315 [D loss: 0.098577, acc.: 96.88%] [G loss: 0.811651]\n",
      "epoch:13 step:10316 [D loss: 0.235807, acc.: 90.62%] [G loss: 0.411628]\n",
      "epoch:13 step:10317 [D loss: 0.106052, acc.: 98.44%] [G loss: 0.383914]\n",
      "epoch:13 step:10318 [D loss: 0.123689, acc.: 96.09%] [G loss: 0.454454]\n",
      "epoch:13 step:10319 [D loss: 0.151325, acc.: 99.22%] [G loss: 0.672705]\n",
      "epoch:13 step:10320 [D loss: 0.088830, acc.: 97.66%] [G loss: 3.716566]\n",
      "epoch:13 step:10321 [D loss: 0.062617, acc.: 97.66%] [G loss: 0.583724]\n",
      "epoch:13 step:10322 [D loss: 0.063100, acc.: 100.00%] [G loss: 0.857657]\n",
      "epoch:13 step:10323 [D loss: 0.048642, acc.: 100.00%] [G loss: 3.959631]\n",
      "epoch:13 step:10324 [D loss: 0.029789, acc.: 100.00%] [G loss: 3.867759]\n",
      "epoch:13 step:10325 [D loss: 0.132700, acc.: 96.09%] [G loss: 3.869566]\n",
      "epoch:13 step:10326 [D loss: 0.093645, acc.: 97.66%] [G loss: 3.973488]\n",
      "epoch:13 step:10327 [D loss: 0.082383, acc.: 99.22%] [G loss: 1.244561]\n",
      "epoch:13 step:10328 [D loss: 0.166534, acc.: 96.09%] [G loss: 3.124994]\n",
      "epoch:13 step:10329 [D loss: 0.286200, acc.: 90.62%] [G loss: 0.713370]\n",
      "epoch:13 step:10330 [D loss: 0.059596, acc.: 98.44%] [G loss: 0.273725]\n",
      "epoch:13 step:10331 [D loss: 0.064317, acc.: 99.22%] [G loss: 2.397050]\n",
      "epoch:13 step:10332 [D loss: 0.164882, acc.: 95.31%] [G loss: 1.480649]\n",
      "epoch:13 step:10333 [D loss: 0.204288, acc.: 89.84%] [G loss: 1.050074]\n",
      "epoch:13 step:10334 [D loss: 0.054792, acc.: 99.22%] [G loss: 0.020680]\n",
      "epoch:13 step:10335 [D loss: 0.046835, acc.: 100.00%] [G loss: 0.051971]\n",
      "epoch:13 step:10336 [D loss: 0.011611, acc.: 100.00%] [G loss: 0.161342]\n",
      "epoch:13 step:10337 [D loss: 0.089779, acc.: 96.88%] [G loss: 0.968736]\n",
      "epoch:13 step:10338 [D loss: 0.748838, acc.: 60.16%] [G loss: 5.891735]\n",
      "epoch:13 step:10339 [D loss: 1.089495, acc.: 54.69%] [G loss: 0.259996]\n",
      "epoch:13 step:10340 [D loss: 0.472844, acc.: 76.56%] [G loss: 5.010810]\n",
      "epoch:13 step:10341 [D loss: 0.057549, acc.: 98.44%] [G loss: 4.378492]\n",
      "epoch:13 step:10342 [D loss: 0.116617, acc.: 96.88%] [G loss: 4.201326]\n",
      "epoch:13 step:10343 [D loss: 0.089613, acc.: 97.66%] [G loss: 1.026888]\n",
      "epoch:13 step:10344 [D loss: 0.094608, acc.: 96.88%] [G loss: 1.105059]\n",
      "epoch:13 step:10345 [D loss: 0.079687, acc.: 98.44%] [G loss: 1.754590]\n",
      "epoch:13 step:10346 [D loss: 0.043898, acc.: 98.44%] [G loss: 0.773743]\n",
      "epoch:13 step:10347 [D loss: 0.336541, acc.: 87.50%] [G loss: 1.352959]\n",
      "epoch:13 step:10348 [D loss: 0.109623, acc.: 97.66%] [G loss: 3.688784]\n",
      "epoch:13 step:10349 [D loss: 0.407322, acc.: 83.59%] [G loss: 1.831567]\n",
      "epoch:13 step:10350 [D loss: 0.064332, acc.: 100.00%] [G loss: 3.759474]\n",
      "epoch:13 step:10351 [D loss: 0.068363, acc.: 99.22%] [G loss: 0.993368]\n",
      "epoch:13 step:10352 [D loss: 0.037330, acc.: 100.00%] [G loss: 0.776296]\n",
      "epoch:13 step:10353 [D loss: 0.060355, acc.: 100.00%] [G loss: 0.435243]\n",
      "epoch:13 step:10354 [D loss: 0.054716, acc.: 99.22%] [G loss: 2.531927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10355 [D loss: 0.117468, acc.: 97.66%] [G loss: 2.298551]\n",
      "epoch:13 step:10356 [D loss: 0.146113, acc.: 95.31%] [G loss: 2.246429]\n",
      "epoch:13 step:10357 [D loss: 0.200356, acc.: 94.53%] [G loss: 1.749047]\n",
      "epoch:13 step:10358 [D loss: 0.080598, acc.: 97.66%] [G loss: 1.642622]\n",
      "epoch:13 step:10359 [D loss: 0.297262, acc.: 88.28%] [G loss: 0.749775]\n",
      "epoch:13 step:10360 [D loss: 0.119603, acc.: 97.66%] [G loss: 2.522727]\n",
      "epoch:13 step:10361 [D loss: 0.044074, acc.: 100.00%] [G loss: 2.811344]\n",
      "epoch:13 step:10362 [D loss: 0.104358, acc.: 98.44%] [G loss: 2.253528]\n",
      "epoch:13 step:10363 [D loss: 0.452583, acc.: 75.00%] [G loss: 6.449667]\n",
      "epoch:13 step:10364 [D loss: 0.224027, acc.: 93.75%] [G loss: 2.071755]\n",
      "epoch:13 step:10365 [D loss: 0.087970, acc.: 96.88%] [G loss: 0.429138]\n",
      "epoch:13 step:10366 [D loss: 0.145558, acc.: 93.75%] [G loss: 4.038794]\n",
      "epoch:13 step:10367 [D loss: 0.056187, acc.: 98.44%] [G loss: 0.073439]\n",
      "epoch:13 step:10368 [D loss: 0.035632, acc.: 100.00%] [G loss: 0.200383]\n",
      "epoch:13 step:10369 [D loss: 0.102309, acc.: 97.66%] [G loss: 0.021975]\n",
      "epoch:13 step:10370 [D loss: 0.086268, acc.: 97.66%] [G loss: 0.059481]\n",
      "epoch:13 step:10371 [D loss: 0.026009, acc.: 100.00%] [G loss: 0.772808]\n",
      "epoch:13 step:10372 [D loss: 0.050945, acc.: 99.22%] [G loss: 0.020761]\n",
      "epoch:13 step:10373 [D loss: 0.060738, acc.: 99.22%] [G loss: 0.038932]\n",
      "epoch:13 step:10374 [D loss: 0.031141, acc.: 100.00%] [G loss: 5.820435]\n",
      "epoch:13 step:10375 [D loss: 0.130787, acc.: 95.31%] [G loss: 3.908298]\n",
      "epoch:13 step:10376 [D loss: 0.039239, acc.: 100.00%] [G loss: 0.025669]\n",
      "epoch:13 step:10377 [D loss: 0.036767, acc.: 100.00%] [G loss: 0.071124]\n",
      "epoch:13 step:10378 [D loss: 0.032317, acc.: 100.00%] [G loss: 0.017789]\n",
      "epoch:13 step:10379 [D loss: 0.028738, acc.: 100.00%] [G loss: 0.023323]\n",
      "epoch:13 step:10380 [D loss: 0.042974, acc.: 99.22%] [G loss: 0.019018]\n",
      "epoch:13 step:10381 [D loss: 0.015599, acc.: 100.00%] [G loss: 0.042669]\n",
      "epoch:13 step:10382 [D loss: 0.071400, acc.: 99.22%] [G loss: 0.021614]\n",
      "epoch:13 step:10383 [D loss: 0.049330, acc.: 100.00%] [G loss: 0.004208]\n",
      "epoch:13 step:10384 [D loss: 0.010460, acc.: 100.00%] [G loss: 0.370248]\n",
      "epoch:13 step:10385 [D loss: 0.068522, acc.: 98.44%] [G loss: 0.163648]\n",
      "epoch:13 step:10386 [D loss: 0.065617, acc.: 98.44%] [G loss: 0.178754]\n",
      "epoch:13 step:10387 [D loss: 0.060689, acc.: 98.44%] [G loss: 0.079315]\n",
      "epoch:13 step:10388 [D loss: 0.079801, acc.: 98.44%] [G loss: 0.525600]\n",
      "epoch:13 step:10389 [D loss: 0.087118, acc.: 99.22%] [G loss: 6.354655]\n",
      "epoch:13 step:10390 [D loss: 0.017925, acc.: 100.00%] [G loss: 0.616751]\n",
      "epoch:13 step:10391 [D loss: 0.224180, acc.: 92.19%] [G loss: 4.342211]\n",
      "epoch:13 step:10392 [D loss: 0.231488, acc.: 91.41%] [G loss: 3.022768]\n",
      "epoch:13 step:10393 [D loss: 0.143893, acc.: 96.09%] [G loss: 1.896045]\n",
      "epoch:13 step:10394 [D loss: 0.072908, acc.: 96.88%] [G loss: 1.448788]\n",
      "epoch:13 step:10395 [D loss: 0.030372, acc.: 100.00%] [G loss: 2.314382]\n",
      "epoch:13 step:10396 [D loss: 0.052470, acc.: 99.22%] [G loss: 0.408778]\n",
      "epoch:13 step:10397 [D loss: 0.078005, acc.: 96.09%] [G loss: 0.231574]\n",
      "epoch:13 step:10398 [D loss: 0.102330, acc.: 96.88%] [G loss: 3.856591]\n",
      "epoch:13 step:10399 [D loss: 0.176760, acc.: 93.75%] [G loss: 0.460907]\n",
      "epoch:13 step:10400 [D loss: 0.121454, acc.: 96.88%] [G loss: 2.340235]\n",
      "epoch:13 step:10401 [D loss: 0.018811, acc.: 99.22%] [G loss: 1.521939]\n",
      "epoch:13 step:10402 [D loss: 0.024916, acc.: 100.00%] [G loss: 0.363686]\n",
      "epoch:13 step:10403 [D loss: 0.169047, acc.: 92.97%] [G loss: 2.721604]\n",
      "epoch:13 step:10404 [D loss: 0.093421, acc.: 96.88%] [G loss: 0.428485]\n",
      "epoch:13 step:10405 [D loss: 0.041287, acc.: 98.44%] [G loss: 0.115320]\n",
      "epoch:13 step:10406 [D loss: 0.024415, acc.: 100.00%] [G loss: 0.098595]\n",
      "epoch:13 step:10407 [D loss: 0.030771, acc.: 99.22%] [G loss: 3.743279]\n",
      "epoch:13 step:10408 [D loss: 0.053259, acc.: 100.00%] [G loss: 0.787372]\n",
      "epoch:13 step:10409 [D loss: 0.026675, acc.: 99.22%] [G loss: 0.157368]\n",
      "epoch:13 step:10410 [D loss: 0.075033, acc.: 98.44%] [G loss: 0.077124]\n",
      "epoch:13 step:10411 [D loss: 0.210451, acc.: 92.97%] [G loss: 0.965901]\n",
      "epoch:13 step:10412 [D loss: 0.043825, acc.: 97.66%] [G loss: 0.919261]\n",
      "epoch:13 step:10413 [D loss: 0.109259, acc.: 95.31%] [G loss: 2.650079]\n",
      "epoch:13 step:10414 [D loss: 0.048837, acc.: 99.22%] [G loss: 0.142972]\n",
      "epoch:13 step:10415 [D loss: 0.029714, acc.: 99.22%] [G loss: 0.545126]\n",
      "epoch:13 step:10416 [D loss: 0.004695, acc.: 100.00%] [G loss: 4.933544]\n",
      "epoch:13 step:10417 [D loss: 0.479993, acc.: 77.34%] [G loss: 2.292382]\n",
      "epoch:13 step:10418 [D loss: 0.300471, acc.: 84.38%] [G loss: 0.102182]\n",
      "epoch:13 step:10419 [D loss: 0.296107, acc.: 90.62%] [G loss: 4.838884]\n",
      "epoch:13 step:10420 [D loss: 2.250770, acc.: 29.69%] [G loss: 4.471687]\n",
      "epoch:13 step:10421 [D loss: 0.081443, acc.: 96.88%] [G loss: 6.735237]\n",
      "epoch:13 step:10422 [D loss: 0.092822, acc.: 96.09%] [G loss: 2.276368]\n",
      "epoch:13 step:10423 [D loss: 0.204202, acc.: 92.19%] [G loss: 3.148350]\n",
      "epoch:13 step:10424 [D loss: 0.158925, acc.: 92.97%] [G loss: 3.383626]\n",
      "epoch:13 step:10425 [D loss: 0.067161, acc.: 98.44%] [G loss: 2.200378]\n",
      "epoch:13 step:10426 [D loss: 0.042883, acc.: 99.22%] [G loss: 4.253396]\n",
      "epoch:13 step:10427 [D loss: 0.082160, acc.: 97.66%] [G loss: 2.525280]\n",
      "epoch:13 step:10428 [D loss: 0.085164, acc.: 97.66%] [G loss: 1.193698]\n",
      "epoch:13 step:10429 [D loss: 0.298557, acc.: 85.16%] [G loss: 5.161681]\n",
      "epoch:13 step:10430 [D loss: 1.004142, acc.: 55.47%] [G loss: 1.975707]\n",
      "epoch:13 step:10431 [D loss: 0.015263, acc.: 100.00%] [G loss: 2.504517]\n",
      "epoch:13 step:10432 [D loss: 0.030731, acc.: 100.00%] [G loss: 4.037076]\n",
      "epoch:13 step:10433 [D loss: 0.101674, acc.: 96.88%] [G loss: 3.332903]\n",
      "epoch:13 step:10434 [D loss: 0.106070, acc.: 93.75%] [G loss: 4.335048]\n",
      "epoch:13 step:10435 [D loss: 0.050375, acc.: 98.44%] [G loss: 5.008436]\n",
      "epoch:13 step:10436 [D loss: 0.066460, acc.: 99.22%] [G loss: 4.354521]\n",
      "epoch:13 step:10437 [D loss: 0.047502, acc.: 99.22%] [G loss: 3.325479]\n",
      "epoch:13 step:10438 [D loss: 0.063109, acc.: 97.66%] [G loss: 0.658474]\n",
      "epoch:13 step:10439 [D loss: 0.143876, acc.: 94.53%] [G loss: 3.400441]\n",
      "epoch:13 step:10440 [D loss: 0.052906, acc.: 99.22%] [G loss: 3.254884]\n",
      "epoch:13 step:10441 [D loss: 0.086879, acc.: 97.66%] [G loss: 3.678793]\n",
      "epoch:13 step:10442 [D loss: 0.115606, acc.: 97.66%] [G loss: 3.380971]\n",
      "epoch:13 step:10443 [D loss: 0.116061, acc.: 96.88%] [G loss: 3.590795]\n",
      "epoch:13 step:10444 [D loss: 0.086458, acc.: 97.66%] [G loss: 3.846888]\n",
      "epoch:13 step:10445 [D loss: 0.128821, acc.: 95.31%] [G loss: 3.445855]\n",
      "epoch:13 step:10446 [D loss: 0.224530, acc.: 92.19%] [G loss: 0.933470]\n",
      "epoch:13 step:10447 [D loss: 0.117559, acc.: 96.88%] [G loss: 5.560650]\n",
      "epoch:13 step:10448 [D loss: 0.270736, acc.: 86.72%] [G loss: 3.548701]\n",
      "epoch:13 step:10449 [D loss: 0.163620, acc.: 92.19%] [G loss: 5.775791]\n",
      "epoch:13 step:10450 [D loss: 0.134734, acc.: 95.31%] [G loss: 4.319521]\n",
      "epoch:13 step:10451 [D loss: 0.036223, acc.: 100.00%] [G loss: 5.509220]\n",
      "epoch:13 step:10452 [D loss: 0.560109, acc.: 72.66%] [G loss: 7.331322]\n",
      "epoch:13 step:10453 [D loss: 0.345401, acc.: 84.38%] [G loss: 5.757994]\n",
      "epoch:13 step:10454 [D loss: 0.003990, acc.: 100.00%] [G loss: 4.314212]\n",
      "epoch:13 step:10455 [D loss: 0.094545, acc.: 96.88%] [G loss: 0.327874]\n",
      "epoch:13 step:10456 [D loss: 0.058637, acc.: 99.22%] [G loss: 5.133702]\n",
      "epoch:13 step:10457 [D loss: 0.029689, acc.: 100.00%] [G loss: 5.011250]\n",
      "epoch:13 step:10458 [D loss: 0.099675, acc.: 96.88%] [G loss: 2.843392]\n",
      "epoch:13 step:10459 [D loss: 0.033739, acc.: 99.22%] [G loss: 2.722915]\n",
      "epoch:13 step:10460 [D loss: 0.038661, acc.: 100.00%] [G loss: 0.925864]\n",
      "epoch:13 step:10461 [D loss: 0.098419, acc.: 98.44%] [G loss: 4.307179]\n",
      "epoch:13 step:10462 [D loss: 0.517346, acc.: 78.91%] [G loss: 3.986843]\n",
      "epoch:13 step:10463 [D loss: 0.014337, acc.: 100.00%] [G loss: 2.735242]\n",
      "epoch:13 step:10464 [D loss: 0.045748, acc.: 99.22%] [G loss: 2.422999]\n",
      "epoch:13 step:10465 [D loss: 0.069309, acc.: 98.44%] [G loss: 2.426701]\n",
      "epoch:13 step:10466 [D loss: 0.074035, acc.: 98.44%] [G loss: 1.545918]\n",
      "epoch:13 step:10467 [D loss: 0.251124, acc.: 88.28%] [G loss: 5.147760]\n",
      "epoch:13 step:10468 [D loss: 0.137468, acc.: 93.75%] [G loss: 3.187420]\n",
      "epoch:13 step:10469 [D loss: 0.042922, acc.: 99.22%] [G loss: 1.753820]\n",
      "epoch:13 step:10470 [D loss: 0.088433, acc.: 98.44%] [G loss: 0.242576]\n",
      "epoch:13 step:10471 [D loss: 0.010202, acc.: 100.00%] [G loss: 4.140563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10472 [D loss: 0.096535, acc.: 98.44%] [G loss: 1.415066]\n",
      "epoch:13 step:10473 [D loss: 0.075654, acc.: 98.44%] [G loss: 1.326711]\n",
      "epoch:13 step:10474 [D loss: 0.037253, acc.: 99.22%] [G loss: 1.271307]\n",
      "epoch:13 step:10475 [D loss: 0.101597, acc.: 97.66%] [G loss: 3.016588]\n",
      "epoch:13 step:10476 [D loss: 0.317335, acc.: 82.81%] [G loss: 4.030879]\n",
      "epoch:13 step:10477 [D loss: 0.134186, acc.: 97.66%] [G loss: 3.363655]\n",
      "epoch:13 step:10478 [D loss: 0.323566, acc.: 85.16%] [G loss: 5.402297]\n",
      "epoch:13 step:10479 [D loss: 0.051318, acc.: 99.22%] [G loss: 5.039246]\n",
      "epoch:13 step:10480 [D loss: 0.165585, acc.: 94.53%] [G loss: 5.192142]\n",
      "epoch:13 step:10481 [D loss: 0.027091, acc.: 100.00%] [G loss: 3.308227]\n",
      "epoch:13 step:10482 [D loss: 0.012357, acc.: 100.00%] [G loss: 6.051672]\n",
      "epoch:13 step:10483 [D loss: 0.185391, acc.: 95.31%] [G loss: 5.632250]\n",
      "epoch:13 step:10484 [D loss: 0.079574, acc.: 98.44%] [G loss: 1.927233]\n",
      "epoch:13 step:10485 [D loss: 0.005896, acc.: 100.00%] [G loss: 3.593611]\n",
      "epoch:13 step:10486 [D loss: 0.057281, acc.: 98.44%] [G loss: 1.330225]\n",
      "epoch:13 step:10487 [D loss: 0.028448, acc.: 99.22%] [G loss: 0.602528]\n",
      "epoch:13 step:10488 [D loss: 0.027475, acc.: 100.00%] [G loss: 0.686311]\n",
      "epoch:13 step:10489 [D loss: 0.008599, acc.: 100.00%] [G loss: 0.774064]\n",
      "epoch:13 step:10490 [D loss: 0.028772, acc.: 99.22%] [G loss: 0.187473]\n",
      "epoch:13 step:10491 [D loss: 0.062605, acc.: 100.00%] [G loss: 0.007954]\n",
      "epoch:13 step:10492 [D loss: 0.029023, acc.: 100.00%] [G loss: 0.006671]\n",
      "epoch:13 step:10493 [D loss: 0.064312, acc.: 99.22%] [G loss: 0.461154]\n",
      "epoch:13 step:10494 [D loss: 0.003790, acc.: 100.00%] [G loss: 4.479947]\n",
      "epoch:13 step:10495 [D loss: 0.070823, acc.: 97.66%] [G loss: 4.062819]\n",
      "epoch:13 step:10496 [D loss: 1.024633, acc.: 68.75%] [G loss: 11.029921]\n",
      "epoch:13 step:10497 [D loss: 3.039068, acc.: 50.00%] [G loss: 2.084947]\n",
      "epoch:13 step:10498 [D loss: 0.024685, acc.: 100.00%] [G loss: 0.767506]\n",
      "epoch:13 step:10499 [D loss: 0.018192, acc.: 100.00%] [G loss: 1.471884]\n",
      "epoch:13 step:10500 [D loss: 0.025066, acc.: 100.00%] [G loss: 0.452149]\n",
      "epoch:13 step:10501 [D loss: 0.009891, acc.: 100.00%] [G loss: 0.337379]\n",
      "epoch:13 step:10502 [D loss: 0.009471, acc.: 100.00%] [G loss: 0.611335]\n",
      "epoch:13 step:10503 [D loss: 0.046019, acc.: 100.00%] [G loss: 0.495933]\n",
      "epoch:13 step:10504 [D loss: 0.019942, acc.: 100.00%] [G loss: 0.246394]\n",
      "epoch:13 step:10505 [D loss: 0.035582, acc.: 100.00%] [G loss: 5.253668]\n",
      "epoch:13 step:10506 [D loss: 0.021231, acc.: 100.00%] [G loss: 0.433640]\n",
      "epoch:13 step:10507 [D loss: 0.014010, acc.: 100.00%] [G loss: 0.482685]\n",
      "epoch:13 step:10508 [D loss: 0.028820, acc.: 100.00%] [G loss: 0.770537]\n",
      "epoch:13 step:10509 [D loss: 0.043102, acc.: 99.22%] [G loss: 4.017837]\n",
      "epoch:13 step:10510 [D loss: 0.017824, acc.: 100.00%] [G loss: 4.189168]\n",
      "epoch:13 step:10511 [D loss: 0.104422, acc.: 96.88%] [G loss: 0.460721]\n",
      "epoch:13 step:10512 [D loss: 0.018855, acc.: 100.00%] [G loss: 0.634168]\n",
      "epoch:13 step:10513 [D loss: 0.013377, acc.: 100.00%] [G loss: 0.384456]\n",
      "epoch:13 step:10514 [D loss: 0.027844, acc.: 100.00%] [G loss: 0.133519]\n",
      "epoch:13 step:10515 [D loss: 0.056624, acc.: 99.22%] [G loss: 1.081955]\n",
      "epoch:13 step:10516 [D loss: 0.015335, acc.: 100.00%] [G loss: 5.600354]\n",
      "epoch:13 step:10517 [D loss: 0.123844, acc.: 96.88%] [G loss: 0.211872]\n",
      "epoch:13 step:10518 [D loss: 0.269610, acc.: 89.06%] [G loss: 5.172507]\n",
      "epoch:13 step:10519 [D loss: 0.325596, acc.: 84.38%] [G loss: 4.108374]\n",
      "epoch:13 step:10520 [D loss: 0.030450, acc.: 99.22%] [G loss: 1.472525]\n",
      "epoch:13 step:10521 [D loss: 0.036205, acc.: 99.22%] [G loss: 2.317836]\n",
      "epoch:13 step:10522 [D loss: 0.020698, acc.: 99.22%] [G loss: 2.921221]\n",
      "epoch:13 step:10523 [D loss: 0.033863, acc.: 99.22%] [G loss: 4.279301]\n",
      "epoch:13 step:10524 [D loss: 0.091122, acc.: 96.88%] [G loss: 3.578001]\n",
      "epoch:13 step:10525 [D loss: 0.015192, acc.: 100.00%] [G loss: 5.239152]\n",
      "epoch:13 step:10526 [D loss: 0.087448, acc.: 96.88%] [G loss: 1.589289]\n",
      "epoch:13 step:10527 [D loss: 0.128025, acc.: 95.31%] [G loss: 4.275494]\n",
      "epoch:13 step:10528 [D loss: 0.112839, acc.: 95.31%] [G loss: 3.154236]\n",
      "epoch:13 step:10529 [D loss: 0.016774, acc.: 100.00%] [G loss: 1.973021]\n",
      "epoch:13 step:10530 [D loss: 0.007434, acc.: 100.00%] [G loss: 2.669258]\n",
      "epoch:13 step:10531 [D loss: 0.009136, acc.: 100.00%] [G loss: 1.529543]\n",
      "epoch:13 step:10532 [D loss: 0.097344, acc.: 96.88%] [G loss: 2.960333]\n",
      "epoch:13 step:10533 [D loss: 0.036837, acc.: 99.22%] [G loss: 3.472507]\n",
      "epoch:13 step:10534 [D loss: 0.023668, acc.: 100.00%] [G loss: 0.714369]\n",
      "epoch:13 step:10535 [D loss: 0.044084, acc.: 99.22%] [G loss: 1.203255]\n",
      "epoch:13 step:10536 [D loss: 0.019836, acc.: 100.00%] [G loss: 0.501275]\n",
      "epoch:13 step:10537 [D loss: 0.008065, acc.: 100.00%] [G loss: 1.032720]\n",
      "epoch:13 step:10538 [D loss: 0.141417, acc.: 96.09%] [G loss: 3.053025]\n",
      "epoch:13 step:10539 [D loss: 0.108310, acc.: 96.09%] [G loss: 1.082260]\n",
      "epoch:13 step:10540 [D loss: 0.093545, acc.: 96.09%] [G loss: 4.575966]\n",
      "epoch:13 step:10541 [D loss: 0.010875, acc.: 100.00%] [G loss: 4.657059]\n",
      "epoch:13 step:10542 [D loss: 0.035356, acc.: 99.22%] [G loss: 3.459485]\n",
      "epoch:13 step:10543 [D loss: 0.263975, acc.: 89.84%] [G loss: 0.882809]\n",
      "epoch:13 step:10544 [D loss: 0.026691, acc.: 100.00%] [G loss: 1.357492]\n",
      "epoch:13 step:10545 [D loss: 0.010968, acc.: 100.00%] [G loss: 2.584486]\n",
      "epoch:13 step:10546 [D loss: 0.036830, acc.: 99.22%] [G loss: 1.402111]\n",
      "epoch:13 step:10547 [D loss: 0.034265, acc.: 100.00%] [G loss: 4.515886]\n",
      "epoch:13 step:10548 [D loss: 0.033712, acc.: 100.00%] [G loss: 2.235260]\n",
      "epoch:13 step:10549 [D loss: 0.097321, acc.: 97.66%] [G loss: 6.370735]\n",
      "epoch:13 step:10550 [D loss: 0.062694, acc.: 97.66%] [G loss: 5.892529]\n",
      "epoch:13 step:10551 [D loss: 0.011657, acc.: 100.00%] [G loss: 3.793571]\n",
      "epoch:13 step:10552 [D loss: 0.014914, acc.: 100.00%] [G loss: 4.875053]\n",
      "epoch:13 step:10553 [D loss: 0.011809, acc.: 100.00%] [G loss: 4.270961]\n",
      "epoch:13 step:10554 [D loss: 0.079014, acc.: 97.66%] [G loss: 3.577756]\n",
      "epoch:13 step:10555 [D loss: 0.010981, acc.: 100.00%] [G loss: 0.529239]\n",
      "epoch:13 step:10556 [D loss: 0.157033, acc.: 96.88%] [G loss: 6.485218]\n",
      "epoch:13 step:10557 [D loss: 0.015502, acc.: 100.00%] [G loss: 7.301791]\n",
      "epoch:13 step:10558 [D loss: 0.275402, acc.: 87.50%] [G loss: 2.269638]\n",
      "epoch:13 step:10559 [D loss: 0.267734, acc.: 88.28%] [G loss: 5.204192]\n",
      "epoch:13 step:10560 [D loss: 0.003212, acc.: 100.00%] [G loss: 7.415142]\n",
      "epoch:13 step:10561 [D loss: 0.231805, acc.: 89.84%] [G loss: 2.518144]\n",
      "epoch:13 step:10562 [D loss: 0.106698, acc.: 96.88%] [G loss: 2.454023]\n",
      "epoch:13 step:10563 [D loss: 0.033014, acc.: 100.00%] [G loss: 4.638291]\n",
      "epoch:13 step:10564 [D loss: 0.311410, acc.: 86.72%] [G loss: 3.229185]\n",
      "epoch:13 step:10565 [D loss: 0.015437, acc.: 100.00%] [G loss: 3.047354]\n",
      "epoch:13 step:10566 [D loss: 0.028495, acc.: 100.00%] [G loss: 1.931637]\n",
      "epoch:13 step:10567 [D loss: 0.018862, acc.: 100.00%] [G loss: 1.262730]\n",
      "epoch:13 step:10568 [D loss: 0.010380, acc.: 100.00%] [G loss: 2.809378]\n",
      "epoch:13 step:10569 [D loss: 0.081671, acc.: 97.66%] [G loss: 2.254578]\n",
      "epoch:13 step:10570 [D loss: 0.052404, acc.: 99.22%] [G loss: 0.829739]\n",
      "epoch:13 step:10571 [D loss: 0.023030, acc.: 100.00%] [G loss: 0.910531]\n",
      "epoch:13 step:10572 [D loss: 0.138165, acc.: 95.31%] [G loss: 2.568163]\n",
      "epoch:13 step:10573 [D loss: 0.350615, acc.: 84.38%] [G loss: 0.137774]\n",
      "epoch:13 step:10574 [D loss: 0.241954, acc.: 91.41%] [G loss: 5.899657]\n",
      "epoch:13 step:10575 [D loss: 0.060995, acc.: 96.09%] [G loss: 4.906408]\n",
      "epoch:13 step:10576 [D loss: 0.086402, acc.: 96.88%] [G loss: 3.180888]\n",
      "epoch:13 step:10577 [D loss: 0.088294, acc.: 96.09%] [G loss: 2.215168]\n",
      "epoch:13 step:10578 [D loss: 0.004777, acc.: 100.00%] [G loss: 6.128953]\n",
      "epoch:13 step:10579 [D loss: 0.015509, acc.: 100.00%] [G loss: 0.784048]\n",
      "epoch:13 step:10580 [D loss: 0.001850, acc.: 100.00%] [G loss: 1.201013]\n",
      "epoch:13 step:10581 [D loss: 0.006698, acc.: 100.00%] [G loss: 2.952950]\n",
      "epoch:13 step:10582 [D loss: 0.016164, acc.: 100.00%] [G loss: 0.896004]\n",
      "epoch:13 step:10583 [D loss: 0.022661, acc.: 100.00%] [G loss: 2.969322]\n",
      "epoch:13 step:10584 [D loss: 0.134388, acc.: 98.44%] [G loss: 0.322482]\n",
      "epoch:13 step:10585 [D loss: 0.027828, acc.: 99.22%] [G loss: 0.243427]\n",
      "epoch:13 step:10586 [D loss: 0.009347, acc.: 100.00%] [G loss: 0.118045]\n",
      "epoch:13 step:10587 [D loss: 0.019693, acc.: 100.00%] [G loss: 0.067111]\n",
      "epoch:13 step:10588 [D loss: 0.004700, acc.: 100.00%] [G loss: 0.079688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10589 [D loss: 0.005917, acc.: 100.00%] [G loss: 0.041194]\n",
      "epoch:13 step:10590 [D loss: 0.018835, acc.: 100.00%] [G loss: 4.009027]\n",
      "epoch:13 step:10591 [D loss: 0.005868, acc.: 100.00%] [G loss: 2.454811]\n",
      "epoch:13 step:10592 [D loss: 0.030009, acc.: 100.00%] [G loss: 0.291689]\n",
      "epoch:13 step:10593 [D loss: 0.016721, acc.: 99.22%] [G loss: 5.416608]\n",
      "epoch:13 step:10594 [D loss: 0.162672, acc.: 96.88%] [G loss: 0.045515]\n",
      "epoch:13 step:10595 [D loss: 0.004181, acc.: 100.00%] [G loss: 0.282119]\n",
      "epoch:13 step:10596 [D loss: 0.009022, acc.: 100.00%] [G loss: 5.030284]\n",
      "epoch:13 step:10597 [D loss: 0.215421, acc.: 88.28%] [G loss: 5.490901]\n",
      "epoch:13 step:10598 [D loss: 0.260295, acc.: 87.50%] [G loss: 1.909095]\n",
      "epoch:13 step:10599 [D loss: 0.216210, acc.: 90.62%] [G loss: 6.499609]\n",
      "epoch:13 step:10600 [D loss: 0.027847, acc.: 99.22%] [G loss: 1.440048]\n",
      "epoch:13 step:10601 [D loss: 0.080569, acc.: 96.09%] [G loss: 7.829119]\n",
      "epoch:13 step:10602 [D loss: 0.017745, acc.: 99.22%] [G loss: 0.024565]\n",
      "epoch:13 step:10603 [D loss: 0.003642, acc.: 100.00%] [G loss: 5.237489]\n",
      "epoch:13 step:10604 [D loss: 0.020767, acc.: 100.00%] [G loss: 0.324774]\n",
      "epoch:13 step:10605 [D loss: 0.007604, acc.: 100.00%] [G loss: 0.130576]\n",
      "epoch:13 step:10606 [D loss: 0.011901, acc.: 100.00%] [G loss: 3.697349]\n",
      "epoch:13 step:10607 [D loss: 0.001561, acc.: 100.00%] [G loss: 0.005304]\n",
      "epoch:13 step:10608 [D loss: 0.002331, acc.: 100.00%] [G loss: 5.028623]\n",
      "epoch:13 step:10609 [D loss: 0.007374, acc.: 100.00%] [G loss: 0.002931]\n",
      "epoch:13 step:10610 [D loss: 0.013940, acc.: 100.00%] [G loss: 0.007834]\n",
      "epoch:13 step:10611 [D loss: 0.019588, acc.: 100.00%] [G loss: 6.190709]\n",
      "epoch:13 step:10612 [D loss: 0.052648, acc.: 100.00%] [G loss: 0.002468]\n",
      "epoch:13 step:10613 [D loss: 0.148495, acc.: 93.75%] [G loss: 0.110973]\n",
      "epoch:13 step:10614 [D loss: 0.037602, acc.: 98.44%] [G loss: 0.386516]\n",
      "epoch:13 step:10615 [D loss: 0.022850, acc.: 100.00%] [G loss: 0.010477]\n",
      "epoch:13 step:10616 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.157533]\n",
      "epoch:13 step:10617 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.008614]\n",
      "epoch:13 step:10618 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.012886]\n",
      "epoch:13 step:10619 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.013198]\n",
      "epoch:13 step:10620 [D loss: 0.012226, acc.: 100.00%] [G loss: 7.738849]\n",
      "epoch:13 step:10621 [D loss: 0.002209, acc.: 100.00%] [G loss: 0.067253]\n",
      "epoch:13 step:10622 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.005322]\n",
      "epoch:13 step:10623 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.000893]\n",
      "epoch:13 step:10624 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.003250]\n",
      "epoch:13 step:10625 [D loss: 0.002369, acc.: 100.00%] [G loss: 6.121409]\n",
      "epoch:13 step:10626 [D loss: 0.002786, acc.: 100.00%] [G loss: 0.011663]\n",
      "epoch:13 step:10627 [D loss: 0.033104, acc.: 99.22%] [G loss: 6.470640]\n",
      "epoch:13 step:10628 [D loss: 0.006956, acc.: 100.00%] [G loss: 0.026963]\n",
      "epoch:13 step:10629 [D loss: 0.002323, acc.: 100.00%] [G loss: 0.007653]\n",
      "epoch:13 step:10630 [D loss: 0.004488, acc.: 100.00%] [G loss: 0.006921]\n",
      "epoch:13 step:10631 [D loss: 0.010926, acc.: 100.00%] [G loss: 0.005499]\n",
      "epoch:13 step:10632 [D loss: 0.007193, acc.: 100.00%] [G loss: 0.033609]\n",
      "epoch:13 step:10633 [D loss: 0.002050, acc.: 100.00%] [G loss: 6.242487]\n",
      "epoch:13 step:10634 [D loss: 0.001347, acc.: 100.00%] [G loss: 4.878238]\n",
      "epoch:13 step:10635 [D loss: 0.012802, acc.: 100.00%] [G loss: 5.106300]\n",
      "epoch:13 step:10636 [D loss: 0.155796, acc.: 92.19%] [G loss: 6.684441]\n",
      "epoch:13 step:10637 [D loss: 1.778405, acc.: 38.28%] [G loss: 4.949224]\n",
      "epoch:13 step:10638 [D loss: 0.293464, acc.: 89.06%] [G loss: 0.674541]\n",
      "epoch:13 step:10639 [D loss: 0.141302, acc.: 94.53%] [G loss: 2.250771]\n",
      "epoch:13 step:10640 [D loss: 0.008467, acc.: 100.00%] [G loss: 2.985400]\n",
      "epoch:13 step:10641 [D loss: 0.055608, acc.: 98.44%] [G loss: 5.143819]\n",
      "epoch:13 step:10642 [D loss: 0.025310, acc.: 100.00%] [G loss: 0.320997]\n",
      "epoch:13 step:10643 [D loss: 0.062934, acc.: 98.44%] [G loss: 1.655857]\n",
      "epoch:13 step:10644 [D loss: 0.089656, acc.: 96.88%] [G loss: 5.040309]\n",
      "epoch:13 step:10645 [D loss: 0.108885, acc.: 96.09%] [G loss: 1.519866]\n",
      "epoch:13 step:10646 [D loss: 0.018397, acc.: 100.00%] [G loss: 1.498611]\n",
      "epoch:13 step:10647 [D loss: 0.058164, acc.: 98.44%] [G loss: 2.343297]\n",
      "epoch:13 step:10648 [D loss: 0.043624, acc.: 100.00%] [G loss: 2.536587]\n",
      "epoch:13 step:10649 [D loss: 0.015556, acc.: 100.00%] [G loss: 3.590410]\n",
      "epoch:13 step:10650 [D loss: 0.033349, acc.: 99.22%] [G loss: 5.845847]\n",
      "epoch:13 step:10651 [D loss: 0.045673, acc.: 99.22%] [G loss: 3.120128]\n",
      "epoch:13 step:10652 [D loss: 0.060020, acc.: 100.00%] [G loss: 4.663051]\n",
      "epoch:13 step:10653 [D loss: 0.042332, acc.: 99.22%] [G loss: 4.338408]\n",
      "epoch:13 step:10654 [D loss: 1.607801, acc.: 43.75%] [G loss: 11.926604]\n",
      "epoch:13 step:10655 [D loss: 3.363220, acc.: 50.00%] [G loss: 7.219393]\n",
      "epoch:13 step:10656 [D loss: 0.477297, acc.: 77.34%] [G loss: 2.914492]\n",
      "epoch:13 step:10657 [D loss: 0.301398, acc.: 85.16%] [G loss: 4.267092]\n",
      "epoch:13 step:10658 [D loss: 0.023012, acc.: 100.00%] [G loss: 5.011271]\n",
      "epoch:13 step:10659 [D loss: 0.007639, acc.: 100.00%] [G loss: 1.115987]\n",
      "epoch:13 step:10660 [D loss: 0.035559, acc.: 100.00%] [G loss: 5.514769]\n",
      "epoch:13 step:10661 [D loss: 0.058877, acc.: 99.22%] [G loss: 5.020590]\n",
      "epoch:13 step:10662 [D loss: 0.054232, acc.: 99.22%] [G loss: 5.110994]\n",
      "epoch:13 step:10663 [D loss: 0.021048, acc.: 99.22%] [G loss: 5.197652]\n",
      "epoch:13 step:10664 [D loss: 0.058725, acc.: 99.22%] [G loss: 4.993404]\n",
      "epoch:13 step:10665 [D loss: 0.044727, acc.: 98.44%] [G loss: 4.414048]\n",
      "epoch:13 step:10666 [D loss: 0.046860, acc.: 100.00%] [G loss: 4.400361]\n",
      "epoch:13 step:10667 [D loss: 0.071345, acc.: 99.22%] [G loss: 5.006177]\n",
      "epoch:13 step:10668 [D loss: 0.009808, acc.: 100.00%] [G loss: 5.282111]\n",
      "epoch:13 step:10669 [D loss: 0.172813, acc.: 95.31%] [G loss: 0.015508]\n",
      "epoch:13 step:10670 [D loss: 0.955810, acc.: 64.06%] [G loss: 4.729500]\n",
      "epoch:13 step:10671 [D loss: 1.825016, acc.: 50.78%] [G loss: 5.070331]\n",
      "epoch:13 step:10672 [D loss: 0.995807, acc.: 63.28%] [G loss: 2.611219]\n",
      "epoch:13 step:10673 [D loss: 0.257859, acc.: 89.06%] [G loss: 2.669658]\n",
      "epoch:13 step:10674 [D loss: 0.080483, acc.: 96.09%] [G loss: 3.469895]\n",
      "epoch:13 step:10675 [D loss: 0.091958, acc.: 100.00%] [G loss: 3.461526]\n",
      "epoch:13 step:10676 [D loss: 0.070637, acc.: 99.22%] [G loss: 3.658635]\n",
      "epoch:13 step:10677 [D loss: 0.129010, acc.: 96.09%] [G loss: 2.412489]\n",
      "epoch:13 step:10678 [D loss: 0.050401, acc.: 99.22%] [G loss: 2.320995]\n",
      "epoch:13 step:10679 [D loss: 0.099556, acc.: 98.44%] [G loss: 2.091861]\n",
      "epoch:13 step:10680 [D loss: 0.051741, acc.: 100.00%] [G loss: 2.208946]\n",
      "epoch:13 step:10681 [D loss: 0.081718, acc.: 99.22%] [G loss: 0.072770]\n",
      "epoch:13 step:10682 [D loss: 0.064992, acc.: 99.22%] [G loss: 1.852948]\n",
      "epoch:13 step:10683 [D loss: 0.036325, acc.: 100.00%] [G loss: 2.214697]\n",
      "epoch:13 step:10684 [D loss: 0.048014, acc.: 100.00%] [G loss: 2.380237]\n",
      "epoch:13 step:10685 [D loss: 0.171017, acc.: 95.31%] [G loss: 3.403884]\n",
      "epoch:13 step:10686 [D loss: 0.051640, acc.: 99.22%] [G loss: 3.999143]\n",
      "epoch:13 step:10687 [D loss: 0.053281, acc.: 99.22%] [G loss: 0.172280]\n",
      "epoch:13 step:10688 [D loss: 0.136472, acc.: 96.09%] [G loss: 3.178876]\n",
      "epoch:13 step:10689 [D loss: 0.052414, acc.: 100.00%] [G loss: 0.029326]\n",
      "epoch:13 step:10690 [D loss: 0.224099, acc.: 92.97%] [G loss: 4.240239]\n",
      "epoch:13 step:10691 [D loss: 0.032078, acc.: 100.00%] [G loss: 3.712371]\n",
      "epoch:13 step:10692 [D loss: 0.021672, acc.: 100.00%] [G loss: 4.472580]\n",
      "epoch:13 step:10693 [D loss: 0.077225, acc.: 99.22%] [G loss: 3.892997]\n",
      "epoch:13 step:10694 [D loss: 0.121862, acc.: 97.66%] [G loss: 4.237530]\n",
      "epoch:13 step:10695 [D loss: 0.101817, acc.: 96.88%] [G loss: 0.501777]\n",
      "epoch:13 step:10696 [D loss: 0.107616, acc.: 96.88%] [G loss: 0.311721]\n",
      "epoch:13 step:10697 [D loss: 0.024054, acc.: 100.00%] [G loss: 4.796171]\n",
      "epoch:13 step:10698 [D loss: 0.229167, acc.: 90.62%] [G loss: 0.329404]\n",
      "epoch:13 step:10699 [D loss: 0.049746, acc.: 99.22%] [G loss: 4.353594]\n",
      "epoch:13 step:10700 [D loss: 0.086243, acc.: 98.44%] [G loss: 4.785189]\n",
      "epoch:13 step:10701 [D loss: 0.056158, acc.: 99.22%] [G loss: 0.397301]\n",
      "epoch:13 step:10702 [D loss: 0.016742, acc.: 100.00%] [G loss: 0.052762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10703 [D loss: 0.410076, acc.: 81.25%] [G loss: 7.078207]\n",
      "epoch:13 step:10704 [D loss: 0.662821, acc.: 64.84%] [G loss: 0.108938]\n",
      "epoch:13 step:10705 [D loss: 1.109217, acc.: 60.16%] [G loss: 6.841435]\n",
      "epoch:13 step:10706 [D loss: 0.787050, acc.: 62.50%] [G loss: 4.755676]\n",
      "epoch:13 step:10707 [D loss: 0.100893, acc.: 96.09%] [G loss: 4.661150]\n",
      "epoch:13 step:10708 [D loss: 0.136373, acc.: 96.09%] [G loss: 2.663800]\n",
      "epoch:13 step:10709 [D loss: 0.066220, acc.: 99.22%] [G loss: 4.890876]\n",
      "epoch:13 step:10710 [D loss: 0.075770, acc.: 99.22%] [G loss: 2.832611]\n",
      "epoch:13 step:10711 [D loss: 0.145274, acc.: 95.31%] [G loss: 1.306927]\n",
      "epoch:13 step:10712 [D loss: 0.132266, acc.: 94.53%] [G loss: 1.156044]\n",
      "epoch:13 step:10713 [D loss: 0.121322, acc.: 96.88%] [G loss: 5.789753]\n",
      "epoch:13 step:10714 [D loss: 0.117261, acc.: 95.31%] [G loss: 5.022717]\n",
      "epoch:13 step:10715 [D loss: 0.222872, acc.: 94.53%] [G loss: 4.392585]\n",
      "epoch:13 step:10716 [D loss: 0.021570, acc.: 100.00%] [G loss: 1.586136]\n",
      "epoch:13 step:10717 [D loss: 0.044592, acc.: 100.00%] [G loss: 3.979565]\n",
      "epoch:13 step:10718 [D loss: 0.036893, acc.: 100.00%] [G loss: 1.999274]\n",
      "epoch:13 step:10719 [D loss: 0.091959, acc.: 96.88%] [G loss: 2.482913]\n",
      "epoch:13 step:10720 [D loss: 0.054138, acc.: 100.00%] [G loss: 2.704427]\n",
      "epoch:13 step:10721 [D loss: 0.015140, acc.: 100.00%] [G loss: 2.689663]\n",
      "epoch:13 step:10722 [D loss: 0.090569, acc.: 96.88%] [G loss: 0.612126]\n",
      "epoch:13 step:10723 [D loss: 0.688617, acc.: 64.84%] [G loss: 6.626462]\n",
      "epoch:13 step:10724 [D loss: 1.216219, acc.: 54.69%] [G loss: 2.589465]\n",
      "epoch:13 step:10725 [D loss: 0.039361, acc.: 99.22%] [G loss: 1.652744]\n",
      "epoch:13 step:10726 [D loss: 0.043335, acc.: 99.22%] [G loss: 0.870512]\n",
      "epoch:13 step:10727 [D loss: 0.043829, acc.: 99.22%] [G loss: 1.907979]\n",
      "epoch:13 step:10728 [D loss: 0.040429, acc.: 100.00%] [G loss: 0.888237]\n",
      "epoch:13 step:10729 [D loss: 0.010982, acc.: 100.00%] [G loss: 0.852320]\n",
      "epoch:13 step:10730 [D loss: 0.073464, acc.: 97.66%] [G loss: 0.648081]\n",
      "epoch:13 step:10731 [D loss: 0.342446, acc.: 81.25%] [G loss: 5.069791]\n",
      "epoch:13 step:10732 [D loss: 0.191874, acc.: 92.97%] [G loss: 5.631175]\n",
      "epoch:13 step:10733 [D loss: 0.318474, acc.: 82.81%] [G loss: 0.404413]\n",
      "epoch:13 step:10734 [D loss: 0.224851, acc.: 89.84%] [G loss: 0.632817]\n",
      "epoch:13 step:10735 [D loss: 0.007171, acc.: 100.00%] [G loss: 2.583120]\n",
      "epoch:13 step:10736 [D loss: 0.010234, acc.: 100.00%] [G loss: 5.034650]\n",
      "epoch:13 step:10737 [D loss: 0.017639, acc.: 100.00%] [G loss: 1.683426]\n",
      "epoch:13 step:10738 [D loss: 0.181560, acc.: 92.19%] [G loss: 0.319069]\n",
      "epoch:13 step:10739 [D loss: 0.589595, acc.: 74.22%] [G loss: 3.525443]\n",
      "epoch:13 step:10740 [D loss: 0.258706, acc.: 85.94%] [G loss: 4.983043]\n",
      "epoch:13 step:10741 [D loss: 0.214537, acc.: 93.75%] [G loss: 1.534672]\n",
      "epoch:13 step:10742 [D loss: 0.069962, acc.: 97.66%] [G loss: 1.730229]\n",
      "epoch:13 step:10743 [D loss: 0.031099, acc.: 100.00%] [G loss: 2.461342]\n",
      "epoch:13 step:10744 [D loss: 0.025950, acc.: 100.00%] [G loss: 1.724368]\n",
      "epoch:13 step:10745 [D loss: 0.057312, acc.: 100.00%] [G loss: 3.239659]\n",
      "epoch:13 step:10746 [D loss: 0.027199, acc.: 99.22%] [G loss: 2.592219]\n",
      "epoch:13 step:10747 [D loss: 0.085669, acc.: 98.44%] [G loss: 1.394509]\n",
      "epoch:13 step:10748 [D loss: 0.018023, acc.: 100.00%] [G loss: 1.812161]\n",
      "epoch:13 step:10749 [D loss: 0.042686, acc.: 100.00%] [G loss: 1.539481]\n",
      "epoch:13 step:10750 [D loss: 0.029621, acc.: 100.00%] [G loss: 1.526359]\n",
      "epoch:13 step:10751 [D loss: 0.044727, acc.: 100.00%] [G loss: 1.787217]\n",
      "epoch:13 step:10752 [D loss: 0.254438, acc.: 92.97%] [G loss: 0.926018]\n",
      "epoch:13 step:10753 [D loss: 0.045404, acc.: 99.22%] [G loss: 1.487954]\n",
      "epoch:13 step:10754 [D loss: 0.328945, acc.: 88.28%] [G loss: 2.201081]\n",
      "epoch:13 step:10755 [D loss: 0.033891, acc.: 99.22%] [G loss: 1.185331]\n",
      "epoch:13 step:10756 [D loss: 0.058352, acc.: 99.22%] [G loss: 1.361852]\n",
      "epoch:13 step:10757 [D loss: 0.168748, acc.: 95.31%] [G loss: 2.230970]\n",
      "epoch:13 step:10758 [D loss: 0.007344, acc.: 100.00%] [G loss: 3.847504]\n",
      "epoch:13 step:10759 [D loss: 0.056888, acc.: 99.22%] [G loss: 1.697500]\n",
      "epoch:13 step:10760 [D loss: 0.031009, acc.: 100.00%] [G loss: 4.229319]\n",
      "epoch:13 step:10761 [D loss: 0.017779, acc.: 100.00%] [G loss: 2.685969]\n",
      "epoch:13 step:10762 [D loss: 0.079766, acc.: 99.22%] [G loss: 1.701337]\n",
      "epoch:13 step:10763 [D loss: 0.081306, acc.: 99.22%] [G loss: 0.930013]\n",
      "epoch:13 step:10764 [D loss: 0.358772, acc.: 82.81%] [G loss: 6.401978]\n",
      "epoch:13 step:10765 [D loss: 0.649768, acc.: 65.62%] [G loss: 1.344738]\n",
      "epoch:13 step:10766 [D loss: 0.288279, acc.: 83.59%] [G loss: 2.213337]\n",
      "epoch:13 step:10767 [D loss: 0.003173, acc.: 100.00%] [G loss: 3.346701]\n",
      "epoch:13 step:10768 [D loss: 0.086736, acc.: 97.66%] [G loss: 1.851720]\n",
      "epoch:13 step:10769 [D loss: 0.007946, acc.: 100.00%] [G loss: 4.080559]\n",
      "epoch:13 step:10770 [D loss: 0.016437, acc.: 100.00%] [G loss: 0.884989]\n",
      "epoch:13 step:10771 [D loss: 0.022537, acc.: 99.22%] [G loss: 3.386142]\n",
      "epoch:13 step:10772 [D loss: 0.016085, acc.: 100.00%] [G loss: 0.970566]\n",
      "epoch:13 step:10773 [D loss: 0.024914, acc.: 100.00%] [G loss: 1.914017]\n",
      "epoch:13 step:10774 [D loss: 0.092096, acc.: 96.88%] [G loss: 1.212052]\n",
      "epoch:13 step:10775 [D loss: 0.023036, acc.: 99.22%] [G loss: 3.565628]\n",
      "epoch:13 step:10776 [D loss: 0.294419, acc.: 85.94%] [G loss: 0.434507]\n",
      "epoch:13 step:10777 [D loss: 0.331715, acc.: 85.16%] [G loss: 3.121674]\n",
      "epoch:13 step:10778 [D loss: 0.043844, acc.: 98.44%] [G loss: 5.315003]\n",
      "epoch:13 step:10779 [D loss: 0.353458, acc.: 82.81%] [G loss: 2.461382]\n",
      "epoch:13 step:10780 [D loss: 0.095904, acc.: 96.88%] [G loss: 4.227483]\n",
      "epoch:13 step:10781 [D loss: 0.037026, acc.: 100.00%] [G loss: 4.279696]\n",
      "epoch:13 step:10782 [D loss: 0.012261, acc.: 100.00%] [G loss: 4.801016]\n",
      "epoch:13 step:10783 [D loss: 0.054206, acc.: 99.22%] [G loss: 3.232489]\n",
      "epoch:13 step:10784 [D loss: 0.040181, acc.: 99.22%] [G loss: 3.575493]\n",
      "epoch:13 step:10785 [D loss: 0.014645, acc.: 100.00%] [G loss: 3.700911]\n",
      "epoch:13 step:10786 [D loss: 0.038755, acc.: 99.22%] [G loss: 4.100589]\n",
      "epoch:13 step:10787 [D loss: 0.016798, acc.: 100.00%] [G loss: 4.267920]\n",
      "epoch:13 step:10788 [D loss: 0.133046, acc.: 97.66%] [G loss: 2.059673]\n",
      "epoch:13 step:10789 [D loss: 0.020977, acc.: 100.00%] [G loss: 4.996076]\n",
      "epoch:13 step:10790 [D loss: 0.019831, acc.: 100.00%] [G loss: 4.448432]\n",
      "epoch:13 step:10791 [D loss: 0.036220, acc.: 100.00%] [G loss: 3.601878]\n",
      "epoch:13 step:10792 [D loss: 0.056862, acc.: 100.00%] [G loss: 1.842023]\n",
      "epoch:13 step:10793 [D loss: 0.022587, acc.: 100.00%] [G loss: 4.745832]\n",
      "epoch:13 step:10794 [D loss: 0.004038, acc.: 100.00%] [G loss: 4.829790]\n",
      "epoch:13 step:10795 [D loss: 0.020615, acc.: 99.22%] [G loss: 4.037745]\n",
      "epoch:13 step:10796 [D loss: 0.005385, acc.: 100.00%] [G loss: 4.088787]\n",
      "epoch:13 step:10797 [D loss: 0.011049, acc.: 100.00%] [G loss: 3.976391]\n",
      "epoch:13 step:10798 [D loss: 0.011477, acc.: 100.00%] [G loss: 3.667227]\n",
      "epoch:13 step:10799 [D loss: 0.032625, acc.: 100.00%] [G loss: 2.261113]\n",
      "epoch:13 step:10800 [D loss: 0.069590, acc.: 99.22%] [G loss: 0.168214]\n",
      "epoch:13 step:10801 [D loss: 0.035088, acc.: 100.00%] [G loss: 4.408568]\n",
      "epoch:13 step:10802 [D loss: 0.037353, acc.: 100.00%] [G loss: 4.738428]\n",
      "epoch:13 step:10803 [D loss: 0.042523, acc.: 100.00%] [G loss: 0.206815]\n",
      "epoch:13 step:10804 [D loss: 0.392416, acc.: 75.78%] [G loss: 4.875791]\n",
      "epoch:13 step:10805 [D loss: 1.294678, acc.: 53.12%] [G loss: 0.186995]\n",
      "epoch:13 step:10806 [D loss: 0.234531, acc.: 87.50%] [G loss: 3.300476]\n",
      "epoch:13 step:10807 [D loss: 0.030216, acc.: 99.22%] [G loss: 3.924024]\n",
      "epoch:13 step:10808 [D loss: 0.053761, acc.: 99.22%] [G loss: 3.525770]\n",
      "epoch:13 step:10809 [D loss: 0.019311, acc.: 100.00%] [G loss: 2.380010]\n",
      "epoch:13 step:10810 [D loss: 0.016989, acc.: 100.00%] [G loss: 1.600609]\n",
      "epoch:13 step:10811 [D loss: 0.021692, acc.: 100.00%] [G loss: 3.561704]\n",
      "epoch:13 step:10812 [D loss: 0.037776, acc.: 100.00%] [G loss: 2.208830]\n",
      "epoch:13 step:10813 [D loss: 0.029203, acc.: 100.00%] [G loss: 1.627087]\n",
      "epoch:13 step:10814 [D loss: 0.037408, acc.: 100.00%] [G loss: 1.314699]\n",
      "epoch:13 step:10815 [D loss: 0.061803, acc.: 99.22%] [G loss: 1.527869]\n",
      "epoch:13 step:10816 [D loss: 0.015749, acc.: 100.00%] [G loss: 1.646384]\n",
      "epoch:13 step:10817 [D loss: 0.018494, acc.: 100.00%] [G loss: 1.551030]\n",
      "epoch:13 step:10818 [D loss: 0.065077, acc.: 99.22%] [G loss: 4.225586]\n",
      "epoch:13 step:10819 [D loss: 0.021491, acc.: 100.00%] [G loss: 1.508300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10820 [D loss: 0.035577, acc.: 100.00%] [G loss: 2.082221]\n",
      "epoch:13 step:10821 [D loss: 0.013695, acc.: 100.00%] [G loss: 2.382735]\n",
      "epoch:13 step:10822 [D loss: 0.016254, acc.: 100.00%] [G loss: 0.395484]\n",
      "epoch:13 step:10823 [D loss: 0.054908, acc.: 100.00%] [G loss: 0.212537]\n",
      "epoch:13 step:10824 [D loss: 0.014652, acc.: 100.00%] [G loss: 3.197439]\n",
      "epoch:13 step:10825 [D loss: 0.038140, acc.: 100.00%] [G loss: 2.187406]\n",
      "epoch:13 step:10826 [D loss: 0.050882, acc.: 99.22%] [G loss: 1.779190]\n",
      "epoch:13 step:10827 [D loss: 0.036980, acc.: 100.00%] [G loss: 1.796012]\n",
      "epoch:13 step:10828 [D loss: 0.004676, acc.: 100.00%] [G loss: 4.756886]\n",
      "epoch:13 step:10829 [D loss: 0.019353, acc.: 100.00%] [G loss: 2.564940]\n",
      "epoch:13 step:10830 [D loss: 0.008891, acc.: 100.00%] [G loss: 2.958485]\n",
      "epoch:13 step:10831 [D loss: 0.196050, acc.: 92.97%] [G loss: 4.027060]\n",
      "epoch:13 step:10832 [D loss: 0.012517, acc.: 100.00%] [G loss: 4.649542]\n",
      "epoch:13 step:10833 [D loss: 0.019245, acc.: 100.00%] [G loss: 0.249504]\n",
      "epoch:13 step:10834 [D loss: 0.036095, acc.: 99.22%] [G loss: 3.954216]\n",
      "epoch:13 step:10835 [D loss: 0.008309, acc.: 100.00%] [G loss: 0.027239]\n",
      "epoch:13 step:10836 [D loss: 0.067884, acc.: 99.22%] [G loss: 0.050122]\n",
      "epoch:13 step:10837 [D loss: 0.053675, acc.: 98.44%] [G loss: 4.564241]\n",
      "epoch:13 step:10838 [D loss: 0.373718, acc.: 85.16%] [G loss: 1.312230]\n",
      "epoch:13 step:10839 [D loss: 0.274740, acc.: 91.41%] [G loss: 2.867179]\n",
      "epoch:13 step:10840 [D loss: 0.008271, acc.: 100.00%] [G loss: 3.772193]\n",
      "epoch:13 step:10841 [D loss: 0.080328, acc.: 96.09%] [G loss: 2.365216]\n",
      "epoch:13 step:10842 [D loss: 0.010964, acc.: 100.00%] [G loss: 2.103415]\n",
      "epoch:13 step:10843 [D loss: 0.012525, acc.: 100.00%] [G loss: 0.855673]\n",
      "epoch:13 step:10844 [D loss: 0.003843, acc.: 100.00%] [G loss: 1.158980]\n",
      "epoch:13 step:10845 [D loss: 0.004943, acc.: 100.00%] [G loss: 0.610777]\n",
      "epoch:13 step:10846 [D loss: 0.011161, acc.: 100.00%] [G loss: 0.846740]\n",
      "epoch:13 step:10847 [D loss: 0.011147, acc.: 100.00%] [G loss: 0.907231]\n",
      "epoch:13 step:10848 [D loss: 0.006411, acc.: 100.00%] [G loss: 0.897550]\n",
      "epoch:13 step:10849 [D loss: 0.009655, acc.: 100.00%] [G loss: 1.189115]\n",
      "epoch:13 step:10850 [D loss: 0.003568, acc.: 100.00%] [G loss: 1.061479]\n",
      "epoch:13 step:10851 [D loss: 0.008889, acc.: 100.00%] [G loss: 0.981725]\n",
      "epoch:13 step:10852 [D loss: 0.006515, acc.: 100.00%] [G loss: 0.901046]\n",
      "epoch:13 step:10853 [D loss: 0.004930, acc.: 100.00%] [G loss: 1.691274]\n",
      "epoch:13 step:10854 [D loss: 0.002810, acc.: 100.00%] [G loss: 1.105346]\n",
      "epoch:13 step:10855 [D loss: 0.005514, acc.: 100.00%] [G loss: 0.705480]\n",
      "epoch:13 step:10856 [D loss: 0.019776, acc.: 100.00%] [G loss: 0.827744]\n",
      "epoch:13 step:10857 [D loss: 0.024891, acc.: 99.22%] [G loss: 0.493603]\n",
      "epoch:13 step:10858 [D loss: 0.036913, acc.: 100.00%] [G loss: 0.780648]\n",
      "epoch:13 step:10859 [D loss: 0.011333, acc.: 100.00%] [G loss: 5.324515]\n",
      "epoch:13 step:10860 [D loss: 0.032998, acc.: 99.22%] [G loss: 0.965338]\n",
      "epoch:13 step:10861 [D loss: 0.038750, acc.: 100.00%] [G loss: 1.253519]\n",
      "epoch:13 step:10862 [D loss: 0.004449, acc.: 100.00%] [G loss: 4.291363]\n",
      "epoch:13 step:10863 [D loss: 0.004982, acc.: 100.00%] [G loss: 1.301950]\n",
      "epoch:13 step:10864 [D loss: 0.108252, acc.: 96.88%] [G loss: 2.512743]\n",
      "epoch:13 step:10865 [D loss: 0.009382, acc.: 100.00%] [G loss: 0.274132]\n",
      "epoch:13 step:10866 [D loss: 0.342568, acc.: 82.81%] [G loss: 4.355385]\n",
      "epoch:13 step:10867 [D loss: 0.023914, acc.: 99.22%] [G loss: 7.365033]\n",
      "epoch:13 step:10868 [D loss: 1.661705, acc.: 39.06%] [G loss: 5.156982]\n",
      "epoch:13 step:10869 [D loss: 0.118814, acc.: 94.53%] [G loss: 0.057845]\n",
      "epoch:13 step:10870 [D loss: 0.032419, acc.: 99.22%] [G loss: 7.819360]\n",
      "epoch:13 step:10871 [D loss: 0.124456, acc.: 92.97%] [G loss: 0.001998]\n",
      "epoch:13 step:10872 [D loss: 0.389369, acc.: 80.47%] [G loss: 1.516971]\n",
      "epoch:13 step:10873 [D loss: 0.083464, acc.: 96.88%] [G loss: 7.619054]\n",
      "epoch:13 step:10874 [D loss: 0.503077, acc.: 78.91%] [G loss: 2.945284]\n",
      "epoch:13 step:10875 [D loss: 0.025828, acc.: 100.00%] [G loss: 4.941421]\n",
      "epoch:13 step:10876 [D loss: 0.258099, acc.: 89.06%] [G loss: 3.178770]\n",
      "epoch:13 step:10877 [D loss: 0.001569, acc.: 100.00%] [G loss: 3.155103]\n",
      "epoch:13 step:10878 [D loss: 0.049991, acc.: 98.44%] [G loss: 2.447424]\n",
      "epoch:13 step:10879 [D loss: 0.002474, acc.: 100.00%] [G loss: 2.242806]\n",
      "epoch:13 step:10880 [D loss: 0.039370, acc.: 99.22%] [G loss: 1.235535]\n",
      "epoch:13 step:10881 [D loss: 0.003003, acc.: 100.00%] [G loss: 5.129275]\n",
      "epoch:13 step:10882 [D loss: 0.011389, acc.: 100.00%] [G loss: 0.647872]\n",
      "epoch:13 step:10883 [D loss: 0.208628, acc.: 89.84%] [G loss: 5.989476]\n",
      "epoch:13 step:10884 [D loss: 0.037570, acc.: 97.66%] [G loss: 5.699337]\n",
      "epoch:13 step:10885 [D loss: 0.409613, acc.: 80.47%] [G loss: 1.446509]\n",
      "epoch:13 step:10886 [D loss: 1.035862, acc.: 56.25%] [G loss: 7.986529]\n",
      "epoch:13 step:10887 [D loss: 0.845381, acc.: 61.72%] [G loss: 5.983258]\n",
      "epoch:13 step:10888 [D loss: 0.078038, acc.: 96.09%] [G loss: 2.785460]\n",
      "epoch:13 step:10889 [D loss: 0.009024, acc.: 100.00%] [G loss: 1.059798]\n",
      "epoch:13 step:10890 [D loss: 0.023102, acc.: 100.00%] [G loss: 1.028223]\n",
      "epoch:13 step:10891 [D loss: 0.013209, acc.: 100.00%] [G loss: 1.795737]\n",
      "epoch:13 step:10892 [D loss: 0.042052, acc.: 99.22%] [G loss: 2.237916]\n",
      "epoch:13 step:10893 [D loss: 0.016467, acc.: 100.00%] [G loss: 0.864387]\n",
      "epoch:13 step:10894 [D loss: 0.015422, acc.: 100.00%] [G loss: 0.914158]\n",
      "epoch:13 step:10895 [D loss: 0.050962, acc.: 100.00%] [G loss: 0.313685]\n",
      "epoch:13 step:10896 [D loss: 0.018679, acc.: 100.00%] [G loss: 3.942521]\n",
      "epoch:13 step:10897 [D loss: 0.018042, acc.: 100.00%] [G loss: 5.189088]\n",
      "epoch:13 step:10898 [D loss: 0.096829, acc.: 97.66%] [G loss: 1.938769]\n",
      "epoch:13 step:10899 [D loss: 0.081768, acc.: 98.44%] [G loss: 0.916805]\n",
      "epoch:13 step:10900 [D loss: 0.087370, acc.: 97.66%] [G loss: 2.913908]\n",
      "epoch:13 step:10901 [D loss: 0.171195, acc.: 93.75%] [G loss: 2.093922]\n",
      "epoch:13 step:10902 [D loss: 0.091745, acc.: 98.44%] [G loss: 1.378390]\n",
      "epoch:13 step:10903 [D loss: 0.065384, acc.: 98.44%] [G loss: 1.709707]\n",
      "epoch:13 step:10904 [D loss: 0.021738, acc.: 99.22%] [G loss: 2.320327]\n",
      "epoch:13 step:10905 [D loss: 0.018667, acc.: 100.00%] [G loss: 0.934665]\n",
      "epoch:13 step:10906 [D loss: 0.075220, acc.: 97.66%] [G loss: 2.061664]\n",
      "epoch:13 step:10907 [D loss: 0.028826, acc.: 100.00%] [G loss: 0.541765]\n",
      "epoch:13 step:10908 [D loss: 0.044185, acc.: 100.00%] [G loss: 1.340240]\n",
      "epoch:13 step:10909 [D loss: 0.052186, acc.: 100.00%] [G loss: 3.284854]\n",
      "epoch:13 step:10910 [D loss: 0.021991, acc.: 100.00%] [G loss: 0.365394]\n",
      "epoch:13 step:10911 [D loss: 0.023163, acc.: 100.00%] [G loss: 0.335222]\n",
      "epoch:13 step:10912 [D loss: 0.077240, acc.: 97.66%] [G loss: 0.979451]\n",
      "epoch:13 step:10913 [D loss: 0.180062, acc.: 92.97%] [G loss: 0.463238]\n",
      "epoch:13 step:10914 [D loss: 0.015762, acc.: 100.00%] [G loss: 0.699854]\n",
      "epoch:13 step:10915 [D loss: 0.932639, acc.: 59.38%] [G loss: 2.221795]\n",
      "epoch:13 step:10916 [D loss: 0.081354, acc.: 99.22%] [G loss: 0.002413]\n",
      "epoch:13 step:10917 [D loss: 0.026068, acc.: 100.00%] [G loss: 0.069057]\n",
      "epoch:13 step:10918 [D loss: 0.005629, acc.: 100.00%] [G loss: 5.395267]\n",
      "epoch:13 step:10919 [D loss: 0.004529, acc.: 100.00%] [G loss: 0.015649]\n",
      "epoch:13 step:10920 [D loss: 0.010983, acc.: 100.00%] [G loss: 0.025858]\n",
      "epoch:13 step:10921 [D loss: 0.016121, acc.: 100.00%] [G loss: 4.633264]\n",
      "epoch:13 step:10922 [D loss: 0.002425, acc.: 100.00%] [G loss: 0.036929]\n",
      "epoch:13 step:10923 [D loss: 0.006879, acc.: 100.00%] [G loss: 3.359319]\n",
      "epoch:13 step:10924 [D loss: 0.093430, acc.: 96.88%] [G loss: 0.067744]\n",
      "epoch:13 step:10925 [D loss: 0.082590, acc.: 96.88%] [G loss: 0.031288]\n",
      "epoch:13 step:10926 [D loss: 0.015624, acc.: 100.00%] [G loss: 0.037943]\n",
      "epoch:13 step:10927 [D loss: 0.012601, acc.: 100.00%] [G loss: 0.031335]\n",
      "epoch:13 step:10928 [D loss: 0.015730, acc.: 100.00%] [G loss: 0.081336]\n",
      "epoch:13 step:10929 [D loss: 0.024485, acc.: 100.00%] [G loss: 0.310626]\n",
      "epoch:13 step:10930 [D loss: 0.054554, acc.: 98.44%] [G loss: 0.050667]\n",
      "epoch:13 step:10931 [D loss: 0.004317, acc.: 100.00%] [G loss: 5.901412]\n",
      "epoch:13 step:10932 [D loss: 0.041044, acc.: 100.00%] [G loss: 4.788559]\n",
      "epoch:13 step:10933 [D loss: 0.004044, acc.: 100.00%] [G loss: 0.249846]\n",
      "epoch:13 step:10934 [D loss: 0.011688, acc.: 100.00%] [G loss: 0.262406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:10935 [D loss: 0.004654, acc.: 100.00%] [G loss: 4.374659]\n",
      "epoch:14 step:10936 [D loss: 0.007946, acc.: 100.00%] [G loss: 0.254623]\n",
      "epoch:14 step:10937 [D loss: 0.053824, acc.: 99.22%] [G loss: 1.762275]\n",
      "epoch:14 step:10938 [D loss: 0.023945, acc.: 100.00%] [G loss: 2.521669]\n",
      "epoch:14 step:10939 [D loss: 0.026243, acc.: 99.22%] [G loss: 0.623382]\n",
      "epoch:14 step:10940 [D loss: 0.240074, acc.: 92.19%] [G loss: 6.386260]\n",
      "epoch:14 step:10941 [D loss: 1.974667, acc.: 28.12%] [G loss: 7.194825]\n",
      "epoch:14 step:10942 [D loss: 0.005804, acc.: 100.00%] [G loss: 8.999512]\n",
      "epoch:14 step:10943 [D loss: 0.754525, acc.: 71.88%] [G loss: 0.795557]\n",
      "epoch:14 step:10944 [D loss: 1.072272, acc.: 63.28%] [G loss: 7.243179]\n",
      "epoch:14 step:10945 [D loss: 0.065255, acc.: 97.66%] [G loss: 9.550806]\n",
      "epoch:14 step:10946 [D loss: 1.458909, acc.: 57.81%] [G loss: 2.454375]\n",
      "epoch:14 step:10947 [D loss: 0.012026, acc.: 100.00%] [G loss: 1.937181]\n",
      "epoch:14 step:10948 [D loss: 0.043664, acc.: 99.22%] [G loss: 1.802136]\n",
      "epoch:14 step:10949 [D loss: 0.185257, acc.: 93.75%] [G loss: 3.113528]\n",
      "epoch:14 step:10950 [D loss: 0.007622, acc.: 100.00%] [G loss: 3.970635]\n",
      "epoch:14 step:10951 [D loss: 0.003510, acc.: 100.00%] [G loss: 4.699346]\n",
      "epoch:14 step:10952 [D loss: 0.016502, acc.: 100.00%] [G loss: 3.045387]\n",
      "epoch:14 step:10953 [D loss: 0.051253, acc.: 99.22%] [G loss: 5.393477]\n",
      "epoch:14 step:10954 [D loss: 0.046395, acc.: 100.00%] [G loss: 2.821029]\n",
      "epoch:14 step:10955 [D loss: 0.055000, acc.: 98.44%] [G loss: 3.223453]\n",
      "epoch:14 step:10956 [D loss: 0.118625, acc.: 96.09%] [G loss: 3.797074]\n",
      "epoch:14 step:10957 [D loss: 0.028026, acc.: 100.00%] [G loss: 3.576162]\n",
      "epoch:14 step:10958 [D loss: 0.230691, acc.: 91.41%] [G loss: 2.911265]\n",
      "epoch:14 step:10959 [D loss: 0.098894, acc.: 94.53%] [G loss: 2.049138]\n",
      "epoch:14 step:10960 [D loss: 0.037705, acc.: 100.00%] [G loss: 2.537913]\n",
      "epoch:14 step:10961 [D loss: 0.059450, acc.: 99.22%] [G loss: 4.526006]\n",
      "epoch:14 step:10962 [D loss: 0.062725, acc.: 100.00%] [G loss: 3.634478]\n",
      "epoch:14 step:10963 [D loss: 0.062662, acc.: 98.44%] [G loss: 0.297591]\n",
      "epoch:14 step:10964 [D loss: 0.025367, acc.: 99.22%] [G loss: 3.653539]\n",
      "epoch:14 step:10965 [D loss: 0.094164, acc.: 97.66%] [G loss: 5.264365]\n",
      "epoch:14 step:10966 [D loss: 0.042832, acc.: 99.22%] [G loss: 5.022147]\n",
      "epoch:14 step:10967 [D loss: 0.080654, acc.: 98.44%] [G loss: 5.512669]\n",
      "epoch:14 step:10968 [D loss: 0.016196, acc.: 100.00%] [G loss: 3.473709]\n",
      "epoch:14 step:10969 [D loss: 0.017997, acc.: 100.00%] [G loss: 0.154056]\n",
      "epoch:14 step:10970 [D loss: 0.144000, acc.: 96.09%] [G loss: 2.742022]\n",
      "epoch:14 step:10971 [D loss: 0.009022, acc.: 100.00%] [G loss: 0.242932]\n",
      "epoch:14 step:10972 [D loss: 0.043050, acc.: 99.22%] [G loss: 1.424610]\n",
      "epoch:14 step:10973 [D loss: 0.029360, acc.: 99.22%] [G loss: 0.279602]\n",
      "epoch:14 step:10974 [D loss: 0.044816, acc.: 100.00%] [G loss: 3.988690]\n",
      "epoch:14 step:10975 [D loss: 0.022032, acc.: 100.00%] [G loss: 3.967908]\n",
      "epoch:14 step:10976 [D loss: 0.004190, acc.: 100.00%] [G loss: 4.054789]\n",
      "epoch:14 step:10977 [D loss: 0.005247, acc.: 100.00%] [G loss: 3.417307]\n",
      "epoch:14 step:10978 [D loss: 0.018753, acc.: 100.00%] [G loss: 3.495355]\n",
      "epoch:14 step:10979 [D loss: 0.014326, acc.: 100.00%] [G loss: 2.996145]\n",
      "epoch:14 step:10980 [D loss: 0.009669, acc.: 100.00%] [G loss: 0.548301]\n",
      "epoch:14 step:10981 [D loss: 0.005990, acc.: 100.00%] [G loss: 1.029221]\n",
      "epoch:14 step:10982 [D loss: 0.006790, acc.: 100.00%] [G loss: 1.387764]\n",
      "epoch:14 step:10983 [D loss: 0.015242, acc.: 100.00%] [G loss: 0.053798]\n",
      "epoch:14 step:10984 [D loss: 0.026173, acc.: 99.22%] [G loss: 0.154274]\n",
      "epoch:14 step:10985 [D loss: 0.724027, acc.: 65.62%] [G loss: 6.467391]\n",
      "epoch:14 step:10986 [D loss: 2.736300, acc.: 50.00%] [G loss: 1.730889]\n",
      "epoch:14 step:10987 [D loss: 0.348602, acc.: 84.38%] [G loss: 5.636837]\n",
      "epoch:14 step:10988 [D loss: 0.011945, acc.: 100.00%] [G loss: 0.006937]\n",
      "epoch:14 step:10989 [D loss: 0.010957, acc.: 100.00%] [G loss: 0.153683]\n",
      "epoch:14 step:10990 [D loss: 0.025459, acc.: 100.00%] [G loss: 3.514483]\n",
      "epoch:14 step:10991 [D loss: 0.032225, acc.: 100.00%] [G loss: 0.012782]\n",
      "epoch:14 step:10992 [D loss: 0.055515, acc.: 99.22%] [G loss: 0.026632]\n",
      "epoch:14 step:10993 [D loss: 0.014570, acc.: 100.00%] [G loss: 0.040591]\n",
      "epoch:14 step:10994 [D loss: 0.044219, acc.: 99.22%] [G loss: 0.124875]\n",
      "epoch:14 step:10995 [D loss: 0.035764, acc.: 100.00%] [G loss: 0.057334]\n",
      "epoch:14 step:10996 [D loss: 0.019904, acc.: 100.00%] [G loss: 4.058825]\n",
      "epoch:14 step:10997 [D loss: 0.044569, acc.: 100.00%] [G loss: 3.740900]\n",
      "epoch:14 step:10998 [D loss: 0.044494, acc.: 100.00%] [G loss: 2.869446]\n",
      "epoch:14 step:10999 [D loss: 0.075718, acc.: 99.22%] [G loss: 0.049046]\n",
      "epoch:14 step:11000 [D loss: 0.037887, acc.: 100.00%] [G loss: 0.099656]\n",
      "epoch:14 step:11001 [D loss: 0.520801, acc.: 75.78%] [G loss: 0.040215]\n",
      "epoch:14 step:11002 [D loss: 0.004988, acc.: 100.00%] [G loss: 3.896722]\n",
      "epoch:14 step:11003 [D loss: 0.027306, acc.: 100.00%] [G loss: 0.078868]\n",
      "epoch:14 step:11004 [D loss: 0.007280, acc.: 100.00%] [G loss: 2.242146]\n",
      "epoch:14 step:11005 [D loss: 0.022012, acc.: 100.00%] [G loss: 0.116525]\n",
      "epoch:14 step:11006 [D loss: 0.016998, acc.: 100.00%] [G loss: 3.166358]\n",
      "epoch:14 step:11007 [D loss: 0.036728, acc.: 100.00%] [G loss: 0.066491]\n",
      "epoch:14 step:11008 [D loss: 0.034082, acc.: 99.22%] [G loss: 0.269515]\n",
      "epoch:14 step:11009 [D loss: 0.032602, acc.: 99.22%] [G loss: 2.801455]\n",
      "epoch:14 step:11010 [D loss: 0.115461, acc.: 96.09%] [G loss: 0.565508]\n",
      "epoch:14 step:11011 [D loss: 0.248138, acc.: 88.28%] [G loss: 0.128769]\n",
      "epoch:14 step:11012 [D loss: 0.021260, acc.: 100.00%] [G loss: 2.150874]\n",
      "epoch:14 step:11013 [D loss: 0.045460, acc.: 100.00%] [G loss: 2.281443]\n",
      "epoch:14 step:11014 [D loss: 0.027974, acc.: 100.00%] [G loss: 1.745052]\n",
      "epoch:14 step:11015 [D loss: 0.045186, acc.: 100.00%] [G loss: 3.166522]\n",
      "epoch:14 step:11016 [D loss: 0.015246, acc.: 100.00%] [G loss: 0.979741]\n",
      "epoch:14 step:11017 [D loss: 0.047327, acc.: 100.00%] [G loss: 1.215657]\n",
      "epoch:14 step:11018 [D loss: 0.113097, acc.: 97.66%] [G loss: 1.711634]\n",
      "epoch:14 step:11019 [D loss: 0.104595, acc.: 96.88%] [G loss: 0.968025]\n",
      "epoch:14 step:11020 [D loss: 0.047553, acc.: 100.00%] [G loss: 1.772994]\n",
      "epoch:14 step:11021 [D loss: 0.007107, acc.: 100.00%] [G loss: 2.870370]\n",
      "epoch:14 step:11022 [D loss: 0.234641, acc.: 89.84%] [G loss: 2.119308]\n",
      "epoch:14 step:11023 [D loss: 0.009786, acc.: 100.00%] [G loss: 3.762586]\n",
      "epoch:14 step:11024 [D loss: 0.082602, acc.: 97.66%] [G loss: 3.250275]\n",
      "epoch:14 step:11025 [D loss: 0.008227, acc.: 100.00%] [G loss: 2.868608]\n",
      "epoch:14 step:11026 [D loss: 0.027396, acc.: 100.00%] [G loss: 0.483259]\n",
      "epoch:14 step:11027 [D loss: 0.009487, acc.: 100.00%] [G loss: 4.237372]\n",
      "epoch:14 step:11028 [D loss: 0.093769, acc.: 98.44%] [G loss: 3.924563]\n",
      "epoch:14 step:11029 [D loss: 0.022448, acc.: 100.00%] [G loss: 5.431417]\n",
      "epoch:14 step:11030 [D loss: 0.014465, acc.: 100.00%] [G loss: 4.694467]\n",
      "epoch:14 step:11031 [D loss: 0.015626, acc.: 99.22%] [G loss: 0.080223]\n",
      "epoch:14 step:11032 [D loss: 0.020466, acc.: 100.00%] [G loss: 0.108727]\n",
      "epoch:14 step:11033 [D loss: 0.177196, acc.: 95.31%] [G loss: 5.865391]\n",
      "epoch:14 step:11034 [D loss: 0.169425, acc.: 93.75%] [G loss: 3.905402]\n",
      "epoch:14 step:11035 [D loss: 0.013937, acc.: 100.00%] [G loss: 3.108296]\n",
      "epoch:14 step:11036 [D loss: 0.030882, acc.: 99.22%] [G loss: 4.151147]\n",
      "epoch:14 step:11037 [D loss: 0.008846, acc.: 100.00%] [G loss: 4.235850]\n",
      "epoch:14 step:11038 [D loss: 0.031358, acc.: 99.22%] [G loss: 0.439032]\n",
      "epoch:14 step:11039 [D loss: 0.009978, acc.: 100.00%] [G loss: 4.603681]\n",
      "epoch:14 step:11040 [D loss: 0.013661, acc.: 100.00%] [G loss: 3.112434]\n",
      "epoch:14 step:11041 [D loss: 0.013580, acc.: 100.00%] [G loss: 3.271140]\n",
      "epoch:14 step:11042 [D loss: 0.009959, acc.: 100.00%] [G loss: 3.071869]\n",
      "epoch:14 step:11043 [D loss: 0.007943, acc.: 100.00%] [G loss: 2.981373]\n",
      "epoch:14 step:11044 [D loss: 0.094035, acc.: 97.66%] [G loss: 3.023224]\n",
      "epoch:14 step:11045 [D loss: 0.014973, acc.: 100.00%] [G loss: 4.249015]\n",
      "epoch:14 step:11046 [D loss: 0.005547, acc.: 100.00%] [G loss: 5.460739]\n",
      "epoch:14 step:11047 [D loss: 0.006352, acc.: 100.00%] [G loss: 0.820319]\n",
      "epoch:14 step:11048 [D loss: 0.042833, acc.: 99.22%] [G loss: 3.553942]\n",
      "epoch:14 step:11049 [D loss: 0.038445, acc.: 100.00%] [G loss: 3.976607]\n",
      "epoch:14 step:11050 [D loss: 0.071594, acc.: 99.22%] [G loss: 4.107330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11051 [D loss: 0.011952, acc.: 100.00%] [G loss: 5.393999]\n",
      "epoch:14 step:11052 [D loss: 0.003454, acc.: 100.00%] [G loss: 4.885250]\n",
      "epoch:14 step:11053 [D loss: 0.020988, acc.: 100.00%] [G loss: 4.353494]\n",
      "epoch:14 step:11054 [D loss: 0.011824, acc.: 100.00%] [G loss: 4.233367]\n",
      "epoch:14 step:11055 [D loss: 0.010320, acc.: 100.00%] [G loss: 3.263171]\n",
      "epoch:14 step:11056 [D loss: 0.013334, acc.: 100.00%] [G loss: 3.818781]\n",
      "epoch:14 step:11057 [D loss: 0.028902, acc.: 99.22%] [G loss: 5.278522]\n",
      "epoch:14 step:11058 [D loss: 0.004181, acc.: 100.00%] [G loss: 4.664553]\n",
      "epoch:14 step:11059 [D loss: 0.031239, acc.: 100.00%] [G loss: 1.062661]\n",
      "epoch:14 step:11060 [D loss: 0.033766, acc.: 100.00%] [G loss: 4.499580]\n",
      "epoch:14 step:11061 [D loss: 0.004115, acc.: 100.00%] [G loss: 4.710658]\n",
      "epoch:14 step:11062 [D loss: 0.017345, acc.: 100.00%] [G loss: 3.624685]\n",
      "epoch:14 step:11063 [D loss: 0.005475, acc.: 100.00%] [G loss: 3.488666]\n",
      "epoch:14 step:11064 [D loss: 0.018025, acc.: 100.00%] [G loss: 0.320874]\n",
      "epoch:14 step:11065 [D loss: 0.009329, acc.: 100.00%] [G loss: 4.274414]\n",
      "epoch:14 step:11066 [D loss: 0.021147, acc.: 100.00%] [G loss: 4.262855]\n",
      "epoch:14 step:11067 [D loss: 0.010293, acc.: 100.00%] [G loss: 4.528915]\n",
      "epoch:14 step:11068 [D loss: 0.010176, acc.: 100.00%] [G loss: 1.109748]\n",
      "epoch:14 step:11069 [D loss: 0.006690, acc.: 100.00%] [G loss: 4.727191]\n",
      "epoch:14 step:11070 [D loss: 0.046174, acc.: 99.22%] [G loss: 3.678548]\n",
      "epoch:14 step:11071 [D loss: 0.016667, acc.: 100.00%] [G loss: 3.959064]\n",
      "epoch:14 step:11072 [D loss: 0.009107, acc.: 100.00%] [G loss: 4.656377]\n",
      "epoch:14 step:11073 [D loss: 0.023988, acc.: 100.00%] [G loss: 4.755863]\n",
      "epoch:14 step:11074 [D loss: 0.027544, acc.: 100.00%] [G loss: 4.003493]\n",
      "epoch:14 step:11075 [D loss: 0.010361, acc.: 100.00%] [G loss: 4.746567]\n",
      "epoch:14 step:11076 [D loss: 0.003585, acc.: 100.00%] [G loss: 4.345181]\n",
      "epoch:14 step:11077 [D loss: 0.008272, acc.: 100.00%] [G loss: 5.447027]\n",
      "epoch:14 step:11078 [D loss: 0.005708, acc.: 100.00%] [G loss: 4.839629]\n",
      "epoch:14 step:11079 [D loss: 0.008810, acc.: 100.00%] [G loss: 0.198680]\n",
      "epoch:14 step:11080 [D loss: 0.010008, acc.: 100.00%] [G loss: 4.600556]\n",
      "epoch:14 step:11081 [D loss: 0.011683, acc.: 100.00%] [G loss: 0.135202]\n",
      "epoch:14 step:11082 [D loss: 0.051467, acc.: 99.22%] [G loss: 5.531920]\n",
      "epoch:14 step:11083 [D loss: 0.018598, acc.: 100.00%] [G loss: 5.236216]\n",
      "epoch:14 step:11084 [D loss: 0.051592, acc.: 97.66%] [G loss: 4.208785]\n",
      "epoch:14 step:11085 [D loss: 0.008814, acc.: 100.00%] [G loss: 0.260048]\n",
      "epoch:14 step:11086 [D loss: 0.013823, acc.: 100.00%] [G loss: 3.900293]\n",
      "epoch:14 step:11087 [D loss: 0.062557, acc.: 98.44%] [G loss: 3.163237]\n",
      "epoch:14 step:11088 [D loss: 0.006696, acc.: 100.00%] [G loss: 3.336406]\n",
      "epoch:14 step:11089 [D loss: 0.002757, acc.: 100.00%] [G loss: 4.315730]\n",
      "epoch:14 step:11090 [D loss: 0.007765, acc.: 100.00%] [G loss: 2.638566]\n",
      "epoch:14 step:11091 [D loss: 0.010680, acc.: 100.00%] [G loss: 2.156456]\n",
      "epoch:14 step:11092 [D loss: 0.018211, acc.: 100.00%] [G loss: 2.277527]\n",
      "epoch:14 step:11093 [D loss: 0.076222, acc.: 98.44%] [G loss: 7.001195]\n",
      "epoch:14 step:11094 [D loss: 0.004958, acc.: 100.00%] [G loss: 5.762522]\n",
      "epoch:14 step:11095 [D loss: 0.243019, acc.: 88.28%] [G loss: 3.113736]\n",
      "epoch:14 step:11096 [D loss: 1.066213, acc.: 54.69%] [G loss: 11.012866]\n",
      "epoch:14 step:11097 [D loss: 4.469516, acc.: 50.00%] [G loss: 6.521167]\n",
      "epoch:14 step:11098 [D loss: 1.978190, acc.: 50.00%] [G loss: 1.738373]\n",
      "epoch:14 step:11099 [D loss: 0.529421, acc.: 71.88%] [G loss: 2.458991]\n",
      "epoch:14 step:11100 [D loss: 0.159018, acc.: 99.22%] [G loss: 2.700353]\n",
      "epoch:14 step:11101 [D loss: 0.178845, acc.: 97.66%] [G loss: 2.987389]\n",
      "epoch:14 step:11102 [D loss: 0.094579, acc.: 100.00%] [G loss: 3.337757]\n",
      "epoch:14 step:11103 [D loss: 0.082184, acc.: 99.22%] [G loss: 2.566901]\n",
      "epoch:14 step:11104 [D loss: 0.099650, acc.: 99.22%] [G loss: 0.834285]\n",
      "epoch:14 step:11105 [D loss: 0.260965, acc.: 90.62%] [G loss: 4.261384]\n",
      "epoch:14 step:11106 [D loss: 0.239064, acc.: 89.84%] [G loss: 0.294072]\n",
      "epoch:14 step:11107 [D loss: 0.036822, acc.: 100.00%] [G loss: 3.291190]\n",
      "epoch:14 step:11108 [D loss: 0.321515, acc.: 86.72%] [G loss: 3.474609]\n",
      "epoch:14 step:11109 [D loss: 0.152728, acc.: 94.53%] [G loss: 1.646397]\n",
      "epoch:14 step:11110 [D loss: 0.260407, acc.: 90.62%] [G loss: 3.502719]\n",
      "epoch:14 step:11111 [D loss: 0.121792, acc.: 97.66%] [G loss: 2.433428]\n",
      "epoch:14 step:11112 [D loss: 0.288246, acc.: 90.62%] [G loss: 2.454755]\n",
      "epoch:14 step:11113 [D loss: 0.097914, acc.: 96.09%] [G loss: 2.309754]\n",
      "epoch:14 step:11114 [D loss: 0.054107, acc.: 100.00%] [G loss: 3.163838]\n",
      "epoch:14 step:11115 [D loss: 0.296139, acc.: 89.84%] [G loss: 2.760546]\n",
      "epoch:14 step:11116 [D loss: 0.103941, acc.: 98.44%] [G loss: 1.950786]\n",
      "epoch:14 step:11117 [D loss: 0.142765, acc.: 93.75%] [G loss: 1.019495]\n",
      "epoch:14 step:11118 [D loss: 0.110229, acc.: 98.44%] [G loss: 1.688725]\n",
      "epoch:14 step:11119 [D loss: 0.122581, acc.: 94.53%] [G loss: 1.880942]\n",
      "epoch:14 step:11120 [D loss: 0.080665, acc.: 100.00%] [G loss: 3.540151]\n",
      "epoch:14 step:11121 [D loss: 0.104957, acc.: 97.66%] [G loss: 1.402800]\n",
      "epoch:14 step:11122 [D loss: 0.074909, acc.: 100.00%] [G loss: 1.729656]\n",
      "epoch:14 step:11123 [D loss: 0.058096, acc.: 100.00%] [G loss: 1.978718]\n",
      "epoch:14 step:11124 [D loss: 0.259371, acc.: 89.84%] [G loss: 2.467658]\n",
      "epoch:14 step:11125 [D loss: 0.061095, acc.: 99.22%] [G loss: 2.919754]\n",
      "epoch:14 step:11126 [D loss: 0.178314, acc.: 93.75%] [G loss: 0.792825]\n",
      "epoch:14 step:11127 [D loss: 0.309624, acc.: 84.38%] [G loss: 5.590760]\n",
      "epoch:14 step:11128 [D loss: 0.589488, acc.: 68.75%] [G loss: 2.227842]\n",
      "epoch:14 step:11129 [D loss: 0.078555, acc.: 97.66%] [G loss: 2.992685]\n",
      "epoch:14 step:11130 [D loss: 0.059370, acc.: 99.22%] [G loss: 1.152820]\n",
      "epoch:14 step:11131 [D loss: 0.029461, acc.: 100.00%] [G loss: 3.839705]\n",
      "epoch:14 step:11132 [D loss: 0.027803, acc.: 100.00%] [G loss: 0.028717]\n",
      "epoch:14 step:11133 [D loss: 0.056333, acc.: 100.00%] [G loss: 3.737574]\n",
      "epoch:14 step:11134 [D loss: 0.042521, acc.: 99.22%] [G loss: 3.517470]\n",
      "epoch:14 step:11135 [D loss: 0.030347, acc.: 98.44%] [G loss: 3.910408]\n",
      "epoch:14 step:11136 [D loss: 0.124826, acc.: 96.09%] [G loss: 2.503880]\n",
      "epoch:14 step:11137 [D loss: 0.018909, acc.: 100.00%] [G loss: 0.134129]\n",
      "epoch:14 step:11138 [D loss: 0.065643, acc.: 100.00%] [G loss: 0.075912]\n",
      "epoch:14 step:11139 [D loss: 0.998066, acc.: 50.78%] [G loss: 7.033526]\n",
      "epoch:14 step:11140 [D loss: 0.748250, acc.: 66.41%] [G loss: 2.826602]\n",
      "epoch:14 step:11141 [D loss: 0.079632, acc.: 96.88%] [G loss: 0.054941]\n",
      "epoch:14 step:11142 [D loss: 0.040449, acc.: 100.00%] [G loss: 0.141130]\n",
      "epoch:14 step:11143 [D loss: 0.053808, acc.: 98.44%] [G loss: 0.686339]\n",
      "epoch:14 step:11144 [D loss: 0.302445, acc.: 85.94%] [G loss: 0.869933]\n",
      "epoch:14 step:11145 [D loss: 0.012101, acc.: 100.00%] [G loss: 0.854360]\n",
      "epoch:14 step:11146 [D loss: 0.104108, acc.: 97.66%] [G loss: 4.332798]\n",
      "epoch:14 step:11147 [D loss: 0.155696, acc.: 96.09%] [G loss: 0.776395]\n",
      "epoch:14 step:11148 [D loss: 0.214326, acc.: 94.53%] [G loss: 1.624213]\n",
      "epoch:14 step:11149 [D loss: 0.032570, acc.: 99.22%] [G loss: 2.321460]\n",
      "epoch:14 step:11150 [D loss: 0.193539, acc.: 93.75%] [G loss: 0.680956]\n",
      "epoch:14 step:11151 [D loss: 0.442450, acc.: 85.94%] [G loss: 5.194186]\n",
      "epoch:14 step:11152 [D loss: 0.285602, acc.: 84.38%] [G loss: 0.174432]\n",
      "epoch:14 step:11153 [D loss: 0.036569, acc.: 99.22%] [G loss: 0.195646]\n",
      "epoch:14 step:11154 [D loss: 0.061687, acc.: 99.22%] [G loss: 2.745563]\n",
      "epoch:14 step:11155 [D loss: 0.012875, acc.: 100.00%] [G loss: 2.111847]\n",
      "epoch:14 step:11156 [D loss: 0.027237, acc.: 100.00%] [G loss: 0.463499]\n",
      "epoch:14 step:11157 [D loss: 0.080019, acc.: 98.44%] [G loss: 0.305013]\n",
      "epoch:14 step:11158 [D loss: 0.078127, acc.: 98.44%] [G loss: 0.126133]\n",
      "epoch:14 step:11159 [D loss: 0.017230, acc.: 100.00%] [G loss: 0.671653]\n",
      "epoch:14 step:11160 [D loss: 0.026325, acc.: 99.22%] [G loss: 0.233188]\n",
      "epoch:14 step:11161 [D loss: 0.016046, acc.: 100.00%] [G loss: 0.147188]\n",
      "epoch:14 step:11162 [D loss: 0.049601, acc.: 99.22%] [G loss: 0.165253]\n",
      "epoch:14 step:11163 [D loss: 0.017415, acc.: 100.00%] [G loss: 0.725901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11164 [D loss: 0.044077, acc.: 100.00%] [G loss: 0.183002]\n",
      "epoch:14 step:11165 [D loss: 0.024877, acc.: 100.00%] [G loss: 0.174998]\n",
      "epoch:14 step:11166 [D loss: 0.084358, acc.: 99.22%] [G loss: 0.391070]\n",
      "epoch:14 step:11167 [D loss: 0.031940, acc.: 100.00%] [G loss: 0.042524]\n",
      "epoch:14 step:11168 [D loss: 0.033211, acc.: 100.00%] [G loss: 1.023414]\n",
      "epoch:14 step:11169 [D loss: 0.177405, acc.: 95.31%] [G loss: 0.972629]\n",
      "epoch:14 step:11170 [D loss: 0.180629, acc.: 93.75%] [G loss: 0.938320]\n",
      "epoch:14 step:11171 [D loss: 0.028438, acc.: 100.00%] [G loss: 1.798945]\n",
      "epoch:14 step:11172 [D loss: 0.052032, acc.: 100.00%] [G loss: 0.879355]\n",
      "epoch:14 step:11173 [D loss: 0.014369, acc.: 100.00%] [G loss: 0.357380]\n",
      "epoch:14 step:11174 [D loss: 0.115091, acc.: 95.31%] [G loss: 0.001151]\n",
      "epoch:14 step:11175 [D loss: 0.245341, acc.: 87.50%] [G loss: 3.935884]\n",
      "epoch:14 step:11176 [D loss: 0.092160, acc.: 96.88%] [G loss: 0.053894]\n",
      "epoch:14 step:11177 [D loss: 0.014407, acc.: 100.00%] [G loss: 5.597133]\n",
      "epoch:14 step:11178 [D loss: 0.011151, acc.: 100.00%] [G loss: 5.793882]\n",
      "epoch:14 step:11179 [D loss: 0.004002, acc.: 100.00%] [G loss: 0.000846]\n",
      "epoch:14 step:11180 [D loss: 0.061032, acc.: 98.44%] [G loss: 0.000110]\n",
      "epoch:14 step:11181 [D loss: 0.020311, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:14 step:11182 [D loss: 0.018243, acc.: 100.00%] [G loss: 3.307305]\n",
      "epoch:14 step:11183 [D loss: 0.007205, acc.: 100.00%] [G loss: 0.000807]\n",
      "epoch:14 step:11184 [D loss: 0.044758, acc.: 99.22%] [G loss: 0.000402]\n",
      "epoch:14 step:11185 [D loss: 0.040229, acc.: 100.00%] [G loss: 0.001452]\n",
      "epoch:14 step:11186 [D loss: 0.004204, acc.: 100.00%] [G loss: 0.014211]\n",
      "epoch:14 step:11187 [D loss: 0.007211, acc.: 100.00%] [G loss: 0.027165]\n",
      "epoch:14 step:11188 [D loss: 0.014171, acc.: 100.00%] [G loss: 4.152157]\n",
      "epoch:14 step:11189 [D loss: 0.006188, acc.: 100.00%] [G loss: 5.194732]\n",
      "epoch:14 step:11190 [D loss: 0.006567, acc.: 100.00%] [G loss: 2.538770]\n",
      "epoch:14 step:11191 [D loss: 0.014070, acc.: 100.00%] [G loss: 2.088938]\n",
      "epoch:14 step:11192 [D loss: 0.037882, acc.: 100.00%] [G loss: 0.522160]\n",
      "epoch:14 step:11193 [D loss: 0.178393, acc.: 92.19%] [G loss: 5.317715]\n",
      "epoch:14 step:11194 [D loss: 0.238416, acc.: 85.94%] [G loss: 0.000116]\n",
      "epoch:14 step:11195 [D loss: 0.017723, acc.: 100.00%] [G loss: 0.002344]\n",
      "epoch:14 step:11196 [D loss: 0.018578, acc.: 100.00%] [G loss: 0.001935]\n",
      "epoch:14 step:11197 [D loss: 0.002091, acc.: 100.00%] [G loss: 5.105720]\n",
      "epoch:14 step:11198 [D loss: 0.006293, acc.: 100.00%] [G loss: 4.643230]\n",
      "epoch:14 step:11199 [D loss: 0.023109, acc.: 100.00%] [G loss: 3.643067]\n",
      "epoch:14 step:11200 [D loss: 0.007985, acc.: 100.00%] [G loss: 0.496748]\n",
      "epoch:14 step:11201 [D loss: 0.089929, acc.: 98.44%] [G loss: 0.000030]\n",
      "epoch:14 step:11202 [D loss: 0.054341, acc.: 98.44%] [G loss: 0.000297]\n",
      "epoch:14 step:11203 [D loss: 0.003384, acc.: 100.00%] [G loss: 5.453319]\n",
      "epoch:14 step:11204 [D loss: 0.022928, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:14 step:11205 [D loss: 0.007742, acc.: 100.00%] [G loss: 5.294048]\n",
      "epoch:14 step:11206 [D loss: 0.008204, acc.: 100.00%] [G loss: 1.089688]\n",
      "epoch:14 step:11207 [D loss: 0.097070, acc.: 96.88%] [G loss: 0.009453]\n",
      "epoch:14 step:11208 [D loss: 0.219571, acc.: 89.84%] [G loss: 0.000090]\n",
      "epoch:14 step:11209 [D loss: 1.181735, acc.: 63.28%] [G loss: 8.688483]\n",
      "epoch:14 step:11210 [D loss: 2.841902, acc.: 50.00%] [G loss: 4.998687]\n",
      "epoch:14 step:11211 [D loss: 1.164434, acc.: 64.06%] [G loss: 0.369654]\n",
      "epoch:14 step:11212 [D loss: 0.114745, acc.: 94.53%] [G loss: 0.623855]\n",
      "epoch:14 step:11213 [D loss: 0.027168, acc.: 99.22%] [G loss: 0.608478]\n",
      "epoch:14 step:11214 [D loss: 0.027275, acc.: 100.00%] [G loss: 0.096791]\n",
      "epoch:14 step:11215 [D loss: 0.068676, acc.: 98.44%] [G loss: 0.277794]\n",
      "epoch:14 step:11216 [D loss: 0.068626, acc.: 100.00%] [G loss: 0.161587]\n",
      "epoch:14 step:11217 [D loss: 0.196369, acc.: 92.97%] [G loss: 0.543814]\n",
      "epoch:14 step:11218 [D loss: 0.047793, acc.: 99.22%] [G loss: 6.840008]\n",
      "epoch:14 step:11219 [D loss: 0.069500, acc.: 98.44%] [G loss: 0.829826]\n",
      "epoch:14 step:11220 [D loss: 0.021539, acc.: 100.00%] [G loss: 4.850184]\n",
      "epoch:14 step:11221 [D loss: 0.623863, acc.: 66.41%] [G loss: 7.482934]\n",
      "epoch:14 step:11222 [D loss: 1.080558, acc.: 56.25%] [G loss: 1.893155]\n",
      "epoch:14 step:11223 [D loss: 0.528897, acc.: 72.66%] [G loss: 0.031749]\n",
      "epoch:14 step:11224 [D loss: 0.665932, acc.: 75.78%] [G loss: 0.785045]\n",
      "epoch:14 step:11225 [D loss: 0.184603, acc.: 93.75%] [G loss: 5.729263]\n",
      "epoch:14 step:11226 [D loss: 0.183989, acc.: 90.62%] [G loss: 0.113148]\n",
      "epoch:14 step:11227 [D loss: 0.063503, acc.: 100.00%] [G loss: 0.064942]\n",
      "epoch:14 step:11228 [D loss: 0.096896, acc.: 97.66%] [G loss: 0.180867]\n",
      "epoch:14 step:11229 [D loss: 0.099896, acc.: 96.88%] [G loss: 5.400312]\n",
      "epoch:14 step:11230 [D loss: 0.067655, acc.: 99.22%] [G loss: 5.123466]\n",
      "epoch:14 step:11231 [D loss: 0.177384, acc.: 94.53%] [G loss: 0.057586]\n",
      "epoch:14 step:11232 [D loss: 0.173857, acc.: 92.19%] [G loss: 0.863164]\n",
      "epoch:14 step:11233 [D loss: 0.126202, acc.: 94.53%] [G loss: 0.570073]\n",
      "epoch:14 step:11234 [D loss: 0.176291, acc.: 95.31%] [G loss: 1.336524]\n",
      "epoch:14 step:11235 [D loss: 0.058641, acc.: 99.22%] [G loss: 0.722011]\n",
      "epoch:14 step:11236 [D loss: 0.129820, acc.: 98.44%] [G loss: 0.299938]\n",
      "epoch:14 step:11237 [D loss: 0.039625, acc.: 99.22%] [G loss: 0.718165]\n",
      "epoch:14 step:11238 [D loss: 0.181147, acc.: 92.97%] [G loss: 5.950791]\n",
      "epoch:14 step:11239 [D loss: 0.226784, acc.: 90.62%] [G loss: 0.503195]\n",
      "epoch:14 step:11240 [D loss: 0.175676, acc.: 95.31%] [G loss: 4.801529]\n",
      "epoch:14 step:11241 [D loss: 0.092852, acc.: 96.88%] [G loss: 1.256175]\n",
      "epoch:14 step:11242 [D loss: 0.034021, acc.: 100.00%] [G loss: 3.524499]\n",
      "epoch:14 step:11243 [D loss: 0.086361, acc.: 98.44%] [G loss: 1.073667]\n",
      "epoch:14 step:11244 [D loss: 0.046923, acc.: 100.00%] [G loss: 1.404460]\n",
      "epoch:14 step:11245 [D loss: 0.253011, acc.: 91.41%] [G loss: 3.887804]\n",
      "epoch:14 step:11246 [D loss: 0.423825, acc.: 78.91%] [G loss: 1.359586]\n",
      "epoch:14 step:11247 [D loss: 0.163657, acc.: 96.88%] [G loss: 1.258794]\n",
      "epoch:14 step:11248 [D loss: 0.022245, acc.: 100.00%] [G loss: 2.257670]\n",
      "epoch:14 step:11249 [D loss: 0.689660, acc.: 72.66%] [G loss: 3.187190]\n",
      "epoch:14 step:11250 [D loss: 0.094092, acc.: 95.31%] [G loss: 4.583825]\n",
      "epoch:14 step:11251 [D loss: 0.149630, acc.: 92.19%] [G loss: 3.948789]\n",
      "epoch:14 step:11252 [D loss: 0.111579, acc.: 95.31%] [G loss: 2.336157]\n",
      "epoch:14 step:11253 [D loss: 0.017034, acc.: 99.22%] [G loss: 2.729161]\n",
      "epoch:14 step:11254 [D loss: 0.365899, acc.: 81.25%] [G loss: 4.521995]\n",
      "epoch:14 step:11255 [D loss: 0.249684, acc.: 90.62%] [G loss: 5.106796]\n",
      "epoch:14 step:11256 [D loss: 0.070072, acc.: 99.22%] [G loss: 3.182064]\n",
      "epoch:14 step:11257 [D loss: 0.043673, acc.: 100.00%] [G loss: 4.184990]\n",
      "epoch:14 step:11258 [D loss: 0.168182, acc.: 92.97%] [G loss: 0.833870]\n",
      "epoch:14 step:11259 [D loss: 0.055150, acc.: 99.22%] [G loss: 4.437467]\n",
      "epoch:14 step:11260 [D loss: 0.062456, acc.: 98.44%] [G loss: 4.556125]\n",
      "epoch:14 step:11261 [D loss: 0.061779, acc.: 99.22%] [G loss: 2.026688]\n",
      "epoch:14 step:11262 [D loss: 0.042637, acc.: 100.00%] [G loss: 3.964484]\n",
      "epoch:14 step:11263 [D loss: 0.035831, acc.: 100.00%] [G loss: 1.354951]\n",
      "epoch:14 step:11264 [D loss: 0.466525, acc.: 82.03%] [G loss: 4.956108]\n",
      "epoch:14 step:11265 [D loss: 0.695092, acc.: 64.84%] [G loss: 4.211260]\n",
      "epoch:14 step:11266 [D loss: 0.044799, acc.: 99.22%] [G loss: 3.151815]\n",
      "epoch:14 step:11267 [D loss: 0.016915, acc.: 100.00%] [G loss: 4.263625]\n",
      "epoch:14 step:11268 [D loss: 0.058162, acc.: 98.44%] [G loss: 3.657906]\n",
      "epoch:14 step:11269 [D loss: 0.027802, acc.: 100.00%] [G loss: 2.445210]\n",
      "epoch:14 step:11270 [D loss: 0.102165, acc.: 98.44%] [G loss: 3.415329]\n",
      "epoch:14 step:11271 [D loss: 0.026724, acc.: 100.00%] [G loss: 2.297290]\n",
      "epoch:14 step:11272 [D loss: 0.068451, acc.: 100.00%] [G loss: 3.293187]\n",
      "epoch:14 step:11273 [D loss: 0.069342, acc.: 98.44%] [G loss: 2.888492]\n",
      "epoch:14 step:11274 [D loss: 0.099773, acc.: 95.31%] [G loss: 2.653135]\n",
      "epoch:14 step:11275 [D loss: 0.031009, acc.: 100.00%] [G loss: 1.991895]\n",
      "epoch:14 step:11276 [D loss: 0.040751, acc.: 99.22%] [G loss: 1.532821]\n",
      "epoch:14 step:11277 [D loss: 0.072177, acc.: 97.66%] [G loss: 1.112697]\n",
      "epoch:14 step:11278 [D loss: 0.049299, acc.: 100.00%] [G loss: 3.203088]\n",
      "epoch:14 step:11279 [D loss: 0.114080, acc.: 96.88%] [G loss: 3.832551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11280 [D loss: 0.031207, acc.: 100.00%] [G loss: 2.262939]\n",
      "epoch:14 step:11281 [D loss: 0.144864, acc.: 93.75%] [G loss: 1.551409]\n",
      "epoch:14 step:11282 [D loss: 0.005917, acc.: 100.00%] [G loss: 3.008674]\n",
      "epoch:14 step:11283 [D loss: 0.034956, acc.: 99.22%] [G loss: 2.394944]\n",
      "epoch:14 step:11284 [D loss: 0.108320, acc.: 96.09%] [G loss: 0.075860]\n",
      "epoch:14 step:11285 [D loss: 0.367657, acc.: 83.59%] [G loss: 2.279300]\n",
      "epoch:14 step:11286 [D loss: 0.190422, acc.: 89.06%] [G loss: 0.792729]\n",
      "epoch:14 step:11287 [D loss: 0.138702, acc.: 92.97%] [G loss: 0.013955]\n",
      "epoch:14 step:11288 [D loss: 0.011639, acc.: 100.00%] [G loss: 4.660202]\n",
      "epoch:14 step:11289 [D loss: 0.007061, acc.: 100.00%] [G loss: 0.004692]\n",
      "epoch:14 step:11290 [D loss: 0.036867, acc.: 99.22%] [G loss: 1.838663]\n",
      "epoch:14 step:11291 [D loss: 0.069386, acc.: 98.44%] [G loss: 0.022295]\n",
      "epoch:14 step:11292 [D loss: 0.005509, acc.: 100.00%] [G loss: 0.310240]\n",
      "epoch:14 step:11293 [D loss: 0.002171, acc.: 100.00%] [G loss: 0.361106]\n",
      "epoch:14 step:11294 [D loss: 0.010086, acc.: 100.00%] [G loss: 0.296188]\n",
      "epoch:14 step:11295 [D loss: 0.118256, acc.: 95.31%] [G loss: 0.425713]\n",
      "epoch:14 step:11296 [D loss: 0.013189, acc.: 100.00%] [G loss: 1.269488]\n",
      "epoch:14 step:11297 [D loss: 0.070456, acc.: 96.88%] [G loss: 0.309095]\n",
      "epoch:14 step:11298 [D loss: 0.057669, acc.: 99.22%] [G loss: 0.303260]\n",
      "epoch:14 step:11299 [D loss: 0.015468, acc.: 99.22%] [G loss: 0.439505]\n",
      "epoch:14 step:11300 [D loss: 0.030488, acc.: 100.00%] [G loss: 2.185900]\n",
      "epoch:14 step:11301 [D loss: 0.027318, acc.: 100.00%] [G loss: 1.518970]\n",
      "epoch:14 step:11302 [D loss: 0.003051, acc.: 100.00%] [G loss: 1.089684]\n",
      "epoch:14 step:11303 [D loss: 0.077931, acc.: 96.88%] [G loss: 6.791236]\n",
      "epoch:14 step:11304 [D loss: 0.138311, acc.: 95.31%] [G loss: 3.365897]\n",
      "epoch:14 step:11305 [D loss: 0.007606, acc.: 100.00%] [G loss: 4.687799]\n",
      "epoch:14 step:11306 [D loss: 0.013071, acc.: 100.00%] [G loss: 4.423652]\n",
      "epoch:14 step:11307 [D loss: 0.056185, acc.: 97.66%] [G loss: 4.978360]\n",
      "epoch:14 step:11308 [D loss: 0.021427, acc.: 100.00%] [G loss: 4.452220]\n",
      "epoch:14 step:11309 [D loss: 0.178256, acc.: 92.97%] [G loss: 4.347572]\n",
      "epoch:14 step:11310 [D loss: 0.007369, acc.: 100.00%] [G loss: 5.282642]\n",
      "epoch:14 step:11311 [D loss: 0.004031, acc.: 100.00%] [G loss: 5.567085]\n",
      "epoch:14 step:11312 [D loss: 0.016107, acc.: 100.00%] [G loss: 4.837319]\n",
      "epoch:14 step:11313 [D loss: 0.010837, acc.: 100.00%] [G loss: 6.196770]\n",
      "epoch:14 step:11314 [D loss: 0.010089, acc.: 100.00%] [G loss: 4.302563]\n",
      "epoch:14 step:11315 [D loss: 0.016254, acc.: 100.00%] [G loss: 3.951218]\n",
      "epoch:14 step:11316 [D loss: 0.029253, acc.: 100.00%] [G loss: 3.121913]\n",
      "epoch:14 step:11317 [D loss: 0.016481, acc.: 99.22%] [G loss: 3.861259]\n",
      "epoch:14 step:11318 [D loss: 0.012648, acc.: 100.00%] [G loss: 2.869927]\n",
      "epoch:14 step:11319 [D loss: 0.024676, acc.: 100.00%] [G loss: 5.974545]\n",
      "epoch:14 step:11320 [D loss: 0.026809, acc.: 99.22%] [G loss: 6.017193]\n",
      "epoch:14 step:11321 [D loss: 0.044634, acc.: 100.00%] [G loss: 4.246785]\n",
      "epoch:14 step:11322 [D loss: 0.018682, acc.: 100.00%] [G loss: 4.952913]\n",
      "epoch:14 step:11323 [D loss: 0.022296, acc.: 100.00%] [G loss: 5.757339]\n",
      "epoch:14 step:11324 [D loss: 0.021761, acc.: 100.00%] [G loss: 6.800508]\n",
      "epoch:14 step:11325 [D loss: 0.008840, acc.: 100.00%] [G loss: 1.176986]\n",
      "epoch:14 step:11326 [D loss: 0.780750, acc.: 64.84%] [G loss: 10.875149]\n",
      "epoch:14 step:11327 [D loss: 3.892860, acc.: 50.00%] [G loss: 5.769170]\n",
      "epoch:14 step:11328 [D loss: 1.189383, acc.: 53.91%] [G loss: 0.629184]\n",
      "epoch:14 step:11329 [D loss: 0.275587, acc.: 91.41%] [G loss: 0.961774]\n",
      "epoch:14 step:11330 [D loss: 0.055258, acc.: 98.44%] [G loss: 1.555085]\n",
      "epoch:14 step:11331 [D loss: 0.427223, acc.: 81.25%] [G loss: 0.676758]\n",
      "epoch:14 step:11332 [D loss: 0.074870, acc.: 99.22%] [G loss: 1.156695]\n",
      "epoch:14 step:11333 [D loss: 0.047439, acc.: 100.00%] [G loss: 4.789329]\n",
      "epoch:14 step:11334 [D loss: 0.113068, acc.: 96.88%] [G loss: 0.888965]\n",
      "epoch:14 step:11335 [D loss: 0.041706, acc.: 100.00%] [G loss: 1.462641]\n",
      "epoch:14 step:11336 [D loss: 0.136945, acc.: 96.09%] [G loss: 1.052717]\n",
      "epoch:14 step:11337 [D loss: 0.147181, acc.: 96.09%] [G loss: 2.530067]\n",
      "epoch:14 step:11338 [D loss: 0.062362, acc.: 97.66%] [G loss: 2.483982]\n",
      "epoch:14 step:11339 [D loss: 0.142599, acc.: 96.09%] [G loss: 1.289231]\n",
      "epoch:14 step:11340 [D loss: 0.140104, acc.: 96.88%] [G loss: 2.948429]\n",
      "epoch:14 step:11341 [D loss: 0.038661, acc.: 99.22%] [G loss: 3.582716]\n",
      "epoch:14 step:11342 [D loss: 0.080525, acc.: 98.44%] [G loss: 2.574428]\n",
      "epoch:14 step:11343 [D loss: 0.387809, acc.: 80.47%] [G loss: 4.790925]\n",
      "epoch:14 step:11344 [D loss: 0.165677, acc.: 92.19%] [G loss: 4.110205]\n",
      "epoch:14 step:11345 [D loss: 0.161272, acc.: 94.53%] [G loss: 4.350425]\n",
      "epoch:14 step:11346 [D loss: 0.019732, acc.: 100.00%] [G loss: 5.039126]\n",
      "epoch:14 step:11347 [D loss: 0.073464, acc.: 97.66%] [G loss: 4.170530]\n",
      "epoch:14 step:11348 [D loss: 0.027801, acc.: 100.00%] [G loss: 4.267132]\n",
      "epoch:14 step:11349 [D loss: 0.033688, acc.: 100.00%] [G loss: 3.733834]\n",
      "epoch:14 step:11350 [D loss: 0.022846, acc.: 100.00%] [G loss: 4.311096]\n",
      "epoch:14 step:11351 [D loss: 0.053422, acc.: 98.44%] [G loss: 1.149269]\n",
      "epoch:14 step:11352 [D loss: 0.054696, acc.: 100.00%] [G loss: 4.117932]\n",
      "epoch:14 step:11353 [D loss: 0.015798, acc.: 100.00%] [G loss: 3.678226]\n",
      "epoch:14 step:11354 [D loss: 0.106194, acc.: 96.09%] [G loss: 0.197573]\n",
      "epoch:14 step:11355 [D loss: 0.067343, acc.: 98.44%] [G loss: 0.022918]\n",
      "epoch:14 step:11356 [D loss: 0.148174, acc.: 96.88%] [G loss: 5.691923]\n",
      "epoch:14 step:11357 [D loss: 0.009913, acc.: 100.00%] [G loss: 5.949511]\n",
      "epoch:14 step:11358 [D loss: 0.081241, acc.: 97.66%] [G loss: 4.832840]\n",
      "epoch:14 step:11359 [D loss: 0.015088, acc.: 100.00%] [G loss: 4.470958]\n",
      "epoch:14 step:11360 [D loss: 0.013680, acc.: 100.00%] [G loss: 4.356559]\n",
      "epoch:14 step:11361 [D loss: 0.011962, acc.: 100.00%] [G loss: 4.380052]\n",
      "epoch:14 step:11362 [D loss: 0.020318, acc.: 100.00%] [G loss: 4.137661]\n",
      "epoch:14 step:11363 [D loss: 0.024541, acc.: 100.00%] [G loss: 4.147596]\n",
      "epoch:14 step:11364 [D loss: 0.051012, acc.: 99.22%] [G loss: 2.784097]\n",
      "epoch:14 step:11365 [D loss: 0.191826, acc.: 94.53%] [G loss: 4.300723]\n",
      "epoch:14 step:11366 [D loss: 0.015324, acc.: 100.00%] [G loss: 5.106983]\n",
      "epoch:14 step:11367 [D loss: 0.069612, acc.: 96.09%] [G loss: 4.191101]\n",
      "epoch:14 step:11368 [D loss: 0.042007, acc.: 98.44%] [G loss: 4.787227]\n",
      "epoch:14 step:11369 [D loss: 0.017065, acc.: 100.00%] [G loss: 4.963215]\n",
      "epoch:14 step:11370 [D loss: 0.023187, acc.: 100.00%] [G loss: 3.505788]\n",
      "epoch:14 step:11371 [D loss: 0.018224, acc.: 100.00%] [G loss: 4.680615]\n",
      "epoch:14 step:11372 [D loss: 0.024140, acc.: 100.00%] [G loss: 5.175118]\n",
      "epoch:14 step:11373 [D loss: 0.054779, acc.: 99.22%] [G loss: 5.242475]\n",
      "epoch:14 step:11374 [D loss: 0.010105, acc.: 100.00%] [G loss: 5.342357]\n",
      "epoch:14 step:11375 [D loss: 0.039084, acc.: 99.22%] [G loss: 4.841854]\n",
      "epoch:14 step:11376 [D loss: 0.013980, acc.: 100.00%] [G loss: 4.975590]\n",
      "epoch:14 step:11377 [D loss: 0.017892, acc.: 100.00%] [G loss: 5.278790]\n",
      "epoch:14 step:11378 [D loss: 0.023241, acc.: 99.22%] [G loss: 5.632420]\n",
      "epoch:14 step:11379 [D loss: 0.011320, acc.: 100.00%] [G loss: 0.114635]\n",
      "epoch:14 step:11380 [D loss: 0.014134, acc.: 100.00%] [G loss: 5.252309]\n",
      "epoch:14 step:11381 [D loss: 0.036024, acc.: 100.00%] [G loss: 5.867994]\n",
      "epoch:14 step:11382 [D loss: 0.028543, acc.: 99.22%] [G loss: 0.053154]\n",
      "epoch:14 step:11383 [D loss: 0.108344, acc.: 94.53%] [G loss: 6.706811]\n",
      "epoch:14 step:11384 [D loss: 0.053787, acc.: 99.22%] [G loss: 6.177171]\n",
      "epoch:14 step:11385 [D loss: 0.067574, acc.: 98.44%] [G loss: 5.576408]\n",
      "epoch:14 step:11386 [D loss: 0.020832, acc.: 100.00%] [G loss: 4.814381]\n",
      "epoch:14 step:11387 [D loss: 0.026204, acc.: 100.00%] [G loss: 4.693053]\n",
      "epoch:14 step:11388 [D loss: 0.008592, acc.: 100.00%] [G loss: 6.118685]\n",
      "epoch:14 step:11389 [D loss: 0.006550, acc.: 100.00%] [G loss: 5.504529]\n",
      "epoch:14 step:11390 [D loss: 0.003856, acc.: 100.00%] [G loss: 5.119946]\n",
      "epoch:14 step:11391 [D loss: 0.003270, acc.: 100.00%] [G loss: 4.887943]\n",
      "epoch:14 step:11392 [D loss: 0.001679, acc.: 100.00%] [G loss: 5.593978]\n",
      "epoch:14 step:11393 [D loss: 0.077807, acc.: 98.44%] [G loss: 3.651222]\n",
      "epoch:14 step:11394 [D loss: 0.106759, acc.: 98.44%] [G loss: 5.377158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11395 [D loss: 0.003521, acc.: 100.00%] [G loss: 6.409871]\n",
      "epoch:14 step:11396 [D loss: 0.022454, acc.: 99.22%] [G loss: 5.983857]\n",
      "epoch:14 step:11397 [D loss: 0.001464, acc.: 100.00%] [G loss: 6.524012]\n",
      "epoch:14 step:11398 [D loss: 0.006407, acc.: 100.00%] [G loss: 0.982826]\n",
      "epoch:14 step:11399 [D loss: 0.010886, acc.: 100.00%] [G loss: 5.308421]\n",
      "epoch:14 step:11400 [D loss: 0.033565, acc.: 100.00%] [G loss: 6.289124]\n",
      "epoch:14 step:11401 [D loss: 0.013436, acc.: 100.00%] [G loss: 6.050509]\n",
      "epoch:14 step:11402 [D loss: 0.049235, acc.: 100.00%] [G loss: 4.685472]\n",
      "epoch:14 step:11403 [D loss: 0.023354, acc.: 100.00%] [G loss: 4.964952]\n",
      "epoch:14 step:11404 [D loss: 0.005262, acc.: 100.00%] [G loss: 6.388921]\n",
      "epoch:14 step:11405 [D loss: 0.009810, acc.: 100.00%] [G loss: 5.086802]\n",
      "epoch:14 step:11406 [D loss: 0.008782, acc.: 100.00%] [G loss: 5.185696]\n",
      "epoch:14 step:11407 [D loss: 0.046719, acc.: 98.44%] [G loss: 5.982407]\n",
      "epoch:14 step:11408 [D loss: 0.124735, acc.: 95.31%] [G loss: 2.173983]\n",
      "epoch:14 step:11409 [D loss: 0.117691, acc.: 95.31%] [G loss: 4.101434]\n",
      "epoch:14 step:11410 [D loss: 0.003146, acc.: 100.00%] [G loss: 7.844571]\n",
      "epoch:14 step:11411 [D loss: 0.122887, acc.: 95.31%] [G loss: 5.026532]\n",
      "epoch:14 step:11412 [D loss: 0.445614, acc.: 79.69%] [G loss: 9.032536]\n",
      "epoch:14 step:11413 [D loss: 2.630481, acc.: 50.00%] [G loss: 2.328653]\n",
      "epoch:14 step:11414 [D loss: 0.241437, acc.: 89.84%] [G loss: 3.460901]\n",
      "epoch:14 step:11415 [D loss: 0.094072, acc.: 99.22%] [G loss: 4.015337]\n",
      "epoch:14 step:11416 [D loss: 0.353530, acc.: 83.59%] [G loss: 3.215403]\n",
      "epoch:14 step:11417 [D loss: 0.060643, acc.: 97.66%] [G loss: 2.482987]\n",
      "epoch:14 step:11418 [D loss: 0.114021, acc.: 97.66%] [G loss: 3.765433]\n",
      "epoch:14 step:11419 [D loss: 0.056972, acc.: 97.66%] [G loss: 4.000434]\n",
      "epoch:14 step:11420 [D loss: 0.084436, acc.: 97.66%] [G loss: 2.615054]\n",
      "epoch:14 step:11421 [D loss: 0.186260, acc.: 92.97%] [G loss: 4.415329]\n",
      "epoch:14 step:11422 [D loss: 0.042158, acc.: 99.22%] [G loss: 4.298142]\n",
      "epoch:14 step:11423 [D loss: 0.022373, acc.: 100.00%] [G loss: 0.862397]\n",
      "epoch:14 step:11424 [D loss: 0.107113, acc.: 97.66%] [G loss: 0.457838]\n",
      "epoch:14 step:11425 [D loss: 0.119475, acc.: 98.44%] [G loss: 0.143957]\n",
      "epoch:14 step:11426 [D loss: 1.493332, acc.: 47.66%] [G loss: 8.287504]\n",
      "epoch:14 step:11427 [D loss: 1.587713, acc.: 51.56%] [G loss: 5.265180]\n",
      "epoch:14 step:11428 [D loss: 0.303129, acc.: 85.16%] [G loss: 5.198542]\n",
      "epoch:14 step:11429 [D loss: 0.016459, acc.: 100.00%] [G loss: 1.601504]\n",
      "epoch:14 step:11430 [D loss: 0.027962, acc.: 100.00%] [G loss: 0.100175]\n",
      "epoch:14 step:11431 [D loss: 0.042802, acc.: 99.22%] [G loss: 3.884089]\n",
      "epoch:14 step:11432 [D loss: 0.029178, acc.: 100.00%] [G loss: 0.142040]\n",
      "epoch:14 step:11433 [D loss: 0.018362, acc.: 99.22%] [G loss: 3.502080]\n",
      "epoch:14 step:11434 [D loss: 0.011863, acc.: 100.00%] [G loss: 2.480601]\n",
      "epoch:14 step:11435 [D loss: 0.022979, acc.: 100.00%] [G loss: 0.173468]\n",
      "epoch:14 step:11436 [D loss: 0.035534, acc.: 100.00%] [G loss: 0.302569]\n",
      "epoch:14 step:11437 [D loss: 0.008603, acc.: 100.00%] [G loss: 0.350506]\n",
      "epoch:14 step:11438 [D loss: 0.028596, acc.: 100.00%] [G loss: 0.277079]\n",
      "epoch:14 step:11439 [D loss: 0.034382, acc.: 100.00%] [G loss: 0.261420]\n",
      "epoch:14 step:11440 [D loss: 0.010832, acc.: 100.00%] [G loss: 0.378801]\n",
      "epoch:14 step:11441 [D loss: 0.097181, acc.: 99.22%] [G loss: 0.372902]\n",
      "epoch:14 step:11442 [D loss: 0.035710, acc.: 100.00%] [G loss: 0.561711]\n",
      "epoch:14 step:11443 [D loss: 0.846213, acc.: 57.81%] [G loss: 4.112851]\n",
      "epoch:14 step:11444 [D loss: 0.426203, acc.: 81.25%] [G loss: 3.091088]\n",
      "epoch:14 step:11445 [D loss: 0.161453, acc.: 92.97%] [G loss: 1.054163]\n",
      "epoch:14 step:11446 [D loss: 0.102474, acc.: 94.53%] [G loss: 1.509770]\n",
      "epoch:14 step:11447 [D loss: 0.055718, acc.: 99.22%] [G loss: 1.891413]\n",
      "epoch:14 step:11448 [D loss: 0.030060, acc.: 100.00%] [G loss: 2.832076]\n",
      "epoch:14 step:11449 [D loss: 0.094003, acc.: 98.44%] [G loss: 3.625723]\n",
      "epoch:14 step:11450 [D loss: 0.071772, acc.: 98.44%] [G loss: 2.096647]\n",
      "epoch:14 step:11451 [D loss: 0.026985, acc.: 100.00%] [G loss: 3.032045]\n",
      "epoch:14 step:11452 [D loss: 0.027549, acc.: 100.00%] [G loss: 3.265099]\n",
      "epoch:14 step:11453 [D loss: 0.039772, acc.: 100.00%] [G loss: 2.455575]\n",
      "epoch:14 step:11454 [D loss: 0.109978, acc.: 98.44%] [G loss: 3.223346]\n",
      "epoch:14 step:11455 [D loss: 0.019416, acc.: 100.00%] [G loss: 4.239702]\n",
      "epoch:14 step:11456 [D loss: 0.117868, acc.: 96.88%] [G loss: 5.008192]\n",
      "epoch:14 step:11457 [D loss: 0.011299, acc.: 100.00%] [G loss: 2.559761]\n",
      "epoch:14 step:11458 [D loss: 0.021506, acc.: 100.00%] [G loss: 1.999749]\n",
      "epoch:14 step:11459 [D loss: 0.029376, acc.: 100.00%] [G loss: 3.564020]\n",
      "epoch:14 step:11460 [D loss: 0.057171, acc.: 98.44%] [G loss: 1.733331]\n",
      "epoch:14 step:11461 [D loss: 0.050896, acc.: 99.22%] [G loss: 2.323442]\n",
      "epoch:14 step:11462 [D loss: 0.020537, acc.: 100.00%] [G loss: 4.223847]\n",
      "epoch:14 step:11463 [D loss: 0.015200, acc.: 100.00%] [G loss: 4.016641]\n",
      "epoch:14 step:11464 [D loss: 0.007595, acc.: 100.00%] [G loss: 3.833476]\n",
      "epoch:14 step:11465 [D loss: 0.034223, acc.: 100.00%] [G loss: 3.226761]\n",
      "epoch:14 step:11466 [D loss: 0.010192, acc.: 100.00%] [G loss: 3.046780]\n",
      "epoch:14 step:11467 [D loss: 0.113741, acc.: 96.88%] [G loss: 2.601484]\n",
      "epoch:14 step:11468 [D loss: 0.039653, acc.: 100.00%] [G loss: 3.698888]\n",
      "epoch:14 step:11469 [D loss: 0.010996, acc.: 100.00%] [G loss: 4.369847]\n",
      "epoch:14 step:11470 [D loss: 0.214433, acc.: 90.62%] [G loss: 0.938723]\n",
      "epoch:14 step:11471 [D loss: 0.116308, acc.: 96.88%] [G loss: 3.892284]\n",
      "epoch:14 step:11472 [D loss: 0.003580, acc.: 100.00%] [G loss: 5.423067]\n",
      "epoch:14 step:11473 [D loss: 0.020445, acc.: 99.22%] [G loss: 4.619863]\n",
      "epoch:14 step:11474 [D loss: 0.009421, acc.: 100.00%] [G loss: 4.854496]\n",
      "epoch:14 step:11475 [D loss: 0.005458, acc.: 100.00%] [G loss: 3.759895]\n",
      "epoch:14 step:11476 [D loss: 0.051850, acc.: 98.44%] [G loss: 5.020491]\n",
      "epoch:14 step:11477 [D loss: 0.006775, acc.: 100.00%] [G loss: 4.954477]\n",
      "epoch:14 step:11478 [D loss: 0.005813, acc.: 100.00%] [G loss: 4.123869]\n",
      "epoch:14 step:11479 [D loss: 0.015308, acc.: 100.00%] [G loss: 4.867798]\n",
      "epoch:14 step:11480 [D loss: 0.010006, acc.: 100.00%] [G loss: 1.824950]\n",
      "epoch:14 step:11481 [D loss: 0.031053, acc.: 100.00%] [G loss: 3.421649]\n",
      "epoch:14 step:11482 [D loss: 0.073074, acc.: 98.44%] [G loss: 0.764504]\n",
      "epoch:14 step:11483 [D loss: 0.026602, acc.: 100.00%] [G loss: 5.681372]\n",
      "epoch:14 step:11484 [D loss: 0.043143, acc.: 100.00%] [G loss: 0.279149]\n",
      "epoch:14 step:11485 [D loss: 0.426492, acc.: 77.34%] [G loss: 8.416388]\n",
      "epoch:14 step:11486 [D loss: 2.421374, acc.: 50.00%] [G loss: 5.658832]\n",
      "epoch:14 step:11487 [D loss: 0.081730, acc.: 96.09%] [G loss: 2.680403]\n",
      "epoch:14 step:11488 [D loss: 0.037887, acc.: 98.44%] [G loss: 1.788263]\n",
      "epoch:14 step:11489 [D loss: 0.016194, acc.: 100.00%] [G loss: 2.437578]\n",
      "epoch:14 step:11490 [D loss: 0.024118, acc.: 100.00%] [G loss: 2.516462]\n",
      "epoch:14 step:11491 [D loss: 0.048910, acc.: 99.22%] [G loss: 1.814731]\n",
      "epoch:14 step:11492 [D loss: 0.121156, acc.: 96.09%] [G loss: 2.699479]\n",
      "epoch:14 step:11493 [D loss: 0.016854, acc.: 100.00%] [G loss: 3.074655]\n",
      "epoch:14 step:11494 [D loss: 0.138566, acc.: 95.31%] [G loss: 0.431940]\n",
      "epoch:14 step:11495 [D loss: 0.246504, acc.: 87.50%] [G loss: 4.001102]\n",
      "epoch:14 step:11496 [D loss: 0.126137, acc.: 96.09%] [G loss: 2.121910]\n",
      "epoch:14 step:11497 [D loss: 0.008700, acc.: 100.00%] [G loss: 1.682476]\n",
      "epoch:14 step:11498 [D loss: 0.012032, acc.: 100.00%] [G loss: 0.751415]\n",
      "epoch:14 step:11499 [D loss: 0.053314, acc.: 98.44%] [G loss: 0.463319]\n",
      "epoch:14 step:11500 [D loss: 0.078708, acc.: 99.22%] [G loss: 0.669433]\n",
      "epoch:14 step:11501 [D loss: 0.014527, acc.: 100.00%] [G loss: 0.633563]\n",
      "epoch:14 step:11502 [D loss: 0.011743, acc.: 100.00%] [G loss: 0.366102]\n",
      "epoch:14 step:11503 [D loss: 0.039961, acc.: 100.00%] [G loss: 1.021868]\n",
      "epoch:14 step:11504 [D loss: 0.066287, acc.: 99.22%] [G loss: 1.674396]\n",
      "epoch:14 step:11505 [D loss: 0.053230, acc.: 98.44%] [G loss: 2.220767]\n",
      "epoch:14 step:11506 [D loss: 0.015121, acc.: 99.22%] [G loss: 1.486861]\n",
      "epoch:14 step:11507 [D loss: 0.055043, acc.: 100.00%] [G loss: 4.594783]\n",
      "epoch:14 step:11508 [D loss: 0.002788, acc.: 100.00%] [G loss: 2.710779]\n",
      "epoch:14 step:11509 [D loss: 0.075300, acc.: 98.44%] [G loss: 1.901681]\n",
      "epoch:14 step:11510 [D loss: 0.010825, acc.: 100.00%] [G loss: 1.933984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11511 [D loss: 0.206916, acc.: 91.41%] [G loss: 5.512931]\n",
      "epoch:14 step:11512 [D loss: 0.186778, acc.: 91.41%] [G loss: 1.381525]\n",
      "epoch:14 step:11513 [D loss: 0.025330, acc.: 100.00%] [G loss: 1.162143]\n",
      "epoch:14 step:11514 [D loss: 0.040271, acc.: 100.00%] [G loss: 2.261484]\n",
      "epoch:14 step:11515 [D loss: 0.018769, acc.: 100.00%] [G loss: 4.760110]\n",
      "epoch:14 step:11516 [D loss: 0.014846, acc.: 100.00%] [G loss: 2.147328]\n",
      "epoch:14 step:11517 [D loss: 0.020981, acc.: 100.00%] [G loss: 2.981421]\n",
      "epoch:14 step:11518 [D loss: 0.010722, acc.: 100.00%] [G loss: 1.113241]\n",
      "epoch:14 step:11519 [D loss: 0.059909, acc.: 99.22%] [G loss: 2.948418]\n",
      "epoch:14 step:11520 [D loss: 0.008499, acc.: 100.00%] [G loss: 3.273265]\n",
      "epoch:14 step:11521 [D loss: 0.053408, acc.: 98.44%] [G loss: 4.044815]\n",
      "epoch:14 step:11522 [D loss: 0.028367, acc.: 99.22%] [G loss: 2.521033]\n",
      "epoch:14 step:11523 [D loss: 0.015637, acc.: 100.00%] [G loss: 2.148242]\n",
      "epoch:14 step:11524 [D loss: 0.027896, acc.: 100.00%] [G loss: 2.970282]\n",
      "epoch:14 step:11525 [D loss: 0.021801, acc.: 100.00%] [G loss: 2.383612]\n",
      "epoch:14 step:11526 [D loss: 0.012719, acc.: 100.00%] [G loss: 1.944629]\n",
      "epoch:14 step:11527 [D loss: 0.063047, acc.: 100.00%] [G loss: 2.697355]\n",
      "epoch:14 step:11528 [D loss: 0.018759, acc.: 100.00%] [G loss: 2.751989]\n",
      "epoch:14 step:11529 [D loss: 0.009315, acc.: 100.00%] [G loss: 2.035804]\n",
      "epoch:14 step:11530 [D loss: 0.025369, acc.: 100.00%] [G loss: 2.776022]\n",
      "epoch:14 step:11531 [D loss: 0.014619, acc.: 100.00%] [G loss: 2.652002]\n",
      "epoch:14 step:11532 [D loss: 0.007270, acc.: 100.00%] [G loss: 2.582244]\n",
      "epoch:14 step:11533 [D loss: 0.071729, acc.: 98.44%] [G loss: 0.943401]\n",
      "epoch:14 step:11534 [D loss: 0.032315, acc.: 99.22%] [G loss: 0.569546]\n",
      "epoch:14 step:11535 [D loss: 0.004279, acc.: 100.00%] [G loss: 3.958508]\n",
      "epoch:14 step:11536 [D loss: 0.014202, acc.: 100.00%] [G loss: 3.829196]\n",
      "epoch:14 step:11537 [D loss: 0.009807, acc.: 100.00%] [G loss: 0.500090]\n",
      "epoch:14 step:11538 [D loss: 0.689606, acc.: 68.75%] [G loss: 7.781274]\n",
      "epoch:14 step:11539 [D loss: 1.266478, acc.: 54.69%] [G loss: 2.795554]\n",
      "epoch:14 step:11540 [D loss: 0.005407, acc.: 100.00%] [G loss: 3.791531]\n",
      "epoch:14 step:11541 [D loss: 0.011292, acc.: 100.00%] [G loss: 0.862266]\n",
      "epoch:14 step:11542 [D loss: 0.014780, acc.: 100.00%] [G loss: 1.257521]\n",
      "epoch:14 step:11543 [D loss: 0.055175, acc.: 97.66%] [G loss: 1.145297]\n",
      "epoch:14 step:11544 [D loss: 0.067809, acc.: 99.22%] [G loss: 2.790011]\n",
      "epoch:14 step:11545 [D loss: 0.014737, acc.: 100.00%] [G loss: 5.084257]\n",
      "epoch:14 step:11546 [D loss: 0.026340, acc.: 99.22%] [G loss: 2.879471]\n",
      "epoch:14 step:11547 [D loss: 0.055951, acc.: 99.22%] [G loss: 0.747943]\n",
      "epoch:14 step:11548 [D loss: 0.143045, acc.: 94.53%] [G loss: 5.805664]\n",
      "epoch:14 step:11549 [D loss: 0.076415, acc.: 96.88%] [G loss: 4.177428]\n",
      "epoch:14 step:11550 [D loss: 0.224764, acc.: 92.19%] [G loss: 2.939327]\n",
      "epoch:14 step:11551 [D loss: 0.027446, acc.: 100.00%] [G loss: 1.797274]\n",
      "epoch:14 step:11552 [D loss: 0.008649, acc.: 100.00%] [G loss: 4.075451]\n",
      "epoch:14 step:11553 [D loss: 0.114900, acc.: 97.66%] [G loss: 4.353168]\n",
      "epoch:14 step:11554 [D loss: 0.037708, acc.: 100.00%] [G loss: 5.580708]\n",
      "epoch:14 step:11555 [D loss: 0.017356, acc.: 100.00%] [G loss: 5.550609]\n",
      "epoch:14 step:11556 [D loss: 0.025884, acc.: 99.22%] [G loss: 5.340349]\n",
      "epoch:14 step:11557 [D loss: 0.033906, acc.: 100.00%] [G loss: 0.712395]\n",
      "epoch:14 step:11558 [D loss: 0.005389, acc.: 100.00%] [G loss: 3.666868]\n",
      "epoch:14 step:11559 [D loss: 0.154791, acc.: 94.53%] [G loss: 6.781929]\n",
      "epoch:14 step:11560 [D loss: 0.094300, acc.: 95.31%] [G loss: 6.678074]\n",
      "epoch:14 step:11561 [D loss: 0.826268, acc.: 64.84%] [G loss: 7.746377]\n",
      "epoch:14 step:11562 [D loss: 0.290341, acc.: 82.81%] [G loss: 6.547962]\n",
      "epoch:14 step:11563 [D loss: 0.016721, acc.: 100.00%] [G loss: 5.455549]\n",
      "epoch:14 step:11564 [D loss: 0.015063, acc.: 100.00%] [G loss: 4.644696]\n",
      "epoch:14 step:11565 [D loss: 0.009154, acc.: 100.00%] [G loss: 5.896128]\n",
      "epoch:14 step:11566 [D loss: 0.003099, acc.: 100.00%] [G loss: 5.711752]\n",
      "epoch:14 step:11567 [D loss: 0.013599, acc.: 100.00%] [G loss: 0.824749]\n",
      "epoch:14 step:11568 [D loss: 0.004521, acc.: 100.00%] [G loss: 5.377545]\n",
      "epoch:14 step:11569 [D loss: 0.009718, acc.: 100.00%] [G loss: 4.993450]\n",
      "epoch:14 step:11570 [D loss: 0.035290, acc.: 100.00%] [G loss: 5.776786]\n",
      "epoch:14 step:11571 [D loss: 0.004592, acc.: 100.00%] [G loss: 5.706399]\n",
      "epoch:14 step:11572 [D loss: 0.025341, acc.: 100.00%] [G loss: 4.605222]\n",
      "epoch:14 step:11573 [D loss: 0.020160, acc.: 100.00%] [G loss: 1.094973]\n",
      "epoch:14 step:11574 [D loss: 0.041941, acc.: 98.44%] [G loss: 1.540821]\n",
      "epoch:14 step:11575 [D loss: 0.008423, acc.: 100.00%] [G loss: 5.772573]\n",
      "epoch:14 step:11576 [D loss: 0.006962, acc.: 100.00%] [G loss: 5.412778]\n",
      "epoch:14 step:11577 [D loss: 0.003996, acc.: 100.00%] [G loss: 0.126569]\n",
      "epoch:14 step:11578 [D loss: 0.061896, acc.: 99.22%] [G loss: 6.532493]\n",
      "epoch:14 step:11579 [D loss: 0.153391, acc.: 96.09%] [G loss: 7.274118]\n",
      "epoch:14 step:11580 [D loss: 1.453797, acc.: 37.50%] [G loss: 4.494187]\n",
      "epoch:14 step:11581 [D loss: 0.062269, acc.: 98.44%] [G loss: 6.269930]\n",
      "epoch:14 step:11582 [D loss: 0.054380, acc.: 99.22%] [G loss: 1.797518]\n",
      "epoch:14 step:11583 [D loss: 0.028265, acc.: 100.00%] [G loss: 0.691059]\n",
      "epoch:14 step:11584 [D loss: 0.029231, acc.: 99.22%] [G loss: 0.556170]\n",
      "epoch:14 step:11585 [D loss: 0.009485, acc.: 100.00%] [G loss: 0.413218]\n",
      "epoch:14 step:11586 [D loss: 0.036574, acc.: 100.00%] [G loss: 0.573280]\n",
      "epoch:14 step:11587 [D loss: 0.198727, acc.: 92.19%] [G loss: 6.052699]\n",
      "epoch:14 step:11588 [D loss: 2.185052, acc.: 24.22%] [G loss: 2.955431]\n",
      "epoch:14 step:11589 [D loss: 0.042198, acc.: 98.44%] [G loss: 5.979810]\n",
      "epoch:14 step:11590 [D loss: 0.946782, acc.: 61.72%] [G loss: 2.876539]\n",
      "epoch:14 step:11591 [D loss: 0.066271, acc.: 98.44%] [G loss: 0.495582]\n",
      "epoch:14 step:11592 [D loss: 0.044260, acc.: 98.44%] [G loss: 0.940671]\n",
      "epoch:14 step:11593 [D loss: 0.020364, acc.: 100.00%] [G loss: 0.321728]\n",
      "epoch:14 step:11594 [D loss: 0.014759, acc.: 100.00%] [G loss: 1.589154]\n",
      "epoch:14 step:11595 [D loss: 0.036305, acc.: 99.22%] [G loss: 1.104573]\n",
      "epoch:14 step:11596 [D loss: 0.007778, acc.: 100.00%] [G loss: 3.090860]\n",
      "epoch:14 step:11597 [D loss: 0.026428, acc.: 100.00%] [G loss: 0.637986]\n",
      "epoch:14 step:11598 [D loss: 0.081728, acc.: 96.88%] [G loss: 1.787124]\n",
      "epoch:14 step:11599 [D loss: 0.095573, acc.: 96.88%] [G loss: 0.978982]\n",
      "epoch:14 step:11600 [D loss: 0.009329, acc.: 100.00%] [G loss: 0.600581]\n",
      "epoch:14 step:11601 [D loss: 0.020382, acc.: 100.00%] [G loss: 0.766224]\n",
      "epoch:14 step:11602 [D loss: 0.021798, acc.: 99.22%] [G loss: 1.038462]\n",
      "epoch:14 step:11603 [D loss: 0.009070, acc.: 100.00%] [G loss: 1.634791]\n",
      "epoch:14 step:11604 [D loss: 0.010092, acc.: 100.00%] [G loss: 0.548707]\n",
      "epoch:14 step:11605 [D loss: 0.009296, acc.: 100.00%] [G loss: 0.197914]\n",
      "epoch:14 step:11606 [D loss: 0.006792, acc.: 100.00%] [G loss: 0.251608]\n",
      "epoch:14 step:11607 [D loss: 0.012831, acc.: 100.00%] [G loss: 0.062584]\n",
      "epoch:14 step:11608 [D loss: 0.017320, acc.: 100.00%] [G loss: 0.529034]\n",
      "epoch:14 step:11609 [D loss: 0.011603, acc.: 100.00%] [G loss: 0.433408]\n",
      "epoch:14 step:11610 [D loss: 0.010858, acc.: 100.00%] [G loss: 0.146310]\n",
      "epoch:14 step:11611 [D loss: 0.009090, acc.: 100.00%] [G loss: 0.692954]\n",
      "epoch:14 step:11612 [D loss: 0.056872, acc.: 99.22%] [G loss: 0.300783]\n",
      "epoch:14 step:11613 [D loss: 0.012758, acc.: 100.00%] [G loss: 0.214538]\n",
      "epoch:14 step:11614 [D loss: 0.046183, acc.: 100.00%] [G loss: 0.595389]\n",
      "epoch:14 step:11615 [D loss: 0.025178, acc.: 100.00%] [G loss: 0.429389]\n",
      "epoch:14 step:11616 [D loss: 0.004852, acc.: 100.00%] [G loss: 0.315059]\n",
      "epoch:14 step:11617 [D loss: 0.017912, acc.: 100.00%] [G loss: 0.484136]\n",
      "epoch:14 step:11618 [D loss: 0.019773, acc.: 100.00%] [G loss: 3.476827]\n",
      "epoch:14 step:11619 [D loss: 0.068961, acc.: 99.22%] [G loss: 1.357864]\n",
      "epoch:14 step:11620 [D loss: 0.019351, acc.: 100.00%] [G loss: 2.051467]\n",
      "epoch:14 step:11621 [D loss: 0.826064, acc.: 60.16%] [G loss: 5.956024]\n",
      "epoch:14 step:11622 [D loss: 0.617117, acc.: 69.53%] [G loss: 1.976343]\n",
      "epoch:14 step:11623 [D loss: 0.019582, acc.: 100.00%] [G loss: 1.526596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11624 [D loss: 0.080467, acc.: 97.66%] [G loss: 1.653654]\n",
      "epoch:14 step:11625 [D loss: 0.033695, acc.: 100.00%] [G loss: 3.022879]\n",
      "epoch:14 step:11626 [D loss: 0.059132, acc.: 97.66%] [G loss: 2.768419]\n",
      "epoch:14 step:11627 [D loss: 0.010860, acc.: 100.00%] [G loss: 3.205876]\n",
      "epoch:14 step:11628 [D loss: 0.018042, acc.: 100.00%] [G loss: 1.280792]\n",
      "epoch:14 step:11629 [D loss: 0.008063, acc.: 100.00%] [G loss: 0.101201]\n",
      "epoch:14 step:11630 [D loss: 0.018557, acc.: 100.00%] [G loss: 0.560048]\n",
      "epoch:14 step:11631 [D loss: 0.031749, acc.: 100.00%] [G loss: 2.322279]\n",
      "epoch:14 step:11632 [D loss: 0.017813, acc.: 100.00%] [G loss: 0.417875]\n",
      "epoch:14 step:11633 [D loss: 0.032859, acc.: 100.00%] [G loss: 2.930969]\n",
      "epoch:14 step:11634 [D loss: 0.039087, acc.: 100.00%] [G loss: 1.446366]\n",
      "epoch:14 step:11635 [D loss: 0.005552, acc.: 100.00%] [G loss: 1.780306]\n",
      "epoch:14 step:11636 [D loss: 0.007945, acc.: 100.00%] [G loss: 2.028334]\n",
      "epoch:14 step:11637 [D loss: 0.046783, acc.: 100.00%] [G loss: 2.115696]\n",
      "epoch:14 step:11638 [D loss: 0.011655, acc.: 100.00%] [G loss: 5.390136]\n",
      "epoch:14 step:11639 [D loss: 0.148947, acc.: 96.88%] [G loss: 0.387878]\n",
      "epoch:14 step:11640 [D loss: 1.204509, acc.: 54.69%] [G loss: 8.696144]\n",
      "epoch:14 step:11641 [D loss: 2.833859, acc.: 50.00%] [G loss: 6.365783]\n",
      "epoch:14 step:11642 [D loss: 1.014289, acc.: 64.06%] [G loss: 2.268302]\n",
      "epoch:14 step:11643 [D loss: 0.198270, acc.: 90.62%] [G loss: 2.434927]\n",
      "epoch:14 step:11644 [D loss: 0.042199, acc.: 100.00%] [G loss: 3.606100]\n",
      "epoch:14 step:11645 [D loss: 0.094440, acc.: 98.44%] [G loss: 2.872059]\n",
      "epoch:14 step:11646 [D loss: 0.058879, acc.: 99.22%] [G loss: 3.398920]\n",
      "epoch:14 step:11647 [D loss: 0.661981, acc.: 64.84%] [G loss: 5.053836]\n",
      "epoch:14 step:11648 [D loss: 0.131119, acc.: 95.31%] [G loss: 0.977145]\n",
      "epoch:14 step:11649 [D loss: 0.861616, acc.: 60.16%] [G loss: 0.030954]\n",
      "epoch:14 step:11650 [D loss: 0.331463, acc.: 83.59%] [G loss: 3.791501]\n",
      "epoch:14 step:11651 [D loss: 0.011785, acc.: 100.00%] [G loss: 4.729506]\n",
      "epoch:14 step:11652 [D loss: 0.050665, acc.: 99.22%] [G loss: 4.284084]\n",
      "epoch:14 step:11653 [D loss: 0.064099, acc.: 97.66%] [G loss: 4.150073]\n",
      "epoch:14 step:11654 [D loss: 0.021766, acc.: 99.22%] [G loss: 2.940118]\n",
      "epoch:14 step:11655 [D loss: 0.013758, acc.: 100.00%] [G loss: 1.431755]\n",
      "epoch:14 step:11656 [D loss: 0.062279, acc.: 97.66%] [G loss: 3.474116]\n",
      "epoch:14 step:11657 [D loss: 0.017141, acc.: 100.00%] [G loss: 2.826297]\n",
      "epoch:14 step:11658 [D loss: 0.023206, acc.: 100.00%] [G loss: 3.194617]\n",
      "epoch:14 step:11659 [D loss: 0.162912, acc.: 93.75%] [G loss: 2.129886]\n",
      "epoch:14 step:11660 [D loss: 0.284094, acc.: 89.06%] [G loss: 2.329412]\n",
      "epoch:14 step:11661 [D loss: 0.084148, acc.: 98.44%] [G loss: 1.414237]\n",
      "epoch:14 step:11662 [D loss: 0.414491, acc.: 81.25%] [G loss: 4.450350]\n",
      "epoch:14 step:11663 [D loss: 0.027045, acc.: 100.00%] [G loss: 0.727341]\n",
      "epoch:14 step:11664 [D loss: 0.127237, acc.: 96.09%] [G loss: 4.772548]\n",
      "epoch:14 step:11665 [D loss: 0.050574, acc.: 99.22%] [G loss: 4.191610]\n",
      "epoch:14 step:11666 [D loss: 0.012266, acc.: 100.00%] [G loss: 3.678805]\n",
      "epoch:14 step:11667 [D loss: 0.073600, acc.: 98.44%] [G loss: 3.528464]\n",
      "epoch:14 step:11668 [D loss: 0.014909, acc.: 100.00%] [G loss: 3.100895]\n",
      "epoch:14 step:11669 [D loss: 0.068251, acc.: 98.44%] [G loss: 2.808249]\n",
      "epoch:14 step:11670 [D loss: 0.047198, acc.: 99.22%] [G loss: 0.303810]\n",
      "epoch:14 step:11671 [D loss: 0.046265, acc.: 99.22%] [G loss: 2.972118]\n",
      "epoch:14 step:11672 [D loss: 0.017211, acc.: 100.00%] [G loss: 3.227830]\n",
      "epoch:14 step:11673 [D loss: 0.114272, acc.: 98.44%] [G loss: 2.279002]\n",
      "epoch:14 step:11674 [D loss: 0.025249, acc.: 100.00%] [G loss: 1.953760]\n",
      "epoch:14 step:11675 [D loss: 0.035865, acc.: 100.00%] [G loss: 3.896678]\n",
      "epoch:14 step:11676 [D loss: 0.021507, acc.: 100.00%] [G loss: 0.633738]\n",
      "epoch:14 step:11677 [D loss: 0.045715, acc.: 99.22%] [G loss: 0.441042]\n",
      "epoch:14 step:11678 [D loss: 0.023537, acc.: 100.00%] [G loss: 2.140451]\n",
      "epoch:14 step:11679 [D loss: 0.019453, acc.: 100.00%] [G loss: 2.290729]\n",
      "epoch:14 step:11680 [D loss: 0.011812, acc.: 100.00%] [G loss: 2.287047]\n",
      "epoch:14 step:11681 [D loss: 0.024783, acc.: 100.00%] [G loss: 1.627487]\n",
      "epoch:14 step:11682 [D loss: 0.013835, acc.: 100.00%] [G loss: 1.617849]\n",
      "epoch:14 step:11683 [D loss: 0.012960, acc.: 100.00%] [G loss: 1.432657]\n",
      "epoch:14 step:11684 [D loss: 0.027771, acc.: 100.00%] [G loss: 0.879206]\n",
      "epoch:14 step:11685 [D loss: 0.027754, acc.: 100.00%] [G loss: 1.637064]\n",
      "epoch:14 step:11686 [D loss: 0.031140, acc.: 100.00%] [G loss: 2.344377]\n",
      "epoch:14 step:11687 [D loss: 0.057518, acc.: 98.44%] [G loss: 2.826120]\n",
      "epoch:14 step:11688 [D loss: 0.064860, acc.: 100.00%] [G loss: 1.144184]\n",
      "epoch:14 step:11689 [D loss: 0.028900, acc.: 100.00%] [G loss: 0.790215]\n",
      "epoch:14 step:11690 [D loss: 0.023077, acc.: 100.00%] [G loss: 1.874041]\n",
      "epoch:14 step:11691 [D loss: 0.016596, acc.: 100.00%] [G loss: 1.861816]\n",
      "epoch:14 step:11692 [D loss: 0.026180, acc.: 100.00%] [G loss: 1.425739]\n",
      "epoch:14 step:11693 [D loss: 0.045745, acc.: 98.44%] [G loss: 0.849945]\n",
      "epoch:14 step:11694 [D loss: 0.074365, acc.: 98.44%] [G loss: 0.311546]\n",
      "epoch:14 step:11695 [D loss: 0.045882, acc.: 100.00%] [G loss: 0.621775]\n",
      "epoch:14 step:11696 [D loss: 0.021568, acc.: 100.00%] [G loss: 0.702938]\n",
      "epoch:14 step:11697 [D loss: 0.010446, acc.: 100.00%] [G loss: 4.234699]\n",
      "epoch:14 step:11698 [D loss: 0.018273, acc.: 100.00%] [G loss: 3.210098]\n",
      "epoch:14 step:11699 [D loss: 0.036121, acc.: 99.22%] [G loss: 1.253205]\n",
      "epoch:14 step:11700 [D loss: 0.020367, acc.: 100.00%] [G loss: 0.988668]\n",
      "epoch:14 step:11701 [D loss: 0.067388, acc.: 100.00%] [G loss: 1.194731]\n",
      "epoch:14 step:11702 [D loss: 0.033936, acc.: 99.22%] [G loss: 0.753239]\n",
      "epoch:14 step:11703 [D loss: 0.075456, acc.: 97.66%] [G loss: 1.457563]\n",
      "epoch:14 step:11704 [D loss: 0.020445, acc.: 100.00%] [G loss: 2.959396]\n",
      "epoch:14 step:11705 [D loss: 0.105927, acc.: 98.44%] [G loss: 0.298647]\n",
      "epoch:14 step:11706 [D loss: 0.030803, acc.: 100.00%] [G loss: 0.340884]\n",
      "epoch:14 step:11707 [D loss: 0.002302, acc.: 100.00%] [G loss: 3.056775]\n",
      "epoch:14 step:11708 [D loss: 0.005640, acc.: 100.00%] [G loss: 0.527761]\n",
      "epoch:14 step:11709 [D loss: 0.016926, acc.: 100.00%] [G loss: 1.116547]\n",
      "epoch:14 step:11710 [D loss: 0.008378, acc.: 100.00%] [G loss: 0.497348]\n",
      "epoch:14 step:11711 [D loss: 0.009843, acc.: 100.00%] [G loss: 0.487080]\n",
      "epoch:14 step:11712 [D loss: 0.008360, acc.: 100.00%] [G loss: 0.715605]\n",
      "epoch:14 step:11713 [D loss: 0.088871, acc.: 98.44%] [G loss: 0.010643]\n",
      "epoch:14 step:11714 [D loss: 0.058002, acc.: 99.22%] [G loss: 0.287649]\n",
      "epoch:14 step:11715 [D loss: 0.006575, acc.: 100.00%] [G loss: 0.109211]\n",
      "epoch:15 step:11716 [D loss: 0.003905, acc.: 100.00%] [G loss: 0.059065]\n",
      "epoch:15 step:11717 [D loss: 0.003548, acc.: 100.00%] [G loss: 0.220527]\n",
      "epoch:15 step:11718 [D loss: 0.003513, acc.: 100.00%] [G loss: 0.100917]\n",
      "epoch:15 step:11719 [D loss: 0.001459, acc.: 100.00%] [G loss: 0.006902]\n",
      "epoch:15 step:11720 [D loss: 0.005082, acc.: 100.00%] [G loss: 0.589829]\n",
      "epoch:15 step:11721 [D loss: 0.004045, acc.: 100.00%] [G loss: 0.733125]\n",
      "epoch:15 step:11722 [D loss: 0.003901, acc.: 100.00%] [G loss: 0.712867]\n",
      "epoch:15 step:11723 [D loss: 0.005691, acc.: 100.00%] [G loss: 0.017770]\n",
      "epoch:15 step:11724 [D loss: 0.006194, acc.: 100.00%] [G loss: 0.071399]\n",
      "epoch:15 step:11725 [D loss: 0.003377, acc.: 100.00%] [G loss: 0.013682]\n",
      "epoch:15 step:11726 [D loss: 0.005919, acc.: 100.00%] [G loss: 0.057454]\n",
      "epoch:15 step:11727 [D loss: 0.016888, acc.: 99.22%] [G loss: 0.206698]\n",
      "epoch:15 step:11728 [D loss: 0.003807, acc.: 100.00%] [G loss: 0.030606]\n",
      "epoch:15 step:11729 [D loss: 0.006178, acc.: 100.00%] [G loss: 1.841413]\n",
      "epoch:15 step:11730 [D loss: 0.021660, acc.: 100.00%] [G loss: 0.006755]\n",
      "epoch:15 step:11731 [D loss: 0.002100, acc.: 100.00%] [G loss: 0.018999]\n",
      "epoch:15 step:11732 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.058057]\n",
      "epoch:15 step:11733 [D loss: 0.023009, acc.: 100.00%] [G loss: 0.093021]\n",
      "epoch:15 step:11734 [D loss: 0.016643, acc.: 99.22%] [G loss: 0.063306]\n",
      "epoch:15 step:11735 [D loss: 0.014299, acc.: 100.00%] [G loss: 0.297029]\n",
      "epoch:15 step:11736 [D loss: 0.011715, acc.: 100.00%] [G loss: 0.594251]\n",
      "epoch:15 step:11737 [D loss: 0.006394, acc.: 100.00%] [G loss: 2.492852]\n",
      "epoch:15 step:11738 [D loss: 0.017292, acc.: 100.00%] [G loss: 0.054310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11739 [D loss: 0.001647, acc.: 100.00%] [G loss: 1.461963]\n",
      "epoch:15 step:11740 [D loss: 0.021973, acc.: 99.22%] [G loss: 0.008662]\n",
      "epoch:15 step:11741 [D loss: 0.044923, acc.: 99.22%] [G loss: 0.299789]\n",
      "epoch:15 step:11742 [D loss: 0.003935, acc.: 100.00%] [G loss: 0.608628]\n",
      "epoch:15 step:11743 [D loss: 0.024032, acc.: 100.00%] [G loss: 4.273812]\n",
      "epoch:15 step:11744 [D loss: 0.025258, acc.: 100.00%] [G loss: 0.265710]\n",
      "epoch:15 step:11745 [D loss: 0.038003, acc.: 100.00%] [G loss: 1.366486]\n",
      "epoch:15 step:11746 [D loss: 0.019856, acc.: 100.00%] [G loss: 0.289285]\n",
      "epoch:15 step:11747 [D loss: 0.017558, acc.: 100.00%] [G loss: 0.172658]\n",
      "epoch:15 step:11748 [D loss: 0.053414, acc.: 99.22%] [G loss: 0.140302]\n",
      "epoch:15 step:11749 [D loss: 0.006383, acc.: 100.00%] [G loss: 2.909596]\n",
      "epoch:15 step:11750 [D loss: 0.009520, acc.: 100.00%] [G loss: 0.523424]\n",
      "epoch:15 step:11751 [D loss: 0.120023, acc.: 96.88%] [G loss: 0.170961]\n",
      "epoch:15 step:11752 [D loss: 0.002274, acc.: 100.00%] [G loss: 1.306221]\n",
      "epoch:15 step:11753 [D loss: 0.021073, acc.: 99.22%] [G loss: 0.246043]\n",
      "epoch:15 step:11754 [D loss: 0.015168, acc.: 100.00%] [G loss: 1.108462]\n",
      "epoch:15 step:11755 [D loss: 0.008976, acc.: 100.00%] [G loss: 0.777017]\n",
      "epoch:15 step:11756 [D loss: 0.003098, acc.: 100.00%] [G loss: 0.109552]\n",
      "epoch:15 step:11757 [D loss: 0.008221, acc.: 100.00%] [G loss: 0.037390]\n",
      "epoch:15 step:11758 [D loss: 0.003565, acc.: 100.00%] [G loss: 4.376403]\n",
      "epoch:15 step:11759 [D loss: 0.014180, acc.: 99.22%] [G loss: 0.046964]\n",
      "epoch:15 step:11760 [D loss: 0.027731, acc.: 100.00%] [G loss: 0.047453]\n",
      "epoch:15 step:11761 [D loss: 0.031351, acc.: 99.22%] [G loss: 0.063920]\n",
      "epoch:15 step:11762 [D loss: 0.039550, acc.: 98.44%] [G loss: 0.971514]\n",
      "epoch:15 step:11763 [D loss: 0.017350, acc.: 100.00%] [G loss: 2.455252]\n",
      "epoch:15 step:11764 [D loss: 0.798165, acc.: 65.62%] [G loss: 10.963865]\n",
      "epoch:15 step:11765 [D loss: 3.995512, acc.: 50.00%] [G loss: 5.583786]\n",
      "epoch:15 step:11766 [D loss: 1.136663, acc.: 53.91%] [G loss: 1.105417]\n",
      "epoch:15 step:11767 [D loss: 0.185728, acc.: 93.75%] [G loss: 3.038162]\n",
      "epoch:15 step:11768 [D loss: 0.022446, acc.: 100.00%] [G loss: 4.582848]\n",
      "epoch:15 step:11769 [D loss: 0.091624, acc.: 97.66%] [G loss: 0.476141]\n",
      "epoch:15 step:11770 [D loss: 0.024390, acc.: 100.00%] [G loss: 3.175520]\n",
      "epoch:15 step:11771 [D loss: 0.017841, acc.: 100.00%] [G loss: 2.797462]\n",
      "epoch:15 step:11772 [D loss: 0.017087, acc.: 100.00%] [G loss: 0.008356]\n",
      "epoch:15 step:11773 [D loss: 0.029821, acc.: 100.00%] [G loss: 2.896158]\n",
      "epoch:15 step:11774 [D loss: 0.045343, acc.: 100.00%] [G loss: 3.191387]\n",
      "epoch:15 step:11775 [D loss: 0.067468, acc.: 98.44%] [G loss: 3.204314]\n",
      "epoch:15 step:11776 [D loss: 0.041950, acc.: 99.22%] [G loss: 3.324988]\n",
      "epoch:15 step:11777 [D loss: 0.009493, acc.: 100.00%] [G loss: 3.086894]\n",
      "epoch:15 step:11778 [D loss: 0.107088, acc.: 96.09%] [G loss: 2.746856]\n",
      "epoch:15 step:11779 [D loss: 0.088274, acc.: 98.44%] [G loss: 3.315220]\n",
      "epoch:15 step:11780 [D loss: 0.271165, acc.: 89.06%] [G loss: 1.717983]\n",
      "epoch:15 step:11781 [D loss: 0.073960, acc.: 98.44%] [G loss: 2.998883]\n",
      "epoch:15 step:11782 [D loss: 0.010184, acc.: 100.00%] [G loss: 4.160440]\n",
      "epoch:15 step:11783 [D loss: 0.029482, acc.: 100.00%] [G loss: 3.127031]\n",
      "epoch:15 step:11784 [D loss: 0.027462, acc.: 100.00%] [G loss: 0.086808]\n",
      "epoch:15 step:11785 [D loss: 0.013999, acc.: 100.00%] [G loss: 2.854266]\n",
      "epoch:15 step:11786 [D loss: 0.019911, acc.: 99.22%] [G loss: 2.961396]\n",
      "epoch:15 step:11787 [D loss: 0.011749, acc.: 100.00%] [G loss: 4.791569]\n",
      "epoch:15 step:11788 [D loss: 0.020656, acc.: 100.00%] [G loss: 3.145994]\n",
      "epoch:15 step:11789 [D loss: 0.016491, acc.: 100.00%] [G loss: 2.766994]\n",
      "epoch:15 step:11790 [D loss: 0.034005, acc.: 100.00%] [G loss: 2.827188]\n",
      "epoch:15 step:11791 [D loss: 0.025343, acc.: 100.00%] [G loss: 3.119643]\n",
      "epoch:15 step:11792 [D loss: 0.054781, acc.: 98.44%] [G loss: 1.863185]\n",
      "epoch:15 step:11793 [D loss: 0.034532, acc.: 100.00%] [G loss: 2.746391]\n",
      "epoch:15 step:11794 [D loss: 0.019971, acc.: 100.00%] [G loss: 3.916379]\n",
      "epoch:15 step:11795 [D loss: 0.022120, acc.: 100.00%] [G loss: 3.427372]\n",
      "epoch:15 step:11796 [D loss: 0.041244, acc.: 100.00%] [G loss: 3.701790]\n",
      "epoch:15 step:11797 [D loss: 0.010171, acc.: 100.00%] [G loss: 3.507731]\n",
      "epoch:15 step:11798 [D loss: 0.026616, acc.: 100.00%] [G loss: 3.775744]\n",
      "epoch:15 step:11799 [D loss: 0.015679, acc.: 100.00%] [G loss: 3.968102]\n",
      "epoch:15 step:11800 [D loss: 0.011161, acc.: 100.00%] [G loss: 3.548083]\n",
      "epoch:15 step:11801 [D loss: 0.135143, acc.: 99.22%] [G loss: 3.555932]\n",
      "epoch:15 step:11802 [D loss: 0.010606, acc.: 100.00%] [G loss: 4.480688]\n",
      "epoch:15 step:11803 [D loss: 0.062386, acc.: 98.44%] [G loss: 3.459077]\n",
      "epoch:15 step:11804 [D loss: 0.038241, acc.: 100.00%] [G loss: 4.565653]\n",
      "epoch:15 step:11805 [D loss: 0.049469, acc.: 100.00%] [G loss: 3.699588]\n",
      "epoch:15 step:11806 [D loss: 0.034769, acc.: 99.22%] [G loss: 3.457421]\n",
      "epoch:15 step:11807 [D loss: 0.009996, acc.: 100.00%] [G loss: 4.816818]\n",
      "epoch:15 step:11808 [D loss: 0.005007, acc.: 100.00%] [G loss: 4.984792]\n",
      "epoch:15 step:11809 [D loss: 0.017664, acc.: 100.00%] [G loss: 0.166980]\n",
      "epoch:15 step:11810 [D loss: 0.046066, acc.: 100.00%] [G loss: 6.141955]\n",
      "epoch:15 step:11811 [D loss: 0.021661, acc.: 99.22%] [G loss: 5.906770]\n",
      "epoch:15 step:11812 [D loss: 0.023018, acc.: 100.00%] [G loss: 5.165609]\n",
      "epoch:15 step:11813 [D loss: 0.008528, acc.: 100.00%] [G loss: 5.519771]\n",
      "epoch:15 step:11814 [D loss: 0.041447, acc.: 100.00%] [G loss: 5.315151]\n",
      "epoch:15 step:11815 [D loss: 0.031399, acc.: 100.00%] [G loss: 5.005172]\n",
      "epoch:15 step:11816 [D loss: 0.011179, acc.: 100.00%] [G loss: 4.799806]\n",
      "epoch:15 step:11817 [D loss: 0.006843, acc.: 100.00%] [G loss: 5.544598]\n",
      "epoch:15 step:11818 [D loss: 0.025614, acc.: 99.22%] [G loss: 4.890208]\n",
      "epoch:15 step:11819 [D loss: 0.047101, acc.: 99.22%] [G loss: 2.782384]\n",
      "epoch:15 step:11820 [D loss: 0.015640, acc.: 100.00%] [G loss: 0.798254]\n",
      "epoch:15 step:11821 [D loss: 0.031506, acc.: 100.00%] [G loss: 5.589679]\n",
      "epoch:15 step:11822 [D loss: 0.137291, acc.: 96.09%] [G loss: 1.659291]\n",
      "epoch:15 step:11823 [D loss: 0.017890, acc.: 99.22%] [G loss: 5.944450]\n",
      "epoch:15 step:11824 [D loss: 0.017627, acc.: 100.00%] [G loss: 5.755805]\n",
      "epoch:15 step:11825 [D loss: 0.044643, acc.: 98.44%] [G loss: 5.119680]\n",
      "epoch:15 step:11826 [D loss: 0.019476, acc.: 100.00%] [G loss: 4.647968]\n",
      "epoch:15 step:11827 [D loss: 0.023265, acc.: 100.00%] [G loss: 0.185795]\n",
      "epoch:15 step:11828 [D loss: 0.009283, acc.: 100.00%] [G loss: 0.482800]\n",
      "epoch:15 step:11829 [D loss: 0.120211, acc.: 96.09%] [G loss: 3.159804]\n",
      "epoch:15 step:11830 [D loss: 0.027876, acc.: 99.22%] [G loss: 6.310724]\n",
      "epoch:15 step:11831 [D loss: 3.156265, acc.: 24.22%] [G loss: 8.338158]\n",
      "epoch:15 step:11832 [D loss: 1.289353, acc.: 56.25%] [G loss: 7.166560]\n",
      "epoch:15 step:11833 [D loss: 0.049370, acc.: 98.44%] [G loss: 4.646729]\n",
      "epoch:15 step:11834 [D loss: 0.049344, acc.: 98.44%] [G loss: 2.856421]\n",
      "epoch:15 step:11835 [D loss: 0.001750, acc.: 100.00%] [G loss: 1.507031]\n",
      "epoch:15 step:11836 [D loss: 0.001847, acc.: 100.00%] [G loss: 0.559272]\n",
      "epoch:15 step:11837 [D loss: 0.004569, acc.: 100.00%] [G loss: 0.504530]\n",
      "epoch:15 step:11838 [D loss: 0.020099, acc.: 100.00%] [G loss: 5.076203]\n",
      "epoch:15 step:11839 [D loss: 0.006548, acc.: 100.00%] [G loss: 0.272125]\n",
      "epoch:15 step:11840 [D loss: 0.009765, acc.: 100.00%] [G loss: 0.257885]\n",
      "epoch:15 step:11841 [D loss: 0.007089, acc.: 100.00%] [G loss: 0.207291]\n",
      "epoch:15 step:11842 [D loss: 0.011608, acc.: 100.00%] [G loss: 0.122022]\n",
      "epoch:15 step:11843 [D loss: 0.034140, acc.: 100.00%] [G loss: 4.805399]\n",
      "epoch:15 step:11844 [D loss: 0.013433, acc.: 100.00%] [G loss: 0.254186]\n",
      "epoch:15 step:11845 [D loss: 0.009691, acc.: 100.00%] [G loss: 2.621517]\n",
      "epoch:15 step:11846 [D loss: 0.168790, acc.: 92.97%] [G loss: 1.576876]\n",
      "epoch:15 step:11847 [D loss: 0.662954, acc.: 64.06%] [G loss: 0.261642]\n",
      "epoch:15 step:11848 [D loss: 0.015614, acc.: 100.00%] [G loss: 7.586095]\n",
      "epoch:15 step:11849 [D loss: 0.040857, acc.: 97.66%] [G loss: 0.247577]\n",
      "epoch:15 step:11850 [D loss: 0.100152, acc.: 96.88%] [G loss: 0.018790]\n",
      "epoch:15 step:11851 [D loss: 0.264953, acc.: 89.84%] [G loss: 6.331053]\n",
      "epoch:15 step:11852 [D loss: 0.051567, acc.: 98.44%] [G loss: 6.232013]\n",
      "epoch:15 step:11853 [D loss: 0.340700, acc.: 78.12%] [G loss: 0.027377]\n",
      "epoch:15 step:11854 [D loss: 0.034332, acc.: 100.00%] [G loss: 0.024036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11855 [D loss: 0.129102, acc.: 96.09%] [G loss: 0.796064]\n",
      "epoch:15 step:11856 [D loss: 0.001972, acc.: 100.00%] [G loss: 1.096362]\n",
      "epoch:15 step:11857 [D loss: 0.001994, acc.: 100.00%] [G loss: 4.566900]\n",
      "epoch:15 step:11858 [D loss: 0.006011, acc.: 100.00%] [G loss: 1.394723]\n",
      "epoch:15 step:11859 [D loss: 0.003955, acc.: 100.00%] [G loss: 0.239832]\n",
      "epoch:15 step:11860 [D loss: 0.019828, acc.: 100.00%] [G loss: 0.238051]\n",
      "epoch:15 step:11861 [D loss: 0.063324, acc.: 99.22%] [G loss: 1.871268]\n",
      "epoch:15 step:11862 [D loss: 0.032521, acc.: 100.00%] [G loss: 1.528759]\n",
      "epoch:15 step:11863 [D loss: 0.102065, acc.: 96.09%] [G loss: 6.012095]\n",
      "epoch:15 step:11864 [D loss: 0.692353, acc.: 68.75%] [G loss: 5.298303]\n",
      "epoch:15 step:11865 [D loss: 0.008587, acc.: 100.00%] [G loss: 7.041366]\n",
      "epoch:15 step:11866 [D loss: 0.139260, acc.: 92.97%] [G loss: 4.092866]\n",
      "epoch:15 step:11867 [D loss: 0.013516, acc.: 100.00%] [G loss: 2.627337]\n",
      "epoch:15 step:11868 [D loss: 0.132629, acc.: 95.31%] [G loss: 5.728145]\n",
      "epoch:15 step:11869 [D loss: 0.006144, acc.: 100.00%] [G loss: 6.238901]\n",
      "epoch:15 step:11870 [D loss: 0.489095, acc.: 77.34%] [G loss: 0.522730]\n",
      "epoch:15 step:11871 [D loss: 0.667281, acc.: 74.22%] [G loss: 7.117189]\n",
      "epoch:15 step:11872 [D loss: 0.766219, acc.: 68.75%] [G loss: 0.786203]\n",
      "epoch:15 step:11873 [D loss: 0.191539, acc.: 92.19%] [G loss: 0.110489]\n",
      "epoch:15 step:11874 [D loss: 0.031606, acc.: 98.44%] [G loss: 4.362258]\n",
      "epoch:15 step:11875 [D loss: 0.026613, acc.: 99.22%] [G loss: 4.036163]\n",
      "epoch:15 step:11876 [D loss: 0.105842, acc.: 96.88%] [G loss: 3.047844]\n",
      "epoch:15 step:11877 [D loss: 0.218025, acc.: 92.97%] [G loss: 5.035829]\n",
      "epoch:15 step:11878 [D loss: 0.181591, acc.: 92.19%] [G loss: 0.367409]\n",
      "epoch:15 step:11879 [D loss: 0.070565, acc.: 99.22%] [G loss: 3.007144]\n",
      "epoch:15 step:11880 [D loss: 0.283756, acc.: 89.06%] [G loss: 4.076640]\n",
      "epoch:15 step:11881 [D loss: 0.458224, acc.: 77.34%] [G loss: 0.650732]\n",
      "epoch:15 step:11882 [D loss: 0.182676, acc.: 90.62%] [G loss: 1.046528]\n",
      "epoch:15 step:11883 [D loss: 0.003539, acc.: 100.00%] [G loss: 3.713095]\n",
      "epoch:15 step:11884 [D loss: 0.017194, acc.: 100.00%] [G loss: 2.779402]\n",
      "epoch:15 step:11885 [D loss: 0.027557, acc.: 100.00%] [G loss: 1.258561]\n",
      "epoch:15 step:11886 [D loss: 0.071123, acc.: 98.44%] [G loss: 0.424669]\n",
      "epoch:15 step:11887 [D loss: 0.024464, acc.: 99.22%] [G loss: 0.718748]\n",
      "epoch:15 step:11888 [D loss: 0.102820, acc.: 99.22%] [G loss: 2.477817]\n",
      "epoch:15 step:11889 [D loss: 0.210705, acc.: 92.97%] [G loss: 0.407880]\n",
      "epoch:15 step:11890 [D loss: 0.062910, acc.: 97.66%] [G loss: 0.926565]\n",
      "epoch:15 step:11891 [D loss: 0.065588, acc.: 99.22%] [G loss: 4.486452]\n",
      "epoch:15 step:11892 [D loss: 0.022850, acc.: 99.22%] [G loss: 0.383775]\n",
      "epoch:15 step:11893 [D loss: 0.050639, acc.: 99.22%] [G loss: 0.880963]\n",
      "epoch:15 step:11894 [D loss: 0.031989, acc.: 100.00%] [G loss: 1.507559]\n",
      "epoch:15 step:11895 [D loss: 0.016377, acc.: 100.00%] [G loss: 0.034310]\n",
      "epoch:15 step:11896 [D loss: 0.059787, acc.: 99.22%] [G loss: 0.726277]\n",
      "epoch:15 step:11897 [D loss: 0.048826, acc.: 98.44%] [G loss: 1.554737]\n",
      "epoch:15 step:11898 [D loss: 0.029968, acc.: 99.22%] [G loss: 1.241570]\n",
      "epoch:15 step:11899 [D loss: 0.027348, acc.: 100.00%] [G loss: 1.202722]\n",
      "epoch:15 step:11900 [D loss: 0.036451, acc.: 100.00%] [G loss: 1.597658]\n",
      "epoch:15 step:11901 [D loss: 0.034521, acc.: 100.00%] [G loss: 1.260516]\n",
      "epoch:15 step:11902 [D loss: 0.145084, acc.: 93.75%] [G loss: 0.534537]\n",
      "epoch:15 step:11903 [D loss: 0.316154, acc.: 85.16%] [G loss: 5.983834]\n",
      "epoch:15 step:11904 [D loss: 0.023792, acc.: 99.22%] [G loss: 4.312907]\n",
      "epoch:15 step:11905 [D loss: 0.384096, acc.: 79.69%] [G loss: 5.940978]\n",
      "epoch:15 step:11906 [D loss: 0.003655, acc.: 100.00%] [G loss: 4.942766]\n",
      "epoch:15 step:11907 [D loss: 0.007850, acc.: 100.00%] [G loss: 2.302891]\n",
      "epoch:15 step:11908 [D loss: 0.133772, acc.: 93.75%] [G loss: 2.893448]\n",
      "epoch:15 step:11909 [D loss: 0.001906, acc.: 100.00%] [G loss: 0.248226]\n",
      "epoch:15 step:11910 [D loss: 0.002286, acc.: 100.00%] [G loss: 0.705196]\n",
      "epoch:15 step:11911 [D loss: 0.014263, acc.: 100.00%] [G loss: 0.260769]\n",
      "epoch:15 step:11912 [D loss: 0.002554, acc.: 100.00%] [G loss: 3.285922]\n",
      "epoch:15 step:11913 [D loss: 0.025524, acc.: 99.22%] [G loss: 4.541830]\n",
      "epoch:15 step:11914 [D loss: 0.023678, acc.: 100.00%] [G loss: 0.594348]\n",
      "epoch:15 step:11915 [D loss: 0.011696, acc.: 100.00%] [G loss: 0.227630]\n",
      "epoch:15 step:11916 [D loss: 0.005635, acc.: 100.00%] [G loss: 0.210055]\n",
      "epoch:15 step:11917 [D loss: 0.076425, acc.: 98.44%] [G loss: 0.179519]\n",
      "epoch:15 step:11918 [D loss: 0.006372, acc.: 100.00%] [G loss: 0.595095]\n",
      "epoch:15 step:11919 [D loss: 0.042278, acc.: 100.00%] [G loss: 0.404367]\n",
      "epoch:15 step:11920 [D loss: 0.607070, acc.: 67.97%] [G loss: 6.517996]\n",
      "epoch:15 step:11921 [D loss: 2.325614, acc.: 50.00%] [G loss: 3.470322]\n",
      "epoch:15 step:11922 [D loss: 0.041170, acc.: 99.22%] [G loss: 2.275381]\n",
      "epoch:15 step:11923 [D loss: 0.010340, acc.: 100.00%] [G loss: 1.270395]\n",
      "epoch:15 step:11924 [D loss: 0.024787, acc.: 100.00%] [G loss: 6.206337]\n",
      "epoch:15 step:11925 [D loss: 0.006923, acc.: 100.00%] [G loss: 0.684794]\n",
      "epoch:15 step:11926 [D loss: 0.019928, acc.: 100.00%] [G loss: 5.689223]\n",
      "epoch:15 step:11927 [D loss: 0.011492, acc.: 100.00%] [G loss: 0.541640]\n",
      "epoch:15 step:11928 [D loss: 0.066709, acc.: 100.00%] [G loss: 1.742484]\n",
      "epoch:15 step:11929 [D loss: 0.391605, acc.: 84.38%] [G loss: 3.248511]\n",
      "epoch:15 step:11930 [D loss: 0.233725, acc.: 88.28%] [G loss: 1.957911]\n",
      "epoch:15 step:11931 [D loss: 0.109869, acc.: 96.09%] [G loss: 3.070350]\n",
      "epoch:15 step:11932 [D loss: 0.018198, acc.: 99.22%] [G loss: 3.011058]\n",
      "epoch:15 step:11933 [D loss: 0.057583, acc.: 98.44%] [G loss: 2.434540]\n",
      "epoch:15 step:11934 [D loss: 0.081606, acc.: 97.66%] [G loss: 3.255031]\n",
      "epoch:15 step:11935 [D loss: 0.109473, acc.: 97.66%] [G loss: 5.137188]\n",
      "epoch:15 step:11936 [D loss: 0.005473, acc.: 100.00%] [G loss: 3.519802]\n",
      "epoch:15 step:11937 [D loss: 0.050763, acc.: 99.22%] [G loss: 5.888704]\n",
      "epoch:15 step:11938 [D loss: 0.056464, acc.: 99.22%] [G loss: 1.715276]\n",
      "epoch:15 step:11939 [D loss: 0.029243, acc.: 100.00%] [G loss: 1.967970]\n",
      "epoch:15 step:11940 [D loss: 0.040729, acc.: 99.22%] [G loss: 1.736153]\n",
      "epoch:15 step:11941 [D loss: 0.020889, acc.: 100.00%] [G loss: 2.962317]\n",
      "epoch:15 step:11942 [D loss: 0.055693, acc.: 100.00%] [G loss: 2.769494]\n",
      "epoch:15 step:11943 [D loss: 0.104989, acc.: 96.09%] [G loss: 3.369602]\n",
      "epoch:15 step:11944 [D loss: 0.237342, acc.: 92.19%] [G loss: 3.262702]\n",
      "epoch:15 step:11945 [D loss: 0.020766, acc.: 100.00%] [G loss: 5.203600]\n",
      "epoch:15 step:11946 [D loss: 0.011241, acc.: 100.00%] [G loss: 3.658494]\n",
      "epoch:15 step:11947 [D loss: 0.592901, acc.: 73.44%] [G loss: 5.674345]\n",
      "epoch:15 step:11948 [D loss: 0.040476, acc.: 99.22%] [G loss: 6.672048]\n",
      "epoch:15 step:11949 [D loss: 0.773037, acc.: 66.41%] [G loss: 0.834915]\n",
      "epoch:15 step:11950 [D loss: 0.728456, acc.: 65.62%] [G loss: 6.360646]\n",
      "epoch:15 step:11951 [D loss: 0.087704, acc.: 97.66%] [G loss: 6.944092]\n",
      "epoch:15 step:11952 [D loss: 0.901598, acc.: 58.59%] [G loss: 1.425801]\n",
      "epoch:15 step:11953 [D loss: 0.282553, acc.: 82.81%] [G loss: 1.901743]\n",
      "epoch:15 step:11954 [D loss: 0.013698, acc.: 100.00%] [G loss: 3.375279]\n",
      "epoch:15 step:11955 [D loss: 0.139559, acc.: 94.53%] [G loss: 4.798716]\n",
      "epoch:15 step:11956 [D loss: 0.120119, acc.: 96.09%] [G loss: 1.677667]\n",
      "epoch:15 step:11957 [D loss: 0.038936, acc.: 100.00%] [G loss: 5.175545]\n",
      "epoch:15 step:11958 [D loss: 0.014261, acc.: 100.00%] [G loss: 2.013409]\n",
      "epoch:15 step:11959 [D loss: 0.070679, acc.: 98.44%] [G loss: 4.840638]\n",
      "epoch:15 step:11960 [D loss: 0.049866, acc.: 100.00%] [G loss: 5.666583]\n",
      "epoch:15 step:11961 [D loss: 0.248938, acc.: 92.19%] [G loss: 3.007617]\n",
      "epoch:15 step:11962 [D loss: 0.176787, acc.: 94.53%] [G loss: 3.514116]\n",
      "epoch:15 step:11963 [D loss: 0.029315, acc.: 99.22%] [G loss: 3.844191]\n",
      "epoch:15 step:11964 [D loss: 0.052242, acc.: 100.00%] [G loss: 4.325822]\n",
      "epoch:15 step:11965 [D loss: 0.033600, acc.: 100.00%] [G loss: 4.016695]\n",
      "epoch:15 step:11966 [D loss: 0.032754, acc.: 99.22%] [G loss: 3.815327]\n",
      "epoch:15 step:11967 [D loss: 0.011310, acc.: 100.00%] [G loss: 4.401877]\n",
      "epoch:15 step:11968 [D loss: 0.008818, acc.: 100.00%] [G loss: 3.182691]\n",
      "epoch:15 step:11969 [D loss: 0.038301, acc.: 99.22%] [G loss: 3.006301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11970 [D loss: 0.081145, acc.: 96.88%] [G loss: 1.684705]\n",
      "epoch:15 step:11971 [D loss: 0.075552, acc.: 97.66%] [G loss: 2.022605]\n",
      "epoch:15 step:11972 [D loss: 0.024660, acc.: 100.00%] [G loss: 2.302784]\n",
      "epoch:15 step:11973 [D loss: 0.022152, acc.: 99.22%] [G loss: 1.773555]\n",
      "epoch:15 step:11974 [D loss: 0.044471, acc.: 100.00%] [G loss: 3.008357]\n",
      "epoch:15 step:11975 [D loss: 0.030538, acc.: 100.00%] [G loss: 2.418315]\n",
      "epoch:15 step:11976 [D loss: 0.160163, acc.: 94.53%] [G loss: 2.713129]\n",
      "epoch:15 step:11977 [D loss: 0.022091, acc.: 99.22%] [G loss: 4.343472]\n",
      "epoch:15 step:11978 [D loss: 0.006747, acc.: 100.00%] [G loss: 4.477320]\n",
      "epoch:15 step:11979 [D loss: 0.317668, acc.: 86.72%] [G loss: 3.171917]\n",
      "epoch:15 step:11980 [D loss: 0.022342, acc.: 100.00%] [G loss: 3.927425]\n",
      "epoch:15 step:11981 [D loss: 0.008832, acc.: 100.00%] [G loss: 4.326519]\n",
      "epoch:15 step:11982 [D loss: 0.012931, acc.: 100.00%] [G loss: 4.216384]\n",
      "epoch:15 step:11983 [D loss: 0.014372, acc.: 100.00%] [G loss: 4.134771]\n",
      "epoch:15 step:11984 [D loss: 0.014018, acc.: 100.00%] [G loss: 4.558136]\n",
      "epoch:15 step:11985 [D loss: 0.012913, acc.: 100.00%] [G loss: 4.307000]\n",
      "epoch:15 step:11986 [D loss: 0.018857, acc.: 100.00%] [G loss: 4.682327]\n",
      "epoch:15 step:11987 [D loss: 0.007730, acc.: 100.00%] [G loss: 4.779487]\n",
      "epoch:15 step:11988 [D loss: 0.013224, acc.: 100.00%] [G loss: 5.098729]\n",
      "epoch:15 step:11989 [D loss: 0.009721, acc.: 100.00%] [G loss: 2.270792]\n",
      "epoch:15 step:11990 [D loss: 0.077211, acc.: 97.66%] [G loss: 5.451844]\n",
      "epoch:15 step:11991 [D loss: 0.010327, acc.: 100.00%] [G loss: 5.827511]\n",
      "epoch:15 step:11992 [D loss: 0.033121, acc.: 100.00%] [G loss: 5.170750]\n",
      "epoch:15 step:11993 [D loss: 0.012124, acc.: 100.00%] [G loss: 0.368500]\n",
      "epoch:15 step:11994 [D loss: 0.102646, acc.: 96.09%] [G loss: 3.716954]\n",
      "epoch:15 step:11995 [D loss: 0.018484, acc.: 100.00%] [G loss: 1.456643]\n",
      "epoch:15 step:11996 [D loss: 0.029395, acc.: 99.22%] [G loss: 0.499192]\n",
      "epoch:15 step:11997 [D loss: 0.038575, acc.: 99.22%] [G loss: 0.437686]\n",
      "epoch:15 step:11998 [D loss: 0.027729, acc.: 99.22%] [G loss: 5.914577]\n",
      "epoch:15 step:11999 [D loss: 0.015892, acc.: 100.00%] [G loss: 6.577182]\n",
      "epoch:15 step:12000 [D loss: 0.026696, acc.: 99.22%] [G loss: 5.528538]\n",
      "epoch:15 step:12001 [D loss: 0.466026, acc.: 77.34%] [G loss: 0.680951]\n",
      "epoch:15 step:12002 [D loss: 0.010226, acc.: 100.00%] [G loss: 5.757209]\n",
      "epoch:15 step:12003 [D loss: 0.030678, acc.: 98.44%] [G loss: 4.907673]\n",
      "epoch:15 step:12004 [D loss: 0.003627, acc.: 100.00%] [G loss: 4.563674]\n",
      "epoch:15 step:12005 [D loss: 0.007692, acc.: 100.00%] [G loss: 2.422711]\n",
      "epoch:15 step:12006 [D loss: 0.069272, acc.: 97.66%] [G loss: 0.311840]\n",
      "epoch:15 step:12007 [D loss: 0.004269, acc.: 100.00%] [G loss: 2.281929]\n",
      "epoch:15 step:12008 [D loss: 0.043500, acc.: 100.00%] [G loss: 1.989762]\n",
      "epoch:15 step:12009 [D loss: 0.009326, acc.: 100.00%] [G loss: 2.632826]\n",
      "epoch:15 step:12010 [D loss: 0.006403, acc.: 100.00%] [G loss: 1.874620]\n",
      "epoch:15 step:12011 [D loss: 0.010258, acc.: 100.00%] [G loss: 1.341724]\n",
      "epoch:15 step:12012 [D loss: 0.033619, acc.: 100.00%] [G loss: 1.120535]\n",
      "epoch:15 step:12013 [D loss: 0.042894, acc.: 100.00%] [G loss: 1.402457]\n",
      "epoch:15 step:12014 [D loss: 0.027308, acc.: 100.00%] [G loss: 1.083278]\n",
      "epoch:15 step:12015 [D loss: 0.003735, acc.: 100.00%] [G loss: 2.802648]\n",
      "epoch:15 step:12016 [D loss: 0.004007, acc.: 100.00%] [G loss: 0.235958]\n",
      "epoch:15 step:12017 [D loss: 0.020501, acc.: 100.00%] [G loss: 1.770129]\n",
      "epoch:15 step:12018 [D loss: 0.031003, acc.: 100.00%] [G loss: 0.212338]\n",
      "epoch:15 step:12019 [D loss: 0.002126, acc.: 100.00%] [G loss: 2.313426]\n",
      "epoch:15 step:12020 [D loss: 0.044278, acc.: 99.22%] [G loss: 3.130478]\n",
      "epoch:15 step:12021 [D loss: 0.023325, acc.: 99.22%] [G loss: 2.335254]\n",
      "epoch:15 step:12022 [D loss: 0.036992, acc.: 100.00%] [G loss: 2.366356]\n",
      "epoch:15 step:12023 [D loss: 0.004058, acc.: 100.00%] [G loss: 1.728289]\n",
      "epoch:15 step:12024 [D loss: 0.038221, acc.: 99.22%] [G loss: 1.190024]\n",
      "epoch:15 step:12025 [D loss: 0.041916, acc.: 99.22%] [G loss: 0.127457]\n",
      "epoch:15 step:12026 [D loss: 0.134311, acc.: 94.53%] [G loss: 5.710132]\n",
      "epoch:15 step:12027 [D loss: 0.022320, acc.: 100.00%] [G loss: 2.816759]\n",
      "epoch:15 step:12028 [D loss: 0.017560, acc.: 100.00%] [G loss: 5.450329]\n",
      "epoch:15 step:12029 [D loss: 0.017910, acc.: 99.22%] [G loss: 1.671781]\n",
      "epoch:15 step:12030 [D loss: 0.042166, acc.: 98.44%] [G loss: 0.238733]\n",
      "epoch:15 step:12031 [D loss: 0.057783, acc.: 97.66%] [G loss: 1.039379]\n",
      "epoch:15 step:12032 [D loss: 0.002732, acc.: 100.00%] [G loss: 1.165176]\n",
      "epoch:15 step:12033 [D loss: 0.002273, acc.: 100.00%] [G loss: 5.279196]\n",
      "epoch:15 step:12034 [D loss: 0.001186, acc.: 100.00%] [G loss: 0.713482]\n",
      "epoch:15 step:12035 [D loss: 0.267429, acc.: 89.06%] [G loss: 0.139741]\n",
      "epoch:15 step:12036 [D loss: 0.002732, acc.: 100.00%] [G loss: 0.042524]\n",
      "epoch:15 step:12037 [D loss: 0.006226, acc.: 100.00%] [G loss: 0.009819]\n",
      "epoch:15 step:12038 [D loss: 0.001762, acc.: 100.00%] [G loss: 0.060990]\n",
      "epoch:15 step:12039 [D loss: 0.233223, acc.: 91.41%] [G loss: 7.491848]\n",
      "epoch:15 step:12040 [D loss: 0.167543, acc.: 92.97%] [G loss: 6.314595]\n",
      "epoch:15 step:12041 [D loss: 0.009006, acc.: 100.00%] [G loss: 1.735937]\n",
      "epoch:15 step:12042 [D loss: 0.020786, acc.: 99.22%] [G loss: 4.296752]\n",
      "epoch:15 step:12043 [D loss: 0.001504, acc.: 100.00%] [G loss: 0.560545]\n",
      "epoch:15 step:12044 [D loss: 0.004339, acc.: 100.00%] [G loss: 0.329507]\n",
      "epoch:15 step:12045 [D loss: 0.003444, acc.: 100.00%] [G loss: 3.898090]\n",
      "epoch:15 step:12046 [D loss: 0.007762, acc.: 100.00%] [G loss: 0.219182]\n",
      "epoch:15 step:12047 [D loss: 0.002352, acc.: 100.00%] [G loss: 0.173011]\n",
      "epoch:15 step:12048 [D loss: 0.005673, acc.: 100.00%] [G loss: 0.396466]\n",
      "epoch:15 step:12049 [D loss: 0.207787, acc.: 93.75%] [G loss: 3.854795]\n",
      "epoch:15 step:12050 [D loss: 0.246627, acc.: 89.84%] [G loss: 5.904493]\n",
      "epoch:15 step:12051 [D loss: 0.033004, acc.: 99.22%] [G loss: 4.001416]\n",
      "epoch:15 step:12052 [D loss: 0.002678, acc.: 100.00%] [G loss: 2.416928]\n",
      "epoch:15 step:12053 [D loss: 0.013554, acc.: 100.00%] [G loss: 0.138955]\n",
      "epoch:15 step:12054 [D loss: 0.007556, acc.: 100.00%] [G loss: 4.763507]\n",
      "epoch:15 step:12055 [D loss: 0.002301, acc.: 100.00%] [G loss: 0.698159]\n",
      "epoch:15 step:12056 [D loss: 0.053133, acc.: 98.44%] [G loss: 0.231704]\n",
      "epoch:15 step:12057 [D loss: 0.009756, acc.: 99.22%] [G loss: 0.987856]\n",
      "epoch:15 step:12058 [D loss: 0.157392, acc.: 93.75%] [G loss: 0.007013]\n",
      "epoch:15 step:12059 [D loss: 0.412531, acc.: 76.56%] [G loss: 3.870301]\n",
      "epoch:15 step:12060 [D loss: 2.016492, acc.: 50.78%] [G loss: 3.333708]\n",
      "epoch:15 step:12061 [D loss: 0.034686, acc.: 99.22%] [G loss: 0.584051]\n",
      "epoch:15 step:12062 [D loss: 0.063897, acc.: 97.66%] [G loss: 0.679043]\n",
      "epoch:15 step:12063 [D loss: 0.009228, acc.: 100.00%] [G loss: 0.327848]\n",
      "epoch:15 step:12064 [D loss: 0.040731, acc.: 98.44%] [G loss: 0.177903]\n",
      "epoch:15 step:12065 [D loss: 0.014566, acc.: 100.00%] [G loss: 0.141343]\n",
      "epoch:15 step:12066 [D loss: 0.006373, acc.: 100.00%] [G loss: 0.149211]\n",
      "epoch:15 step:12067 [D loss: 0.003564, acc.: 100.00%] [G loss: 0.148659]\n",
      "epoch:15 step:12068 [D loss: 0.013944, acc.: 100.00%] [G loss: 0.252027]\n",
      "epoch:15 step:12069 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.552571]\n",
      "epoch:15 step:12070 [D loss: 0.014835, acc.: 99.22%] [G loss: 3.167973]\n",
      "epoch:15 step:12071 [D loss: 0.015485, acc.: 100.00%] [G loss: 0.234926]\n",
      "epoch:15 step:12072 [D loss: 0.014304, acc.: 100.00%] [G loss: 1.673035]\n",
      "epoch:15 step:12073 [D loss: 0.003840, acc.: 100.00%] [G loss: 0.110430]\n",
      "epoch:15 step:12074 [D loss: 0.006224, acc.: 100.00%] [G loss: 0.523391]\n",
      "epoch:15 step:12075 [D loss: 0.010139, acc.: 100.00%] [G loss: 0.457080]\n",
      "epoch:15 step:12076 [D loss: 0.004540, acc.: 100.00%] [G loss: 0.335484]\n",
      "epoch:15 step:12077 [D loss: 0.023272, acc.: 100.00%] [G loss: 0.376300]\n",
      "epoch:15 step:12078 [D loss: 0.005413, acc.: 100.00%] [G loss: 0.157765]\n",
      "epoch:15 step:12079 [D loss: 0.130346, acc.: 96.88%] [G loss: 0.924990]\n",
      "epoch:15 step:12080 [D loss: 0.001671, acc.: 100.00%] [G loss: 5.336931]\n",
      "epoch:15 step:12081 [D loss: 0.003820, acc.: 100.00%] [G loss: 1.983041]\n",
      "epoch:15 step:12082 [D loss: 0.005179, acc.: 100.00%] [G loss: 3.566222]\n",
      "epoch:15 step:12083 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.405664]\n",
      "epoch:15 step:12084 [D loss: 0.003824, acc.: 100.00%] [G loss: 0.502133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12085 [D loss: 0.030686, acc.: 99.22%] [G loss: 0.583743]\n",
      "epoch:15 step:12086 [D loss: 0.032554, acc.: 99.22%] [G loss: 1.059231]\n",
      "epoch:15 step:12087 [D loss: 0.014248, acc.: 100.00%] [G loss: 4.987678]\n",
      "epoch:15 step:12088 [D loss: 0.245521, acc.: 91.41%] [G loss: 0.543228]\n",
      "epoch:15 step:12089 [D loss: 0.033206, acc.: 100.00%] [G loss: 5.920964]\n",
      "epoch:15 step:12090 [D loss: 0.071282, acc.: 98.44%] [G loss: 0.276331]\n",
      "epoch:15 step:12091 [D loss: 0.001894, acc.: 100.00%] [G loss: 4.178555]\n",
      "epoch:15 step:12092 [D loss: 0.008520, acc.: 100.00%] [G loss: 0.036914]\n",
      "epoch:15 step:12093 [D loss: 0.001527, acc.: 100.00%] [G loss: 3.418182]\n",
      "epoch:15 step:12094 [D loss: 0.007605, acc.: 100.00%] [G loss: 0.018069]\n",
      "epoch:15 step:12095 [D loss: 0.001808, acc.: 100.00%] [G loss: 3.874578]\n",
      "epoch:15 step:12096 [D loss: 0.001629, acc.: 100.00%] [G loss: 0.095054]\n",
      "epoch:15 step:12097 [D loss: 0.006942, acc.: 100.00%] [G loss: 0.111643]\n",
      "epoch:15 step:12098 [D loss: 0.011262, acc.: 100.00%] [G loss: 2.299681]\n",
      "epoch:15 step:12099 [D loss: 0.011446, acc.: 100.00%] [G loss: 0.149346]\n",
      "epoch:15 step:12100 [D loss: 0.005817, acc.: 100.00%] [G loss: 1.949669]\n",
      "epoch:15 step:12101 [D loss: 0.018934, acc.: 99.22%] [G loss: 1.005697]\n",
      "epoch:15 step:12102 [D loss: 0.026080, acc.: 100.00%] [G loss: 0.789895]\n",
      "epoch:15 step:12103 [D loss: 0.013616, acc.: 100.00%] [G loss: 0.085896]\n",
      "epoch:15 step:12104 [D loss: 0.006283, acc.: 100.00%] [G loss: 1.598198]\n",
      "epoch:15 step:12105 [D loss: 0.002410, acc.: 100.00%] [G loss: 0.017475]\n",
      "epoch:15 step:12106 [D loss: 0.003061, acc.: 100.00%] [G loss: 1.176182]\n",
      "epoch:15 step:12107 [D loss: 0.003426, acc.: 100.00%] [G loss: 0.575605]\n",
      "epoch:15 step:12108 [D loss: 0.010748, acc.: 100.00%] [G loss: 0.111708]\n",
      "epoch:15 step:12109 [D loss: 0.005220, acc.: 100.00%] [G loss: 0.205798]\n",
      "epoch:15 step:12110 [D loss: 0.023851, acc.: 99.22%] [G loss: 0.014924]\n",
      "epoch:15 step:12111 [D loss: 0.002518, acc.: 100.00%] [G loss: 0.018484]\n",
      "epoch:15 step:12112 [D loss: 0.002757, acc.: 100.00%] [G loss: 0.006942]\n",
      "epoch:15 step:12113 [D loss: 0.004215, acc.: 100.00%] [G loss: 1.388092]\n",
      "epoch:15 step:12114 [D loss: 0.291748, acc.: 85.16%] [G loss: 10.534651]\n",
      "epoch:15 step:12115 [D loss: 0.320835, acc.: 82.81%] [G loss: 0.772638]\n",
      "epoch:15 step:12116 [D loss: 0.034581, acc.: 98.44%] [G loss: 6.817355]\n",
      "epoch:15 step:12117 [D loss: 0.000167, acc.: 100.00%] [G loss: 8.680943]\n",
      "epoch:15 step:12118 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.020851]\n",
      "epoch:15 step:12119 [D loss: 0.000121, acc.: 100.00%] [G loss: 6.733664]\n",
      "epoch:15 step:12120 [D loss: 0.001751, acc.: 100.00%] [G loss: 6.900491]\n",
      "epoch:15 step:12121 [D loss: 0.017368, acc.: 99.22%] [G loss: 2.770275]\n",
      "epoch:15 step:12122 [D loss: 0.024666, acc.: 100.00%] [G loss: 1.862484]\n",
      "epoch:15 step:12123 [D loss: 0.326608, acc.: 83.59%] [G loss: 3.629154]\n",
      "epoch:15 step:12124 [D loss: 0.579805, acc.: 67.97%] [G loss: 0.190926]\n",
      "epoch:15 step:12125 [D loss: 0.007610, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:15 step:12126 [D loss: 0.018781, acc.: 100.00%] [G loss: 0.000793]\n",
      "epoch:15 step:12127 [D loss: 0.031506, acc.: 98.44%] [G loss: 0.003104]\n",
      "epoch:15 step:12128 [D loss: 0.002311, acc.: 100.00%] [G loss: 9.212497]\n",
      "epoch:15 step:12129 [D loss: 0.003472, acc.: 100.00%] [G loss: 0.095599]\n",
      "epoch:15 step:12130 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.034383]\n",
      "epoch:15 step:12131 [D loss: 0.001947, acc.: 100.00%] [G loss: 8.074972]\n",
      "epoch:15 step:12132 [D loss: 0.001335, acc.: 100.00%] [G loss: 0.021796]\n",
      "epoch:15 step:12133 [D loss: 0.001964, acc.: 100.00%] [G loss: 0.030811]\n",
      "epoch:15 step:12134 [D loss: 0.008440, acc.: 100.00%] [G loss: 0.008496]\n",
      "epoch:15 step:12135 [D loss: 0.029075, acc.: 99.22%] [G loss: 0.084032]\n",
      "epoch:15 step:12136 [D loss: 0.013260, acc.: 100.00%] [G loss: 0.004163]\n",
      "epoch:15 step:12137 [D loss: 0.008305, acc.: 100.00%] [G loss: 0.006433]\n",
      "epoch:15 step:12138 [D loss: 0.007895, acc.: 100.00%] [G loss: 0.010899]\n",
      "epoch:15 step:12139 [D loss: 0.004470, acc.: 100.00%] [G loss: 6.939479]\n",
      "epoch:15 step:12140 [D loss: 0.004827, acc.: 100.00%] [G loss: 0.038316]\n",
      "epoch:15 step:12141 [D loss: 0.001441, acc.: 100.00%] [G loss: 1.015856]\n",
      "epoch:15 step:12142 [D loss: 0.120360, acc.: 94.53%] [G loss: 0.434102]\n",
      "epoch:15 step:12143 [D loss: 3.332819, acc.: 18.75%] [G loss: 11.234550]\n",
      "epoch:15 step:12144 [D loss: 2.677321, acc.: 50.00%] [G loss: 4.733792]\n",
      "epoch:15 step:12145 [D loss: 2.108259, acc.: 50.00%] [G loss: 1.191305]\n",
      "epoch:15 step:12146 [D loss: 0.298924, acc.: 84.38%] [G loss: 4.533326]\n",
      "epoch:15 step:12147 [D loss: 0.049890, acc.: 100.00%] [G loss: 0.037024]\n",
      "epoch:15 step:12148 [D loss: 0.036821, acc.: 100.00%] [G loss: 0.925882]\n",
      "epoch:15 step:12149 [D loss: 0.063264, acc.: 100.00%] [G loss: 3.630383]\n",
      "epoch:15 step:12150 [D loss: 0.050946, acc.: 100.00%] [G loss: 0.001643]\n",
      "epoch:15 step:12151 [D loss: 0.074315, acc.: 100.00%] [G loss: 0.065778]\n",
      "epoch:15 step:12152 [D loss: 0.083182, acc.: 100.00%] [G loss: 0.028861]\n",
      "epoch:15 step:12153 [D loss: 0.053383, acc.: 100.00%] [G loss: 0.007656]\n",
      "epoch:15 step:12154 [D loss: 0.101052, acc.: 99.22%] [G loss: 0.108109]\n",
      "epoch:15 step:12155 [D loss: 0.116068, acc.: 97.66%] [G loss: 0.051097]\n",
      "epoch:15 step:12156 [D loss: 0.085743, acc.: 98.44%] [G loss: 0.046989]\n",
      "epoch:15 step:12157 [D loss: 0.092474, acc.: 99.22%] [G loss: 4.673796]\n",
      "epoch:15 step:12158 [D loss: 0.021390, acc.: 100.00%] [G loss: 0.374111]\n",
      "epoch:15 step:12159 [D loss: 0.029161, acc.: 100.00%] [G loss: 0.224548]\n",
      "epoch:15 step:12160 [D loss: 0.046963, acc.: 100.00%] [G loss: 0.140628]\n",
      "epoch:15 step:12161 [D loss: 0.078820, acc.: 100.00%] [G loss: 0.110125]\n",
      "epoch:15 step:12162 [D loss: 0.053477, acc.: 99.22%] [G loss: 0.198828]\n",
      "epoch:15 step:12163 [D loss: 0.190713, acc.: 93.75%] [G loss: 5.753625]\n",
      "epoch:15 step:12164 [D loss: 0.363992, acc.: 78.91%] [G loss: 1.355870]\n",
      "epoch:15 step:12165 [D loss: 0.040119, acc.: 100.00%] [G loss: 3.860488]\n",
      "epoch:15 step:12166 [D loss: 0.114043, acc.: 97.66%] [G loss: 0.672365]\n",
      "epoch:15 step:12167 [D loss: 0.010328, acc.: 100.00%] [G loss: 1.825116]\n",
      "epoch:15 step:12168 [D loss: 0.099698, acc.: 96.09%] [G loss: 0.746523]\n",
      "epoch:15 step:12169 [D loss: 0.187463, acc.: 91.41%] [G loss: 2.708160]\n",
      "epoch:15 step:12170 [D loss: 0.250640, acc.: 89.06%] [G loss: 1.830951]\n",
      "epoch:15 step:12171 [D loss: 0.094083, acc.: 97.66%] [G loss: 0.510335]\n",
      "epoch:15 step:12172 [D loss: 0.036275, acc.: 100.00%] [G loss: 0.387582]\n",
      "epoch:15 step:12173 [D loss: 0.083795, acc.: 98.44%] [G loss: 3.325931]\n",
      "epoch:15 step:12174 [D loss: 0.043862, acc.: 100.00%] [G loss: 1.436396]\n",
      "epoch:15 step:12175 [D loss: 0.438035, acc.: 77.34%] [G loss: 5.565540]\n",
      "epoch:15 step:12176 [D loss: 0.780432, acc.: 67.19%] [G loss: 4.717418]\n",
      "epoch:15 step:12177 [D loss: 0.131625, acc.: 95.31%] [G loss: 3.451485]\n",
      "epoch:15 step:12178 [D loss: 0.089854, acc.: 96.88%] [G loss: 4.030532]\n",
      "epoch:15 step:12179 [D loss: 0.118877, acc.: 97.66%] [G loss: 4.921866]\n",
      "epoch:15 step:12180 [D loss: 0.028840, acc.: 100.00%] [G loss: 4.802069]\n",
      "epoch:15 step:12181 [D loss: 0.062825, acc.: 98.44%] [G loss: 3.783855]\n",
      "epoch:15 step:12182 [D loss: 0.080957, acc.: 98.44%] [G loss: 4.177696]\n",
      "epoch:15 step:12183 [D loss: 0.076903, acc.: 96.88%] [G loss: 3.304711]\n",
      "epoch:15 step:12184 [D loss: 0.068537, acc.: 99.22%] [G loss: 4.272684]\n",
      "epoch:15 step:12185 [D loss: 0.169071, acc.: 92.97%] [G loss: 5.192143]\n",
      "epoch:15 step:12186 [D loss: 0.117473, acc.: 97.66%] [G loss: 2.241125]\n",
      "epoch:15 step:12187 [D loss: 0.117947, acc.: 98.44%] [G loss: 5.804595]\n",
      "epoch:15 step:12188 [D loss: 0.021856, acc.: 100.00%] [G loss: 6.686846]\n",
      "epoch:15 step:12189 [D loss: 0.407286, acc.: 82.03%] [G loss: 5.086586]\n",
      "epoch:15 step:12190 [D loss: 0.081318, acc.: 96.88%] [G loss: 0.794197]\n",
      "epoch:15 step:12191 [D loss: 0.972303, acc.: 67.19%] [G loss: 9.427172]\n",
      "epoch:15 step:12192 [D loss: 1.703561, acc.: 54.69%] [G loss: 5.576665]\n",
      "epoch:15 step:12193 [D loss: 0.255598, acc.: 85.16%] [G loss: 3.597581]\n",
      "epoch:15 step:12194 [D loss: 0.073029, acc.: 98.44%] [G loss: 3.317213]\n",
      "epoch:15 step:12195 [D loss: 0.045507, acc.: 99.22%] [G loss: 3.500001]\n",
      "epoch:15 step:12196 [D loss: 0.030474, acc.: 100.00%] [G loss: 3.690685]\n",
      "epoch:15 step:12197 [D loss: 0.024263, acc.: 100.00%] [G loss: 3.273714]\n",
      "epoch:15 step:12198 [D loss: 0.029697, acc.: 100.00%] [G loss: 2.854503]\n",
      "epoch:15 step:12199 [D loss: 0.027405, acc.: 100.00%] [G loss: 2.709506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12200 [D loss: 0.025086, acc.: 100.00%] [G loss: 3.894337]\n",
      "epoch:15 step:12201 [D loss: 0.034027, acc.: 100.00%] [G loss: 3.362828]\n",
      "epoch:15 step:12202 [D loss: 0.122672, acc.: 98.44%] [G loss: 2.932991]\n",
      "epoch:15 step:12203 [D loss: 0.032832, acc.: 100.00%] [G loss: 3.526833]\n",
      "epoch:15 step:12204 [D loss: 0.097075, acc.: 98.44%] [G loss: 3.386551]\n",
      "epoch:15 step:12205 [D loss: 0.052699, acc.: 100.00%] [G loss: 2.302930]\n",
      "epoch:15 step:12206 [D loss: 0.063562, acc.: 99.22%] [G loss: 2.134313]\n",
      "epoch:15 step:12207 [D loss: 0.104324, acc.: 98.44%] [G loss: 2.236458]\n",
      "epoch:15 step:12208 [D loss: 0.142963, acc.: 95.31%] [G loss: 5.108035]\n",
      "epoch:15 step:12209 [D loss: 0.470759, acc.: 79.69%] [G loss: 3.646667]\n",
      "epoch:15 step:12210 [D loss: 0.088794, acc.: 97.66%] [G loss: 2.074030]\n",
      "epoch:15 step:12211 [D loss: 0.084926, acc.: 99.22%] [G loss: 3.076424]\n",
      "epoch:15 step:12212 [D loss: 0.070362, acc.: 98.44%] [G loss: 3.348286]\n",
      "epoch:15 step:12213 [D loss: 0.041544, acc.: 99.22%] [G loss: 0.961389]\n",
      "epoch:15 step:12214 [D loss: 0.019184, acc.: 100.00%] [G loss: 2.856157]\n",
      "epoch:15 step:12215 [D loss: 0.031903, acc.: 100.00%] [G loss: 2.952151]\n",
      "epoch:15 step:12216 [D loss: 0.037305, acc.: 100.00%] [G loss: 4.011220]\n",
      "epoch:15 step:12217 [D loss: 0.062804, acc.: 99.22%] [G loss: 2.704532]\n",
      "epoch:15 step:12218 [D loss: 0.024465, acc.: 100.00%] [G loss: 2.177424]\n",
      "epoch:15 step:12219 [D loss: 0.022634, acc.: 100.00%] [G loss: 2.632730]\n",
      "epoch:15 step:12220 [D loss: 0.197862, acc.: 92.97%] [G loss: 4.178426]\n",
      "epoch:15 step:12221 [D loss: 0.055757, acc.: 98.44%] [G loss: 4.592501]\n",
      "epoch:15 step:12222 [D loss: 0.178864, acc.: 95.31%] [G loss: 3.962678]\n",
      "epoch:15 step:12223 [D loss: 0.065407, acc.: 98.44%] [G loss: 3.340042]\n",
      "epoch:15 step:12224 [D loss: 0.144072, acc.: 94.53%] [G loss: 1.657812]\n",
      "epoch:15 step:12225 [D loss: 0.138288, acc.: 96.88%] [G loss: 3.482626]\n",
      "epoch:15 step:12226 [D loss: 0.012188, acc.: 100.00%] [G loss: 4.166689]\n",
      "epoch:15 step:12227 [D loss: 0.147645, acc.: 92.97%] [G loss: 2.662611]\n",
      "epoch:15 step:12228 [D loss: 0.062583, acc.: 98.44%] [G loss: 2.338198]\n",
      "epoch:15 step:12229 [D loss: 0.016519, acc.: 100.00%] [G loss: 3.479703]\n",
      "epoch:15 step:12230 [D loss: 0.044639, acc.: 100.00%] [G loss: 4.282341]\n",
      "epoch:15 step:12231 [D loss: 0.068777, acc.: 99.22%] [G loss: 2.670094]\n",
      "epoch:15 step:12232 [D loss: 0.020032, acc.: 100.00%] [G loss: 4.043975]\n",
      "epoch:15 step:12233 [D loss: 0.042554, acc.: 98.44%] [G loss: 1.879734]\n",
      "epoch:15 step:12234 [D loss: 0.696303, acc.: 64.84%] [G loss: 5.205853]\n",
      "epoch:15 step:12235 [D loss: 0.223166, acc.: 86.72%] [G loss: 4.701745]\n",
      "epoch:15 step:12236 [D loss: 0.049567, acc.: 99.22%] [G loss: 4.656216]\n",
      "epoch:15 step:12237 [D loss: 0.017550, acc.: 100.00%] [G loss: 2.075404]\n",
      "epoch:15 step:12238 [D loss: 0.025216, acc.: 100.00%] [G loss: 1.311125]\n",
      "epoch:15 step:12239 [D loss: 0.051235, acc.: 97.66%] [G loss: 1.136512]\n",
      "epoch:15 step:12240 [D loss: 0.005933, acc.: 100.00%] [G loss: 1.798107]\n",
      "epoch:15 step:12241 [D loss: 0.031019, acc.: 100.00%] [G loss: 1.282874]\n",
      "epoch:15 step:12242 [D loss: 0.110199, acc.: 97.66%] [G loss: 1.481631]\n",
      "epoch:15 step:12243 [D loss: 0.029043, acc.: 100.00%] [G loss: 1.620381]\n",
      "epoch:15 step:12244 [D loss: 0.074895, acc.: 97.66%] [G loss: 3.453115]\n",
      "epoch:15 step:12245 [D loss: 0.024050, acc.: 100.00%] [G loss: 3.411854]\n",
      "epoch:15 step:12246 [D loss: 0.447505, acc.: 81.25%] [G loss: 4.467915]\n",
      "epoch:15 step:12247 [D loss: 0.310323, acc.: 83.59%] [G loss: 3.279520]\n",
      "epoch:15 step:12248 [D loss: 0.062462, acc.: 99.22%] [G loss: 2.698341]\n",
      "epoch:15 step:12249 [D loss: 0.051217, acc.: 99.22%] [G loss: 4.733329]\n",
      "epoch:15 step:12250 [D loss: 0.006773, acc.: 100.00%] [G loss: 5.635754]\n",
      "epoch:15 step:12251 [D loss: 0.062837, acc.: 97.66%] [G loss: 3.507199]\n",
      "epoch:15 step:12252 [D loss: 0.033430, acc.: 99.22%] [G loss: 4.178399]\n",
      "epoch:15 step:12253 [D loss: 0.009113, acc.: 100.00%] [G loss: 4.698594]\n",
      "epoch:15 step:12254 [D loss: 0.007091, acc.: 100.00%] [G loss: 4.370655]\n",
      "epoch:15 step:12255 [D loss: 0.003982, acc.: 100.00%] [G loss: 1.559733]\n",
      "epoch:15 step:12256 [D loss: 0.039884, acc.: 100.00%] [G loss: 5.250166]\n",
      "epoch:15 step:12257 [D loss: 0.034318, acc.: 100.00%] [G loss: 5.947047]\n",
      "epoch:15 step:12258 [D loss: 0.011502, acc.: 100.00%] [G loss: 5.706587]\n",
      "epoch:15 step:12259 [D loss: 0.004492, acc.: 100.00%] [G loss: 0.796627]\n",
      "epoch:15 step:12260 [D loss: 0.020225, acc.: 100.00%] [G loss: 4.730267]\n",
      "epoch:15 step:12261 [D loss: 0.014220, acc.: 100.00%] [G loss: 4.171409]\n",
      "epoch:15 step:12262 [D loss: 0.066869, acc.: 100.00%] [G loss: 0.464696]\n",
      "epoch:15 step:12263 [D loss: 0.032102, acc.: 100.00%] [G loss: 5.819564]\n",
      "epoch:15 step:12264 [D loss: 0.026959, acc.: 99.22%] [G loss: 7.095557]\n",
      "epoch:15 step:12265 [D loss: 0.032478, acc.: 100.00%] [G loss: 6.102815]\n",
      "epoch:15 step:12266 [D loss: 0.023796, acc.: 100.00%] [G loss: 0.508229]\n",
      "epoch:15 step:12267 [D loss: 0.066003, acc.: 99.22%] [G loss: 7.090726]\n",
      "epoch:15 step:12268 [D loss: 0.043568, acc.: 97.66%] [G loss: 7.058174]\n",
      "epoch:15 step:12269 [D loss: 0.043258, acc.: 99.22%] [G loss: 6.349131]\n",
      "epoch:15 step:12270 [D loss: 0.006008, acc.: 100.00%] [G loss: 5.857566]\n",
      "epoch:15 step:12271 [D loss: 0.017586, acc.: 100.00%] [G loss: 5.318330]\n",
      "epoch:15 step:12272 [D loss: 0.029622, acc.: 99.22%] [G loss: 4.732990]\n",
      "epoch:15 step:12273 [D loss: 0.014217, acc.: 100.00%] [G loss: 4.053568]\n",
      "epoch:15 step:12274 [D loss: 0.009328, acc.: 100.00%] [G loss: 3.748593]\n",
      "epoch:15 step:12275 [D loss: 0.012818, acc.: 100.00%] [G loss: 0.726594]\n",
      "epoch:15 step:12276 [D loss: 0.042815, acc.: 100.00%] [G loss: 4.670105]\n",
      "epoch:15 step:12277 [D loss: 0.054625, acc.: 100.00%] [G loss: 5.329976]\n",
      "epoch:15 step:12278 [D loss: 0.132886, acc.: 95.31%] [G loss: 4.084717]\n",
      "epoch:15 step:12279 [D loss: 0.010098, acc.: 100.00%] [G loss: 0.675386]\n",
      "epoch:15 step:12280 [D loss: 0.209606, acc.: 92.97%] [G loss: 7.689444]\n",
      "epoch:15 step:12281 [D loss: 1.170403, acc.: 47.66%] [G loss: 1.760242]\n",
      "epoch:15 step:12282 [D loss: 0.005903, acc.: 100.00%] [G loss: 5.867514]\n",
      "epoch:15 step:12283 [D loss: 0.058189, acc.: 97.66%] [G loss: 4.564811]\n",
      "epoch:15 step:12284 [D loss: 0.006087, acc.: 100.00%] [G loss: 3.804372]\n",
      "epoch:15 step:12285 [D loss: 0.013444, acc.: 100.00%] [G loss: 3.861532]\n",
      "epoch:15 step:12286 [D loss: 0.010099, acc.: 100.00%] [G loss: 3.682930]\n",
      "epoch:15 step:12287 [D loss: 0.020197, acc.: 100.00%] [G loss: 2.746156]\n",
      "epoch:15 step:12288 [D loss: 0.016493, acc.: 100.00%] [G loss: 2.994406]\n",
      "epoch:15 step:12289 [D loss: 0.012352, acc.: 100.00%] [G loss: 2.541438]\n",
      "epoch:15 step:12290 [D loss: 0.044001, acc.: 100.00%] [G loss: 5.657849]\n",
      "epoch:15 step:12291 [D loss: 0.012499, acc.: 100.00%] [G loss: 3.166688]\n",
      "epoch:15 step:12292 [D loss: 0.092889, acc.: 98.44%] [G loss: 1.910535]\n",
      "epoch:15 step:12293 [D loss: 0.174897, acc.: 91.41%] [G loss: 6.567219]\n",
      "epoch:15 step:12294 [D loss: 1.050140, acc.: 52.34%] [G loss: 4.920815]\n",
      "epoch:15 step:12295 [D loss: 0.006879, acc.: 100.00%] [G loss: 6.489535]\n",
      "epoch:15 step:12296 [D loss: 0.003316, acc.: 100.00%] [G loss: 6.696792]\n",
      "epoch:15 step:12297 [D loss: 0.046091, acc.: 98.44%] [G loss: 6.403062]\n",
      "epoch:15 step:12298 [D loss: 0.002451, acc.: 100.00%] [G loss: 6.999391]\n",
      "epoch:15 step:12299 [D loss: 0.005768, acc.: 100.00%] [G loss: 6.668797]\n",
      "epoch:15 step:12300 [D loss: 0.003719, acc.: 100.00%] [G loss: 1.371792]\n",
      "epoch:15 step:12301 [D loss: 0.003126, acc.: 100.00%] [G loss: 6.680599]\n",
      "epoch:15 step:12302 [D loss: 0.005106, acc.: 100.00%] [G loss: 6.439867]\n",
      "epoch:15 step:12303 [D loss: 0.004384, acc.: 100.00%] [G loss: 5.586991]\n",
      "epoch:15 step:12304 [D loss: 0.023242, acc.: 100.00%] [G loss: 4.933901]\n",
      "epoch:15 step:12305 [D loss: 0.047851, acc.: 100.00%] [G loss: 5.988194]\n",
      "epoch:15 step:12306 [D loss: 0.028797, acc.: 99.22%] [G loss: 0.414540]\n",
      "epoch:15 step:12307 [D loss: 0.005972, acc.: 100.00%] [G loss: 7.135078]\n",
      "epoch:15 step:12308 [D loss: 0.055717, acc.: 99.22%] [G loss: 5.894170]\n",
      "epoch:15 step:12309 [D loss: 0.006271, acc.: 100.00%] [G loss: 5.518494]\n",
      "epoch:15 step:12310 [D loss: 0.012732, acc.: 100.00%] [G loss: 4.792345]\n",
      "epoch:15 step:12311 [D loss: 0.030695, acc.: 100.00%] [G loss: 5.704084]\n",
      "epoch:15 step:12312 [D loss: 0.008550, acc.: 99.22%] [G loss: 0.522485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12313 [D loss: 0.014256, acc.: 100.00%] [G loss: 0.185626]\n",
      "epoch:15 step:12314 [D loss: 0.052170, acc.: 98.44%] [G loss: 0.140568]\n",
      "epoch:15 step:12315 [D loss: 1.404244, acc.: 50.78%] [G loss: 10.003454]\n",
      "epoch:15 step:12316 [D loss: 3.910909, acc.: 50.00%] [G loss: 6.800622]\n",
      "epoch:15 step:12317 [D loss: 2.365021, acc.: 50.00%] [G loss: 3.483130]\n",
      "epoch:15 step:12318 [D loss: 0.982964, acc.: 53.91%] [G loss: 1.666831]\n",
      "epoch:15 step:12319 [D loss: 0.194448, acc.: 95.31%] [G loss: 1.118810]\n",
      "epoch:15 step:12320 [D loss: 0.103231, acc.: 100.00%] [G loss: 1.191412]\n",
      "epoch:15 step:12321 [D loss: 0.111767, acc.: 98.44%] [G loss: 0.885613]\n",
      "epoch:15 step:12322 [D loss: 0.107100, acc.: 96.88%] [G loss: 2.999401]\n",
      "epoch:15 step:12323 [D loss: 0.138654, acc.: 97.66%] [G loss: 2.314471]\n",
      "epoch:15 step:12324 [D loss: 0.148437, acc.: 98.44%] [G loss: 0.930571]\n",
      "epoch:15 step:12325 [D loss: 0.270282, acc.: 90.62%] [G loss: 2.318933]\n",
      "epoch:15 step:12326 [D loss: 0.157991, acc.: 94.53%] [G loss: 1.728414]\n",
      "epoch:15 step:12327 [D loss: 0.407344, acc.: 81.25%] [G loss: 1.534586]\n",
      "epoch:15 step:12328 [D loss: 0.249711, acc.: 90.62%] [G loss: 2.186330]\n",
      "epoch:15 step:12329 [D loss: 0.782887, acc.: 49.22%] [G loss: 0.229246]\n",
      "epoch:15 step:12330 [D loss: 0.292072, acc.: 91.41%] [G loss: 0.734600]\n",
      "epoch:15 step:12331 [D loss: 0.243345, acc.: 92.97%] [G loss: 0.098904]\n",
      "epoch:15 step:12332 [D loss: 0.545076, acc.: 72.66%] [G loss: 2.517349]\n",
      "epoch:15 step:12333 [D loss: 0.226303, acc.: 90.62%] [G loss: 2.465559]\n",
      "epoch:15 step:12334 [D loss: 0.875469, acc.: 57.03%] [G loss: 3.335541]\n",
      "epoch:15 step:12335 [D loss: 0.121120, acc.: 98.44%] [G loss: 0.243841]\n",
      "epoch:15 step:12336 [D loss: 0.122656, acc.: 96.88%] [G loss: 0.807597]\n",
      "epoch:15 step:12337 [D loss: 0.099169, acc.: 99.22%] [G loss: 2.915743]\n",
      "epoch:15 step:12338 [D loss: 0.055938, acc.: 99.22%] [G loss: 3.007294]\n",
      "epoch:15 step:12339 [D loss: 0.047009, acc.: 100.00%] [G loss: 0.325617]\n",
      "epoch:15 step:12340 [D loss: 0.030407, acc.: 100.00%] [G loss: 0.602214]\n",
      "epoch:15 step:12341 [D loss: 0.074296, acc.: 100.00%] [G loss: 0.094637]\n",
      "epoch:15 step:12342 [D loss: 0.026577, acc.: 100.00%] [G loss: 3.782974]\n",
      "epoch:15 step:12343 [D loss: 0.066251, acc.: 99.22%] [G loss: 0.229374]\n",
      "epoch:15 step:12344 [D loss: 0.060538, acc.: 99.22%] [G loss: 3.049989]\n",
      "epoch:15 step:12345 [D loss: 0.031153, acc.: 100.00%] [G loss: 0.135271]\n",
      "epoch:15 step:12346 [D loss: 0.035325, acc.: 99.22%] [G loss: 2.863078]\n",
      "epoch:15 step:12347 [D loss: 0.065268, acc.: 100.00%] [G loss: 0.169160]\n",
      "epoch:15 step:12348 [D loss: 0.031279, acc.: 100.00%] [G loss: 0.023022]\n",
      "epoch:15 step:12349 [D loss: 0.184759, acc.: 92.19%] [G loss: 0.318943]\n",
      "epoch:15 step:12350 [D loss: 0.073232, acc.: 98.44%] [G loss: 3.347940]\n",
      "epoch:15 step:12351 [D loss: 0.065291, acc.: 98.44%] [G loss: 1.781743]\n",
      "epoch:15 step:12352 [D loss: 0.043154, acc.: 100.00%] [G loss: 0.224936]\n",
      "epoch:15 step:12353 [D loss: 0.044126, acc.: 99.22%] [G loss: 1.290818]\n",
      "epoch:15 step:12354 [D loss: 0.117958, acc.: 97.66%] [G loss: 0.052906]\n",
      "epoch:15 step:12355 [D loss: 0.026097, acc.: 99.22%] [G loss: 0.053972]\n",
      "epoch:15 step:12356 [D loss: 0.034564, acc.: 98.44%] [G loss: 0.752183]\n",
      "epoch:15 step:12357 [D loss: 0.043720, acc.: 100.00%] [G loss: 0.339649]\n",
      "epoch:15 step:12358 [D loss: 0.035739, acc.: 99.22%] [G loss: 0.012319]\n",
      "epoch:15 step:12359 [D loss: 0.039716, acc.: 100.00%] [G loss: 0.305910]\n",
      "epoch:15 step:12360 [D loss: 0.124194, acc.: 96.88%] [G loss: 0.022432]\n",
      "epoch:15 step:12361 [D loss: 0.032853, acc.: 99.22%] [G loss: 1.866046]\n",
      "epoch:15 step:12362 [D loss: 0.040452, acc.: 99.22%] [G loss: 0.014098]\n",
      "epoch:15 step:12363 [D loss: 0.018160, acc.: 100.00%] [G loss: 0.005983]\n",
      "epoch:15 step:12364 [D loss: 0.052198, acc.: 100.00%] [G loss: 0.008003]\n",
      "epoch:15 step:12365 [D loss: 0.017014, acc.: 100.00%] [G loss: 2.029490]\n",
      "epoch:15 step:12366 [D loss: 0.020824, acc.: 100.00%] [G loss: 1.550014]\n",
      "epoch:15 step:12367 [D loss: 0.041512, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:15 step:12368 [D loss: 0.010487, acc.: 100.00%] [G loss: 0.011299]\n",
      "epoch:15 step:12369 [D loss: 0.058340, acc.: 99.22%] [G loss: 0.008002]\n",
      "epoch:15 step:12370 [D loss: 0.020469, acc.: 100.00%] [G loss: 0.011119]\n",
      "epoch:15 step:12371 [D loss: 0.175186, acc.: 95.31%] [G loss: 0.013994]\n",
      "epoch:15 step:12372 [D loss: 0.053830, acc.: 99.22%] [G loss: 3.022598]\n",
      "epoch:15 step:12373 [D loss: 0.027704, acc.: 100.00%] [G loss: 0.001379]\n",
      "epoch:15 step:12374 [D loss: 0.044773, acc.: 99.22%] [G loss: 2.087477]\n",
      "epoch:15 step:12375 [D loss: 0.106992, acc.: 97.66%] [G loss: 0.225747]\n",
      "epoch:15 step:12376 [D loss: 0.095160, acc.: 96.09%] [G loss: 0.062115]\n",
      "epoch:15 step:12377 [D loss: 0.261665, acc.: 90.62%] [G loss: 0.000169]\n",
      "epoch:15 step:12378 [D loss: 0.020330, acc.: 100.00%] [G loss: 2.383039]\n",
      "epoch:15 step:12379 [D loss: 0.019665, acc.: 100.00%] [G loss: 0.006806]\n",
      "epoch:15 step:12380 [D loss: 0.019788, acc.: 100.00%] [G loss: 0.004465]\n",
      "epoch:15 step:12381 [D loss: 0.007368, acc.: 100.00%] [G loss: 0.093158]\n",
      "epoch:15 step:12382 [D loss: 0.013408, acc.: 100.00%] [G loss: 0.024089]\n",
      "epoch:15 step:12383 [D loss: 0.006733, acc.: 100.00%] [G loss: 0.038338]\n",
      "epoch:15 step:12384 [D loss: 0.038568, acc.: 98.44%] [G loss: 0.929430]\n",
      "epoch:15 step:12385 [D loss: 0.023467, acc.: 100.00%] [G loss: 0.042335]\n",
      "epoch:15 step:12386 [D loss: 0.347523, acc.: 84.38%] [G loss: 5.761503]\n",
      "epoch:15 step:12387 [D loss: 0.897058, acc.: 64.06%] [G loss: 0.157139]\n",
      "epoch:15 step:12388 [D loss: 0.265550, acc.: 92.19%] [G loss: 6.566636]\n",
      "epoch:15 step:12389 [D loss: 0.019569, acc.: 99.22%] [G loss: 8.528266]\n",
      "epoch:15 step:12390 [D loss: 0.185151, acc.: 92.19%] [G loss: 2.802083]\n",
      "epoch:15 step:12391 [D loss: 0.131841, acc.: 95.31%] [G loss: 3.587983]\n",
      "epoch:15 step:12392 [D loss: 0.053495, acc.: 98.44%] [G loss: 3.886703]\n",
      "epoch:15 step:12393 [D loss: 0.029921, acc.: 99.22%] [G loss: 3.330945]\n",
      "epoch:15 step:12394 [D loss: 0.009928, acc.: 100.00%] [G loss: 2.611200]\n",
      "epoch:15 step:12395 [D loss: 0.053746, acc.: 98.44%] [G loss: 1.790470]\n",
      "epoch:15 step:12396 [D loss: 0.069735, acc.: 98.44%] [G loss: 3.973673]\n",
      "epoch:15 step:12397 [D loss: 0.121421, acc.: 97.66%] [G loss: 2.272027]\n",
      "epoch:15 step:12398 [D loss: 0.034358, acc.: 99.22%] [G loss: 2.292683]\n",
      "epoch:15 step:12399 [D loss: 0.043636, acc.: 100.00%] [G loss: 2.846371]\n",
      "epoch:15 step:12400 [D loss: 0.079067, acc.: 97.66%] [G loss: 1.824897]\n",
      "epoch:15 step:12401 [D loss: 0.054912, acc.: 99.22%] [G loss: 1.501560]\n",
      "epoch:15 step:12402 [D loss: 0.065014, acc.: 99.22%] [G loss: 2.537915]\n",
      "epoch:15 step:12403 [D loss: 0.041630, acc.: 100.00%] [G loss: 2.491132]\n",
      "epoch:15 step:12404 [D loss: 0.005646, acc.: 100.00%] [G loss: 2.786755]\n",
      "epoch:15 step:12405 [D loss: 0.038366, acc.: 100.00%] [G loss: 2.461801]\n",
      "epoch:15 step:12406 [D loss: 0.036254, acc.: 100.00%] [G loss: 2.634862]\n",
      "epoch:15 step:12407 [D loss: 0.097252, acc.: 97.66%] [G loss: 3.575413]\n",
      "epoch:15 step:12408 [D loss: 0.073194, acc.: 97.66%] [G loss: 4.329488]\n",
      "epoch:15 step:12409 [D loss: 0.017176, acc.: 100.00%] [G loss: 2.027304]\n",
      "epoch:15 step:12410 [D loss: 0.027688, acc.: 100.00%] [G loss: 4.628446]\n",
      "epoch:15 step:12411 [D loss: 0.005958, acc.: 100.00%] [G loss: 5.032000]\n",
      "epoch:15 step:12412 [D loss: 0.040870, acc.: 100.00%] [G loss: 1.171104]\n",
      "epoch:15 step:12413 [D loss: 0.113652, acc.: 98.44%] [G loss: 5.987814]\n",
      "epoch:15 step:12414 [D loss: 0.178544, acc.: 91.41%] [G loss: 3.234715]\n",
      "epoch:15 step:12415 [D loss: 0.052731, acc.: 99.22%] [G loss: 3.912766]\n",
      "epoch:15 step:12416 [D loss: 0.015704, acc.: 100.00%] [G loss: 3.829158]\n",
      "epoch:15 step:12417 [D loss: 0.005025, acc.: 100.00%] [G loss: 4.355559]\n",
      "epoch:15 step:12418 [D loss: 0.021874, acc.: 99.22%] [G loss: 4.119408]\n",
      "epoch:15 step:12419 [D loss: 0.025246, acc.: 100.00%] [G loss: 3.870688]\n",
      "epoch:15 step:12420 [D loss: 0.050198, acc.: 98.44%] [G loss: 3.463983]\n",
      "epoch:15 step:12421 [D loss: 0.063894, acc.: 98.44%] [G loss: 6.021738]\n",
      "epoch:15 step:12422 [D loss: 0.021465, acc.: 99.22%] [G loss: 5.523573]\n",
      "epoch:15 step:12423 [D loss: 0.028499, acc.: 99.22%] [G loss: 4.725440]\n",
      "epoch:15 step:12424 [D loss: 0.031069, acc.: 100.00%] [G loss: 4.889786]\n",
      "epoch:15 step:12425 [D loss: 0.049617, acc.: 98.44%] [G loss: 3.481504]\n",
      "epoch:15 step:12426 [D loss: 0.021440, acc.: 100.00%] [G loss: 4.695419]\n",
      "epoch:15 step:12427 [D loss: 0.003310, acc.: 100.00%] [G loss: 4.191660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12428 [D loss: 0.040664, acc.: 99.22%] [G loss: 3.083726]\n",
      "epoch:15 step:12429 [D loss: 0.019987, acc.: 100.00%] [G loss: 4.453010]\n",
      "epoch:15 step:12430 [D loss: 0.331105, acc.: 85.16%] [G loss: 5.364576]\n",
      "epoch:15 step:12431 [D loss: 0.106594, acc.: 94.53%] [G loss: 6.581285]\n",
      "epoch:15 step:12432 [D loss: 0.026877, acc.: 99.22%] [G loss: 5.346220]\n",
      "epoch:15 step:12433 [D loss: 0.026137, acc.: 100.00%] [G loss: 4.005535]\n",
      "epoch:15 step:12434 [D loss: 0.005123, acc.: 100.00%] [G loss: 3.884129]\n",
      "epoch:15 step:12435 [D loss: 0.008622, acc.: 100.00%] [G loss: 0.870077]\n",
      "epoch:15 step:12436 [D loss: 0.043356, acc.: 99.22%] [G loss: 4.144663]\n",
      "epoch:15 step:12437 [D loss: 0.000673, acc.: 100.00%] [G loss: 4.294844]\n",
      "epoch:15 step:12438 [D loss: 0.002124, acc.: 100.00%] [G loss: 0.239894]\n",
      "epoch:15 step:12439 [D loss: 0.059106, acc.: 99.22%] [G loss: 4.204021]\n",
      "epoch:15 step:12440 [D loss: 0.038428, acc.: 99.22%] [G loss: 2.888705]\n",
      "epoch:15 step:12441 [D loss: 0.002017, acc.: 100.00%] [G loss: 2.677317]\n",
      "epoch:15 step:12442 [D loss: 0.281544, acc.: 86.72%] [G loss: 8.325266]\n",
      "epoch:15 step:12443 [D loss: 0.226676, acc.: 89.06%] [G loss: 6.931660]\n",
      "epoch:15 step:12444 [D loss: 0.007652, acc.: 100.00%] [G loss: 7.200315]\n",
      "epoch:15 step:12445 [D loss: 0.001640, acc.: 100.00%] [G loss: 6.730000]\n",
      "epoch:15 step:12446 [D loss: 0.006203, acc.: 100.00%] [G loss: 6.219863]\n",
      "epoch:15 step:12447 [D loss: 0.003161, acc.: 100.00%] [G loss: 6.167837]\n",
      "epoch:15 step:12448 [D loss: 0.005482, acc.: 100.00%] [G loss: 6.232657]\n",
      "epoch:15 step:12449 [D loss: 0.005078, acc.: 100.00%] [G loss: 6.049315]\n",
      "epoch:15 step:12450 [D loss: 0.006031, acc.: 100.00%] [G loss: 6.130253]\n",
      "epoch:15 step:12451 [D loss: 0.056131, acc.: 98.44%] [G loss: 3.717468]\n",
      "epoch:15 step:12452 [D loss: 0.030110, acc.: 99.22%] [G loss: 4.632260]\n",
      "epoch:15 step:12453 [D loss: 0.002002, acc.: 100.00%] [G loss: 5.881597]\n",
      "epoch:15 step:12454 [D loss: 0.004575, acc.: 100.00%] [G loss: 1.210304]\n",
      "epoch:15 step:12455 [D loss: 0.023564, acc.: 100.00%] [G loss: 5.617145]\n",
      "epoch:15 step:12456 [D loss: 0.016470, acc.: 100.00%] [G loss: 6.648192]\n",
      "epoch:15 step:12457 [D loss: 0.005870, acc.: 100.00%] [G loss: 7.307063]\n",
      "epoch:15 step:12458 [D loss: 0.002618, acc.: 100.00%] [G loss: 1.072759]\n",
      "epoch:15 step:12459 [D loss: 0.002736, acc.: 100.00%] [G loss: 6.565661]\n",
      "epoch:15 step:12460 [D loss: 0.010624, acc.: 100.00%] [G loss: 1.119365]\n",
      "epoch:15 step:12461 [D loss: 0.041457, acc.: 99.22%] [G loss: 7.439372]\n",
      "epoch:15 step:12462 [D loss: 0.033626, acc.: 99.22%] [G loss: 6.980350]\n",
      "epoch:15 step:12463 [D loss: 0.002299, acc.: 100.00%] [G loss: 7.054209]\n",
      "epoch:15 step:12464 [D loss: 0.002654, acc.: 100.00%] [G loss: 3.279836]\n",
      "epoch:15 step:12465 [D loss: 0.005583, acc.: 100.00%] [G loss: 7.132792]\n",
      "epoch:15 step:12466 [D loss: 0.005860, acc.: 100.00%] [G loss: 6.142818]\n",
      "epoch:15 step:12467 [D loss: 0.003289, acc.: 100.00%] [G loss: 6.166070]\n",
      "epoch:15 step:12468 [D loss: 0.012593, acc.: 100.00%] [G loss: 6.108701]\n",
      "epoch:15 step:12469 [D loss: 0.011174, acc.: 100.00%] [G loss: 5.336438]\n",
      "epoch:15 step:12470 [D loss: 0.006272, acc.: 100.00%] [G loss: 4.625396]\n",
      "epoch:15 step:12471 [D loss: 0.001321, acc.: 100.00%] [G loss: 4.077735]\n",
      "epoch:15 step:12472 [D loss: 0.003679, acc.: 100.00%] [G loss: 0.682436]\n",
      "epoch:15 step:12473 [D loss: 0.004331, acc.: 100.00%] [G loss: 0.134884]\n",
      "epoch:15 step:12474 [D loss: 0.010957, acc.: 100.00%] [G loss: 0.105365]\n",
      "epoch:15 step:12475 [D loss: 0.038759, acc.: 100.00%] [G loss: 4.271813]\n",
      "epoch:15 step:12476 [D loss: 0.002127, acc.: 100.00%] [G loss: 5.153072]\n",
      "epoch:15 step:12477 [D loss: 0.302020, acc.: 85.94%] [G loss: 0.050940]\n",
      "epoch:15 step:12478 [D loss: 0.249814, acc.: 90.62%] [G loss: 6.155100]\n",
      "epoch:15 step:12479 [D loss: 0.008088, acc.: 99.22%] [G loss: 8.147880]\n",
      "epoch:15 step:12480 [D loss: 0.044072, acc.: 98.44%] [G loss: 7.600927]\n",
      "epoch:15 step:12481 [D loss: 0.058596, acc.: 99.22%] [G loss: 6.290028]\n",
      "epoch:15 step:12482 [D loss: 0.001675, acc.: 100.00%] [G loss: 5.044265]\n",
      "epoch:15 step:12483 [D loss: 0.000150, acc.: 100.00%] [G loss: 3.244133]\n",
      "epoch:15 step:12484 [D loss: 0.007148, acc.: 100.00%] [G loss: 1.928979]\n",
      "epoch:15 step:12485 [D loss: 0.000918, acc.: 100.00%] [G loss: 1.421056]\n",
      "epoch:15 step:12486 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.366799]\n",
      "epoch:15 step:12487 [D loss: 0.000881, acc.: 100.00%] [G loss: 2.348546]\n",
      "epoch:15 step:12488 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.682030]\n",
      "epoch:15 step:12489 [D loss: 0.001555, acc.: 100.00%] [G loss: 0.038434]\n",
      "epoch:15 step:12490 [D loss: 0.245878, acc.: 91.41%] [G loss: 6.014912]\n",
      "epoch:15 step:12491 [D loss: 0.353968, acc.: 84.38%] [G loss: 0.522942]\n",
      "epoch:15 step:12492 [D loss: 0.001559, acc.: 100.00%] [G loss: 1.058430]\n",
      "epoch:15 step:12493 [D loss: 0.451146, acc.: 79.69%] [G loss: 8.849917]\n",
      "epoch:15 step:12494 [D loss: 3.770150, acc.: 50.00%] [G loss: 4.421111]\n",
      "epoch:15 step:12495 [D loss: 1.181962, acc.: 56.25%] [G loss: 0.392060]\n",
      "epoch:15 step:12496 [D loss: 1.041976, acc.: 55.47%] [G loss: 4.813782]\n",
      "epoch:16 step:12497 [D loss: 0.348027, acc.: 75.78%] [G loss: 4.763371]\n",
      "epoch:16 step:12498 [D loss: 0.753106, acc.: 60.94%] [G loss: 3.189132]\n",
      "epoch:16 step:12499 [D loss: 0.064242, acc.: 99.22%] [G loss: 1.965040]\n",
      "epoch:16 step:12500 [D loss: 0.050292, acc.: 100.00%] [G loss: 1.761769]\n",
      "epoch:16 step:12501 [D loss: 0.068689, acc.: 99.22%] [G loss: 1.094973]\n",
      "epoch:16 step:12502 [D loss: 0.031562, acc.: 100.00%] [G loss: 3.260908]\n",
      "epoch:16 step:12503 [D loss: 0.090155, acc.: 98.44%] [G loss: 2.393122]\n",
      "epoch:16 step:12504 [D loss: 0.042577, acc.: 100.00%] [G loss: 2.393903]\n",
      "epoch:16 step:12505 [D loss: 0.064989, acc.: 99.22%] [G loss: 2.617213]\n",
      "epoch:16 step:12506 [D loss: 0.048204, acc.: 100.00%] [G loss: 2.677983]\n",
      "epoch:16 step:12507 [D loss: 0.065272, acc.: 98.44%] [G loss: 2.284655]\n",
      "epoch:16 step:12508 [D loss: 0.166568, acc.: 96.88%] [G loss: 0.398226]\n",
      "epoch:16 step:12509 [D loss: 0.066468, acc.: 99.22%] [G loss: 1.326859]\n",
      "epoch:16 step:12510 [D loss: 0.073399, acc.: 97.66%] [G loss: 2.169566]\n",
      "epoch:16 step:12511 [D loss: 0.271190, acc.: 87.50%] [G loss: 3.925310]\n",
      "epoch:16 step:12512 [D loss: 0.158086, acc.: 92.97%] [G loss: 3.855177]\n",
      "epoch:16 step:12513 [D loss: 0.072010, acc.: 97.66%] [G loss: 2.563097]\n",
      "epoch:16 step:12514 [D loss: 0.013673, acc.: 100.00%] [G loss: 1.922988]\n",
      "epoch:16 step:12515 [D loss: 0.076918, acc.: 99.22%] [G loss: 1.471678]\n",
      "epoch:16 step:12516 [D loss: 0.008304, acc.: 100.00%] [G loss: 2.499784]\n",
      "epoch:16 step:12517 [D loss: 0.029112, acc.: 100.00%] [G loss: 2.722623]\n",
      "epoch:16 step:12518 [D loss: 0.015167, acc.: 100.00%] [G loss: 2.111498]\n",
      "epoch:16 step:12519 [D loss: 0.024836, acc.: 100.00%] [G loss: 1.493158]\n",
      "epoch:16 step:12520 [D loss: 0.042492, acc.: 100.00%] [G loss: 0.921613]\n",
      "epoch:16 step:12521 [D loss: 0.015925, acc.: 100.00%] [G loss: 1.129570]\n",
      "epoch:16 step:12522 [D loss: 0.061625, acc.: 99.22%] [G loss: 1.000133]\n",
      "epoch:16 step:12523 [D loss: 0.042818, acc.: 99.22%] [G loss: 1.001916]\n",
      "epoch:16 step:12524 [D loss: 0.074030, acc.: 97.66%] [G loss: 1.195692]\n",
      "epoch:16 step:12525 [D loss: 0.364618, acc.: 85.16%] [G loss: 3.239985]\n",
      "epoch:16 step:12526 [D loss: 0.144350, acc.: 92.97%] [G loss: 3.630988]\n",
      "epoch:16 step:12527 [D loss: 0.291879, acc.: 87.50%] [G loss: 1.856729]\n",
      "epoch:16 step:12528 [D loss: 0.176139, acc.: 91.41%] [G loss: 4.235225]\n",
      "epoch:16 step:12529 [D loss: 0.027045, acc.: 100.00%] [G loss: 4.283759]\n",
      "epoch:16 step:12530 [D loss: 0.007386, acc.: 100.00%] [G loss: 4.657017]\n",
      "epoch:16 step:12531 [D loss: 0.028473, acc.: 99.22%] [G loss: 4.385553]\n",
      "epoch:16 step:12532 [D loss: 0.044248, acc.: 98.44%] [G loss: 3.452034]\n",
      "epoch:16 step:12533 [D loss: 0.013685, acc.: 100.00%] [G loss: 3.481309]\n",
      "epoch:16 step:12534 [D loss: 0.042819, acc.: 98.44%] [G loss: 4.003113]\n",
      "epoch:16 step:12535 [D loss: 0.022224, acc.: 100.00%] [G loss: 4.031549]\n",
      "epoch:16 step:12536 [D loss: 0.010755, acc.: 100.00%] [G loss: 4.621694]\n",
      "epoch:16 step:12537 [D loss: 0.010896, acc.: 100.00%] [G loss: 1.804825]\n",
      "epoch:16 step:12538 [D loss: 0.009404, acc.: 100.00%] [G loss: 0.932937]\n",
      "epoch:16 step:12539 [D loss: 0.011643, acc.: 100.00%] [G loss: 4.817033]\n",
      "epoch:16 step:12540 [D loss: 0.035780, acc.: 100.00%] [G loss: 3.725946]\n",
      "epoch:16 step:12541 [D loss: 0.014435, acc.: 100.00%] [G loss: 4.142685]\n",
      "epoch:16 step:12542 [D loss: 0.049432, acc.: 100.00%] [G loss: 4.014289]\n",
      "epoch:16 step:12543 [D loss: 0.008213, acc.: 100.00%] [G loss: 3.943641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12544 [D loss: 0.009802, acc.: 100.00%] [G loss: 4.088722]\n",
      "epoch:16 step:12545 [D loss: 0.013983, acc.: 100.00%] [G loss: 4.361564]\n",
      "epoch:16 step:12546 [D loss: 0.012821, acc.: 100.00%] [G loss: 4.021571]\n",
      "epoch:16 step:12547 [D loss: 0.023808, acc.: 100.00%] [G loss: 0.264130]\n",
      "epoch:16 step:12548 [D loss: 0.022154, acc.: 100.00%] [G loss: 4.471891]\n",
      "epoch:16 step:12549 [D loss: 0.009748, acc.: 100.00%] [G loss: 4.498448]\n",
      "epoch:16 step:12550 [D loss: 0.016176, acc.: 100.00%] [G loss: 3.890328]\n",
      "epoch:16 step:12551 [D loss: 0.009083, acc.: 100.00%] [G loss: 2.648175]\n",
      "epoch:16 step:12552 [D loss: 0.023807, acc.: 99.22%] [G loss: 0.310078]\n",
      "epoch:16 step:12553 [D loss: 0.025207, acc.: 100.00%] [G loss: 3.850324]\n",
      "epoch:16 step:12554 [D loss: 0.028068, acc.: 100.00%] [G loss: 3.686971]\n",
      "epoch:16 step:12555 [D loss: 0.010938, acc.: 100.00%] [G loss: 0.101486]\n",
      "epoch:16 step:12556 [D loss: 0.026941, acc.: 100.00%] [G loss: 4.230635]\n",
      "epoch:16 step:12557 [D loss: 0.091519, acc.: 98.44%] [G loss: 2.465146]\n",
      "epoch:16 step:12558 [D loss: 0.032184, acc.: 100.00%] [G loss: 2.383354]\n",
      "epoch:16 step:12559 [D loss: 0.036218, acc.: 99.22%] [G loss: 2.801084]\n",
      "epoch:16 step:12560 [D loss: 0.007178, acc.: 100.00%] [G loss: 1.537439]\n",
      "epoch:16 step:12561 [D loss: 0.127589, acc.: 97.66%] [G loss: 4.745740]\n",
      "epoch:16 step:12562 [D loss: 0.024989, acc.: 99.22%] [G loss: 7.011052]\n",
      "epoch:16 step:12563 [D loss: 0.556718, acc.: 73.44%] [G loss: 4.310512]\n",
      "epoch:16 step:12564 [D loss: 0.001475, acc.: 100.00%] [G loss: 5.691576]\n",
      "epoch:16 step:12565 [D loss: 0.081569, acc.: 96.88%] [G loss: 7.361556]\n",
      "epoch:16 step:12566 [D loss: 0.007628, acc.: 100.00%] [G loss: 7.900033]\n",
      "epoch:16 step:12567 [D loss: 0.078029, acc.: 97.66%] [G loss: 1.614218]\n",
      "epoch:16 step:12568 [D loss: 0.016750, acc.: 100.00%] [G loss: 4.697445]\n",
      "epoch:16 step:12569 [D loss: 0.049413, acc.: 100.00%] [G loss: 6.728001]\n",
      "epoch:16 step:12570 [D loss: 0.003180, acc.: 100.00%] [G loss: 7.376000]\n",
      "epoch:16 step:12571 [D loss: 0.005163, acc.: 100.00%] [G loss: 3.287236]\n",
      "epoch:16 step:12572 [D loss: 0.032408, acc.: 97.66%] [G loss: 7.122641]\n",
      "epoch:16 step:12573 [D loss: 1.363323, acc.: 53.91%] [G loss: 11.528583]\n",
      "epoch:16 step:12574 [D loss: 3.112265, acc.: 50.00%] [G loss: 3.478836]\n",
      "epoch:16 step:12575 [D loss: 0.694047, acc.: 70.31%] [G loss: 2.949397]\n",
      "epoch:16 step:12576 [D loss: 0.066158, acc.: 97.66%] [G loss: 2.991994]\n",
      "epoch:16 step:12577 [D loss: 0.077044, acc.: 99.22%] [G loss: 2.981090]\n",
      "epoch:16 step:12578 [D loss: 0.066895, acc.: 99.22%] [G loss: 1.524996]\n",
      "epoch:16 step:12579 [D loss: 0.136930, acc.: 96.88%] [G loss: 3.019767]\n",
      "epoch:16 step:12580 [D loss: 0.639996, acc.: 66.41%] [G loss: 2.046526]\n",
      "epoch:16 step:12581 [D loss: 0.022530, acc.: 100.00%] [G loss: 3.413778]\n",
      "epoch:16 step:12582 [D loss: 0.317866, acc.: 86.72%] [G loss: 2.486945]\n",
      "epoch:16 step:12583 [D loss: 0.057632, acc.: 100.00%] [G loss: 0.769765]\n",
      "epoch:16 step:12584 [D loss: 0.051688, acc.: 99.22%] [G loss: 0.448850]\n",
      "epoch:16 step:12585 [D loss: 0.064436, acc.: 100.00%] [G loss: 0.576806]\n",
      "epoch:16 step:12586 [D loss: 0.081867, acc.: 99.22%] [G loss: 0.690099]\n",
      "epoch:16 step:12587 [D loss: 0.160068, acc.: 96.09%] [G loss: 0.452835]\n",
      "epoch:16 step:12588 [D loss: 0.029732, acc.: 100.00%] [G loss: 3.213504]\n",
      "epoch:16 step:12589 [D loss: 0.297021, acc.: 85.94%] [G loss: 0.070600]\n",
      "epoch:16 step:12590 [D loss: 0.677638, acc.: 65.62%] [G loss: 2.039155]\n",
      "epoch:16 step:12591 [D loss: 0.470635, acc.: 71.88%] [G loss: 4.990349]\n",
      "epoch:16 step:12592 [D loss: 0.409471, acc.: 78.12%] [G loss: 0.001107]\n",
      "epoch:16 step:12593 [D loss: 0.023716, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:16 step:12594 [D loss: 0.028620, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:16 step:12595 [D loss: 0.039372, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:16 step:12596 [D loss: 0.008091, acc.: 100.00%] [G loss: 0.001269]\n",
      "epoch:16 step:12597 [D loss: 0.007666, acc.: 100.00%] [G loss: 0.008924]\n",
      "epoch:16 step:12598 [D loss: 0.021603, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:16 step:12599 [D loss: 0.012984, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:16 step:12600 [D loss: 0.022148, acc.: 100.00%] [G loss: 0.000538]\n",
      "epoch:16 step:12601 [D loss: 0.027805, acc.: 100.00%] [G loss: 0.009694]\n",
      "epoch:16 step:12602 [D loss: 0.024298, acc.: 100.00%] [G loss: 3.618467]\n",
      "epoch:16 step:12603 [D loss: 0.012353, acc.: 100.00%] [G loss: 0.464120]\n",
      "epoch:16 step:12604 [D loss: 0.065693, acc.: 100.00%] [G loss: 0.001064]\n",
      "epoch:16 step:12605 [D loss: 0.027295, acc.: 100.00%] [G loss: 4.032804]\n",
      "epoch:16 step:12606 [D loss: 0.043677, acc.: 99.22%] [G loss: 0.003181]\n",
      "epoch:16 step:12607 [D loss: 0.038765, acc.: 99.22%] [G loss: 0.002804]\n",
      "epoch:16 step:12608 [D loss: 0.013733, acc.: 100.00%] [G loss: 0.002631]\n",
      "epoch:16 step:12609 [D loss: 0.052785, acc.: 100.00%] [G loss: 0.018112]\n",
      "epoch:16 step:12610 [D loss: 0.305915, acc.: 87.50%] [G loss: 0.109572]\n",
      "epoch:16 step:12611 [D loss: 0.114298, acc.: 96.09%] [G loss: 4.463344]\n",
      "epoch:16 step:12612 [D loss: 0.041302, acc.: 100.00%] [G loss: 0.103102]\n",
      "epoch:16 step:12613 [D loss: 0.056062, acc.: 99.22%] [G loss: 0.173152]\n",
      "epoch:16 step:12614 [D loss: 0.046232, acc.: 98.44%] [G loss: 3.630427]\n",
      "epoch:16 step:12615 [D loss: 0.245658, acc.: 88.28%] [G loss: 0.010768]\n",
      "epoch:16 step:12616 [D loss: 0.078787, acc.: 97.66%] [G loss: 1.960297]\n",
      "epoch:16 step:12617 [D loss: 0.010113, acc.: 100.00%] [G loss: 1.580934]\n",
      "epoch:16 step:12618 [D loss: 0.030085, acc.: 99.22%] [G loss: 0.311940]\n",
      "epoch:16 step:12619 [D loss: 0.010604, acc.: 100.00%] [G loss: 0.126112]\n",
      "epoch:16 step:12620 [D loss: 0.013610, acc.: 100.00%] [G loss: 0.263164]\n",
      "epoch:16 step:12621 [D loss: 0.016187, acc.: 100.00%] [G loss: 0.257129]\n",
      "epoch:16 step:12622 [D loss: 0.023372, acc.: 100.00%] [G loss: 0.064885]\n",
      "epoch:16 step:12623 [D loss: 0.066254, acc.: 99.22%] [G loss: 0.011383]\n",
      "epoch:16 step:12624 [D loss: 0.026655, acc.: 100.00%] [G loss: 0.491574]\n",
      "epoch:16 step:12625 [D loss: 0.034317, acc.: 99.22%] [G loss: 0.041584]\n",
      "epoch:16 step:12626 [D loss: 0.046420, acc.: 99.22%] [G loss: 0.017042]\n",
      "epoch:16 step:12627 [D loss: 0.030917, acc.: 100.00%] [G loss: 0.005765]\n",
      "epoch:16 step:12628 [D loss: 0.010251, acc.: 100.00%] [G loss: 0.016179]\n",
      "epoch:16 step:12629 [D loss: 0.003579, acc.: 100.00%] [G loss: 0.141620]\n",
      "epoch:16 step:12630 [D loss: 0.004505, acc.: 100.00%] [G loss: 0.006855]\n",
      "epoch:16 step:12631 [D loss: 0.023993, acc.: 99.22%] [G loss: 0.002859]\n",
      "epoch:16 step:12632 [D loss: 0.001529, acc.: 100.00%] [G loss: 0.002522]\n",
      "epoch:16 step:12633 [D loss: 0.004000, acc.: 100.00%] [G loss: 0.003397]\n",
      "epoch:16 step:12634 [D loss: 0.003330, acc.: 100.00%] [G loss: 0.002048]\n",
      "epoch:16 step:12635 [D loss: 0.004116, acc.: 100.00%] [G loss: 0.019368]\n",
      "epoch:16 step:12636 [D loss: 0.008436, acc.: 100.00%] [G loss: 0.002437]\n",
      "epoch:16 step:12637 [D loss: 0.023230, acc.: 99.22%] [G loss: 0.000556]\n",
      "epoch:16 step:12638 [D loss: 0.004136, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:16 step:12639 [D loss: 0.009017, acc.: 100.00%] [G loss: 0.001764]\n",
      "epoch:16 step:12640 [D loss: 0.002540, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:16 step:12641 [D loss: 0.001196, acc.: 100.00%] [G loss: 0.000692]\n",
      "epoch:16 step:12642 [D loss: 0.009001, acc.: 100.00%] [G loss: 0.002581]\n",
      "epoch:16 step:12643 [D loss: 0.002633, acc.: 100.00%] [G loss: 0.005381]\n",
      "epoch:16 step:12644 [D loss: 0.004434, acc.: 100.00%] [G loss: 0.017865]\n",
      "epoch:16 step:12645 [D loss: 0.005014, acc.: 100.00%] [G loss: 0.014092]\n",
      "epoch:16 step:12646 [D loss: 0.037041, acc.: 99.22%] [G loss: 0.000163]\n",
      "epoch:16 step:12647 [D loss: 0.026364, acc.: 99.22%] [G loss: 0.004031]\n",
      "epoch:16 step:12648 [D loss: 0.032452, acc.: 100.00%] [G loss: 1.406764]\n",
      "epoch:16 step:12649 [D loss: 0.004144, acc.: 100.00%] [G loss: 0.070520]\n",
      "epoch:16 step:12650 [D loss: 0.011791, acc.: 100.00%] [G loss: 0.351644]\n",
      "epoch:16 step:12651 [D loss: 0.030132, acc.: 99.22%] [G loss: 0.010176]\n",
      "epoch:16 step:12652 [D loss: 0.106908, acc.: 96.88%] [G loss: 0.000043]\n",
      "epoch:16 step:12653 [D loss: 0.906025, acc.: 60.94%] [G loss: 10.652950]\n",
      "epoch:16 step:12654 [D loss: 2.960829, acc.: 50.00%] [G loss: 6.477293]\n",
      "epoch:16 step:12655 [D loss: 1.697208, acc.: 50.78%] [G loss: 2.587675]\n",
      "epoch:16 step:12656 [D loss: 0.102319, acc.: 96.88%] [G loss: 0.159828]\n",
      "epoch:16 step:12657 [D loss: 0.013775, acc.: 100.00%] [G loss: 0.281131]\n",
      "epoch:16 step:12658 [D loss: 0.012229, acc.: 100.00%] [G loss: 0.157797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12659 [D loss: 0.043035, acc.: 100.00%] [G loss: 0.029661]\n",
      "epoch:16 step:12660 [D loss: 0.059653, acc.: 100.00%] [G loss: 0.024846]\n",
      "epoch:16 step:12661 [D loss: 0.024008, acc.: 100.00%] [G loss: 0.521414]\n",
      "epoch:16 step:12662 [D loss: 0.016655, acc.: 100.00%] [G loss: 0.037669]\n",
      "epoch:16 step:12663 [D loss: 0.009655, acc.: 100.00%] [G loss: 0.046907]\n",
      "epoch:16 step:12664 [D loss: 0.035581, acc.: 100.00%] [G loss: 0.011617]\n",
      "epoch:16 step:12665 [D loss: 0.013803, acc.: 100.00%] [G loss: 0.053903]\n",
      "epoch:16 step:12666 [D loss: 0.026018, acc.: 100.00%] [G loss: 0.018703]\n",
      "epoch:16 step:12667 [D loss: 0.055412, acc.: 99.22%] [G loss: 0.028656]\n",
      "epoch:16 step:12668 [D loss: 0.012089, acc.: 100.00%] [G loss: 0.023970]\n",
      "epoch:16 step:12669 [D loss: 0.019333, acc.: 100.00%] [G loss: 0.089667]\n",
      "epoch:16 step:12670 [D loss: 0.026889, acc.: 100.00%] [G loss: 0.015536]\n",
      "epoch:16 step:12671 [D loss: 0.012730, acc.: 100.00%] [G loss: 0.722227]\n",
      "epoch:16 step:12672 [D loss: 0.010974, acc.: 100.00%] [G loss: 0.011788]\n",
      "epoch:16 step:12673 [D loss: 0.068582, acc.: 99.22%] [G loss: 0.034703]\n",
      "epoch:16 step:12674 [D loss: 0.036848, acc.: 100.00%] [G loss: 0.023369]\n",
      "epoch:16 step:12675 [D loss: 0.008709, acc.: 100.00%] [G loss: 0.045864]\n",
      "epoch:16 step:12676 [D loss: 0.009780, acc.: 100.00%] [G loss: 0.013769]\n",
      "epoch:16 step:12677 [D loss: 0.098594, acc.: 97.66%] [G loss: 0.012652]\n",
      "epoch:16 step:12678 [D loss: 0.033675, acc.: 100.00%] [G loss: 0.153581]\n",
      "epoch:16 step:12679 [D loss: 0.016708, acc.: 100.00%] [G loss: 0.465652]\n",
      "epoch:16 step:12680 [D loss: 0.037353, acc.: 100.00%] [G loss: 1.055834]\n",
      "epoch:16 step:12681 [D loss: 0.084764, acc.: 97.66%] [G loss: 0.902737]\n",
      "epoch:16 step:12682 [D loss: 0.092896, acc.: 96.88%] [G loss: 0.005012]\n",
      "epoch:16 step:12683 [D loss: 0.196462, acc.: 91.41%] [G loss: 0.589355]\n",
      "epoch:16 step:12684 [D loss: 0.256570, acc.: 88.28%] [G loss: 0.571850]\n",
      "epoch:16 step:12685 [D loss: 0.031811, acc.: 100.00%] [G loss: 1.158529]\n",
      "epoch:16 step:12686 [D loss: 0.012390, acc.: 100.00%] [G loss: 0.355184]\n",
      "epoch:16 step:12687 [D loss: 0.014742, acc.: 100.00%] [G loss: 0.760375]\n",
      "epoch:16 step:12688 [D loss: 0.159877, acc.: 93.75%] [G loss: 1.980213]\n",
      "epoch:16 step:12689 [D loss: 0.097935, acc.: 94.53%] [G loss: 3.361168]\n",
      "epoch:16 step:12690 [D loss: 0.268563, acc.: 89.84%] [G loss: 0.018635]\n",
      "epoch:16 step:12691 [D loss: 0.093701, acc.: 96.09%] [G loss: 0.103866]\n",
      "epoch:16 step:12692 [D loss: 0.357618, acc.: 79.69%] [G loss: 0.000067]\n",
      "epoch:16 step:12693 [D loss: 0.109642, acc.: 97.66%] [G loss: 0.008351]\n",
      "epoch:16 step:12694 [D loss: 0.004812, acc.: 100.00%] [G loss: 0.239186]\n",
      "epoch:16 step:12695 [D loss: 0.044521, acc.: 99.22%] [G loss: 0.076583]\n",
      "epoch:16 step:12696 [D loss: 0.010939, acc.: 100.00%] [G loss: 0.165165]\n",
      "epoch:16 step:12697 [D loss: 0.018086, acc.: 100.00%] [G loss: 0.880722]\n",
      "epoch:16 step:12698 [D loss: 0.087160, acc.: 98.44%] [G loss: 0.008775]\n",
      "epoch:16 step:12699 [D loss: 0.050010, acc.: 99.22%] [G loss: 0.034296]\n",
      "epoch:16 step:12700 [D loss: 0.023683, acc.: 100.00%] [G loss: 0.141661]\n",
      "epoch:16 step:12701 [D loss: 0.317909, acc.: 85.94%] [G loss: 0.360377]\n",
      "epoch:16 step:12702 [D loss: 0.020069, acc.: 100.00%] [G loss: 0.710961]\n",
      "epoch:16 step:12703 [D loss: 0.051193, acc.: 97.66%] [G loss: 0.382600]\n",
      "epoch:16 step:12704 [D loss: 0.069507, acc.: 98.44%] [G loss: 0.378773]\n",
      "epoch:16 step:12705 [D loss: 0.053250, acc.: 99.22%] [G loss: 6.104280]\n",
      "epoch:16 step:12706 [D loss: 0.041736, acc.: 98.44%] [G loss: 0.516132]\n",
      "epoch:16 step:12707 [D loss: 0.030327, acc.: 100.00%] [G loss: 0.751659]\n",
      "epoch:16 step:12708 [D loss: 0.079505, acc.: 98.44%] [G loss: 4.673539]\n",
      "epoch:16 step:12709 [D loss: 0.082722, acc.: 98.44%] [G loss: 0.253860]\n",
      "epoch:16 step:12710 [D loss: 0.324441, acc.: 84.38%] [G loss: 7.195549]\n",
      "epoch:16 step:12711 [D loss: 1.501894, acc.: 53.91%] [G loss: 1.303839]\n",
      "epoch:16 step:12712 [D loss: 0.115648, acc.: 96.09%] [G loss: 3.080132]\n",
      "epoch:16 step:12713 [D loss: 0.014945, acc.: 99.22%] [G loss: 4.034528]\n",
      "epoch:16 step:12714 [D loss: 0.063864, acc.: 98.44%] [G loss: 1.807554]\n",
      "epoch:16 step:12715 [D loss: 0.074096, acc.: 97.66%] [G loss: 1.174923]\n",
      "epoch:16 step:12716 [D loss: 0.052094, acc.: 98.44%] [G loss: 2.377951]\n",
      "epoch:16 step:12717 [D loss: 0.048762, acc.: 99.22%] [G loss: 1.741916]\n",
      "epoch:16 step:12718 [D loss: 0.060667, acc.: 100.00%] [G loss: 1.941938]\n",
      "epoch:16 step:12719 [D loss: 0.144804, acc.: 97.66%] [G loss: 3.758206]\n",
      "epoch:16 step:12720 [D loss: 0.170325, acc.: 94.53%] [G loss: 0.982869]\n",
      "epoch:16 step:12721 [D loss: 0.034081, acc.: 99.22%] [G loss: 2.772052]\n",
      "epoch:16 step:12722 [D loss: 0.661822, acc.: 66.41%] [G loss: 7.951687]\n",
      "epoch:16 step:12723 [D loss: 1.656875, acc.: 53.12%] [G loss: 6.079074]\n",
      "epoch:16 step:12724 [D loss: 0.674199, acc.: 67.97%] [G loss: 3.426007]\n",
      "epoch:16 step:12725 [D loss: 0.091039, acc.: 96.88%] [G loss: 3.160740]\n",
      "epoch:16 step:12726 [D loss: 0.075506, acc.: 98.44%] [G loss: 3.229970]\n",
      "epoch:16 step:12727 [D loss: 0.011120, acc.: 100.00%] [G loss: 3.687559]\n",
      "epoch:16 step:12728 [D loss: 0.055242, acc.: 99.22%] [G loss: 3.587256]\n",
      "epoch:16 step:12729 [D loss: 0.051962, acc.: 99.22%] [G loss: 3.327544]\n",
      "epoch:16 step:12730 [D loss: 0.022644, acc.: 100.00%] [G loss: 3.309659]\n",
      "epoch:16 step:12731 [D loss: 0.040180, acc.: 99.22%] [G loss: 3.235910]\n",
      "epoch:16 step:12732 [D loss: 0.025118, acc.: 100.00%] [G loss: 2.925037]\n",
      "epoch:16 step:12733 [D loss: 0.049048, acc.: 99.22%] [G loss: 2.604662]\n",
      "epoch:16 step:12734 [D loss: 0.027203, acc.: 100.00%] [G loss: 3.335929]\n",
      "epoch:16 step:12735 [D loss: 0.030432, acc.: 100.00%] [G loss: 3.039325]\n",
      "epoch:16 step:12736 [D loss: 0.047830, acc.: 99.22%] [G loss: 2.458173]\n",
      "epoch:16 step:12737 [D loss: 0.036360, acc.: 100.00%] [G loss: 1.939794]\n",
      "epoch:16 step:12738 [D loss: 0.081723, acc.: 99.22%] [G loss: 3.123803]\n",
      "epoch:16 step:12739 [D loss: 0.014731, acc.: 100.00%] [G loss: 2.956962]\n",
      "epoch:16 step:12740 [D loss: 0.031266, acc.: 100.00%] [G loss: 3.170705]\n",
      "epoch:16 step:12741 [D loss: 0.131870, acc.: 97.66%] [G loss: 3.227559]\n",
      "epoch:16 step:12742 [D loss: 0.065221, acc.: 99.22%] [G loss: 3.070721]\n",
      "epoch:16 step:12743 [D loss: 0.049065, acc.: 99.22%] [G loss: 1.186792]\n",
      "epoch:16 step:12744 [D loss: 0.031320, acc.: 100.00%] [G loss: 3.839313]\n",
      "epoch:16 step:12745 [D loss: 0.023276, acc.: 100.00%] [G loss: 3.609556]\n",
      "epoch:16 step:12746 [D loss: 0.485943, acc.: 77.34%] [G loss: 3.672289]\n",
      "epoch:16 step:12747 [D loss: 0.014285, acc.: 100.00%] [G loss: 4.579102]\n",
      "epoch:16 step:12748 [D loss: 0.113753, acc.: 93.75%] [G loss: 3.494454]\n",
      "epoch:16 step:12749 [D loss: 0.006340, acc.: 100.00%] [G loss: 1.247470]\n",
      "epoch:16 step:12750 [D loss: 0.039228, acc.: 98.44%] [G loss: 1.569586]\n",
      "epoch:16 step:12751 [D loss: 0.027735, acc.: 100.00%] [G loss: 0.036586]\n",
      "epoch:16 step:12752 [D loss: 0.011981, acc.: 100.00%] [G loss: 1.330907]\n",
      "epoch:16 step:12753 [D loss: 0.026260, acc.: 100.00%] [G loss: 0.804950]\n",
      "epoch:16 step:12754 [D loss: 0.184604, acc.: 90.62%] [G loss: 2.565883]\n",
      "epoch:16 step:12755 [D loss: 0.062804, acc.: 98.44%] [G loss: 2.599550]\n",
      "epoch:16 step:12756 [D loss: 0.023063, acc.: 100.00%] [G loss: 3.177172]\n",
      "epoch:16 step:12757 [D loss: 0.060621, acc.: 100.00%] [G loss: 1.151693]\n",
      "epoch:16 step:12758 [D loss: 0.020221, acc.: 100.00%] [G loss: 2.240426]\n",
      "epoch:16 step:12759 [D loss: 1.002655, acc.: 51.56%] [G loss: 5.143520]\n",
      "epoch:16 step:12760 [D loss: 1.264843, acc.: 55.47%] [G loss: 3.626429]\n",
      "epoch:16 step:12761 [D loss: 0.073894, acc.: 100.00%] [G loss: 2.482945]\n",
      "epoch:16 step:12762 [D loss: 0.074875, acc.: 96.88%] [G loss: 2.128326]\n",
      "epoch:16 step:12763 [D loss: 0.346539, acc.: 85.94%] [G loss: 4.069268]\n",
      "epoch:16 step:12764 [D loss: 0.009428, acc.: 100.00%] [G loss: 4.107985]\n",
      "epoch:16 step:12765 [D loss: 0.125808, acc.: 93.75%] [G loss: 3.662756]\n",
      "epoch:16 step:12766 [D loss: 0.016011, acc.: 100.00%] [G loss: 3.269600]\n",
      "epoch:16 step:12767 [D loss: 0.016103, acc.: 100.00%] [G loss: 2.874984]\n",
      "epoch:16 step:12768 [D loss: 0.051237, acc.: 100.00%] [G loss: 3.444029]\n",
      "epoch:16 step:12769 [D loss: 0.019078, acc.: 100.00%] [G loss: 3.359864]\n",
      "epoch:16 step:12770 [D loss: 0.043792, acc.: 98.44%] [G loss: 3.213151]\n",
      "epoch:16 step:12771 [D loss: 0.015922, acc.: 100.00%] [G loss: 3.174540]\n",
      "epoch:16 step:12772 [D loss: 0.022986, acc.: 100.00%] [G loss: 3.189147]\n",
      "epoch:16 step:12773 [D loss: 0.042781, acc.: 99.22%] [G loss: 3.258592]\n",
      "epoch:16 step:12774 [D loss: 0.015615, acc.: 100.00%] [G loss: 4.205380]\n",
      "epoch:16 step:12775 [D loss: 0.022892, acc.: 100.00%] [G loss: 0.976952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12776 [D loss: 0.026367, acc.: 100.00%] [G loss: 3.620015]\n",
      "epoch:16 step:12777 [D loss: 0.033904, acc.: 100.00%] [G loss: 3.631538]\n",
      "epoch:16 step:12778 [D loss: 0.019732, acc.: 100.00%] [G loss: 3.326923]\n",
      "epoch:16 step:12779 [D loss: 0.037343, acc.: 99.22%] [G loss: 3.639142]\n",
      "epoch:16 step:12780 [D loss: 0.021227, acc.: 100.00%] [G loss: 3.789676]\n",
      "epoch:16 step:12781 [D loss: 0.021188, acc.: 100.00%] [G loss: 3.330446]\n",
      "epoch:16 step:12782 [D loss: 0.025388, acc.: 100.00%] [G loss: 2.615190]\n",
      "epoch:16 step:12783 [D loss: 0.010973, acc.: 100.00%] [G loss: 2.810021]\n",
      "epoch:16 step:12784 [D loss: 0.021621, acc.: 100.00%] [G loss: 3.407382]\n",
      "epoch:16 step:12785 [D loss: 0.036914, acc.: 98.44%] [G loss: 3.128684]\n",
      "epoch:16 step:12786 [D loss: 0.063704, acc.: 100.00%] [G loss: 3.192213]\n",
      "epoch:16 step:12787 [D loss: 0.038738, acc.: 100.00%] [G loss: 3.391423]\n",
      "epoch:16 step:12788 [D loss: 0.028249, acc.: 100.00%] [G loss: 0.711178]\n",
      "epoch:16 step:12789 [D loss: 0.116235, acc.: 94.53%] [G loss: 5.878325]\n",
      "epoch:16 step:12790 [D loss: 0.054237, acc.: 98.44%] [G loss: 5.873886]\n",
      "epoch:16 step:12791 [D loss: 0.321373, acc.: 83.59%] [G loss: 0.076325]\n",
      "epoch:16 step:12792 [D loss: 0.170678, acc.: 91.41%] [G loss: 7.220421]\n",
      "epoch:16 step:12793 [D loss: 0.108194, acc.: 96.09%] [G loss: 7.553495]\n",
      "epoch:16 step:12794 [D loss: 0.003194, acc.: 100.00%] [G loss: 1.099320]\n",
      "epoch:16 step:12795 [D loss: 0.015862, acc.: 100.00%] [G loss: 7.203570]\n",
      "epoch:16 step:12796 [D loss: 0.012448, acc.: 99.22%] [G loss: 6.797398]\n",
      "epoch:16 step:12797 [D loss: 0.005446, acc.: 100.00%] [G loss: 6.756238]\n",
      "epoch:16 step:12798 [D loss: 0.022276, acc.: 99.22%] [G loss: 6.765174]\n",
      "epoch:16 step:12799 [D loss: 0.008459, acc.: 100.00%] [G loss: 7.015882]\n",
      "epoch:16 step:12800 [D loss: 0.003039, acc.: 100.00%] [G loss: 6.075234]\n",
      "epoch:16 step:12801 [D loss: 0.004034, acc.: 100.00%] [G loss: 6.495394]\n",
      "epoch:16 step:12802 [D loss: 0.002518, acc.: 100.00%] [G loss: 6.255352]\n",
      "epoch:16 step:12803 [D loss: 0.031541, acc.: 99.22%] [G loss: 2.777768]\n",
      "epoch:16 step:12804 [D loss: 0.010081, acc.: 99.22%] [G loss: 4.777761]\n",
      "epoch:16 step:12805 [D loss: 0.029788, acc.: 99.22%] [G loss: 0.263249]\n",
      "epoch:16 step:12806 [D loss: 0.012076, acc.: 100.00%] [G loss: 2.196123]\n",
      "epoch:16 step:12807 [D loss: 0.044443, acc.: 100.00%] [G loss: 0.531995]\n",
      "epoch:16 step:12808 [D loss: 0.050704, acc.: 100.00%] [G loss: 0.497885]\n",
      "epoch:16 step:12809 [D loss: 0.069555, acc.: 99.22%] [G loss: 7.949218]\n",
      "epoch:16 step:12810 [D loss: 0.233723, acc.: 91.41%] [G loss: 5.178662]\n",
      "epoch:16 step:12811 [D loss: 0.280234, acc.: 89.06%] [G loss: 7.542406]\n",
      "epoch:16 step:12812 [D loss: 0.206489, acc.: 91.41%] [G loss: 7.723538]\n",
      "epoch:16 step:12813 [D loss: 0.046304, acc.: 98.44%] [G loss: 6.876334]\n",
      "epoch:16 step:12814 [D loss: 0.001776, acc.: 100.00%] [G loss: 2.488124]\n",
      "epoch:16 step:12815 [D loss: 0.005098, acc.: 100.00%] [G loss: 2.091716]\n",
      "epoch:16 step:12816 [D loss: 0.014159, acc.: 100.00%] [G loss: 1.350541]\n",
      "epoch:16 step:12817 [D loss: 0.020335, acc.: 100.00%] [G loss: 5.338468]\n",
      "epoch:16 step:12818 [D loss: 0.060271, acc.: 99.22%] [G loss: 5.895851]\n",
      "epoch:16 step:12819 [D loss: 0.004889, acc.: 100.00%] [G loss: 2.714735]\n",
      "epoch:16 step:12820 [D loss: 0.053692, acc.: 97.66%] [G loss: 0.539336]\n",
      "epoch:16 step:12821 [D loss: 0.052641, acc.: 99.22%] [G loss: 4.982154]\n",
      "epoch:16 step:12822 [D loss: 0.008826, acc.: 100.00%] [G loss: 5.540619]\n",
      "epoch:16 step:12823 [D loss: 0.010896, acc.: 100.00%] [G loss: 4.207459]\n",
      "epoch:16 step:12824 [D loss: 0.022341, acc.: 99.22%] [G loss: 1.165198]\n",
      "epoch:16 step:12825 [D loss: 0.155951, acc.: 96.09%] [G loss: 4.491339]\n",
      "epoch:16 step:12826 [D loss: 0.059510, acc.: 98.44%] [G loss: 5.036359]\n",
      "epoch:16 step:12827 [D loss: 0.009298, acc.: 100.00%] [G loss: 2.892623]\n",
      "epoch:16 step:12828 [D loss: 0.027302, acc.: 99.22%] [G loss: 1.457205]\n",
      "epoch:16 step:12829 [D loss: 0.049008, acc.: 99.22%] [G loss: 3.332311]\n",
      "epoch:16 step:12830 [D loss: 0.057130, acc.: 100.00%] [G loss: 3.019591]\n",
      "epoch:16 step:12831 [D loss: 0.025320, acc.: 100.00%] [G loss: 3.741751]\n",
      "epoch:16 step:12832 [D loss: 0.431620, acc.: 79.69%] [G loss: 5.731701]\n",
      "epoch:16 step:12833 [D loss: 2.192219, acc.: 43.75%] [G loss: 1.515526]\n",
      "epoch:16 step:12834 [D loss: 0.021542, acc.: 100.00%] [G loss: 1.765043]\n",
      "epoch:16 step:12835 [D loss: 0.491126, acc.: 75.78%] [G loss: 1.028270]\n",
      "epoch:16 step:12836 [D loss: 0.030845, acc.: 99.22%] [G loss: 1.518929]\n",
      "epoch:16 step:12837 [D loss: 0.045759, acc.: 99.22%] [G loss: 5.548630]\n",
      "epoch:16 step:12838 [D loss: 0.049172, acc.: 98.44%] [G loss: 0.341707]\n",
      "epoch:16 step:12839 [D loss: 0.034131, acc.: 100.00%] [G loss: 6.613055]\n",
      "epoch:16 step:12840 [D loss: 0.029544, acc.: 100.00%] [G loss: 0.231110]\n",
      "epoch:16 step:12841 [D loss: 0.149314, acc.: 95.31%] [G loss: 1.301991]\n",
      "epoch:16 step:12842 [D loss: 0.384166, acc.: 82.81%] [G loss: 4.933588]\n",
      "epoch:16 step:12843 [D loss: 0.050373, acc.: 100.00%] [G loss: 0.004150]\n",
      "epoch:16 step:12844 [D loss: 0.009890, acc.: 100.00%] [G loss: 0.089090]\n",
      "epoch:16 step:12845 [D loss: 0.005688, acc.: 100.00%] [G loss: 4.831715]\n",
      "epoch:16 step:12846 [D loss: 0.012387, acc.: 100.00%] [G loss: 0.169241]\n",
      "epoch:16 step:12847 [D loss: 0.010772, acc.: 100.00%] [G loss: 2.884997]\n",
      "epoch:16 step:12848 [D loss: 0.087660, acc.: 98.44%] [G loss: 4.825637]\n",
      "epoch:16 step:12849 [D loss: 0.482481, acc.: 79.69%] [G loss: 4.806415]\n",
      "epoch:16 step:12850 [D loss: 0.592177, acc.: 71.09%] [G loss: 0.086750]\n",
      "epoch:16 step:12851 [D loss: 0.177713, acc.: 92.97%] [G loss: 2.507090]\n",
      "epoch:16 step:12852 [D loss: 0.027764, acc.: 100.00%] [G loss: 4.000706]\n",
      "epoch:16 step:12853 [D loss: 0.050999, acc.: 97.66%] [G loss: 1.444898]\n",
      "epoch:16 step:12854 [D loss: 0.101769, acc.: 98.44%] [G loss: 6.850135]\n",
      "epoch:16 step:12855 [D loss: 0.005841, acc.: 100.00%] [G loss: 0.467851]\n",
      "epoch:16 step:12856 [D loss: 0.006853, acc.: 100.00%] [G loss: 0.623407]\n",
      "epoch:16 step:12857 [D loss: 0.034709, acc.: 100.00%] [G loss: 0.469431]\n",
      "epoch:16 step:12858 [D loss: 0.051009, acc.: 100.00%] [G loss: 0.509485]\n",
      "epoch:16 step:12859 [D loss: 0.148630, acc.: 96.09%] [G loss: 3.913237]\n",
      "epoch:16 step:12860 [D loss: 0.445849, acc.: 80.47%] [G loss: 4.338769]\n",
      "epoch:16 step:12861 [D loss: 0.333379, acc.: 84.38%] [G loss: 5.619380]\n",
      "epoch:16 step:12862 [D loss: 0.023690, acc.: 99.22%] [G loss: 1.294236]\n",
      "epoch:16 step:12863 [D loss: 0.021847, acc.: 100.00%] [G loss: 4.494883]\n",
      "epoch:16 step:12864 [D loss: 0.006573, acc.: 100.00%] [G loss: 0.957856]\n",
      "epoch:16 step:12865 [D loss: 0.021053, acc.: 99.22%] [G loss: 0.463269]\n",
      "epoch:16 step:12866 [D loss: 0.174208, acc.: 96.09%] [G loss: 3.952069]\n",
      "epoch:16 step:12867 [D loss: 0.188373, acc.: 89.84%] [G loss: 1.269804]\n",
      "epoch:16 step:12868 [D loss: 0.114670, acc.: 96.09%] [G loss: 6.354191]\n",
      "epoch:16 step:12869 [D loss: 0.102617, acc.: 95.31%] [G loss: 5.014659]\n",
      "epoch:16 step:12870 [D loss: 0.038802, acc.: 98.44%] [G loss: 4.895917]\n",
      "epoch:16 step:12871 [D loss: 0.036595, acc.: 99.22%] [G loss: 1.630813]\n",
      "epoch:16 step:12872 [D loss: 0.014068, acc.: 100.00%] [G loss: 0.903784]\n",
      "epoch:16 step:12873 [D loss: 0.066513, acc.: 97.66%] [G loss: 4.298018]\n",
      "epoch:16 step:12874 [D loss: 0.037209, acc.: 99.22%] [G loss: 3.118932]\n",
      "epoch:16 step:12875 [D loss: 0.027495, acc.: 98.44%] [G loss: 0.935888]\n",
      "epoch:16 step:12876 [D loss: 0.013434, acc.: 100.00%] [G loss: 1.411205]\n",
      "epoch:16 step:12877 [D loss: 0.008210, acc.: 100.00%] [G loss: 0.861983]\n",
      "epoch:16 step:12878 [D loss: 0.017731, acc.: 100.00%] [G loss: 0.408302]\n",
      "epoch:16 step:12879 [D loss: 0.004609, acc.: 100.00%] [G loss: 2.121569]\n",
      "epoch:16 step:12880 [D loss: 0.002827, acc.: 100.00%] [G loss: 0.988982]\n",
      "epoch:16 step:12881 [D loss: 0.022758, acc.: 100.00%] [G loss: 0.783386]\n",
      "epoch:16 step:12882 [D loss: 0.311467, acc.: 85.94%] [G loss: 4.041968]\n",
      "epoch:16 step:12883 [D loss: 0.893604, acc.: 62.50%] [G loss: 0.113897]\n",
      "epoch:16 step:12884 [D loss: 0.091541, acc.: 97.66%] [G loss: 0.256894]\n",
      "epoch:16 step:12885 [D loss: 0.001859, acc.: 100.00%] [G loss: 0.313783]\n",
      "epoch:16 step:12886 [D loss: 0.004631, acc.: 100.00%] [G loss: 2.971563]\n",
      "epoch:16 step:12887 [D loss: 0.030684, acc.: 98.44%] [G loss: 1.483634]\n",
      "epoch:16 step:12888 [D loss: 0.003953, acc.: 100.00%] [G loss: 1.115455]\n",
      "epoch:16 step:12889 [D loss: 0.004783, acc.: 100.00%] [G loss: 1.545200]\n",
      "epoch:16 step:12890 [D loss: 0.007194, acc.: 100.00%] [G loss: 4.726168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12891 [D loss: 0.019949, acc.: 100.00%] [G loss: 0.334244]\n",
      "epoch:16 step:12892 [D loss: 0.089434, acc.: 96.09%] [G loss: 2.062455]\n",
      "epoch:16 step:12893 [D loss: 0.016699, acc.: 100.00%] [G loss: 2.928592]\n",
      "epoch:16 step:12894 [D loss: 0.017713, acc.: 100.00%] [G loss: 1.533861]\n",
      "epoch:16 step:12895 [D loss: 0.007819, acc.: 100.00%] [G loss: 3.511835]\n",
      "epoch:16 step:12896 [D loss: 0.101665, acc.: 97.66%] [G loss: 3.725192]\n",
      "epoch:16 step:12897 [D loss: 1.743099, acc.: 35.94%] [G loss: 9.010660]\n",
      "epoch:16 step:12898 [D loss: 2.069209, acc.: 51.56%] [G loss: 2.791554]\n",
      "epoch:16 step:12899 [D loss: 0.775196, acc.: 69.53%] [G loss: 0.005282]\n",
      "epoch:16 step:12900 [D loss: 0.062351, acc.: 96.88%] [G loss: 2.224858]\n",
      "epoch:16 step:12901 [D loss: 0.007461, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:16 step:12902 [D loss: 0.083146, acc.: 98.44%] [G loss: 0.000117]\n",
      "epoch:16 step:12903 [D loss: 0.014173, acc.: 100.00%] [G loss: 0.490346]\n",
      "epoch:16 step:12904 [D loss: 0.011250, acc.: 100.00%] [G loss: 0.008026]\n",
      "epoch:16 step:12905 [D loss: 0.041183, acc.: 100.00%] [G loss: 0.005088]\n",
      "epoch:16 step:12906 [D loss: 0.015905, acc.: 100.00%] [G loss: 0.008075]\n",
      "epoch:16 step:12907 [D loss: 0.017740, acc.: 100.00%] [G loss: 0.005123]\n",
      "epoch:16 step:12908 [D loss: 0.105401, acc.: 97.66%] [G loss: 0.045894]\n",
      "epoch:16 step:12909 [D loss: 0.087571, acc.: 97.66%] [G loss: 0.018559]\n",
      "epoch:16 step:12910 [D loss: 0.007401, acc.: 100.00%] [G loss: 0.025652]\n",
      "epoch:16 step:12911 [D loss: 0.039838, acc.: 100.00%] [G loss: 0.030933]\n",
      "epoch:16 step:12912 [D loss: 0.018741, acc.: 100.00%] [G loss: 0.052832]\n",
      "epoch:16 step:12913 [D loss: 0.010614, acc.: 100.00%] [G loss: 0.070698]\n",
      "epoch:16 step:12914 [D loss: 0.052886, acc.: 99.22%] [G loss: 0.469017]\n",
      "epoch:16 step:12915 [D loss: 0.039790, acc.: 100.00%] [G loss: 0.814129]\n",
      "epoch:16 step:12916 [D loss: 0.164162, acc.: 97.66%] [G loss: 0.349307]\n",
      "epoch:16 step:12917 [D loss: 0.143077, acc.: 96.09%] [G loss: 2.604295]\n",
      "epoch:16 step:12918 [D loss: 0.070918, acc.: 97.66%] [G loss: 2.779951]\n",
      "epoch:16 step:12919 [D loss: 0.067586, acc.: 98.44%] [G loss: 2.141035]\n",
      "epoch:16 step:12920 [D loss: 0.348774, acc.: 89.84%] [G loss: 5.259851]\n",
      "epoch:16 step:12921 [D loss: 0.137206, acc.: 94.53%] [G loss: 7.132043]\n",
      "epoch:16 step:12922 [D loss: 0.061148, acc.: 97.66%] [G loss: 3.353434]\n",
      "epoch:16 step:12923 [D loss: 0.025023, acc.: 99.22%] [G loss: 3.632227]\n",
      "epoch:16 step:12924 [D loss: 0.003950, acc.: 100.00%] [G loss: 3.035030]\n",
      "epoch:16 step:12925 [D loss: 0.068829, acc.: 96.88%] [G loss: 4.065405]\n",
      "epoch:16 step:12926 [D loss: 0.010713, acc.: 100.00%] [G loss: 6.381394]\n",
      "epoch:16 step:12927 [D loss: 0.105016, acc.: 98.44%] [G loss: 3.828128]\n",
      "epoch:16 step:12928 [D loss: 0.069881, acc.: 98.44%] [G loss: 3.573565]\n",
      "epoch:16 step:12929 [D loss: 0.013458, acc.: 100.00%] [G loss: 5.087091]\n",
      "epoch:16 step:12930 [D loss: 0.044469, acc.: 99.22%] [G loss: 3.759013]\n",
      "epoch:16 step:12931 [D loss: 0.022663, acc.: 100.00%] [G loss: 3.538529]\n",
      "epoch:16 step:12932 [D loss: 0.007422, acc.: 100.00%] [G loss: 3.225778]\n",
      "epoch:16 step:12933 [D loss: 0.148973, acc.: 96.09%] [G loss: 4.013335]\n",
      "epoch:16 step:12934 [D loss: 0.024897, acc.: 100.00%] [G loss: 4.432214]\n",
      "epoch:16 step:12935 [D loss: 0.069681, acc.: 97.66%] [G loss: 4.992883]\n",
      "epoch:16 step:12936 [D loss: 0.015985, acc.: 100.00%] [G loss: 2.484040]\n",
      "epoch:16 step:12937 [D loss: 0.039424, acc.: 99.22%] [G loss: 5.365005]\n",
      "epoch:16 step:12938 [D loss: 0.016725, acc.: 100.00%] [G loss: 3.234197]\n",
      "epoch:16 step:12939 [D loss: 0.004279, acc.: 100.00%] [G loss: 4.128414]\n",
      "epoch:16 step:12940 [D loss: 0.024267, acc.: 100.00%] [G loss: 2.178788]\n",
      "epoch:16 step:12941 [D loss: 0.036766, acc.: 98.44%] [G loss: 3.565166]\n",
      "epoch:16 step:12942 [D loss: 0.047552, acc.: 99.22%] [G loss: 3.471324]\n",
      "epoch:16 step:12943 [D loss: 0.056574, acc.: 100.00%] [G loss: 4.307860]\n",
      "epoch:16 step:12944 [D loss: 0.093412, acc.: 97.66%] [G loss: 3.248638]\n",
      "epoch:16 step:12945 [D loss: 0.085359, acc.: 96.88%] [G loss: 4.857192]\n",
      "epoch:16 step:12946 [D loss: 0.006081, acc.: 100.00%] [G loss: 6.319575]\n",
      "epoch:16 step:12947 [D loss: 0.055138, acc.: 99.22%] [G loss: 5.330295]\n",
      "epoch:16 step:12948 [D loss: 0.028810, acc.: 99.22%] [G loss: 5.848942]\n",
      "epoch:16 step:12949 [D loss: 0.041064, acc.: 100.00%] [G loss: 4.462514]\n",
      "epoch:16 step:12950 [D loss: 0.004778, acc.: 100.00%] [G loss: 4.478235]\n",
      "epoch:16 step:12951 [D loss: 0.011319, acc.: 100.00%] [G loss: 5.168503]\n",
      "epoch:16 step:12952 [D loss: 0.013633, acc.: 100.00%] [G loss: 4.642413]\n",
      "epoch:16 step:12953 [D loss: 0.016190, acc.: 100.00%] [G loss: 3.459377]\n",
      "epoch:16 step:12954 [D loss: 0.011255, acc.: 100.00%] [G loss: 4.784618]\n",
      "epoch:16 step:12955 [D loss: 0.237383, acc.: 88.28%] [G loss: 5.486706]\n",
      "epoch:16 step:12956 [D loss: 0.121593, acc.: 96.09%] [G loss: 5.170635]\n",
      "epoch:16 step:12957 [D loss: 0.002701, acc.: 100.00%] [G loss: 5.792855]\n",
      "epoch:16 step:12958 [D loss: 0.009635, acc.: 100.00%] [G loss: 4.852347]\n",
      "epoch:16 step:12959 [D loss: 0.011818, acc.: 99.22%] [G loss: 5.070066]\n",
      "epoch:16 step:12960 [D loss: 0.012455, acc.: 100.00%] [G loss: 5.178961]\n",
      "epoch:16 step:12961 [D loss: 0.004930, acc.: 100.00%] [G loss: 5.047226]\n",
      "epoch:16 step:12962 [D loss: 0.007317, acc.: 100.00%] [G loss: 4.723251]\n",
      "epoch:16 step:12963 [D loss: 0.006066, acc.: 100.00%] [G loss: 4.847199]\n",
      "epoch:16 step:12964 [D loss: 0.014641, acc.: 100.00%] [G loss: 4.640430]\n",
      "epoch:16 step:12965 [D loss: 0.026370, acc.: 100.00%] [G loss: 1.619975]\n",
      "epoch:16 step:12966 [D loss: 0.005417, acc.: 100.00%] [G loss: 5.519538]\n",
      "epoch:16 step:12967 [D loss: 0.016002, acc.: 100.00%] [G loss: 5.271136]\n",
      "epoch:16 step:12968 [D loss: 0.004688, acc.: 100.00%] [G loss: 4.683523]\n",
      "epoch:16 step:12969 [D loss: 0.011942, acc.: 100.00%] [G loss: 4.686632]\n",
      "epoch:16 step:12970 [D loss: 0.029421, acc.: 100.00%] [G loss: 3.590684]\n",
      "epoch:16 step:12971 [D loss: 0.073207, acc.: 97.66%] [G loss: 6.212368]\n",
      "epoch:16 step:12972 [D loss: 0.011357, acc.: 99.22%] [G loss: 7.182190]\n",
      "epoch:16 step:12973 [D loss: 0.026644, acc.: 100.00%] [G loss: 6.036109]\n",
      "epoch:16 step:12974 [D loss: 0.093942, acc.: 96.88%] [G loss: 0.392754]\n",
      "epoch:16 step:12975 [D loss: 0.096684, acc.: 96.88%] [G loss: 7.080475]\n",
      "epoch:16 step:12976 [D loss: 0.007177, acc.: 100.00%] [G loss: 2.418297]\n",
      "epoch:16 step:12977 [D loss: 0.006631, acc.: 100.00%] [G loss: 7.612794]\n",
      "epoch:16 step:12978 [D loss: 0.041799, acc.: 99.22%] [G loss: 6.731136]\n",
      "epoch:16 step:12979 [D loss: 0.004982, acc.: 100.00%] [G loss: 6.276195]\n",
      "epoch:16 step:12980 [D loss: 0.023075, acc.: 99.22%] [G loss: 6.357460]\n",
      "epoch:16 step:12981 [D loss: 0.010617, acc.: 100.00%] [G loss: 5.451821]\n",
      "epoch:16 step:12982 [D loss: 0.004166, acc.: 100.00%] [G loss: 1.107699]\n",
      "epoch:16 step:12983 [D loss: 0.463669, acc.: 75.78%] [G loss: 7.728233]\n",
      "epoch:16 step:12984 [D loss: 2.309875, acc.: 50.00%] [G loss: 4.638750]\n",
      "epoch:16 step:12985 [D loss: 0.010822, acc.: 100.00%] [G loss: 1.506037]\n",
      "epoch:16 step:12986 [D loss: 0.006865, acc.: 100.00%] [G loss: 2.967351]\n",
      "epoch:16 step:12987 [D loss: 0.014357, acc.: 100.00%] [G loss: 0.518559]\n",
      "epoch:16 step:12988 [D loss: 0.075231, acc.: 97.66%] [G loss: 3.333261]\n",
      "epoch:16 step:12989 [D loss: 0.012828, acc.: 100.00%] [G loss: 3.455121]\n",
      "epoch:16 step:12990 [D loss: 0.004934, acc.: 100.00%] [G loss: 3.625026]\n",
      "epoch:16 step:12991 [D loss: 0.032905, acc.: 99.22%] [G loss: 2.875253]\n",
      "epoch:16 step:12992 [D loss: 0.018909, acc.: 100.00%] [G loss: 1.005311]\n",
      "epoch:16 step:12993 [D loss: 0.138761, acc.: 96.09%] [G loss: 3.877075]\n",
      "epoch:16 step:12994 [D loss: 0.049288, acc.: 98.44%] [G loss: 3.867776]\n",
      "epoch:16 step:12995 [D loss: 0.030641, acc.: 99.22%] [G loss: 3.128088]\n",
      "epoch:16 step:12996 [D loss: 0.032165, acc.: 100.00%] [G loss: 2.162397]\n",
      "epoch:16 step:12997 [D loss: 0.053406, acc.: 98.44%] [G loss: 1.053441]\n",
      "epoch:16 step:12998 [D loss: 0.152247, acc.: 96.09%] [G loss: 5.265008]\n",
      "epoch:16 step:12999 [D loss: 0.217557, acc.: 88.28%] [G loss: 2.786000]\n",
      "epoch:16 step:13000 [D loss: 0.033255, acc.: 98.44%] [G loss: 4.525998]\n",
      "epoch:16 step:13001 [D loss: 0.010403, acc.: 100.00%] [G loss: 3.171428]\n",
      "epoch:16 step:13002 [D loss: 0.007922, acc.: 100.00%] [G loss: 2.702960]\n",
      "epoch:16 step:13003 [D loss: 0.015463, acc.: 100.00%] [G loss: 2.406387]\n",
      "epoch:16 step:13004 [D loss: 0.259515, acc.: 88.28%] [G loss: 5.011390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13005 [D loss: 0.637798, acc.: 69.53%] [G loss: 0.644954]\n",
      "epoch:16 step:13006 [D loss: 0.719659, acc.: 67.97%] [G loss: 5.694353]\n",
      "epoch:16 step:13007 [D loss: 1.383414, acc.: 55.47%] [G loss: 1.979135]\n",
      "epoch:16 step:13008 [D loss: 0.093025, acc.: 98.44%] [G loss: 0.867017]\n",
      "epoch:16 step:13009 [D loss: 0.251215, acc.: 90.62%] [G loss: 4.638293]\n",
      "epoch:16 step:13010 [D loss: 0.150625, acc.: 94.53%] [G loss: 3.714151]\n",
      "epoch:16 step:13011 [D loss: 0.213453, acc.: 92.19%] [G loss: 2.726005]\n",
      "epoch:16 step:13012 [D loss: 0.031867, acc.: 100.00%] [G loss: 2.308746]\n",
      "epoch:16 step:13013 [D loss: 0.100289, acc.: 97.66%] [G loss: 1.514875]\n",
      "epoch:16 step:13014 [D loss: 0.187403, acc.: 93.75%] [G loss: 2.656879]\n",
      "epoch:16 step:13015 [D loss: 0.889431, acc.: 53.12%] [G loss: 2.563695]\n",
      "epoch:16 step:13016 [D loss: 0.058199, acc.: 97.66%] [G loss: 3.613204]\n",
      "epoch:16 step:13017 [D loss: 0.217939, acc.: 89.84%] [G loss: 0.397672]\n",
      "epoch:16 step:13018 [D loss: 0.116845, acc.: 94.53%] [G loss: 0.857781]\n",
      "epoch:16 step:13019 [D loss: 0.008437, acc.: 100.00%] [G loss: 3.384649]\n",
      "epoch:16 step:13020 [D loss: 0.057427, acc.: 99.22%] [G loss: 2.077239]\n",
      "epoch:16 step:13021 [D loss: 0.116280, acc.: 98.44%] [G loss: 0.400306]\n",
      "epoch:16 step:13022 [D loss: 0.105015, acc.: 96.88%] [G loss: 3.986547]\n",
      "epoch:16 step:13023 [D loss: 0.035946, acc.: 100.00%] [G loss: 1.407493]\n",
      "epoch:16 step:13024 [D loss: 0.060320, acc.: 99.22%] [G loss: 2.730487]\n",
      "epoch:16 step:13025 [D loss: 0.055037, acc.: 99.22%] [G loss: 1.040041]\n",
      "epoch:16 step:13026 [D loss: 0.056262, acc.: 99.22%] [G loss: 1.665464]\n",
      "epoch:16 step:13027 [D loss: 0.056765, acc.: 98.44%] [G loss: 1.715541]\n",
      "epoch:16 step:13028 [D loss: 0.089208, acc.: 98.44%] [G loss: 3.490804]\n",
      "epoch:16 step:13029 [D loss: 0.039899, acc.: 98.44%] [G loss: 2.492846]\n",
      "epoch:16 step:13030 [D loss: 0.008678, acc.: 100.00%] [G loss: 2.021550]\n",
      "epoch:16 step:13031 [D loss: 0.069768, acc.: 99.22%] [G loss: 2.588190]\n",
      "epoch:16 step:13032 [D loss: 0.084914, acc.: 97.66%] [G loss: 1.881418]\n",
      "epoch:16 step:13033 [D loss: 0.012410, acc.: 100.00%] [G loss: 2.315445]\n",
      "epoch:16 step:13034 [D loss: 0.027008, acc.: 99.22%] [G loss: 3.424739]\n",
      "epoch:16 step:13035 [D loss: 0.011353, acc.: 100.00%] [G loss: 2.172779]\n",
      "epoch:16 step:13036 [D loss: 0.016784, acc.: 100.00%] [G loss: 2.685676]\n",
      "epoch:16 step:13037 [D loss: 0.068376, acc.: 99.22%] [G loss: 5.014633]\n",
      "epoch:16 step:13038 [D loss: 0.160171, acc.: 94.53%] [G loss: 0.655599]\n",
      "epoch:16 step:13039 [D loss: 0.006997, acc.: 100.00%] [G loss: 3.445848]\n",
      "epoch:16 step:13040 [D loss: 0.011762, acc.: 100.00%] [G loss: 3.617450]\n",
      "epoch:16 step:13041 [D loss: 0.003183, acc.: 100.00%] [G loss: 2.166770]\n",
      "epoch:16 step:13042 [D loss: 0.020309, acc.: 100.00%] [G loss: 3.024895]\n",
      "epoch:16 step:13043 [D loss: 0.015316, acc.: 100.00%] [G loss: 3.609815]\n",
      "epoch:16 step:13044 [D loss: 0.002880, acc.: 100.00%] [G loss: 2.705274]\n",
      "epoch:16 step:13045 [D loss: 0.002961, acc.: 100.00%] [G loss: 1.942217]\n",
      "epoch:16 step:13046 [D loss: 0.008065, acc.: 100.00%] [G loss: 1.486677]\n",
      "epoch:16 step:13047 [D loss: 0.001371, acc.: 100.00%] [G loss: 0.378831]\n",
      "epoch:16 step:13048 [D loss: 0.021379, acc.: 100.00%] [G loss: 0.090429]\n",
      "epoch:16 step:13049 [D loss: 0.035207, acc.: 100.00%] [G loss: 0.184628]\n",
      "epoch:16 step:13050 [D loss: 0.008820, acc.: 100.00%] [G loss: 0.349909]\n",
      "epoch:16 step:13051 [D loss: 0.228695, acc.: 88.28%] [G loss: 6.492202]\n",
      "epoch:16 step:13052 [D loss: 0.966171, acc.: 56.25%] [G loss: 0.997339]\n",
      "epoch:16 step:13053 [D loss: 0.039099, acc.: 99.22%] [G loss: 0.439539]\n",
      "epoch:16 step:13054 [D loss: 0.003282, acc.: 100.00%] [G loss: 0.528395]\n",
      "epoch:16 step:13055 [D loss: 0.008023, acc.: 100.00%] [G loss: 2.120522]\n",
      "epoch:16 step:13056 [D loss: 0.005152, acc.: 100.00%] [G loss: 0.775621]\n",
      "epoch:16 step:13057 [D loss: 0.010426, acc.: 100.00%] [G loss: 0.906323]\n",
      "epoch:16 step:13058 [D loss: 0.011089, acc.: 100.00%] [G loss: 1.027824]\n",
      "epoch:16 step:13059 [D loss: 0.011449, acc.: 100.00%] [G loss: 0.987043]\n",
      "epoch:16 step:13060 [D loss: 0.018729, acc.: 100.00%] [G loss: 0.914505]\n",
      "epoch:16 step:13061 [D loss: 0.010047, acc.: 100.00%] [G loss: 0.920035]\n",
      "epoch:16 step:13062 [D loss: 0.005746, acc.: 100.00%] [G loss: 2.750638]\n",
      "epoch:16 step:13063 [D loss: 0.023226, acc.: 100.00%] [G loss: 0.403857]\n",
      "epoch:16 step:13064 [D loss: 0.036025, acc.: 99.22%] [G loss: 2.452419]\n",
      "epoch:16 step:13065 [D loss: 0.036555, acc.: 98.44%] [G loss: 0.647233]\n",
      "epoch:16 step:13066 [D loss: 0.038355, acc.: 100.00%] [G loss: 0.743491]\n",
      "epoch:16 step:13067 [D loss: 0.025658, acc.: 100.00%] [G loss: 1.190299]\n",
      "epoch:16 step:13068 [D loss: 0.010102, acc.: 100.00%] [G loss: 2.198328]\n",
      "epoch:16 step:13069 [D loss: 0.020610, acc.: 100.00%] [G loss: 0.394999]\n",
      "epoch:16 step:13070 [D loss: 0.035371, acc.: 100.00%] [G loss: 0.285040]\n",
      "epoch:16 step:13071 [D loss: 0.082325, acc.: 98.44%] [G loss: 1.602711]\n",
      "epoch:16 step:13072 [D loss: 0.042427, acc.: 100.00%] [G loss: 0.913657]\n",
      "epoch:16 step:13073 [D loss: 0.067406, acc.: 99.22%] [G loss: 0.701273]\n",
      "epoch:16 step:13074 [D loss: 0.003401, acc.: 100.00%] [G loss: 3.435387]\n",
      "epoch:16 step:13075 [D loss: 0.010995, acc.: 99.22%] [G loss: 2.039441]\n",
      "epoch:16 step:13076 [D loss: 0.039135, acc.: 99.22%] [G loss: 0.955998]\n",
      "epoch:16 step:13077 [D loss: 0.007003, acc.: 100.00%] [G loss: 0.163844]\n",
      "epoch:16 step:13078 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.661493]\n",
      "epoch:16 step:13079 [D loss: 0.008541, acc.: 100.00%] [G loss: 1.856100]\n",
      "epoch:16 step:13080 [D loss: 0.065675, acc.: 98.44%] [G loss: 3.586702]\n",
      "epoch:16 step:13081 [D loss: 0.031480, acc.: 100.00%] [G loss: 2.686596]\n",
      "epoch:16 step:13082 [D loss: 0.003739, acc.: 100.00%] [G loss: 2.592888]\n",
      "epoch:16 step:13083 [D loss: 0.017727, acc.: 100.00%] [G loss: 1.751927]\n",
      "epoch:16 step:13084 [D loss: 0.093933, acc.: 98.44%] [G loss: 2.045744]\n",
      "epoch:16 step:13085 [D loss: 0.018996, acc.: 99.22%] [G loss: 2.454440]\n",
      "epoch:16 step:13086 [D loss: 0.015066, acc.: 100.00%] [G loss: 3.856755]\n",
      "epoch:16 step:13087 [D loss: 0.001348, acc.: 100.00%] [G loss: 2.936799]\n",
      "epoch:16 step:13088 [D loss: 0.003126, acc.: 100.00%] [G loss: 4.518759]\n",
      "epoch:16 step:13089 [D loss: 0.009537, acc.: 100.00%] [G loss: 3.771987]\n",
      "epoch:16 step:13090 [D loss: 0.014296, acc.: 100.00%] [G loss: 2.168599]\n",
      "epoch:16 step:13091 [D loss: 0.087023, acc.: 96.09%] [G loss: 6.321913]\n",
      "epoch:16 step:13092 [D loss: 0.112115, acc.: 93.75%] [G loss: 3.858222]\n",
      "epoch:16 step:13093 [D loss: 0.015628, acc.: 100.00%] [G loss: 0.238677]\n",
      "epoch:16 step:13094 [D loss: 0.006152, acc.: 100.00%] [G loss: 4.700677]\n",
      "epoch:16 step:13095 [D loss: 0.002067, acc.: 100.00%] [G loss: 2.922525]\n",
      "epoch:16 step:13096 [D loss: 0.004199, acc.: 100.00%] [G loss: 1.337104]\n",
      "epoch:16 step:13097 [D loss: 0.000774, acc.: 100.00%] [G loss: 3.365185]\n",
      "epoch:16 step:13098 [D loss: 0.001691, acc.: 100.00%] [G loss: 2.435853]\n",
      "epoch:16 step:13099 [D loss: 0.005039, acc.: 100.00%] [G loss: 1.479980]\n",
      "epoch:16 step:13100 [D loss: 0.011548, acc.: 100.00%] [G loss: 1.166366]\n",
      "epoch:16 step:13101 [D loss: 0.018959, acc.: 100.00%] [G loss: 4.551183]\n",
      "epoch:16 step:13102 [D loss: 0.005014, acc.: 100.00%] [G loss: 3.886421]\n",
      "epoch:16 step:13103 [D loss: 0.010138, acc.: 100.00%] [G loss: 2.507496]\n",
      "epoch:16 step:13104 [D loss: 0.026938, acc.: 100.00%] [G loss: 4.880207]\n",
      "epoch:16 step:13105 [D loss: 0.027309, acc.: 99.22%] [G loss: 1.086940]\n",
      "epoch:16 step:13106 [D loss: 0.004733, acc.: 100.00%] [G loss: 4.976602]\n",
      "epoch:16 step:13107 [D loss: 0.004798, acc.: 100.00%] [G loss: 3.704134]\n",
      "epoch:16 step:13108 [D loss: 0.008124, acc.: 100.00%] [G loss: 0.062007]\n",
      "epoch:16 step:13109 [D loss: 0.019347, acc.: 99.22%] [G loss: 4.185006]\n",
      "epoch:16 step:13110 [D loss: 0.008737, acc.: 100.00%] [G loss: 3.416641]\n",
      "epoch:16 step:13111 [D loss: 0.003398, acc.: 100.00%] [G loss: 3.591922]\n",
      "epoch:16 step:13112 [D loss: 0.012075, acc.: 100.00%] [G loss: 2.583774]\n",
      "epoch:16 step:13113 [D loss: 0.001608, acc.: 100.00%] [G loss: 0.135729]\n",
      "epoch:16 step:13114 [D loss: 0.040239, acc.: 100.00%] [G loss: 5.395531]\n",
      "epoch:16 step:13115 [D loss: 0.028901, acc.: 99.22%] [G loss: 2.987064]\n",
      "epoch:16 step:13116 [D loss: 0.004750, acc.: 100.00%] [G loss: 3.098205]\n",
      "epoch:16 step:13117 [D loss: 0.026722, acc.: 99.22%] [G loss: 1.075305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13118 [D loss: 0.022751, acc.: 100.00%] [G loss: 4.984010]\n",
      "epoch:16 step:13119 [D loss: 0.041863, acc.: 98.44%] [G loss: 1.448512]\n",
      "epoch:16 step:13120 [D loss: 0.005786, acc.: 100.00%] [G loss: 2.357878]\n",
      "epoch:16 step:13121 [D loss: 0.007168, acc.: 100.00%] [G loss: 3.560207]\n",
      "epoch:16 step:13122 [D loss: 0.029203, acc.: 99.22%] [G loss: 0.974734]\n",
      "epoch:16 step:13123 [D loss: 0.033282, acc.: 98.44%] [G loss: 2.715996]\n",
      "epoch:16 step:13124 [D loss: 0.029070, acc.: 100.00%] [G loss: 6.388843]\n",
      "epoch:16 step:13125 [D loss: 0.009327, acc.: 100.00%] [G loss: 6.695899]\n",
      "epoch:16 step:13126 [D loss: 0.032522, acc.: 100.00%] [G loss: 4.582908]\n",
      "epoch:16 step:13127 [D loss: 0.012810, acc.: 100.00%] [G loss: 5.869455]\n",
      "epoch:16 step:13128 [D loss: 0.013391, acc.: 100.00%] [G loss: 4.624624]\n",
      "epoch:16 step:13129 [D loss: 0.001339, acc.: 100.00%] [G loss: 2.982477]\n",
      "epoch:16 step:13130 [D loss: 0.004628, acc.: 100.00%] [G loss: 3.369058]\n",
      "epoch:16 step:13131 [D loss: 0.005389, acc.: 100.00%] [G loss: 1.908278]\n",
      "epoch:16 step:13132 [D loss: 0.074404, acc.: 98.44%] [G loss: 4.553961]\n",
      "epoch:16 step:13133 [D loss: 0.001852, acc.: 100.00%] [G loss: 8.729538]\n",
      "epoch:16 step:13134 [D loss: 0.111289, acc.: 96.09%] [G loss: 5.532985]\n",
      "epoch:16 step:13135 [D loss: 0.355145, acc.: 81.25%] [G loss: 10.826747]\n",
      "epoch:16 step:13136 [D loss: 3.003513, acc.: 43.75%] [G loss: 0.640045]\n",
      "epoch:16 step:13137 [D loss: 0.061283, acc.: 98.44%] [G loss: 4.894101]\n",
      "epoch:16 step:13138 [D loss: 0.013174, acc.: 100.00%] [G loss: 4.483849]\n",
      "epoch:16 step:13139 [D loss: 0.007996, acc.: 100.00%] [G loss: 5.218944]\n",
      "epoch:16 step:13140 [D loss: 0.012723, acc.: 100.00%] [G loss: 4.884462]\n",
      "epoch:16 step:13141 [D loss: 0.009951, acc.: 100.00%] [G loss: 4.812196]\n",
      "epoch:16 step:13142 [D loss: 0.017924, acc.: 100.00%] [G loss: 5.063086]\n",
      "epoch:16 step:13143 [D loss: 0.025311, acc.: 99.22%] [G loss: 2.620550]\n",
      "epoch:16 step:13144 [D loss: 0.020795, acc.: 100.00%] [G loss: 5.278378]\n",
      "epoch:16 step:13145 [D loss: 0.006347, acc.: 100.00%] [G loss: 5.084936]\n",
      "epoch:16 step:13146 [D loss: 0.015918, acc.: 100.00%] [G loss: 5.302805]\n",
      "epoch:16 step:13147 [D loss: 0.004699, acc.: 100.00%] [G loss: 4.774101]\n",
      "epoch:16 step:13148 [D loss: 0.012476, acc.: 100.00%] [G loss: 4.832726]\n",
      "epoch:16 step:13149 [D loss: 0.015212, acc.: 100.00%] [G loss: 4.642207]\n",
      "epoch:16 step:13150 [D loss: 0.012067, acc.: 100.00%] [G loss: 1.363094]\n",
      "epoch:16 step:13151 [D loss: 0.015181, acc.: 100.00%] [G loss: 5.409586]\n",
      "epoch:16 step:13152 [D loss: 0.017508, acc.: 100.00%] [G loss: 5.038730]\n",
      "epoch:16 step:13153 [D loss: 0.010556, acc.: 100.00%] [G loss: 5.178089]\n",
      "epoch:16 step:13154 [D loss: 0.008692, acc.: 100.00%] [G loss: 5.569139]\n",
      "epoch:16 step:13155 [D loss: 0.011424, acc.: 100.00%] [G loss: 4.984820]\n",
      "epoch:16 step:13156 [D loss: 0.008489, acc.: 100.00%] [G loss: 4.842373]\n",
      "epoch:16 step:13157 [D loss: 0.015249, acc.: 100.00%] [G loss: 0.117896]\n",
      "epoch:16 step:13158 [D loss: 0.010035, acc.: 100.00%] [G loss: 0.093237]\n",
      "epoch:16 step:13159 [D loss: 0.448025, acc.: 73.44%] [G loss: 7.809532]\n",
      "epoch:16 step:13160 [D loss: 1.284796, acc.: 56.25%] [G loss: 6.295707]\n",
      "epoch:16 step:13161 [D loss: 0.039918, acc.: 99.22%] [G loss: 5.139959]\n",
      "epoch:16 step:13162 [D loss: 0.013438, acc.: 100.00%] [G loss: 5.442662]\n",
      "epoch:16 step:13163 [D loss: 0.006484, acc.: 100.00%] [G loss: 5.260984]\n",
      "epoch:16 step:13164 [D loss: 0.004750, acc.: 100.00%] [G loss: 5.474838]\n",
      "epoch:16 step:13165 [D loss: 0.006153, acc.: 100.00%] [G loss: 5.128925]\n",
      "epoch:16 step:13166 [D loss: 0.006694, acc.: 100.00%] [G loss: 4.984775]\n",
      "epoch:16 step:13167 [D loss: 0.004503, acc.: 100.00%] [G loss: 5.211100]\n",
      "epoch:16 step:13168 [D loss: 0.008738, acc.: 100.00%] [G loss: 4.873019]\n",
      "epoch:16 step:13169 [D loss: 0.016795, acc.: 100.00%] [G loss: 5.325353]\n",
      "epoch:16 step:13170 [D loss: 0.011948, acc.: 100.00%] [G loss: 4.915093]\n",
      "epoch:16 step:13171 [D loss: 0.005244, acc.: 100.00%] [G loss: 2.509851]\n",
      "epoch:16 step:13172 [D loss: 0.011727, acc.: 100.00%] [G loss: 5.087949]\n",
      "epoch:16 step:13173 [D loss: 0.006939, acc.: 100.00%] [G loss: 5.323681]\n",
      "epoch:16 step:13174 [D loss: 0.011753, acc.: 100.00%] [G loss: 4.974880]\n",
      "epoch:16 step:13175 [D loss: 0.007000, acc.: 100.00%] [G loss: 4.776131]\n",
      "epoch:16 step:13176 [D loss: 0.005337, acc.: 100.00%] [G loss: 5.228158]\n",
      "epoch:16 step:13177 [D loss: 0.032334, acc.: 100.00%] [G loss: 5.280986]\n",
      "epoch:16 step:13178 [D loss: 0.003507, acc.: 100.00%] [G loss: 5.470560]\n",
      "epoch:16 step:13179 [D loss: 0.006611, acc.: 100.00%] [G loss: 1.709979]\n",
      "epoch:16 step:13180 [D loss: 0.031695, acc.: 99.22%] [G loss: 5.664742]\n",
      "epoch:16 step:13181 [D loss: 0.108341, acc.: 97.66%] [G loss: 3.385662]\n",
      "epoch:16 step:13182 [D loss: 0.049992, acc.: 98.44%] [G loss: 4.756795]\n",
      "epoch:16 step:13183 [D loss: 0.015871, acc.: 100.00%] [G loss: 5.243932]\n",
      "epoch:16 step:13184 [D loss: 0.027312, acc.: 99.22%] [G loss: 5.678344]\n",
      "epoch:16 step:13185 [D loss: 0.008161, acc.: 100.00%] [G loss: 5.975160]\n",
      "epoch:16 step:13186 [D loss: 0.009155, acc.: 100.00%] [G loss: 5.475909]\n",
      "epoch:16 step:13187 [D loss: 0.007018, acc.: 100.00%] [G loss: 5.594927]\n",
      "epoch:16 step:13188 [D loss: 0.006881, acc.: 100.00%] [G loss: 5.632039]\n",
      "epoch:16 step:13189 [D loss: 0.008884, acc.: 100.00%] [G loss: 0.116124]\n",
      "epoch:16 step:13190 [D loss: 0.093350, acc.: 98.44%] [G loss: 3.874208]\n",
      "epoch:16 step:13191 [D loss: 0.018796, acc.: 100.00%] [G loss: 7.050211]\n",
      "epoch:16 step:13192 [D loss: 0.065556, acc.: 99.22%] [G loss: 6.546344]\n",
      "epoch:16 step:13193 [D loss: 0.012163, acc.: 100.00%] [G loss: 2.963890]\n",
      "epoch:16 step:13194 [D loss: 0.011227, acc.: 100.00%] [G loss: 6.739818]\n",
      "epoch:16 step:13195 [D loss: 0.003373, acc.: 100.00%] [G loss: 6.420336]\n",
      "epoch:16 step:13196 [D loss: 0.024932, acc.: 100.00%] [G loss: 0.018326]\n",
      "epoch:16 step:13197 [D loss: 0.089769, acc.: 98.44%] [G loss: 7.650007]\n",
      "epoch:16 step:13198 [D loss: 0.028333, acc.: 100.00%] [G loss: 7.528430]\n",
      "epoch:16 step:13199 [D loss: 0.243340, acc.: 89.06%] [G loss: 4.603558]\n",
      "epoch:16 step:13200 [D loss: 0.548163, acc.: 73.44%] [G loss: 9.281282]\n",
      "epoch:16 step:13201 [D loss: 1.056397, acc.: 63.28%] [G loss: 1.812750]\n",
      "epoch:16 step:13202 [D loss: 0.009432, acc.: 100.00%] [G loss: 5.402011]\n",
      "epoch:16 step:13203 [D loss: 0.008826, acc.: 100.00%] [G loss: 5.477089]\n",
      "epoch:16 step:13204 [D loss: 0.022919, acc.: 99.22%] [G loss: 4.932922]\n",
      "epoch:16 step:13205 [D loss: 0.070941, acc.: 97.66%] [G loss: 2.268806]\n",
      "epoch:16 step:13206 [D loss: 0.007466, acc.: 100.00%] [G loss: 3.208112]\n",
      "epoch:16 step:13207 [D loss: 0.012805, acc.: 100.00%] [G loss: 5.047899]\n",
      "epoch:16 step:13208 [D loss: 0.013691, acc.: 100.00%] [G loss: 4.029351]\n",
      "epoch:16 step:13209 [D loss: 0.208504, acc.: 91.41%] [G loss: 0.207113]\n",
      "epoch:16 step:13210 [D loss: 0.007063, acc.: 100.00%] [G loss: 6.126721]\n",
      "epoch:16 step:13211 [D loss: 4.030326, acc.: 11.72%] [G loss: 7.306637]\n",
      "epoch:16 step:13212 [D loss: 0.683892, acc.: 64.84%] [G loss: 6.397889]\n",
      "epoch:16 step:13213 [D loss: 0.165614, acc.: 93.75%] [G loss: 5.112478]\n",
      "epoch:16 step:13214 [D loss: 0.015174, acc.: 100.00%] [G loss: 4.180752]\n",
      "epoch:16 step:13215 [D loss: 0.058326, acc.: 100.00%] [G loss: 4.324418]\n",
      "epoch:16 step:13216 [D loss: 0.010460, acc.: 100.00%] [G loss: 3.889345]\n",
      "epoch:16 step:13217 [D loss: 0.012122, acc.: 100.00%] [G loss: 3.803296]\n",
      "epoch:16 step:13218 [D loss: 0.016297, acc.: 100.00%] [G loss: 3.207371]\n",
      "epoch:16 step:13219 [D loss: 0.060311, acc.: 98.44%] [G loss: 3.857329]\n",
      "epoch:16 step:13220 [D loss: 0.319694, acc.: 84.38%] [G loss: 4.633425]\n",
      "epoch:16 step:13221 [D loss: 0.419771, acc.: 80.47%] [G loss: 2.791288]\n",
      "epoch:16 step:13222 [D loss: 0.074509, acc.: 98.44%] [G loss: 3.778405]\n",
      "epoch:16 step:13223 [D loss: 0.166188, acc.: 96.09%] [G loss: 3.242885]\n",
      "epoch:16 step:13224 [D loss: 0.014885, acc.: 100.00%] [G loss: 4.857868]\n",
      "epoch:16 step:13225 [D loss: 0.049474, acc.: 100.00%] [G loss: 4.564567]\n",
      "epoch:16 step:13226 [D loss: 0.049557, acc.: 100.00%] [G loss: 3.380090]\n",
      "epoch:16 step:13227 [D loss: 0.025602, acc.: 100.00%] [G loss: 4.081368]\n",
      "epoch:16 step:13228 [D loss: 0.035670, acc.: 100.00%] [G loss: 3.453085]\n",
      "epoch:16 step:13229 [D loss: 0.104622, acc.: 99.22%] [G loss: 3.376170]\n",
      "epoch:16 step:13230 [D loss: 0.017176, acc.: 100.00%] [G loss: 4.076683]\n",
      "epoch:16 step:13231 [D loss: 0.170990, acc.: 95.31%] [G loss: 0.297097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13232 [D loss: 0.082826, acc.: 100.00%] [G loss: 3.202734]\n",
      "epoch:16 step:13233 [D loss: 0.033999, acc.: 100.00%] [G loss: 3.679698]\n",
      "epoch:16 step:13234 [D loss: 0.039457, acc.: 99.22%] [G loss: 3.943648]\n",
      "epoch:16 step:13235 [D loss: 0.022001, acc.: 100.00%] [G loss: 3.595555]\n",
      "epoch:16 step:13236 [D loss: 0.137421, acc.: 97.66%] [G loss: 3.381154]\n",
      "epoch:16 step:13237 [D loss: 0.027781, acc.: 100.00%] [G loss: 1.908534]\n",
      "epoch:16 step:13238 [D loss: 0.005414, acc.: 100.00%] [G loss: 4.341434]\n",
      "epoch:16 step:13239 [D loss: 0.061040, acc.: 100.00%] [G loss: 3.845588]\n",
      "epoch:16 step:13240 [D loss: 0.023447, acc.: 100.00%] [G loss: 3.280813]\n",
      "epoch:16 step:13241 [D loss: 0.026900, acc.: 100.00%] [G loss: 2.653422]\n",
      "epoch:16 step:13242 [D loss: 0.028942, acc.: 100.00%] [G loss: 3.308336]\n",
      "epoch:16 step:13243 [D loss: 0.120612, acc.: 96.88%] [G loss: 4.015453]\n",
      "epoch:16 step:13244 [D loss: 0.010029, acc.: 100.00%] [G loss: 4.857565]\n",
      "epoch:16 step:13245 [D loss: 0.085555, acc.: 97.66%] [G loss: 4.360819]\n",
      "epoch:16 step:13246 [D loss: 0.026015, acc.: 100.00%] [G loss: 4.605451]\n",
      "epoch:16 step:13247 [D loss: 0.014473, acc.: 100.00%] [G loss: 4.904008]\n",
      "epoch:16 step:13248 [D loss: 0.011440, acc.: 100.00%] [G loss: 4.287863]\n",
      "epoch:16 step:13249 [D loss: 0.020628, acc.: 100.00%] [G loss: 4.622202]\n",
      "epoch:16 step:13250 [D loss: 0.020858, acc.: 100.00%] [G loss: 0.751864]\n",
      "epoch:16 step:13251 [D loss: 0.004295, acc.: 100.00%] [G loss: 4.493427]\n",
      "epoch:16 step:13252 [D loss: 0.037045, acc.: 100.00%] [G loss: 5.774455]\n",
      "epoch:16 step:13253 [D loss: 0.025581, acc.: 100.00%] [G loss: 5.152673]\n",
      "epoch:16 step:13254 [D loss: 0.005164, acc.: 100.00%] [G loss: 5.095330]\n",
      "epoch:16 step:13255 [D loss: 0.027571, acc.: 100.00%] [G loss: 4.820119]\n",
      "epoch:16 step:13256 [D loss: 0.083303, acc.: 99.22%] [G loss: 5.005770]\n",
      "epoch:16 step:13257 [D loss: 0.005148, acc.: 100.00%] [G loss: 0.106188]\n",
      "epoch:16 step:13258 [D loss: 0.050592, acc.: 99.22%] [G loss: 2.149049]\n",
      "epoch:16 step:13259 [D loss: 0.154943, acc.: 95.31%] [G loss: 7.517778]\n",
      "epoch:16 step:13260 [D loss: 0.146378, acc.: 91.41%] [G loss: 0.195144]\n",
      "epoch:16 step:13261 [D loss: 0.055300, acc.: 96.88%] [G loss: 6.010601]\n",
      "epoch:16 step:13262 [D loss: 0.030228, acc.: 100.00%] [G loss: 6.364259]\n",
      "epoch:16 step:13263 [D loss: 0.007818, acc.: 100.00%] [G loss: 2.503594]\n",
      "epoch:16 step:13264 [D loss: 0.048277, acc.: 99.22%] [G loss: 0.123611]\n",
      "epoch:16 step:13265 [D loss: 0.045331, acc.: 99.22%] [G loss: 6.852005]\n",
      "epoch:16 step:13266 [D loss: 0.051405, acc.: 99.22%] [G loss: 5.493120]\n",
      "epoch:16 step:13267 [D loss: 0.273159, acc.: 85.16%] [G loss: 6.737948]\n",
      "epoch:16 step:13268 [D loss: 0.174308, acc.: 92.19%] [G loss: 5.335555]\n",
      "epoch:16 step:13269 [D loss: 0.001564, acc.: 100.00%] [G loss: 1.815326]\n",
      "epoch:16 step:13270 [D loss: 0.011348, acc.: 99.22%] [G loss: 5.256955]\n",
      "epoch:16 step:13271 [D loss: 0.002756, acc.: 100.00%] [G loss: 1.284014]\n",
      "epoch:16 step:13272 [D loss: 0.006465, acc.: 100.00%] [G loss: 0.387117]\n",
      "epoch:16 step:13273 [D loss: 0.040238, acc.: 100.00%] [G loss: 5.728294]\n",
      "epoch:16 step:13274 [D loss: 0.011697, acc.: 100.00%] [G loss: 0.666512]\n",
      "epoch:16 step:13275 [D loss: 0.068375, acc.: 98.44%] [G loss: 0.048069]\n",
      "epoch:16 step:13276 [D loss: 0.443207, acc.: 77.34%] [G loss: 6.885408]\n",
      "epoch:16 step:13277 [D loss: 0.638090, acc.: 71.88%] [G loss: 6.054084]\n",
      "epoch:17 step:13278 [D loss: 0.017402, acc.: 100.00%] [G loss: 5.018785]\n",
      "epoch:17 step:13279 [D loss: 0.006172, acc.: 100.00%] [G loss: 4.374171]\n",
      "epoch:17 step:13280 [D loss: 0.007578, acc.: 100.00%] [G loss: 3.353860]\n",
      "epoch:17 step:13281 [D loss: 0.009361, acc.: 100.00%] [G loss: 3.555476]\n",
      "epoch:17 step:13282 [D loss: 0.013990, acc.: 100.00%] [G loss: 3.077983]\n",
      "epoch:17 step:13283 [D loss: 0.045331, acc.: 99.22%] [G loss: 2.679763]\n",
      "epoch:17 step:13284 [D loss: 0.018230, acc.: 100.00%] [G loss: 2.880047]\n",
      "epoch:17 step:13285 [D loss: 0.004060, acc.: 100.00%] [G loss: 2.206407]\n",
      "epoch:17 step:13286 [D loss: 0.061884, acc.: 99.22%] [G loss: 3.178219]\n",
      "epoch:17 step:13287 [D loss: 0.021153, acc.: 100.00%] [G loss: 4.715285]\n",
      "epoch:17 step:13288 [D loss: 0.005973, acc.: 100.00%] [G loss: 3.312787]\n",
      "epoch:17 step:13289 [D loss: 0.008542, acc.: 100.00%] [G loss: 1.847969]\n",
      "epoch:17 step:13290 [D loss: 0.022046, acc.: 100.00%] [G loss: 2.083420]\n",
      "epoch:17 step:13291 [D loss: 0.028540, acc.: 100.00%] [G loss: 0.727908]\n",
      "epoch:17 step:13292 [D loss: 0.237642, acc.: 87.50%] [G loss: 3.743187]\n",
      "epoch:17 step:13293 [D loss: 0.648142, acc.: 69.53%] [G loss: 0.508605]\n",
      "epoch:17 step:13294 [D loss: 0.014551, acc.: 99.22%] [G loss: 0.440920]\n",
      "epoch:17 step:13295 [D loss: 0.061143, acc.: 97.66%] [G loss: 1.069422]\n",
      "epoch:17 step:13296 [D loss: 0.003759, acc.: 100.00%] [G loss: 1.551151]\n",
      "epoch:17 step:13297 [D loss: 0.008211, acc.: 100.00%] [G loss: 1.560732]\n",
      "epoch:17 step:13298 [D loss: 0.003544, acc.: 100.00%] [G loss: 2.162962]\n",
      "epoch:17 step:13299 [D loss: 0.006114, acc.: 100.00%] [G loss: 1.050087]\n",
      "epoch:17 step:13300 [D loss: 0.022196, acc.: 100.00%] [G loss: 1.426561]\n",
      "epoch:17 step:13301 [D loss: 0.017146, acc.: 100.00%] [G loss: 2.563444]\n",
      "epoch:17 step:13302 [D loss: 0.032080, acc.: 99.22%] [G loss: 0.738427]\n",
      "epoch:17 step:13303 [D loss: 0.016848, acc.: 100.00%] [G loss: 0.450050]\n",
      "epoch:17 step:13304 [D loss: 0.009221, acc.: 100.00%] [G loss: 0.435272]\n",
      "epoch:17 step:13305 [D loss: 0.013662, acc.: 100.00%] [G loss: 0.250201]\n",
      "epoch:17 step:13306 [D loss: 0.034754, acc.: 99.22%] [G loss: 1.941100]\n",
      "epoch:17 step:13307 [D loss: 0.004382, acc.: 100.00%] [G loss: 0.194361]\n",
      "epoch:17 step:13308 [D loss: 0.019367, acc.: 100.00%] [G loss: 0.101395]\n",
      "epoch:17 step:13309 [D loss: 0.022111, acc.: 100.00%] [G loss: 1.852516]\n",
      "epoch:17 step:13310 [D loss: 0.007227, acc.: 100.00%] [G loss: 1.520094]\n",
      "epoch:17 step:13311 [D loss: 0.006321, acc.: 100.00%] [G loss: 1.192854]\n",
      "epoch:17 step:13312 [D loss: 0.008347, acc.: 100.00%] [G loss: 1.140970]\n",
      "epoch:17 step:13313 [D loss: 0.036523, acc.: 99.22%] [G loss: 0.953524]\n",
      "epoch:17 step:13314 [D loss: 0.010488, acc.: 100.00%] [G loss: 1.031677]\n",
      "epoch:17 step:13315 [D loss: 0.035904, acc.: 99.22%] [G loss: 0.809576]\n",
      "epoch:17 step:13316 [D loss: 0.001464, acc.: 100.00%] [G loss: 1.631298]\n",
      "epoch:17 step:13317 [D loss: 0.024836, acc.: 99.22%] [G loss: 0.847297]\n",
      "epoch:17 step:13318 [D loss: 0.004177, acc.: 100.00%] [G loss: 0.129524]\n",
      "epoch:17 step:13319 [D loss: 0.014506, acc.: 100.00%] [G loss: 2.257072]\n",
      "epoch:17 step:13320 [D loss: 0.003051, acc.: 100.00%] [G loss: 0.433907]\n",
      "epoch:17 step:13321 [D loss: 0.048627, acc.: 99.22%] [G loss: 1.150701]\n",
      "epoch:17 step:13322 [D loss: 0.048204, acc.: 99.22%] [G loss: 0.398380]\n",
      "epoch:17 step:13323 [D loss: 0.033530, acc.: 100.00%] [G loss: 1.493279]\n",
      "epoch:17 step:13324 [D loss: 0.005126, acc.: 100.00%] [G loss: 0.145062]\n",
      "epoch:17 step:13325 [D loss: 0.003057, acc.: 100.00%] [G loss: 1.365182]\n",
      "epoch:17 step:13326 [D loss: 0.018732, acc.: 100.00%] [G loss: 0.389036]\n",
      "epoch:17 step:13327 [D loss: 0.006476, acc.: 100.00%] [G loss: 0.137712]\n",
      "epoch:17 step:13328 [D loss: 0.005622, acc.: 100.00%] [G loss: 0.120671]\n",
      "epoch:17 step:13329 [D loss: 0.008185, acc.: 100.00%] [G loss: 0.233418]\n",
      "epoch:17 step:13330 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.158570]\n",
      "epoch:17 step:13331 [D loss: 0.018298, acc.: 100.00%] [G loss: 0.066072]\n",
      "epoch:17 step:13332 [D loss: 0.004060, acc.: 100.00%] [G loss: 0.135099]\n",
      "epoch:17 step:13333 [D loss: 0.014077, acc.: 100.00%] [G loss: 0.139013]\n",
      "epoch:17 step:13334 [D loss: 0.003832, acc.: 100.00%] [G loss: 0.258240]\n",
      "epoch:17 step:13335 [D loss: 0.027701, acc.: 100.00%] [G loss: 0.209114]\n",
      "epoch:17 step:13336 [D loss: 0.000715, acc.: 100.00%] [G loss: 0.275747]\n",
      "epoch:17 step:13337 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.715444]\n",
      "epoch:17 step:13338 [D loss: 0.044109, acc.: 99.22%] [G loss: 0.008364]\n",
      "epoch:17 step:13339 [D loss: 0.014903, acc.: 100.00%] [G loss: 0.026644]\n",
      "epoch:17 step:13340 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.047931]\n",
      "epoch:17 step:13341 [D loss: 0.002297, acc.: 100.00%] [G loss: 0.009545]\n",
      "epoch:17 step:13342 [D loss: 0.013803, acc.: 100.00%] [G loss: 0.018582]\n",
      "epoch:17 step:13343 [D loss: 0.000751, acc.: 100.00%] [G loss: 0.008064]\n",
      "epoch:17 step:13344 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.005226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13345 [D loss: 0.003537, acc.: 100.00%] [G loss: 0.038444]\n",
      "epoch:17 step:13346 [D loss: 0.007112, acc.: 100.00%] [G loss: 0.057568]\n",
      "epoch:17 step:13347 [D loss: 0.001063, acc.: 100.00%] [G loss: 2.384502]\n",
      "epoch:17 step:13348 [D loss: 0.003908, acc.: 100.00%] [G loss: 0.054647]\n",
      "epoch:17 step:13349 [D loss: 0.010928, acc.: 100.00%] [G loss: 0.123135]\n",
      "epoch:17 step:13350 [D loss: 0.001388, acc.: 100.00%] [G loss: 0.070734]\n",
      "epoch:17 step:13351 [D loss: 0.016283, acc.: 100.00%] [G loss: 0.174385]\n",
      "epoch:17 step:13352 [D loss: 0.004159, acc.: 100.00%] [G loss: 4.139520]\n",
      "epoch:17 step:13353 [D loss: 0.007923, acc.: 100.00%] [G loss: 0.168638]\n",
      "epoch:17 step:13354 [D loss: 0.405286, acc.: 78.12%] [G loss: 7.854498]\n",
      "epoch:17 step:13355 [D loss: 2.537301, acc.: 50.00%] [G loss: 2.670587]\n",
      "epoch:17 step:13356 [D loss: 0.305474, acc.: 88.28%] [G loss: 1.569170]\n",
      "epoch:17 step:13357 [D loss: 0.037423, acc.: 99.22%] [G loss: 1.711986]\n",
      "epoch:17 step:13358 [D loss: 0.091603, acc.: 95.31%] [G loss: 2.046194]\n",
      "epoch:17 step:13359 [D loss: 0.060585, acc.: 98.44%] [G loss: 1.039511]\n",
      "epoch:17 step:13360 [D loss: 0.075459, acc.: 99.22%] [G loss: 0.512537]\n",
      "epoch:17 step:13361 [D loss: 0.295672, acc.: 85.94%] [G loss: 2.861132]\n",
      "epoch:17 step:13362 [D loss: 0.018519, acc.: 100.00%] [G loss: 4.577857]\n",
      "epoch:17 step:13363 [D loss: 0.099037, acc.: 97.66%] [G loss: 2.685460]\n",
      "epoch:17 step:13364 [D loss: 0.021727, acc.: 100.00%] [G loss: 2.660721]\n",
      "epoch:17 step:13365 [D loss: 0.108405, acc.: 97.66%] [G loss: 3.760729]\n",
      "epoch:17 step:13366 [D loss: 0.005068, acc.: 100.00%] [G loss: 5.312901]\n",
      "epoch:17 step:13367 [D loss: 0.840066, acc.: 59.38%] [G loss: 4.793134]\n",
      "epoch:17 step:13368 [D loss: 0.004352, acc.: 100.00%] [G loss: 6.611814]\n",
      "epoch:17 step:13369 [D loss: 0.013351, acc.: 100.00%] [G loss: 0.385744]\n",
      "epoch:17 step:13370 [D loss: 0.005082, acc.: 100.00%] [G loss: 0.319304]\n",
      "epoch:17 step:13371 [D loss: 0.185142, acc.: 91.41%] [G loss: 0.000840]\n",
      "epoch:17 step:13372 [D loss: 0.292370, acc.: 83.59%] [G loss: 7.458381]\n",
      "epoch:17 step:13373 [D loss: 0.014863, acc.: 100.00%] [G loss: 6.737390]\n",
      "epoch:17 step:13374 [D loss: 0.135815, acc.: 93.75%] [G loss: 5.476406]\n",
      "epoch:17 step:13375 [D loss: 0.006296, acc.: 100.00%] [G loss: 5.210397]\n",
      "epoch:17 step:13376 [D loss: 0.007858, acc.: 100.00%] [G loss: 3.913270]\n",
      "epoch:17 step:13377 [D loss: 0.002561, acc.: 100.00%] [G loss: 0.535466]\n",
      "epoch:17 step:13378 [D loss: 0.009504, acc.: 100.00%] [G loss: 2.288291]\n",
      "epoch:17 step:13379 [D loss: 0.021496, acc.: 100.00%] [G loss: 3.614350]\n",
      "epoch:17 step:13380 [D loss: 0.018017, acc.: 100.00%] [G loss: 2.857366]\n",
      "epoch:17 step:13381 [D loss: 0.004032, acc.: 100.00%] [G loss: 0.189610]\n",
      "epoch:17 step:13382 [D loss: 0.012815, acc.: 100.00%] [G loss: 2.319641]\n",
      "epoch:17 step:13383 [D loss: 0.007711, acc.: 100.00%] [G loss: 4.517556]\n",
      "epoch:17 step:13384 [D loss: 0.018690, acc.: 99.22%] [G loss: 1.061921]\n",
      "epoch:17 step:13385 [D loss: 0.007376, acc.: 100.00%] [G loss: 0.057479]\n",
      "epoch:17 step:13386 [D loss: 0.012928, acc.: 100.00%] [G loss: 1.448152]\n",
      "epoch:17 step:13387 [D loss: 0.016732, acc.: 100.00%] [G loss: 4.418276]\n",
      "epoch:17 step:13388 [D loss: 0.007742, acc.: 100.00%] [G loss: 2.113511]\n",
      "epoch:17 step:13389 [D loss: 0.005647, acc.: 100.00%] [G loss: 2.538343]\n",
      "epoch:17 step:13390 [D loss: 0.012487, acc.: 100.00%] [G loss: 0.591090]\n",
      "epoch:17 step:13391 [D loss: 0.021550, acc.: 100.00%] [G loss: 0.809786]\n",
      "epoch:17 step:13392 [D loss: 0.009673, acc.: 100.00%] [G loss: 0.104287]\n",
      "epoch:17 step:13393 [D loss: 0.008708, acc.: 100.00%] [G loss: 0.095921]\n",
      "epoch:17 step:13394 [D loss: 0.009206, acc.: 100.00%] [G loss: 0.227113]\n",
      "epoch:17 step:13395 [D loss: 0.002675, acc.: 100.00%] [G loss: 1.017844]\n",
      "epoch:17 step:13396 [D loss: 0.006671, acc.: 100.00%] [G loss: 0.136853]\n",
      "epoch:17 step:13397 [D loss: 0.010268, acc.: 100.00%] [G loss: 0.539326]\n",
      "epoch:17 step:13398 [D loss: 0.002594, acc.: 100.00%] [G loss: 0.225319]\n",
      "epoch:17 step:13399 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.967401]\n",
      "epoch:17 step:13400 [D loss: 0.007340, acc.: 100.00%] [G loss: 1.613952]\n",
      "epoch:17 step:13401 [D loss: 0.004400, acc.: 100.00%] [G loss: 1.189914]\n",
      "epoch:17 step:13402 [D loss: 0.003314, acc.: 100.00%] [G loss: 1.735426]\n",
      "epoch:17 step:13403 [D loss: 0.024814, acc.: 100.00%] [G loss: 0.552218]\n",
      "epoch:17 step:13404 [D loss: 0.049668, acc.: 100.00%] [G loss: 0.064331]\n",
      "epoch:17 step:13405 [D loss: 0.013156, acc.: 100.00%] [G loss: 3.261162]\n",
      "epoch:17 step:13406 [D loss: 0.012851, acc.: 100.00%] [G loss: 0.253685]\n",
      "epoch:17 step:13407 [D loss: 0.070164, acc.: 96.88%] [G loss: 0.060316]\n",
      "epoch:17 step:13408 [D loss: 0.035649, acc.: 99.22%] [G loss: 0.663409]\n",
      "epoch:17 step:13409 [D loss: 0.009580, acc.: 100.00%] [G loss: 0.258411]\n",
      "epoch:17 step:13410 [D loss: 0.009306, acc.: 100.00%] [G loss: 2.768628]\n",
      "epoch:17 step:13411 [D loss: 0.102674, acc.: 98.44%] [G loss: 1.679319]\n",
      "epoch:17 step:13412 [D loss: 0.059956, acc.: 99.22%] [G loss: 5.025939]\n",
      "epoch:17 step:13413 [D loss: 0.214501, acc.: 89.06%] [G loss: 0.061642]\n",
      "epoch:17 step:13414 [D loss: 0.065979, acc.: 99.22%] [G loss: 0.061996]\n",
      "epoch:17 step:13415 [D loss: 0.007841, acc.: 100.00%] [G loss: 1.512220]\n",
      "epoch:17 step:13416 [D loss: 0.001351, acc.: 100.00%] [G loss: 0.589790]\n",
      "epoch:17 step:13417 [D loss: 0.033338, acc.: 100.00%] [G loss: 3.041283]\n",
      "epoch:17 step:13418 [D loss: 0.006073, acc.: 100.00%] [G loss: 3.600308]\n",
      "epoch:17 step:13419 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.317339]\n",
      "epoch:17 step:13420 [D loss: 0.016007, acc.: 100.00%] [G loss: 1.434517]\n",
      "epoch:17 step:13421 [D loss: 0.003921, acc.: 100.00%] [G loss: 0.108054]\n",
      "epoch:17 step:13422 [D loss: 0.121055, acc.: 96.88%] [G loss: 1.511413]\n",
      "epoch:17 step:13423 [D loss: 0.154016, acc.: 92.97%] [G loss: 0.301473]\n",
      "epoch:17 step:13424 [D loss: 0.000929, acc.: 100.00%] [G loss: 0.062822]\n",
      "epoch:17 step:13425 [D loss: 0.016417, acc.: 100.00%] [G loss: 0.034529]\n",
      "epoch:17 step:13426 [D loss: 0.003179, acc.: 100.00%] [G loss: 2.203338]\n",
      "epoch:17 step:13427 [D loss: 0.001076, acc.: 100.00%] [G loss: 1.362808]\n",
      "epoch:17 step:13428 [D loss: 0.003563, acc.: 100.00%] [G loss: 0.053706]\n",
      "epoch:17 step:13429 [D loss: 0.004202, acc.: 100.00%] [G loss: 0.965263]\n",
      "epoch:17 step:13430 [D loss: 0.013553, acc.: 99.22%] [G loss: 0.314253]\n",
      "epoch:17 step:13431 [D loss: 0.010768, acc.: 100.00%] [G loss: 0.307297]\n",
      "epoch:17 step:13432 [D loss: 0.004916, acc.: 100.00%] [G loss: 0.049582]\n",
      "epoch:17 step:13433 [D loss: 0.004688, acc.: 100.00%] [G loss: 3.643756]\n",
      "epoch:17 step:13434 [D loss: 0.037128, acc.: 100.00%] [G loss: 0.908763]\n",
      "epoch:17 step:13435 [D loss: 0.005184, acc.: 100.00%] [G loss: 0.943338]\n",
      "epoch:17 step:13436 [D loss: 0.019633, acc.: 100.00%] [G loss: 1.537329]\n",
      "epoch:17 step:13437 [D loss: 0.013801, acc.: 100.00%] [G loss: 1.367838]\n",
      "epoch:17 step:13438 [D loss: 0.026513, acc.: 100.00%] [G loss: 0.386441]\n",
      "epoch:17 step:13439 [D loss: 0.008779, acc.: 100.00%] [G loss: 0.370848]\n",
      "epoch:17 step:13440 [D loss: 0.024591, acc.: 99.22%] [G loss: 0.365242]\n",
      "epoch:17 step:13441 [D loss: 0.045916, acc.: 98.44%] [G loss: 0.403377]\n",
      "epoch:17 step:13442 [D loss: 0.010803, acc.: 100.00%] [G loss: 4.944882]\n",
      "epoch:17 step:13443 [D loss: 0.013404, acc.: 100.00%] [G loss: 0.077111]\n",
      "epoch:17 step:13444 [D loss: 0.011912, acc.: 100.00%] [G loss: 2.261250]\n",
      "epoch:17 step:13445 [D loss: 0.009385, acc.: 100.00%] [G loss: 0.092463]\n",
      "epoch:17 step:13446 [D loss: 0.004799, acc.: 100.00%] [G loss: 1.945329]\n",
      "epoch:17 step:13447 [D loss: 0.025391, acc.: 100.00%] [G loss: 4.740435]\n",
      "epoch:17 step:13448 [D loss: 0.006858, acc.: 100.00%] [G loss: 0.875477]\n",
      "epoch:17 step:13449 [D loss: 0.010665, acc.: 100.00%] [G loss: 0.461936]\n",
      "epoch:17 step:13450 [D loss: 0.044623, acc.: 99.22%] [G loss: 0.006133]\n",
      "epoch:17 step:13451 [D loss: 0.003320, acc.: 100.00%] [G loss: 0.013680]\n",
      "epoch:17 step:13452 [D loss: 0.002228, acc.: 100.00%] [G loss: 0.044887]\n",
      "epoch:17 step:13453 [D loss: 0.155200, acc.: 93.75%] [G loss: 0.000952]\n",
      "epoch:17 step:13454 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.025678]\n",
      "epoch:17 step:13455 [D loss: 0.003553, acc.: 100.00%] [G loss: 0.357226]\n",
      "epoch:17 step:13456 [D loss: 0.033081, acc.: 98.44%] [G loss: 0.326026]\n",
      "epoch:17 step:13457 [D loss: 0.001059, acc.: 100.00%] [G loss: 0.802828]\n",
      "epoch:17 step:13458 [D loss: 0.003024, acc.: 100.00%] [G loss: 5.967530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13459 [D loss: 0.001959, acc.: 100.00%] [G loss: 4.360162]\n",
      "epoch:17 step:13460 [D loss: 0.002329, acc.: 100.00%] [G loss: 0.448085]\n",
      "epoch:17 step:13461 [D loss: 0.003959, acc.: 100.00%] [G loss: 0.506720]\n",
      "epoch:17 step:13462 [D loss: 0.002727, acc.: 100.00%] [G loss: 1.356547]\n",
      "epoch:17 step:13463 [D loss: 0.001336, acc.: 100.00%] [G loss: 1.415017]\n",
      "epoch:17 step:13464 [D loss: 0.011758, acc.: 100.00%] [G loss: 1.226157]\n",
      "epoch:17 step:13465 [D loss: 0.006448, acc.: 100.00%] [G loss: 0.774271]\n",
      "epoch:17 step:13466 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.451106]\n",
      "epoch:17 step:13467 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.314483]\n",
      "epoch:17 step:13468 [D loss: 0.022470, acc.: 99.22%] [G loss: 0.169303]\n",
      "epoch:17 step:13469 [D loss: 0.001900, acc.: 100.00%] [G loss: 0.518018]\n",
      "epoch:17 step:13470 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.955181]\n",
      "epoch:17 step:13471 [D loss: 0.004917, acc.: 100.00%] [G loss: 2.032567]\n",
      "epoch:17 step:13472 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.163458]\n",
      "epoch:17 step:13473 [D loss: 0.013019, acc.: 100.00%] [G loss: 0.335762]\n",
      "epoch:17 step:13474 [D loss: 0.002960, acc.: 100.00%] [G loss: 0.676213]\n",
      "epoch:17 step:13475 [D loss: 0.003483, acc.: 100.00%] [G loss: 1.026817]\n",
      "epoch:17 step:13476 [D loss: 0.003182, acc.: 100.00%] [G loss: 0.194666]\n",
      "epoch:17 step:13477 [D loss: 0.009889, acc.: 100.00%] [G loss: 0.242675]\n",
      "epoch:17 step:13478 [D loss: 0.012036, acc.: 100.00%] [G loss: 0.182280]\n",
      "epoch:17 step:13479 [D loss: 0.005071, acc.: 100.00%] [G loss: 0.657873]\n",
      "epoch:17 step:13480 [D loss: 0.002459, acc.: 100.00%] [G loss: 0.141500]\n",
      "epoch:17 step:13481 [D loss: 0.006465, acc.: 100.00%] [G loss: 0.241093]\n",
      "epoch:17 step:13482 [D loss: 0.005041, acc.: 100.00%] [G loss: 0.136332]\n",
      "epoch:17 step:13483 [D loss: 0.030861, acc.: 100.00%] [G loss: 0.136621]\n",
      "epoch:17 step:13484 [D loss: 0.015778, acc.: 100.00%] [G loss: 0.178415]\n",
      "epoch:17 step:13485 [D loss: 0.001160, acc.: 100.00%] [G loss: 2.447591]\n",
      "epoch:17 step:13486 [D loss: 0.035074, acc.: 99.22%] [G loss: 0.103131]\n",
      "epoch:17 step:13487 [D loss: 0.074964, acc.: 99.22%] [G loss: 1.645386]\n",
      "epoch:17 step:13488 [D loss: 0.001100, acc.: 100.00%] [G loss: 7.765797]\n",
      "epoch:17 step:13489 [D loss: 0.432262, acc.: 84.38%] [G loss: 0.026283]\n",
      "epoch:17 step:13490 [D loss: 0.875529, acc.: 74.22%] [G loss: 8.168950]\n",
      "epoch:17 step:13491 [D loss: 1.327537, acc.: 57.03%] [G loss: 6.913158]\n",
      "epoch:17 step:13492 [D loss: 0.021190, acc.: 98.44%] [G loss: 0.814555]\n",
      "epoch:17 step:13493 [D loss: 0.073352, acc.: 96.88%] [G loss: 4.640980]\n",
      "epoch:17 step:13494 [D loss: 0.013137, acc.: 100.00%] [G loss: 0.667445]\n",
      "epoch:17 step:13495 [D loss: 0.019241, acc.: 100.00%] [G loss: 0.288176]\n",
      "epoch:17 step:13496 [D loss: 0.001923, acc.: 100.00%] [G loss: 0.264652]\n",
      "epoch:17 step:13497 [D loss: 0.001950, acc.: 100.00%] [G loss: 0.396884]\n",
      "epoch:17 step:13498 [D loss: 0.007240, acc.: 100.00%] [G loss: 4.893814]\n",
      "epoch:17 step:13499 [D loss: 0.009392, acc.: 100.00%] [G loss: 2.649251]\n",
      "epoch:17 step:13500 [D loss: 0.763790, acc.: 72.66%] [G loss: 8.117427]\n",
      "epoch:17 step:13501 [D loss: 3.861780, acc.: 50.00%] [G loss: 5.874542]\n",
      "epoch:17 step:13502 [D loss: 0.688363, acc.: 68.75%] [G loss: 1.580619]\n",
      "epoch:17 step:13503 [D loss: 0.015578, acc.: 100.00%] [G loss: 4.614824]\n",
      "epoch:17 step:13504 [D loss: 0.043054, acc.: 99.22%] [G loss: 0.575813]\n",
      "epoch:17 step:13505 [D loss: 0.013291, acc.: 100.00%] [G loss: 0.303819]\n",
      "epoch:17 step:13506 [D loss: 0.034923, acc.: 100.00%] [G loss: 4.674847]\n",
      "epoch:17 step:13507 [D loss: 0.013093, acc.: 100.00%] [G loss: 3.519362]\n",
      "epoch:17 step:13508 [D loss: 0.007809, acc.: 100.00%] [G loss: 0.592219]\n",
      "epoch:17 step:13509 [D loss: 0.048237, acc.: 99.22%] [G loss: 3.521778]\n",
      "epoch:17 step:13510 [D loss: 0.013514, acc.: 100.00%] [G loss: 0.150616]\n",
      "epoch:17 step:13511 [D loss: 0.019552, acc.: 99.22%] [G loss: 2.220916]\n",
      "epoch:17 step:13512 [D loss: 0.015142, acc.: 100.00%] [G loss: 2.825159]\n",
      "epoch:17 step:13513 [D loss: 0.314394, acc.: 88.28%] [G loss: 2.212174]\n",
      "epoch:17 step:13514 [D loss: 0.251285, acc.: 87.50%] [G loss: 5.058238]\n",
      "epoch:17 step:13515 [D loss: 0.061336, acc.: 97.66%] [G loss: 0.050385]\n",
      "epoch:17 step:13516 [D loss: 0.003290, acc.: 100.00%] [G loss: 1.117491]\n",
      "epoch:17 step:13517 [D loss: 0.005704, acc.: 100.00%] [G loss: 4.188071]\n",
      "epoch:17 step:13518 [D loss: 0.011903, acc.: 100.00%] [G loss: 0.025416]\n",
      "epoch:17 step:13519 [D loss: 0.060743, acc.: 99.22%] [G loss: 4.797728]\n",
      "epoch:17 step:13520 [D loss: 0.012195, acc.: 100.00%] [G loss: 4.715120]\n",
      "epoch:17 step:13521 [D loss: 0.005080, acc.: 100.00%] [G loss: 0.317667]\n",
      "epoch:17 step:13522 [D loss: 0.010568, acc.: 100.00%] [G loss: 4.117548]\n",
      "epoch:17 step:13523 [D loss: 0.006174, acc.: 100.00%] [G loss: 0.068787]\n",
      "epoch:17 step:13524 [D loss: 0.057471, acc.: 97.66%] [G loss: 3.504902]\n",
      "epoch:17 step:13525 [D loss: 0.042811, acc.: 99.22%] [G loss: 0.011917]\n",
      "epoch:17 step:13526 [D loss: 0.009492, acc.: 100.00%] [G loss: 0.528006]\n",
      "epoch:17 step:13527 [D loss: 0.005907, acc.: 100.00%] [G loss: 0.143830]\n",
      "epoch:17 step:13528 [D loss: 0.009927, acc.: 100.00%] [G loss: 0.032967]\n",
      "epoch:17 step:13529 [D loss: 0.002637, acc.: 100.00%] [G loss: 0.856831]\n",
      "epoch:17 step:13530 [D loss: 0.009106, acc.: 100.00%] [G loss: 4.458018]\n",
      "epoch:17 step:13531 [D loss: 0.009382, acc.: 100.00%] [G loss: 4.745609]\n",
      "epoch:17 step:13532 [D loss: 0.016871, acc.: 100.00%] [G loss: 0.088672]\n",
      "epoch:17 step:13533 [D loss: 0.033181, acc.: 100.00%] [G loss: 0.030748]\n",
      "epoch:17 step:13534 [D loss: 0.035265, acc.: 100.00%] [G loss: 0.093185]\n",
      "epoch:17 step:13535 [D loss: 0.014282, acc.: 100.00%] [G loss: 5.060738]\n",
      "epoch:17 step:13536 [D loss: 0.012401, acc.: 100.00%] [G loss: 0.065489]\n",
      "epoch:17 step:13537 [D loss: 0.005239, acc.: 100.00%] [G loss: 0.062254]\n",
      "epoch:17 step:13538 [D loss: 0.137598, acc.: 96.88%] [G loss: 1.520988]\n",
      "epoch:17 step:13539 [D loss: 0.209085, acc.: 92.97%] [G loss: 0.079313]\n",
      "epoch:17 step:13540 [D loss: 0.012528, acc.: 99.22%] [G loss: 7.772968]\n",
      "epoch:17 step:13541 [D loss: 0.651666, acc.: 71.09%] [G loss: 4.890303]\n",
      "epoch:17 step:13542 [D loss: 0.028028, acc.: 99.22%] [G loss: 0.001482]\n",
      "epoch:17 step:13543 [D loss: 0.025504, acc.: 100.00%] [G loss: 4.541263]\n",
      "epoch:17 step:13544 [D loss: 0.013842, acc.: 100.00%] [G loss: 2.933372]\n",
      "epoch:17 step:13545 [D loss: 0.005260, acc.: 100.00%] [G loss: 2.343677]\n",
      "epoch:17 step:13546 [D loss: 0.206471, acc.: 90.62%] [G loss: 0.403437]\n",
      "epoch:17 step:13547 [D loss: 0.011345, acc.: 100.00%] [G loss: 6.645285]\n",
      "epoch:17 step:13548 [D loss: 0.309848, acc.: 84.38%] [G loss: 0.001883]\n",
      "epoch:17 step:13549 [D loss: 0.010927, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:17 step:13550 [D loss: 0.063285, acc.: 98.44%] [G loss: 3.692439]\n",
      "epoch:17 step:13551 [D loss: 0.007323, acc.: 100.00%] [G loss: 0.003570]\n",
      "epoch:17 step:13552 [D loss: 0.008038, acc.: 100.00%] [G loss: 0.008631]\n",
      "epoch:17 step:13553 [D loss: 0.001070, acc.: 100.00%] [G loss: 3.569247]\n",
      "epoch:17 step:13554 [D loss: 0.006910, acc.: 100.00%] [G loss: 0.090977]\n",
      "epoch:17 step:13555 [D loss: 0.020630, acc.: 100.00%] [G loss: 0.015148]\n",
      "epoch:17 step:13556 [D loss: 0.005348, acc.: 100.00%] [G loss: 2.800104]\n",
      "epoch:17 step:13557 [D loss: 0.002624, acc.: 100.00%] [G loss: 2.133504]\n",
      "epoch:17 step:13558 [D loss: 0.011196, acc.: 100.00%] [G loss: 0.024229]\n",
      "epoch:17 step:13559 [D loss: 0.047636, acc.: 99.22%] [G loss: 2.306984]\n",
      "epoch:17 step:13560 [D loss: 0.005086, acc.: 100.00%] [G loss: 1.443728]\n",
      "epoch:17 step:13561 [D loss: 0.010399, acc.: 100.00%] [G loss: 0.307711]\n",
      "epoch:17 step:13562 [D loss: 0.019481, acc.: 100.00%] [G loss: 1.296316]\n",
      "epoch:17 step:13563 [D loss: 0.004054, acc.: 100.00%] [G loss: 2.519242]\n",
      "epoch:17 step:13564 [D loss: 0.070079, acc.: 97.66%] [G loss: 0.056220]\n",
      "epoch:17 step:13565 [D loss: 0.003953, acc.: 100.00%] [G loss: 2.885829]\n",
      "epoch:17 step:13566 [D loss: 0.046348, acc.: 98.44%] [G loss: 0.005041]\n",
      "epoch:17 step:13567 [D loss: 0.022854, acc.: 100.00%] [G loss: 0.004054]\n",
      "epoch:17 step:13568 [D loss: 0.054086, acc.: 98.44%] [G loss: 2.038492]\n",
      "epoch:17 step:13569 [D loss: 0.012664, acc.: 100.00%] [G loss: 0.000941]\n",
      "epoch:17 step:13570 [D loss: 0.025438, acc.: 100.00%] [G loss: 3.339998]\n",
      "epoch:17 step:13571 [D loss: 0.003913, acc.: 100.00%] [G loss: 0.599825]\n",
      "epoch:17 step:13572 [D loss: 0.014691, acc.: 100.00%] [G loss: 0.001785]\n",
      "epoch:17 step:13573 [D loss: 0.004039, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:17 step:13574 [D loss: 0.009369, acc.: 100.00%] [G loss: 0.000963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13575 [D loss: 0.006958, acc.: 100.00%] [G loss: 0.005720]\n",
      "epoch:17 step:13576 [D loss: 0.001939, acc.: 100.00%] [G loss: 0.001584]\n",
      "epoch:17 step:13577 [D loss: 0.013300, acc.: 100.00%] [G loss: 0.006882]\n",
      "epoch:17 step:13578 [D loss: 0.002251, acc.: 100.00%] [G loss: 3.932409]\n",
      "epoch:17 step:13579 [D loss: 0.006118, acc.: 100.00%] [G loss: 0.000932]\n",
      "epoch:17 step:13580 [D loss: 0.007374, acc.: 100.00%] [G loss: 0.002859]\n",
      "epoch:17 step:13581 [D loss: 0.115320, acc.: 97.66%] [G loss: 7.364937]\n",
      "epoch:17 step:13582 [D loss: 0.615151, acc.: 69.53%] [G loss: 2.815581]\n",
      "epoch:17 step:13583 [D loss: 1.163548, acc.: 67.97%] [G loss: 8.299731]\n",
      "epoch:17 step:13584 [D loss: 1.862886, acc.: 53.12%] [G loss: 6.024134]\n",
      "epoch:17 step:13585 [D loss: 0.509691, acc.: 76.56%] [G loss: 5.421926]\n",
      "epoch:17 step:13586 [D loss: 0.011879, acc.: 100.00%] [G loss: 1.240962]\n",
      "epoch:17 step:13587 [D loss: 0.006497, acc.: 100.00%] [G loss: 4.980065]\n",
      "epoch:17 step:13588 [D loss: 0.012127, acc.: 100.00%] [G loss: 0.548034]\n",
      "epoch:17 step:13589 [D loss: 0.015638, acc.: 100.00%] [G loss: 0.795939]\n",
      "epoch:17 step:13590 [D loss: 0.048908, acc.: 99.22%] [G loss: 0.430493]\n",
      "epoch:17 step:13591 [D loss: 0.126956, acc.: 96.88%] [G loss: 5.321615]\n",
      "epoch:17 step:13592 [D loss: 0.168538, acc.: 95.31%] [G loss: 5.162599]\n",
      "epoch:17 step:13593 [D loss: 0.020616, acc.: 100.00%] [G loss: 0.576034]\n",
      "epoch:17 step:13594 [D loss: 0.058027, acc.: 98.44%] [G loss: 0.762668]\n",
      "epoch:17 step:13595 [D loss: 0.059598, acc.: 99.22%] [G loss: 3.921292]\n",
      "epoch:17 step:13596 [D loss: 0.033850, acc.: 99.22%] [G loss: 0.755654]\n",
      "epoch:17 step:13597 [D loss: 0.083159, acc.: 99.22%] [G loss: 0.785673]\n",
      "epoch:17 step:13598 [D loss: 0.039111, acc.: 100.00%] [G loss: 0.414525]\n",
      "epoch:17 step:13599 [D loss: 0.090807, acc.: 96.88%] [G loss: 0.392575]\n",
      "epoch:17 step:13600 [D loss: 0.031915, acc.: 100.00%] [G loss: 0.719822]\n",
      "epoch:17 step:13601 [D loss: 0.033737, acc.: 99.22%] [G loss: 3.760165]\n",
      "epoch:17 step:13602 [D loss: 0.045785, acc.: 99.22%] [G loss: 3.229290]\n",
      "epoch:17 step:13603 [D loss: 0.271051, acc.: 87.50%] [G loss: 4.107001]\n",
      "epoch:17 step:13604 [D loss: 0.569423, acc.: 72.66%] [G loss: 0.822392]\n",
      "epoch:17 step:13605 [D loss: 0.056191, acc.: 99.22%] [G loss: 1.833236]\n",
      "epoch:17 step:13606 [D loss: 0.028066, acc.: 99.22%] [G loss: 0.457858]\n",
      "epoch:17 step:13607 [D loss: 0.008341, acc.: 100.00%] [G loss: 0.503792]\n",
      "epoch:17 step:13608 [D loss: 0.013787, acc.: 100.00%] [G loss: 0.482210]\n",
      "epoch:17 step:13609 [D loss: 0.043977, acc.: 98.44%] [G loss: 4.720794]\n",
      "epoch:17 step:13610 [D loss: 0.017515, acc.: 99.22%] [G loss: 1.482579]\n",
      "epoch:17 step:13611 [D loss: 0.046199, acc.: 99.22%] [G loss: 0.501473]\n",
      "epoch:17 step:13612 [D loss: 0.028559, acc.: 100.00%] [G loss: 0.836880]\n",
      "epoch:17 step:13613 [D loss: 0.020865, acc.: 100.00%] [G loss: 0.445580]\n",
      "epoch:17 step:13614 [D loss: 0.043424, acc.: 99.22%] [G loss: 0.745306]\n",
      "epoch:17 step:13615 [D loss: 0.035374, acc.: 100.00%] [G loss: 0.646117]\n",
      "epoch:17 step:13616 [D loss: 0.052615, acc.: 99.22%] [G loss: 1.197644]\n",
      "epoch:17 step:13617 [D loss: 0.030779, acc.: 100.00%] [G loss: 0.746238]\n",
      "epoch:17 step:13618 [D loss: 0.095766, acc.: 96.88%] [G loss: 2.194866]\n",
      "epoch:17 step:13619 [D loss: 0.075517, acc.: 96.88%] [G loss: 6.397077]\n",
      "epoch:17 step:13620 [D loss: 0.205408, acc.: 91.41%] [G loss: 3.291962]\n",
      "epoch:17 step:13621 [D loss: 0.351937, acc.: 83.59%] [G loss: 4.639051]\n",
      "epoch:17 step:13622 [D loss: 0.166569, acc.: 89.06%] [G loss: 5.684756]\n",
      "epoch:17 step:13623 [D loss: 0.368726, acc.: 82.81%] [G loss: 4.151739]\n",
      "epoch:17 step:13624 [D loss: 0.311281, acc.: 85.94%] [G loss: 3.881492]\n",
      "epoch:17 step:13625 [D loss: 0.030502, acc.: 98.44%] [G loss: 3.403034]\n",
      "epoch:17 step:13626 [D loss: 0.514425, acc.: 80.47%] [G loss: 1.954252]\n",
      "epoch:17 step:13627 [D loss: 0.029992, acc.: 100.00%] [G loss: 0.191099]\n",
      "epoch:17 step:13628 [D loss: 0.049949, acc.: 98.44%] [G loss: 2.409274]\n",
      "epoch:17 step:13629 [D loss: 0.043574, acc.: 100.00%] [G loss: 2.706141]\n",
      "epoch:17 step:13630 [D loss: 0.010926, acc.: 100.00%] [G loss: 1.094282]\n",
      "epoch:17 step:13631 [D loss: 0.021819, acc.: 100.00%] [G loss: 0.963333]\n",
      "epoch:17 step:13632 [D loss: 0.279178, acc.: 88.28%] [G loss: 3.510823]\n",
      "epoch:17 step:13633 [D loss: 0.077157, acc.: 96.88%] [G loss: 6.122413]\n",
      "epoch:17 step:13634 [D loss: 0.071438, acc.: 97.66%] [G loss: 2.663143]\n",
      "epoch:17 step:13635 [D loss: 0.015190, acc.: 100.00%] [G loss: 2.194624]\n",
      "epoch:17 step:13636 [D loss: 0.048046, acc.: 99.22%] [G loss: 0.265276]\n",
      "epoch:17 step:13637 [D loss: 0.015128, acc.: 100.00%] [G loss: 1.826629]\n",
      "epoch:17 step:13638 [D loss: 0.008491, acc.: 100.00%] [G loss: 0.323207]\n",
      "epoch:17 step:13639 [D loss: 0.010916, acc.: 100.00%] [G loss: 0.912518]\n",
      "epoch:17 step:13640 [D loss: 0.084697, acc.: 98.44%] [G loss: 3.166571]\n",
      "epoch:17 step:13641 [D loss: 0.041604, acc.: 99.22%] [G loss: 0.059271]\n",
      "epoch:17 step:13642 [D loss: 0.043922, acc.: 100.00%] [G loss: 1.928143]\n",
      "epoch:17 step:13643 [D loss: 0.020880, acc.: 100.00%] [G loss: 0.099687]\n",
      "epoch:17 step:13644 [D loss: 0.051163, acc.: 99.22%] [G loss: 0.483027]\n",
      "epoch:17 step:13645 [D loss: 0.069855, acc.: 99.22%] [G loss: 0.231469]\n",
      "epoch:17 step:13646 [D loss: 0.039060, acc.: 99.22%] [G loss: 0.135792]\n",
      "epoch:17 step:13647 [D loss: 0.079251, acc.: 98.44%] [G loss: 2.800615]\n",
      "epoch:17 step:13648 [D loss: 0.101623, acc.: 97.66%] [G loss: 4.450476]\n",
      "epoch:17 step:13649 [D loss: 0.013142, acc.: 100.00%] [G loss: 4.311816]\n",
      "epoch:17 step:13650 [D loss: 0.101264, acc.: 95.31%] [G loss: 1.156097]\n",
      "epoch:17 step:13651 [D loss: 0.324550, acc.: 83.59%] [G loss: 1.243736]\n",
      "epoch:17 step:13652 [D loss: 0.452639, acc.: 80.47%] [G loss: 0.003917]\n",
      "epoch:17 step:13653 [D loss: 0.055259, acc.: 99.22%] [G loss: 0.005891]\n",
      "epoch:17 step:13654 [D loss: 0.058817, acc.: 99.22%] [G loss: 6.296311]\n",
      "epoch:17 step:13655 [D loss: 0.001830, acc.: 100.00%] [G loss: 0.053760]\n",
      "epoch:17 step:13656 [D loss: 0.011312, acc.: 100.00%] [G loss: 0.002141]\n",
      "epoch:17 step:13657 [D loss: 0.003495, acc.: 100.00%] [G loss: 0.013931]\n",
      "epoch:17 step:13658 [D loss: 0.004887, acc.: 100.00%] [G loss: 0.002267]\n",
      "epoch:17 step:13659 [D loss: 0.006023, acc.: 100.00%] [G loss: 0.001262]\n",
      "epoch:17 step:13660 [D loss: 0.007071, acc.: 100.00%] [G loss: 6.512552]\n",
      "epoch:17 step:13661 [D loss: 0.004824, acc.: 100.00%] [G loss: 0.002992]\n",
      "epoch:17 step:13662 [D loss: 0.014849, acc.: 100.00%] [G loss: 6.456235]\n",
      "epoch:17 step:13663 [D loss: 0.003439, acc.: 100.00%] [G loss: 0.012994]\n",
      "epoch:17 step:13664 [D loss: 0.019695, acc.: 99.22%] [G loss: 0.018460]\n",
      "epoch:17 step:13665 [D loss: 0.065289, acc.: 98.44%] [G loss: 0.002734]\n",
      "epoch:17 step:13666 [D loss: 0.090101, acc.: 98.44%] [G loss: 6.579036]\n",
      "epoch:17 step:13667 [D loss: 0.017243, acc.: 99.22%] [G loss: 0.425881]\n",
      "epoch:17 step:13668 [D loss: 0.164919, acc.: 95.31%] [G loss: 5.331409]\n",
      "epoch:17 step:13669 [D loss: 0.012429, acc.: 100.00%] [G loss: 4.464437]\n",
      "epoch:17 step:13670 [D loss: 0.004490, acc.: 100.00%] [G loss: 0.030369]\n",
      "epoch:17 step:13671 [D loss: 0.019609, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:17 step:13672 [D loss: 0.004946, acc.: 100.00%] [G loss: 0.002762]\n",
      "epoch:17 step:13673 [D loss: 0.005538, acc.: 100.00%] [G loss: 4.717527]\n",
      "epoch:17 step:13674 [D loss: 0.016867, acc.: 100.00%] [G loss: 0.001496]\n",
      "epoch:17 step:13675 [D loss: 0.002750, acc.: 100.00%] [G loss: 0.010029]\n",
      "epoch:17 step:13676 [D loss: 0.007302, acc.: 100.00%] [G loss: 0.002078]\n",
      "epoch:17 step:13677 [D loss: 0.002546, acc.: 100.00%] [G loss: 0.002542]\n",
      "epoch:17 step:13678 [D loss: 0.006107, acc.: 100.00%] [G loss: 0.009825]\n",
      "epoch:17 step:13679 [D loss: 0.006852, acc.: 100.00%] [G loss: 0.031908]\n",
      "epoch:17 step:13680 [D loss: 0.012438, acc.: 100.00%] [G loss: 4.831227]\n",
      "epoch:17 step:13681 [D loss: 0.015160, acc.: 100.00%] [G loss: 4.128169]\n",
      "epoch:17 step:13682 [D loss: 0.048817, acc.: 100.00%] [G loss: 4.749540]\n",
      "epoch:17 step:13683 [D loss: 0.004023, acc.: 100.00%] [G loss: 4.704609]\n",
      "epoch:17 step:13684 [D loss: 0.188726, acc.: 93.75%] [G loss: 0.088482]\n",
      "epoch:17 step:13685 [D loss: 0.005600, acc.: 100.00%] [G loss: 1.007433]\n",
      "epoch:17 step:13686 [D loss: 0.021001, acc.: 100.00%] [G loss: 0.085988]\n",
      "epoch:17 step:13687 [D loss: 0.005916, acc.: 100.00%] [G loss: 0.084225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13688 [D loss: 0.013478, acc.: 100.00%] [G loss: 0.030918]\n",
      "epoch:17 step:13689 [D loss: 0.014934, acc.: 100.00%] [G loss: 5.205486]\n",
      "epoch:17 step:13690 [D loss: 0.075424, acc.: 97.66%] [G loss: 4.535465]\n",
      "epoch:17 step:13691 [D loss: 0.006848, acc.: 100.00%] [G loss: 2.376948]\n",
      "epoch:17 step:13692 [D loss: 0.016409, acc.: 100.00%] [G loss: 1.394501]\n",
      "epoch:17 step:13693 [D loss: 0.018040, acc.: 100.00%] [G loss: 0.147373]\n",
      "epoch:17 step:13694 [D loss: 0.016872, acc.: 100.00%] [G loss: 1.684620]\n",
      "epoch:17 step:13695 [D loss: 0.008236, acc.: 100.00%] [G loss: 0.236333]\n",
      "epoch:17 step:13696 [D loss: 0.066398, acc.: 98.44%] [G loss: 0.235736]\n",
      "epoch:17 step:13697 [D loss: 0.155566, acc.: 95.31%] [G loss: 4.177155]\n",
      "epoch:17 step:13698 [D loss: 1.708014, acc.: 39.06%] [G loss: 9.183831]\n",
      "epoch:17 step:13699 [D loss: 1.638988, acc.: 52.34%] [G loss: 4.635223]\n",
      "epoch:17 step:13700 [D loss: 0.036463, acc.: 99.22%] [G loss: 4.817957]\n",
      "epoch:17 step:13701 [D loss: 0.018789, acc.: 100.00%] [G loss: 3.042729]\n",
      "epoch:17 step:13702 [D loss: 0.006490, acc.: 100.00%] [G loss: 1.359440]\n",
      "epoch:17 step:13703 [D loss: 0.141915, acc.: 95.31%] [G loss: 0.558376]\n",
      "epoch:17 step:13704 [D loss: 0.018951, acc.: 100.00%] [G loss: 0.826336]\n",
      "epoch:17 step:13705 [D loss: 0.014285, acc.: 99.22%] [G loss: 1.275381]\n",
      "epoch:17 step:13706 [D loss: 0.108633, acc.: 96.09%] [G loss: 5.043868]\n",
      "epoch:17 step:13707 [D loss: 0.056216, acc.: 98.44%] [G loss: 1.767562]\n",
      "epoch:17 step:13708 [D loss: 0.043040, acc.: 98.44%] [G loss: 0.900701]\n",
      "epoch:17 step:13709 [D loss: 0.081383, acc.: 99.22%] [G loss: 1.270109]\n",
      "epoch:17 step:13710 [D loss: 0.470418, acc.: 78.12%] [G loss: 3.107830]\n",
      "epoch:17 step:13711 [D loss: 0.747592, acc.: 67.97%] [G loss: 1.390592]\n",
      "epoch:17 step:13712 [D loss: 0.011384, acc.: 100.00%] [G loss: 1.487426]\n",
      "epoch:17 step:13713 [D loss: 0.092029, acc.: 96.88%] [G loss: 2.786889]\n",
      "epoch:17 step:13714 [D loss: 0.115606, acc.: 96.09%] [G loss: 5.354806]\n",
      "epoch:17 step:13715 [D loss: 0.084165, acc.: 97.66%] [G loss: 8.051167]\n",
      "epoch:17 step:13716 [D loss: 0.049687, acc.: 98.44%] [G loss: 3.653049]\n",
      "epoch:17 step:13717 [D loss: 0.009152, acc.: 100.00%] [G loss: 2.960201]\n",
      "epoch:17 step:13718 [D loss: 0.015933, acc.: 100.00%] [G loss: 2.093966]\n",
      "epoch:17 step:13719 [D loss: 0.050380, acc.: 99.22%] [G loss: 3.321488]\n",
      "epoch:17 step:13720 [D loss: 0.023725, acc.: 100.00%] [G loss: 3.993317]\n",
      "epoch:17 step:13721 [D loss: 0.005981, acc.: 100.00%] [G loss: 3.633830]\n",
      "epoch:17 step:13722 [D loss: 0.039654, acc.: 99.22%] [G loss: 6.156287]\n",
      "epoch:17 step:13723 [D loss: 0.013310, acc.: 100.00%] [G loss: 3.140786]\n",
      "epoch:17 step:13724 [D loss: 0.030084, acc.: 99.22%] [G loss: 2.635722]\n",
      "epoch:17 step:13725 [D loss: 0.032072, acc.: 98.44%] [G loss: 2.159656]\n",
      "epoch:17 step:13726 [D loss: 0.020575, acc.: 99.22%] [G loss: 1.865526]\n",
      "epoch:17 step:13727 [D loss: 0.013158, acc.: 100.00%] [G loss: 1.996524]\n",
      "epoch:17 step:13728 [D loss: 0.047056, acc.: 99.22%] [G loss: 4.336258]\n",
      "epoch:17 step:13729 [D loss: 0.004959, acc.: 100.00%] [G loss: 5.744126]\n",
      "epoch:17 step:13730 [D loss: 0.111462, acc.: 96.09%] [G loss: 1.533977]\n",
      "epoch:17 step:13731 [D loss: 0.057283, acc.: 98.44%] [G loss: 4.385832]\n",
      "epoch:17 step:13732 [D loss: 0.016252, acc.: 100.00%] [G loss: 5.413686]\n",
      "epoch:17 step:13733 [D loss: 0.012360, acc.: 100.00%] [G loss: 1.636375]\n",
      "epoch:17 step:13734 [D loss: 0.008631, acc.: 100.00%] [G loss: 3.173998]\n",
      "epoch:17 step:13735 [D loss: 0.046818, acc.: 99.22%] [G loss: 4.468198]\n",
      "epoch:17 step:13736 [D loss: 0.112262, acc.: 96.88%] [G loss: 3.197456]\n",
      "epoch:17 step:13737 [D loss: 0.112700, acc.: 96.88%] [G loss: 6.258917]\n",
      "epoch:17 step:13738 [D loss: 0.012485, acc.: 100.00%] [G loss: 7.755312]\n",
      "epoch:17 step:13739 [D loss: 0.163430, acc.: 92.19%] [G loss: 1.798531]\n",
      "epoch:17 step:13740 [D loss: 0.438809, acc.: 82.03%] [G loss: 6.065251]\n",
      "epoch:17 step:13741 [D loss: 0.400337, acc.: 84.38%] [G loss: 0.899696]\n",
      "epoch:17 step:13742 [D loss: 0.133101, acc.: 94.53%] [G loss: 9.534985]\n",
      "epoch:17 step:13743 [D loss: 0.023966, acc.: 99.22%] [G loss: 9.670881]\n",
      "epoch:17 step:13744 [D loss: 0.052313, acc.: 97.66%] [G loss: 9.449032]\n",
      "epoch:17 step:13745 [D loss: 0.011304, acc.: 100.00%] [G loss: 8.738529]\n",
      "epoch:17 step:13746 [D loss: 0.003148, acc.: 100.00%] [G loss: 8.260792]\n",
      "epoch:17 step:13747 [D loss: 0.007736, acc.: 100.00%] [G loss: 8.436676]\n",
      "epoch:17 step:13748 [D loss: 0.015114, acc.: 99.22%] [G loss: 7.365271]\n",
      "epoch:17 step:13749 [D loss: 0.000965, acc.: 100.00%] [G loss: 1.437993]\n",
      "epoch:17 step:13750 [D loss: 0.005269, acc.: 100.00%] [G loss: 0.115733]\n",
      "epoch:17 step:13751 [D loss: 0.015777, acc.: 100.00%] [G loss: 6.888790]\n",
      "epoch:17 step:13752 [D loss: 0.013507, acc.: 100.00%] [G loss: 7.527634]\n",
      "epoch:17 step:13753 [D loss: 0.009938, acc.: 100.00%] [G loss: 6.998353]\n",
      "epoch:17 step:13754 [D loss: 0.007439, acc.: 100.00%] [G loss: 6.856762]\n",
      "epoch:17 step:13755 [D loss: 0.009662, acc.: 100.00%] [G loss: 6.763015]\n",
      "epoch:17 step:13756 [D loss: 0.042623, acc.: 99.22%] [G loss: 6.312527]\n",
      "epoch:17 step:13757 [D loss: 0.002660, acc.: 100.00%] [G loss: 6.606435]\n",
      "epoch:17 step:13758 [D loss: 0.001784, acc.: 100.00%] [G loss: 6.149331]\n",
      "epoch:17 step:13759 [D loss: 0.003884, acc.: 100.00%] [G loss: 5.499245]\n",
      "epoch:17 step:13760 [D loss: 0.003417, acc.: 100.00%] [G loss: 4.918169]\n",
      "epoch:17 step:13761 [D loss: 0.004713, acc.: 100.00%] [G loss: 4.771251]\n",
      "epoch:17 step:13762 [D loss: 0.010376, acc.: 100.00%] [G loss: 5.005488]\n",
      "epoch:17 step:13763 [D loss: 0.003846, acc.: 100.00%] [G loss: 5.191754]\n",
      "epoch:17 step:13764 [D loss: 0.012590, acc.: 100.00%] [G loss: 4.502734]\n",
      "epoch:17 step:13765 [D loss: 0.002109, acc.: 100.00%] [G loss: 4.015082]\n",
      "epoch:17 step:13766 [D loss: 0.009668, acc.: 100.00%] [G loss: 3.702281]\n",
      "epoch:17 step:13767 [D loss: 0.052278, acc.: 99.22%] [G loss: 5.676066]\n",
      "epoch:17 step:13768 [D loss: 0.003100, acc.: 100.00%] [G loss: 5.855677]\n",
      "epoch:17 step:13769 [D loss: 0.112282, acc.: 95.31%] [G loss: 3.192715]\n",
      "epoch:17 step:13770 [D loss: 0.023604, acc.: 100.00%] [G loss: 3.268120]\n",
      "epoch:17 step:13771 [D loss: 0.002037, acc.: 100.00%] [G loss: 1.916312]\n",
      "epoch:17 step:13772 [D loss: 0.009267, acc.: 100.00%] [G loss: 2.456589]\n",
      "epoch:17 step:13773 [D loss: 0.015249, acc.: 100.00%] [G loss: 2.010140]\n",
      "epoch:17 step:13774 [D loss: 1.023976, acc.: 61.72%] [G loss: 12.396106]\n",
      "epoch:17 step:13775 [D loss: 4.679436, acc.: 50.00%] [G loss: 6.474541]\n",
      "epoch:17 step:13776 [D loss: 1.801334, acc.: 50.00%] [G loss: 3.046124]\n",
      "epoch:17 step:13777 [D loss: 0.897145, acc.: 57.03%] [G loss: 2.815734]\n",
      "epoch:17 step:13778 [D loss: 0.048442, acc.: 99.22%] [G loss: 3.491012]\n",
      "epoch:17 step:13779 [D loss: 0.059009, acc.: 100.00%] [G loss: 3.294623]\n",
      "epoch:17 step:13780 [D loss: 0.090418, acc.: 98.44%] [G loss: 2.671379]\n",
      "epoch:17 step:13781 [D loss: 0.058721, acc.: 100.00%] [G loss: 3.108574]\n",
      "epoch:17 step:13782 [D loss: 0.035700, acc.: 100.00%] [G loss: 2.963376]\n",
      "epoch:17 step:13783 [D loss: 0.068188, acc.: 100.00%] [G loss: 2.992693]\n",
      "epoch:17 step:13784 [D loss: 0.078773, acc.: 100.00%] [G loss: 3.374998]\n",
      "epoch:17 step:13785 [D loss: 0.031901, acc.: 100.00%] [G loss: 2.076462]\n",
      "epoch:17 step:13786 [D loss: 0.055212, acc.: 100.00%] [G loss: 3.094522]\n",
      "epoch:17 step:13787 [D loss: 0.061399, acc.: 100.00%] [G loss: 1.910482]\n",
      "epoch:17 step:13788 [D loss: 0.087106, acc.: 100.00%] [G loss: 2.165095]\n",
      "epoch:17 step:13789 [D loss: 0.068957, acc.: 99.22%] [G loss: 1.082200]\n",
      "epoch:17 step:13790 [D loss: 0.105691, acc.: 98.44%] [G loss: 0.349981]\n",
      "epoch:17 step:13791 [D loss: 0.048975, acc.: 100.00%] [G loss: 0.246512]\n",
      "epoch:17 step:13792 [D loss: 0.035368, acc.: 100.00%] [G loss: 0.715906]\n",
      "epoch:17 step:13793 [D loss: 0.053134, acc.: 100.00%] [G loss: 1.341778]\n",
      "epoch:17 step:13794 [D loss: 0.055651, acc.: 99.22%] [G loss: 1.313352]\n",
      "epoch:17 step:13795 [D loss: 0.015853, acc.: 100.00%] [G loss: 1.440662]\n",
      "epoch:17 step:13796 [D loss: 0.149906, acc.: 96.09%] [G loss: 1.028041]\n",
      "epoch:17 step:13797 [D loss: 0.064079, acc.: 99.22%] [G loss: 0.562470]\n",
      "epoch:17 step:13798 [D loss: 0.074733, acc.: 100.00%] [G loss: 1.177524]\n",
      "epoch:17 step:13799 [D loss: 0.017723, acc.: 100.00%] [G loss: 1.512862]\n",
      "epoch:17 step:13800 [D loss: 0.058024, acc.: 96.88%] [G loss: 0.765177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13801 [D loss: 0.499803, acc.: 73.44%] [G loss: 2.252297]\n",
      "epoch:17 step:13802 [D loss: 0.104485, acc.: 94.53%] [G loss: 2.641674]\n",
      "epoch:17 step:13803 [D loss: 0.719043, acc.: 69.53%] [G loss: 0.079103]\n",
      "epoch:17 step:13804 [D loss: 0.279620, acc.: 86.72%] [G loss: 0.529246]\n",
      "epoch:17 step:13805 [D loss: 0.010308, acc.: 100.00%] [G loss: 1.367915]\n",
      "epoch:17 step:13806 [D loss: 0.021488, acc.: 100.00%] [G loss: 1.702269]\n",
      "epoch:17 step:13807 [D loss: 0.021096, acc.: 100.00%] [G loss: 0.995859]\n",
      "epoch:17 step:13808 [D loss: 0.016389, acc.: 100.00%] [G loss: 0.636268]\n",
      "epoch:17 step:13809 [D loss: 0.133391, acc.: 93.75%] [G loss: 0.578332]\n",
      "epoch:17 step:13810 [D loss: 0.062953, acc.: 99.22%] [G loss: 5.577656]\n",
      "epoch:17 step:13811 [D loss: 0.115310, acc.: 96.09%] [G loss: 2.006784]\n",
      "epoch:17 step:13812 [D loss: 0.048212, acc.: 100.00%] [G loss: 0.687790]\n",
      "epoch:17 step:13813 [D loss: 0.090025, acc.: 96.88%] [G loss: 1.013028]\n",
      "epoch:17 step:13814 [D loss: 0.148368, acc.: 92.97%] [G loss: 0.227782]\n",
      "epoch:17 step:13815 [D loss: 0.079762, acc.: 98.44%] [G loss: 0.402493]\n",
      "epoch:17 step:13816 [D loss: 0.019361, acc.: 100.00%] [G loss: 0.775260]\n",
      "epoch:17 step:13817 [D loss: 0.022399, acc.: 100.00%] [G loss: 2.989519]\n",
      "epoch:17 step:13818 [D loss: 0.016430, acc.: 100.00%] [G loss: 0.907567]\n",
      "epoch:17 step:13819 [D loss: 0.183819, acc.: 91.41%] [G loss: 2.578484]\n",
      "epoch:17 step:13820 [D loss: 0.081704, acc.: 99.22%] [G loss: 2.326874]\n",
      "epoch:17 step:13821 [D loss: 0.074984, acc.: 98.44%] [G loss: 2.914241]\n",
      "epoch:17 step:13822 [D loss: 0.115907, acc.: 96.88%] [G loss: 0.793113]\n",
      "epoch:17 step:13823 [D loss: 0.062919, acc.: 99.22%] [G loss: 1.540640]\n",
      "epoch:17 step:13824 [D loss: 0.130814, acc.: 96.09%] [G loss: 2.369060]\n",
      "epoch:17 step:13825 [D loss: 0.090492, acc.: 98.44%] [G loss: 0.608492]\n",
      "epoch:17 step:13826 [D loss: 0.045668, acc.: 100.00%] [G loss: 0.630661]\n",
      "epoch:17 step:13827 [D loss: 0.063464, acc.: 98.44%] [G loss: 0.586998]\n",
      "epoch:17 step:13828 [D loss: 0.166848, acc.: 96.09%] [G loss: 3.407833]\n",
      "epoch:17 step:13829 [D loss: 0.114967, acc.: 96.09%] [G loss: 2.750200]\n",
      "epoch:17 step:13830 [D loss: 0.072342, acc.: 96.88%] [G loss: 1.262896]\n",
      "epoch:17 step:13831 [D loss: 0.109062, acc.: 97.66%] [G loss: 1.017965]\n",
      "epoch:17 step:13832 [D loss: 0.012957, acc.: 100.00%] [G loss: 2.638659]\n",
      "epoch:17 step:13833 [D loss: 0.039181, acc.: 99.22%] [G loss: 1.550124]\n",
      "epoch:17 step:13834 [D loss: 0.340987, acc.: 85.94%] [G loss: 4.661558]\n",
      "epoch:17 step:13835 [D loss: 0.410101, acc.: 81.25%] [G loss: 1.102123]\n",
      "epoch:17 step:13836 [D loss: 0.093249, acc.: 96.88%] [G loss: 1.725259]\n",
      "epoch:17 step:13837 [D loss: 0.056987, acc.: 98.44%] [G loss: 3.440590]\n",
      "epoch:17 step:13838 [D loss: 0.040015, acc.: 99.22%] [G loss: 4.782051]\n",
      "epoch:17 step:13839 [D loss: 0.115123, acc.: 97.66%] [G loss: 3.051436]\n",
      "epoch:17 step:13840 [D loss: 0.381865, acc.: 82.81%] [G loss: 3.789658]\n",
      "epoch:17 step:13841 [D loss: 0.015710, acc.: 100.00%] [G loss: 5.137994]\n",
      "epoch:17 step:13842 [D loss: 0.124161, acc.: 94.53%] [G loss: 3.322509]\n",
      "epoch:17 step:13843 [D loss: 0.058754, acc.: 98.44%] [G loss: 3.723971]\n",
      "epoch:17 step:13844 [D loss: 0.080699, acc.: 98.44%] [G loss: 3.172495]\n",
      "epoch:17 step:13845 [D loss: 0.008902, acc.: 100.00%] [G loss: 4.086882]\n",
      "epoch:17 step:13846 [D loss: 0.020678, acc.: 100.00%] [G loss: 3.683596]\n",
      "epoch:17 step:13847 [D loss: 0.008781, acc.: 100.00%] [G loss: 3.546529]\n",
      "epoch:17 step:13848 [D loss: 0.042594, acc.: 100.00%] [G loss: 3.021698]\n",
      "epoch:17 step:13849 [D loss: 0.061475, acc.: 100.00%] [G loss: 2.687300]\n",
      "epoch:17 step:13850 [D loss: 0.034338, acc.: 99.22%] [G loss: 5.070543]\n",
      "epoch:17 step:13851 [D loss: 0.242452, acc.: 91.41%] [G loss: 2.020736]\n",
      "epoch:17 step:13852 [D loss: 0.058954, acc.: 98.44%] [G loss: 2.829478]\n",
      "epoch:17 step:13853 [D loss: 0.061527, acc.: 98.44%] [G loss: 4.539917]\n",
      "epoch:17 step:13854 [D loss: 0.042947, acc.: 98.44%] [G loss: 4.494227]\n",
      "epoch:17 step:13855 [D loss: 0.015631, acc.: 100.00%] [G loss: 2.276221]\n",
      "epoch:17 step:13856 [D loss: 0.011114, acc.: 100.00%] [G loss: 3.015195]\n",
      "epoch:17 step:13857 [D loss: 0.115622, acc.: 97.66%] [G loss: 5.971953]\n",
      "epoch:17 step:13858 [D loss: 0.101495, acc.: 95.31%] [G loss: 4.386038]\n",
      "epoch:17 step:13859 [D loss: 0.017534, acc.: 100.00%] [G loss: 4.408119]\n",
      "epoch:17 step:13860 [D loss: 0.004493, acc.: 100.00%] [G loss: 3.836231]\n",
      "epoch:17 step:13861 [D loss: 0.034742, acc.: 100.00%] [G loss: 4.037500]\n",
      "epoch:17 step:13862 [D loss: 0.096294, acc.: 97.66%] [G loss: 5.166622]\n",
      "epoch:17 step:13863 [D loss: 0.008370, acc.: 100.00%] [G loss: 4.936450]\n",
      "epoch:17 step:13864 [D loss: 0.662207, acc.: 71.09%] [G loss: 3.426293]\n",
      "epoch:17 step:13865 [D loss: 0.005425, acc.: 100.00%] [G loss: 3.592385]\n",
      "epoch:17 step:13866 [D loss: 0.016649, acc.: 100.00%] [G loss: 4.240108]\n",
      "epoch:17 step:13867 [D loss: 0.052939, acc.: 98.44%] [G loss: 4.134173]\n",
      "epoch:17 step:13868 [D loss: 0.003091, acc.: 100.00%] [G loss: 4.629293]\n",
      "epoch:17 step:13869 [D loss: 0.004970, acc.: 100.00%] [G loss: 3.413122]\n",
      "epoch:17 step:13870 [D loss: 0.051998, acc.: 97.66%] [G loss: 6.520642]\n",
      "epoch:17 step:13871 [D loss: 0.005468, acc.: 100.00%] [G loss: 5.720611]\n",
      "epoch:17 step:13872 [D loss: 0.039311, acc.: 98.44%] [G loss: 4.195931]\n",
      "epoch:17 step:13873 [D loss: 0.050004, acc.: 99.22%] [G loss: 3.867181]\n",
      "epoch:17 step:13874 [D loss: 0.020315, acc.: 100.00%] [G loss: 3.470383]\n",
      "epoch:17 step:13875 [D loss: 0.055061, acc.: 97.66%] [G loss: 2.092850]\n",
      "epoch:17 step:13876 [D loss: 0.014438, acc.: 100.00%] [G loss: 1.741798]\n",
      "epoch:17 step:13877 [D loss: 0.036808, acc.: 100.00%] [G loss: 4.924818]\n",
      "epoch:17 step:13878 [D loss: 0.021724, acc.: 100.00%] [G loss: 5.386671]\n",
      "epoch:17 step:13879 [D loss: 0.041658, acc.: 100.00%] [G loss: 1.967705]\n",
      "epoch:17 step:13880 [D loss: 0.013130, acc.: 100.00%] [G loss: 1.714427]\n",
      "epoch:17 step:13881 [D loss: 0.375921, acc.: 85.16%] [G loss: 4.487193]\n",
      "epoch:17 step:13882 [D loss: 0.005168, acc.: 100.00%] [G loss: 5.485765]\n",
      "epoch:17 step:13883 [D loss: 0.473434, acc.: 80.47%] [G loss: 1.471086]\n",
      "epoch:17 step:13884 [D loss: 0.314609, acc.: 83.59%] [G loss: 3.423775]\n",
      "epoch:17 step:13885 [D loss: 0.000246, acc.: 100.00%] [G loss: 7.970261]\n",
      "epoch:17 step:13886 [D loss: 0.147315, acc.: 92.97%] [G loss: 1.834965]\n",
      "epoch:17 step:13887 [D loss: 0.002076, acc.: 100.00%] [G loss: 1.432587]\n",
      "epoch:17 step:13888 [D loss: 0.005680, acc.: 100.00%] [G loss: 1.393404]\n",
      "epoch:17 step:13889 [D loss: 0.014127, acc.: 100.00%] [G loss: 0.822405]\n",
      "epoch:17 step:13890 [D loss: 0.029959, acc.: 99.22%] [G loss: 4.760466]\n",
      "epoch:17 step:13891 [D loss: 0.001432, acc.: 100.00%] [G loss: 4.476440]\n",
      "epoch:17 step:13892 [D loss: 0.021439, acc.: 99.22%] [G loss: 1.656766]\n",
      "epoch:17 step:13893 [D loss: 0.004259, acc.: 100.00%] [G loss: 1.243716]\n",
      "epoch:17 step:13894 [D loss: 0.011363, acc.: 100.00%] [G loss: 1.172289]\n",
      "epoch:17 step:13895 [D loss: 0.098913, acc.: 96.88%] [G loss: 4.685576]\n",
      "epoch:17 step:13896 [D loss: 0.274048, acc.: 88.28%] [G loss: 1.307930]\n",
      "epoch:17 step:13897 [D loss: 0.721140, acc.: 72.66%] [G loss: 6.128101]\n",
      "epoch:17 step:13898 [D loss: 1.225628, acc.: 56.25%] [G loss: 0.788945]\n",
      "epoch:17 step:13899 [D loss: 0.034345, acc.: 98.44%] [G loss: 0.708853]\n",
      "epoch:17 step:13900 [D loss: 0.018732, acc.: 100.00%] [G loss: 0.799760]\n",
      "epoch:17 step:13901 [D loss: 0.009133, acc.: 100.00%] [G loss: 0.568947]\n",
      "epoch:17 step:13902 [D loss: 0.016980, acc.: 100.00%] [G loss: 0.962841]\n",
      "epoch:17 step:13903 [D loss: 0.011282, acc.: 100.00%] [G loss: 0.396107]\n",
      "epoch:17 step:13904 [D loss: 0.231546, acc.: 90.62%] [G loss: 3.188749]\n",
      "epoch:17 step:13905 [D loss: 0.032008, acc.: 99.22%] [G loss: 4.593836]\n",
      "epoch:17 step:13906 [D loss: 0.208919, acc.: 89.06%] [G loss: 1.277633]\n",
      "epoch:17 step:13907 [D loss: 0.057632, acc.: 99.22%] [G loss: 2.305286]\n",
      "epoch:17 step:13908 [D loss: 0.009423, acc.: 100.00%] [G loss: 3.841384]\n",
      "epoch:17 step:13909 [D loss: 0.004415, acc.: 100.00%] [G loss: 3.496934]\n",
      "epoch:17 step:13910 [D loss: 0.010195, acc.: 100.00%] [G loss: 3.256774]\n",
      "epoch:17 step:13911 [D loss: 0.010470, acc.: 100.00%] [G loss: 1.864375]\n",
      "epoch:17 step:13912 [D loss: 0.016258, acc.: 100.00%] [G loss: 3.567644]\n",
      "epoch:17 step:13913 [D loss: 0.013686, acc.: 100.00%] [G loss: 3.402767]\n",
      "epoch:17 step:13914 [D loss: 0.014707, acc.: 100.00%] [G loss: 2.578715]\n",
      "epoch:17 step:13915 [D loss: 0.023488, acc.: 100.00%] [G loss: 2.099701]\n",
      "epoch:17 step:13916 [D loss: 0.061763, acc.: 99.22%] [G loss: 0.890965]\n",
      "epoch:17 step:13917 [D loss: 0.073408, acc.: 98.44%] [G loss: 3.028623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13918 [D loss: 0.007530, acc.: 100.00%] [G loss: 4.534474]\n",
      "epoch:17 step:13919 [D loss: 0.121808, acc.: 96.88%] [G loss: 1.911816]\n",
      "epoch:17 step:13920 [D loss: 0.300908, acc.: 88.28%] [G loss: 5.834521]\n",
      "epoch:17 step:13921 [D loss: 0.014032, acc.: 100.00%] [G loss: 4.119349]\n",
      "epoch:17 step:13922 [D loss: 0.568970, acc.: 71.09%] [G loss: 3.308870]\n",
      "epoch:17 step:13923 [D loss: 0.226476, acc.: 87.50%] [G loss: 0.762099]\n",
      "epoch:17 step:13924 [D loss: 0.002396, acc.: 100.00%] [G loss: 6.146035]\n",
      "epoch:17 step:13925 [D loss: 0.089443, acc.: 96.88%] [G loss: 3.950756]\n",
      "epoch:17 step:13926 [D loss: 0.061392, acc.: 98.44%] [G loss: 2.150133]\n",
      "epoch:17 step:13927 [D loss: 0.024418, acc.: 100.00%] [G loss: 0.992506]\n",
      "epoch:17 step:13928 [D loss: 0.060406, acc.: 97.66%] [G loss: 3.119727]\n",
      "epoch:17 step:13929 [D loss: 0.001795, acc.: 100.00%] [G loss: 3.302578]\n",
      "epoch:17 step:13930 [D loss: 0.002234, acc.: 100.00%] [G loss: 2.714410]\n",
      "epoch:17 step:13931 [D loss: 0.007737, acc.: 100.00%] [G loss: 1.873965]\n",
      "epoch:17 step:13932 [D loss: 0.002971, acc.: 100.00%] [G loss: 0.930619]\n",
      "epoch:17 step:13933 [D loss: 0.008098, acc.: 100.00%] [G loss: 1.894794]\n",
      "epoch:17 step:13934 [D loss: 0.005015, acc.: 100.00%] [G loss: 6.192819]\n",
      "epoch:17 step:13935 [D loss: 0.022215, acc.: 100.00%] [G loss: 1.649571]\n",
      "epoch:17 step:13936 [D loss: 0.008369, acc.: 100.00%] [G loss: 1.192285]\n",
      "epoch:17 step:13937 [D loss: 0.005494, acc.: 100.00%] [G loss: 0.576775]\n",
      "epoch:17 step:13938 [D loss: 0.123048, acc.: 96.09%] [G loss: 0.698447]\n",
      "epoch:17 step:13939 [D loss: 0.040242, acc.: 97.66%] [G loss: 5.188067]\n",
      "epoch:17 step:13940 [D loss: 0.088743, acc.: 97.66%] [G loss: 1.815833]\n",
      "epoch:17 step:13941 [D loss: 0.008857, acc.: 100.00%] [G loss: 4.429609]\n",
      "epoch:17 step:13942 [D loss: 3.778549, acc.: 7.03%] [G loss: 6.464999]\n",
      "epoch:17 step:13943 [D loss: 1.739701, acc.: 51.56%] [G loss: 6.807098]\n",
      "epoch:17 step:13944 [D loss: 0.926614, acc.: 60.94%] [G loss: 1.649338]\n",
      "epoch:17 step:13945 [D loss: 0.029640, acc.: 100.00%] [G loss: 0.946311]\n",
      "epoch:17 step:13946 [D loss: 0.083928, acc.: 98.44%] [G loss: 0.692144]\n",
      "epoch:17 step:13947 [D loss: 0.012237, acc.: 100.00%] [G loss: 0.567021]\n",
      "epoch:17 step:13948 [D loss: 0.120378, acc.: 94.53%] [G loss: 0.496856]\n",
      "epoch:17 step:13949 [D loss: 0.127810, acc.: 96.88%] [G loss: 4.806776]\n",
      "epoch:17 step:13950 [D loss: 0.110093, acc.: 95.31%] [G loss: 4.001596]\n",
      "epoch:17 step:13951 [D loss: 0.012939, acc.: 100.00%] [G loss: 0.165374]\n",
      "epoch:17 step:13952 [D loss: 0.337956, acc.: 83.59%] [G loss: 0.740063]\n",
      "epoch:17 step:13953 [D loss: 0.007385, acc.: 100.00%] [G loss: 1.851290]\n",
      "epoch:17 step:13954 [D loss: 0.083489, acc.: 97.66%] [G loss: 3.682752]\n",
      "epoch:17 step:13955 [D loss: 0.276260, acc.: 86.72%] [G loss: 4.411365]\n",
      "epoch:17 step:13956 [D loss: 0.359369, acc.: 82.03%] [G loss: 0.938471]\n",
      "epoch:17 step:13957 [D loss: 0.022822, acc.: 100.00%] [G loss: 1.982286]\n",
      "epoch:17 step:13958 [D loss: 0.528356, acc.: 70.31%] [G loss: 1.167652]\n",
      "epoch:17 step:13959 [D loss: 0.069180, acc.: 98.44%] [G loss: 1.082351]\n",
      "epoch:17 step:13960 [D loss: 0.043297, acc.: 100.00%] [G loss: 4.796982]\n",
      "epoch:17 step:13961 [D loss: 0.104966, acc.: 97.66%] [G loss: 1.502877]\n",
      "epoch:17 step:13962 [D loss: 0.096198, acc.: 97.66%] [G loss: 1.354287]\n",
      "epoch:17 step:13963 [D loss: 0.119085, acc.: 96.09%] [G loss: 0.253833]\n",
      "epoch:17 step:13964 [D loss: 0.102921, acc.: 96.88%] [G loss: 0.789201]\n",
      "epoch:17 step:13965 [D loss: 0.041412, acc.: 99.22%] [G loss: 1.585267]\n",
      "epoch:17 step:13966 [D loss: 0.043593, acc.: 100.00%] [G loss: 1.377144]\n",
      "epoch:17 step:13967 [D loss: 0.120656, acc.: 96.88%] [G loss: 5.727607]\n",
      "epoch:17 step:13968 [D loss: 0.035374, acc.: 100.00%] [G loss: 2.204934]\n",
      "epoch:17 step:13969 [D loss: 0.014514, acc.: 100.00%] [G loss: 1.889276]\n",
      "epoch:17 step:13970 [D loss: 0.040625, acc.: 100.00%] [G loss: 5.285772]\n",
      "epoch:17 step:13971 [D loss: 0.039850, acc.: 99.22%] [G loss: 1.802450]\n",
      "epoch:17 step:13972 [D loss: 0.023543, acc.: 100.00%] [G loss: 1.841907]\n",
      "epoch:17 step:13973 [D loss: 0.009526, acc.: 100.00%] [G loss: 4.738341]\n",
      "epoch:17 step:13974 [D loss: 0.028092, acc.: 100.00%] [G loss: 4.052746]\n",
      "epoch:17 step:13975 [D loss: 0.034569, acc.: 100.00%] [G loss: 4.520825]\n",
      "epoch:17 step:13976 [D loss: 0.018655, acc.: 100.00%] [G loss: 3.342951]\n",
      "epoch:17 step:13977 [D loss: 0.082422, acc.: 99.22%] [G loss: 1.268274]\n",
      "epoch:17 step:13978 [D loss: 0.081799, acc.: 99.22%] [G loss: 1.292176]\n",
      "epoch:17 step:13979 [D loss: 0.018858, acc.: 100.00%] [G loss: 1.473576]\n",
      "epoch:17 step:13980 [D loss: 0.065106, acc.: 100.00%] [G loss: 0.714505]\n",
      "epoch:17 step:13981 [D loss: 0.021425, acc.: 100.00%] [G loss: 0.416105]\n",
      "epoch:17 step:13982 [D loss: 0.031219, acc.: 100.00%] [G loss: 3.299819]\n",
      "epoch:17 step:13983 [D loss: 0.053667, acc.: 99.22%] [G loss: 3.992330]\n",
      "epoch:17 step:13984 [D loss: 0.046023, acc.: 99.22%] [G loss: 0.327359]\n",
      "epoch:17 step:13985 [D loss: 0.037131, acc.: 100.00%] [G loss: 3.274481]\n",
      "epoch:17 step:13986 [D loss: 0.011450, acc.: 100.00%] [G loss: 0.624881]\n",
      "epoch:17 step:13987 [D loss: 0.019132, acc.: 100.00%] [G loss: 2.212758]\n",
      "epoch:17 step:13988 [D loss: 0.064396, acc.: 99.22%] [G loss: 2.565855]\n",
      "epoch:17 step:13989 [D loss: 0.089566, acc.: 97.66%] [G loss: 2.895565]\n",
      "epoch:17 step:13990 [D loss: 0.490519, acc.: 75.00%] [G loss: 0.031506]\n",
      "epoch:17 step:13991 [D loss: 0.021730, acc.: 100.00%] [G loss: 0.109478]\n",
      "epoch:17 step:13992 [D loss: 0.013935, acc.: 100.00%] [G loss: 0.012130]\n",
      "epoch:17 step:13993 [D loss: 0.007226, acc.: 100.00%] [G loss: 0.037968]\n",
      "epoch:17 step:13994 [D loss: 0.014876, acc.: 100.00%] [G loss: 0.037799]\n",
      "epoch:17 step:13995 [D loss: 0.005047, acc.: 100.00%] [G loss: 0.090213]\n",
      "epoch:17 step:13996 [D loss: 0.034930, acc.: 99.22%] [G loss: 0.284950]\n",
      "epoch:17 step:13997 [D loss: 0.011393, acc.: 100.00%] [G loss: 0.424494]\n",
      "epoch:17 step:13998 [D loss: 0.042926, acc.: 99.22%] [G loss: 0.090732]\n",
      "epoch:17 step:13999 [D loss: 0.012478, acc.: 100.00%] [G loss: 0.201777]\n",
      "epoch:17 step:14000 [D loss: 0.010289, acc.: 100.00%] [G loss: 0.357576]\n",
      "epoch:17 step:14001 [D loss: 0.049884, acc.: 100.00%] [G loss: 4.964831]\n",
      "epoch:17 step:14002 [D loss: 0.055961, acc.: 98.44%] [G loss: 3.498955]\n",
      "epoch:17 step:14003 [D loss: 0.005485, acc.: 100.00%] [G loss: 0.331208]\n",
      "epoch:17 step:14004 [D loss: 0.022541, acc.: 99.22%] [G loss: 0.108659]\n",
      "epoch:17 step:14005 [D loss: 0.013751, acc.: 100.00%] [G loss: 0.106392]\n",
      "epoch:17 step:14006 [D loss: 0.028842, acc.: 100.00%] [G loss: 3.252083]\n",
      "epoch:17 step:14007 [D loss: 0.007229, acc.: 100.00%] [G loss: 0.783933]\n",
      "epoch:17 step:14008 [D loss: 0.030823, acc.: 100.00%] [G loss: 0.405889]\n",
      "epoch:17 step:14009 [D loss: 0.241327, acc.: 88.28%] [G loss: 5.784721]\n",
      "epoch:17 step:14010 [D loss: 0.871258, acc.: 62.50%] [G loss: 1.239980]\n",
      "epoch:17 step:14011 [D loss: 0.086282, acc.: 96.88%] [G loss: 2.164578]\n",
      "epoch:17 step:14012 [D loss: 0.019553, acc.: 100.00%] [G loss: 3.089545]\n",
      "epoch:17 step:14013 [D loss: 0.049578, acc.: 98.44%] [G loss: 3.118812]\n",
      "epoch:17 step:14014 [D loss: 0.006633, acc.: 100.00%] [G loss: 2.465888]\n",
      "epoch:17 step:14015 [D loss: 0.048338, acc.: 98.44%] [G loss: 3.407799]\n",
      "epoch:17 step:14016 [D loss: 0.016076, acc.: 100.00%] [G loss: 3.537980]\n",
      "epoch:17 step:14017 [D loss: 0.034212, acc.: 100.00%] [G loss: 3.196319]\n",
      "epoch:17 step:14018 [D loss: 0.008097, acc.: 100.00%] [G loss: 2.303576]\n",
      "epoch:17 step:14019 [D loss: 0.048937, acc.: 99.22%] [G loss: 1.980060]\n",
      "epoch:17 step:14020 [D loss: 0.065795, acc.: 97.66%] [G loss: 5.115357]\n",
      "epoch:17 step:14021 [D loss: 0.017562, acc.: 100.00%] [G loss: 4.987360]\n",
      "epoch:17 step:14022 [D loss: 0.054326, acc.: 96.88%] [G loss: 4.441806]\n",
      "epoch:17 step:14023 [D loss: 0.012654, acc.: 100.00%] [G loss: 4.332415]\n",
      "epoch:17 step:14024 [D loss: 0.018278, acc.: 100.00%] [G loss: 4.800397]\n",
      "epoch:17 step:14025 [D loss: 0.010858, acc.: 100.00%] [G loss: 3.186829]\n",
      "epoch:17 step:14026 [D loss: 0.032100, acc.: 99.22%] [G loss: 0.899216]\n",
      "epoch:17 step:14027 [D loss: 0.034856, acc.: 100.00%] [G loss: 5.855469]\n",
      "epoch:17 step:14028 [D loss: 0.019280, acc.: 100.00%] [G loss: 0.083763]\n",
      "epoch:17 step:14029 [D loss: 0.103680, acc.: 96.09%] [G loss: 7.142777]\n",
      "epoch:17 step:14030 [D loss: 0.097117, acc.: 96.09%] [G loss: 0.183802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:14031 [D loss: 0.040602, acc.: 100.00%] [G loss: 5.694131]\n",
      "epoch:17 step:14032 [D loss: 0.003324, acc.: 100.00%] [G loss: 5.406212]\n",
      "epoch:17 step:14033 [D loss: 0.018939, acc.: 100.00%] [G loss: 5.941173]\n",
      "epoch:17 step:14034 [D loss: 0.018642, acc.: 100.00%] [G loss: 5.757215]\n",
      "epoch:17 step:14035 [D loss: 0.003284, acc.: 100.00%] [G loss: 6.018984]\n",
      "epoch:17 step:14036 [D loss: 0.006190, acc.: 100.00%] [G loss: 6.084552]\n",
      "epoch:17 step:14037 [D loss: 0.007706, acc.: 100.00%] [G loss: 0.157332]\n",
      "epoch:17 step:14038 [D loss: 0.004408, acc.: 100.00%] [G loss: 5.134947]\n",
      "epoch:17 step:14039 [D loss: 0.018316, acc.: 100.00%] [G loss: 4.969913]\n",
      "epoch:17 step:14040 [D loss: 0.015627, acc.: 100.00%] [G loss: 0.022209]\n",
      "epoch:17 step:14041 [D loss: 0.030479, acc.: 100.00%] [G loss: 5.259229]\n",
      "epoch:17 step:14042 [D loss: 0.005303, acc.: 100.00%] [G loss: 0.080951]\n",
      "epoch:17 step:14043 [D loss: 0.004580, acc.: 100.00%] [G loss: 4.970581]\n",
      "epoch:17 step:14044 [D loss: 0.014165, acc.: 100.00%] [G loss: 0.692953]\n",
      "epoch:17 step:14045 [D loss: 0.015372, acc.: 100.00%] [G loss: 4.299541]\n",
      "epoch:17 step:14046 [D loss: 0.008316, acc.: 100.00%] [G loss: 4.193131]\n",
      "epoch:17 step:14047 [D loss: 0.006721, acc.: 100.00%] [G loss: 0.236361]\n",
      "epoch:17 step:14048 [D loss: 0.031218, acc.: 100.00%] [G loss: 3.865350]\n",
      "epoch:17 step:14049 [D loss: 0.005502, acc.: 100.00%] [G loss: 4.003513]\n",
      "epoch:17 step:14050 [D loss: 0.009220, acc.: 100.00%] [G loss: 2.982683]\n",
      "epoch:17 step:14051 [D loss: 0.008484, acc.: 100.00%] [G loss: 0.046666]\n",
      "epoch:17 step:14052 [D loss: 0.006281, acc.: 100.00%] [G loss: 2.911782]\n",
      "epoch:17 step:14053 [D loss: 0.010558, acc.: 100.00%] [G loss: 1.641671]\n",
      "epoch:17 step:14054 [D loss: 0.014749, acc.: 100.00%] [G loss: 2.237218]\n",
      "epoch:17 step:14055 [D loss: 0.040287, acc.: 100.00%] [G loss: 3.132612]\n",
      "epoch:17 step:14056 [D loss: 0.229512, acc.: 90.62%] [G loss: 1.121631]\n",
      "epoch:17 step:14057 [D loss: 0.298105, acc.: 88.28%] [G loss: 4.476085]\n",
      "epoch:17 step:14058 [D loss: 0.002861, acc.: 100.00%] [G loss: 6.424319]\n",
      "epoch:18 step:14059 [D loss: 0.194246, acc.: 91.41%] [G loss: 3.950505]\n",
      "epoch:18 step:14060 [D loss: 0.015048, acc.: 99.22%] [G loss: 2.095553]\n",
      "epoch:18 step:14061 [D loss: 0.010269, acc.: 100.00%] [G loss: 1.775812]\n",
      "epoch:18 step:14062 [D loss: 0.068079, acc.: 99.22%] [G loss: 2.764009]\n",
      "epoch:18 step:14063 [D loss: 0.005405, acc.: 100.00%] [G loss: 3.479176]\n",
      "epoch:18 step:14064 [D loss: 0.024954, acc.: 99.22%] [G loss: 5.829644]\n",
      "epoch:18 step:14065 [D loss: 0.029636, acc.: 98.44%] [G loss: 2.369249]\n",
      "epoch:18 step:14066 [D loss: 0.004821, acc.: 100.00%] [G loss: 2.405341]\n",
      "epoch:18 step:14067 [D loss: 0.020318, acc.: 100.00%] [G loss: 2.972874]\n",
      "epoch:18 step:14068 [D loss: 0.015817, acc.: 100.00%] [G loss: 0.490616]\n",
      "epoch:18 step:14069 [D loss: 0.043738, acc.: 99.22%] [G loss: 3.530182]\n",
      "epoch:18 step:14070 [D loss: 0.463997, acc.: 77.34%] [G loss: 0.584322]\n",
      "epoch:18 step:14071 [D loss: 0.002063, acc.: 100.00%] [G loss: 3.858731]\n",
      "epoch:18 step:14072 [D loss: 0.254835, acc.: 87.50%] [G loss: 2.307192]\n",
      "epoch:18 step:14073 [D loss: 0.004821, acc.: 100.00%] [G loss: 1.173208]\n",
      "epoch:18 step:14074 [D loss: 0.085240, acc.: 96.88%] [G loss: 6.176986]\n",
      "epoch:18 step:14075 [D loss: 0.005797, acc.: 100.00%] [G loss: 2.184943]\n",
      "epoch:18 step:14076 [D loss: 0.004788, acc.: 100.00%] [G loss: 4.118762]\n",
      "epoch:18 step:14077 [D loss: 0.007054, acc.: 100.00%] [G loss: 2.128160]\n",
      "epoch:18 step:14078 [D loss: 0.030956, acc.: 99.22%] [G loss: 0.765148]\n",
      "epoch:18 step:14079 [D loss: 0.003989, acc.: 100.00%] [G loss: 0.940984]\n",
      "epoch:18 step:14080 [D loss: 0.005520, acc.: 100.00%] [G loss: 0.293761]\n",
      "epoch:18 step:14081 [D loss: 0.013115, acc.: 100.00%] [G loss: 0.385114]\n",
      "epoch:18 step:14082 [D loss: 0.003579, acc.: 100.00%] [G loss: 2.312320]\n",
      "epoch:18 step:14083 [D loss: 0.035403, acc.: 100.00%] [G loss: 0.206319]\n",
      "epoch:18 step:14084 [D loss: 0.081938, acc.: 96.09%] [G loss: 0.009490]\n",
      "epoch:18 step:14085 [D loss: 0.005380, acc.: 100.00%] [G loss: 0.063763]\n",
      "epoch:18 step:14086 [D loss: 0.083072, acc.: 97.66%] [G loss: 0.042112]\n",
      "epoch:18 step:14087 [D loss: 0.001206, acc.: 100.00%] [G loss: 1.866218]\n",
      "epoch:18 step:14088 [D loss: 0.031185, acc.: 99.22%] [G loss: 3.066850]\n",
      "epoch:18 step:14089 [D loss: 0.002173, acc.: 100.00%] [G loss: 0.092250]\n",
      "epoch:18 step:14090 [D loss: 0.004422, acc.: 100.00%] [G loss: 0.649451]\n",
      "epoch:18 step:14091 [D loss: 0.016738, acc.: 100.00%] [G loss: 0.102485]\n",
      "epoch:18 step:14092 [D loss: 0.006299, acc.: 100.00%] [G loss: 0.078080]\n",
      "epoch:18 step:14093 [D loss: 0.015613, acc.: 100.00%] [G loss: 0.268079]\n",
      "epoch:18 step:14094 [D loss: 0.009179, acc.: 100.00%] [G loss: 0.114538]\n",
      "epoch:18 step:14095 [D loss: 0.005174, acc.: 100.00%] [G loss: 0.201981]\n",
      "epoch:18 step:14096 [D loss: 0.004224, acc.: 100.00%] [G loss: 0.129361]\n",
      "epoch:18 step:14097 [D loss: 0.003234, acc.: 100.00%] [G loss: 0.077597]\n",
      "epoch:18 step:14098 [D loss: 0.097710, acc.: 96.09%] [G loss: 0.209497]\n",
      "epoch:18 step:14099 [D loss: 0.004300, acc.: 100.00%] [G loss: 1.654575]\n",
      "epoch:18 step:14100 [D loss: 0.018034, acc.: 100.00%] [G loss: 0.733504]\n",
      "epoch:18 step:14101 [D loss: 0.006045, acc.: 100.00%] [G loss: 0.270816]\n",
      "epoch:18 step:14102 [D loss: 0.135766, acc.: 94.53%] [G loss: 5.631361]\n",
      "epoch:18 step:14103 [D loss: 0.286803, acc.: 89.84%] [G loss: 1.851752]\n",
      "epoch:18 step:14104 [D loss: 0.441552, acc.: 79.69%] [G loss: 9.640532]\n",
      "epoch:18 step:14105 [D loss: 0.405830, acc.: 84.38%] [G loss: 8.558874]\n",
      "epoch:18 step:14106 [D loss: 0.003934, acc.: 100.00%] [G loss: 7.433333]\n",
      "epoch:18 step:14107 [D loss: 0.020796, acc.: 100.00%] [G loss: 7.358292]\n",
      "epoch:18 step:14108 [D loss: 0.008468, acc.: 100.00%] [G loss: 6.655664]\n",
      "epoch:18 step:14109 [D loss: 0.011412, acc.: 100.00%] [G loss: 0.440746]\n",
      "epoch:18 step:14110 [D loss: 0.002994, acc.: 100.00%] [G loss: 6.882324]\n",
      "epoch:18 step:14111 [D loss: 0.018532, acc.: 100.00%] [G loss: 6.416716]\n",
      "epoch:18 step:14112 [D loss: 0.003336, acc.: 100.00%] [G loss: 6.593367]\n",
      "epoch:18 step:14113 [D loss: 0.000859, acc.: 100.00%] [G loss: 6.545938]\n",
      "epoch:18 step:14114 [D loss: 0.002176, acc.: 100.00%] [G loss: 6.201942]\n",
      "epoch:18 step:14115 [D loss: 0.037858, acc.: 98.44%] [G loss: 0.041239]\n",
      "epoch:18 step:14116 [D loss: 0.025564, acc.: 100.00%] [G loss: 0.005054]\n",
      "epoch:18 step:14117 [D loss: 0.000701, acc.: 100.00%] [G loss: 4.692107]\n",
      "epoch:18 step:14118 [D loss: 0.001159, acc.: 100.00%] [G loss: 5.532324]\n",
      "epoch:18 step:14119 [D loss: 0.082581, acc.: 98.44%] [G loss: 5.204412]\n",
      "epoch:18 step:14120 [D loss: 0.001304, acc.: 100.00%] [G loss: 0.287185]\n",
      "epoch:18 step:14121 [D loss: 0.107444, acc.: 96.09%] [G loss: 2.284313]\n",
      "epoch:18 step:14122 [D loss: 0.155880, acc.: 90.62%] [G loss: 4.438485]\n",
      "epoch:18 step:14123 [D loss: 0.008019, acc.: 100.00%] [G loss: 5.580967]\n",
      "epoch:18 step:14124 [D loss: 0.016191, acc.: 99.22%] [G loss: 4.578101]\n",
      "epoch:18 step:14125 [D loss: 0.125920, acc.: 94.53%] [G loss: 0.657755]\n",
      "epoch:18 step:14126 [D loss: 0.001177, acc.: 100.00%] [G loss: 0.501835]\n",
      "epoch:18 step:14127 [D loss: 0.018220, acc.: 100.00%] [G loss: 1.390073]\n",
      "epoch:18 step:14128 [D loss: 0.053660, acc.: 97.66%] [G loss: 3.517012]\n",
      "epoch:18 step:14129 [D loss: 0.009878, acc.: 100.00%] [G loss: 7.657722]\n",
      "epoch:18 step:14130 [D loss: 0.055793, acc.: 98.44%] [G loss: 3.015321]\n",
      "epoch:18 step:14131 [D loss: 0.012095, acc.: 100.00%] [G loss: 5.129614]\n",
      "epoch:18 step:14132 [D loss: 0.053691, acc.: 99.22%] [G loss: 3.840489]\n",
      "epoch:18 step:14133 [D loss: 0.006306, acc.: 100.00%] [G loss: 6.491249]\n",
      "epoch:18 step:14134 [D loss: 0.113029, acc.: 96.09%] [G loss: 7.165386]\n",
      "epoch:18 step:14135 [D loss: 0.433859, acc.: 78.91%] [G loss: 7.974369]\n",
      "epoch:18 step:14136 [D loss: 0.001185, acc.: 100.00%] [G loss: 5.803844]\n",
      "epoch:18 step:14137 [D loss: 0.454947, acc.: 79.69%] [G loss: 3.377196]\n",
      "epoch:18 step:14138 [D loss: 1.258747, acc.: 62.50%] [G loss: 9.702652]\n",
      "epoch:18 step:14139 [D loss: 2.071017, acc.: 51.56%] [G loss: 4.092745]\n",
      "epoch:18 step:14140 [D loss: 0.025981, acc.: 99.22%] [G loss: 2.487031]\n",
      "epoch:18 step:14141 [D loss: 0.085743, acc.: 98.44%] [G loss: 1.272436]\n",
      "epoch:18 step:14142 [D loss: 0.149836, acc.: 94.53%] [G loss: 3.267853]\n",
      "epoch:18 step:14143 [D loss: 0.113396, acc.: 97.66%] [G loss: 2.634372]\n",
      "epoch:18 step:14144 [D loss: 0.091196, acc.: 96.88%] [G loss: 1.235879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14145 [D loss: 0.007584, acc.: 100.00%] [G loss: 1.529474]\n",
      "epoch:18 step:14146 [D loss: 0.090036, acc.: 97.66%] [G loss: 0.026251]\n",
      "epoch:18 step:14147 [D loss: 0.517502, acc.: 74.22%] [G loss: 2.929674]\n",
      "epoch:18 step:14148 [D loss: 1.615543, acc.: 52.34%] [G loss: 0.146420]\n",
      "epoch:18 step:14149 [D loss: 0.126717, acc.: 96.88%] [G loss: 3.679372]\n",
      "epoch:18 step:14150 [D loss: 0.002985, acc.: 100.00%] [G loss: 0.030909]\n",
      "epoch:18 step:14151 [D loss: 0.007357, acc.: 100.00%] [G loss: 0.043322]\n",
      "epoch:18 step:14152 [D loss: 0.048065, acc.: 99.22%] [G loss: 2.272055]\n",
      "epoch:18 step:14153 [D loss: 0.015308, acc.: 100.00%] [G loss: 0.923122]\n",
      "epoch:18 step:14154 [D loss: 0.028869, acc.: 100.00%] [G loss: 0.076241]\n",
      "epoch:18 step:14155 [D loss: 0.031193, acc.: 100.00%] [G loss: 1.936427]\n",
      "epoch:18 step:14156 [D loss: 0.094631, acc.: 97.66%] [G loss: 0.150849]\n",
      "epoch:18 step:14157 [D loss: 0.168735, acc.: 93.75%] [G loss: 0.032911]\n",
      "epoch:18 step:14158 [D loss: 0.014344, acc.: 100.00%] [G loss: 0.013411]\n",
      "epoch:18 step:14159 [D loss: 0.046321, acc.: 99.22%] [G loss: 0.012567]\n",
      "epoch:18 step:14160 [D loss: 0.078286, acc.: 99.22%] [G loss: 2.942338]\n",
      "epoch:18 step:14161 [D loss: 0.051956, acc.: 98.44%] [G loss: 0.110357]\n",
      "epoch:18 step:14162 [D loss: 0.085951, acc.: 98.44%] [G loss: 2.386125]\n",
      "epoch:18 step:14163 [D loss: 0.433441, acc.: 78.91%] [G loss: 5.962702]\n",
      "epoch:18 step:14164 [D loss: 0.204487, acc.: 89.84%] [G loss: 2.396403]\n",
      "epoch:18 step:14165 [D loss: 0.242518, acc.: 89.84%] [G loss: 0.039212]\n",
      "epoch:18 step:14166 [D loss: 0.127769, acc.: 94.53%] [G loss: 3.998459]\n",
      "epoch:18 step:14167 [D loss: 0.003763, acc.: 100.00%] [G loss: 0.447614]\n",
      "epoch:18 step:14168 [D loss: 0.088359, acc.: 95.31%] [G loss: 2.724660]\n",
      "epoch:18 step:14169 [D loss: 0.103723, acc.: 96.88%] [G loss: 0.454123]\n",
      "epoch:18 step:14170 [D loss: 0.001230, acc.: 100.00%] [G loss: 4.974443]\n",
      "epoch:18 step:14171 [D loss: 0.012887, acc.: 100.00%] [G loss: 1.301591]\n",
      "epoch:18 step:14172 [D loss: 0.091257, acc.: 96.88%] [G loss: 0.143205]\n",
      "epoch:18 step:14173 [D loss: 0.008328, acc.: 100.00%] [G loss: 2.390625]\n",
      "epoch:18 step:14174 [D loss: 0.075634, acc.: 96.88%] [G loss: 2.894435]\n",
      "epoch:18 step:14175 [D loss: 0.013766, acc.: 100.00%] [G loss: 1.054845]\n",
      "epoch:18 step:14176 [D loss: 0.019685, acc.: 100.00%] [G loss: 0.955835]\n",
      "epoch:18 step:14177 [D loss: 0.037663, acc.: 100.00%] [G loss: 0.482545]\n",
      "epoch:18 step:14178 [D loss: 0.015733, acc.: 100.00%] [G loss: 0.776024]\n",
      "epoch:18 step:14179 [D loss: 0.074918, acc.: 98.44%] [G loss: 4.184901]\n",
      "epoch:18 step:14180 [D loss: 0.021564, acc.: 100.00%] [G loss: 2.307020]\n",
      "epoch:18 step:14181 [D loss: 0.179148, acc.: 95.31%] [G loss: 2.869938]\n",
      "epoch:18 step:14182 [D loss: 0.004797, acc.: 100.00%] [G loss: 1.039329]\n",
      "epoch:18 step:14183 [D loss: 0.029919, acc.: 99.22%] [G loss: 0.811528]\n",
      "epoch:18 step:14184 [D loss: 0.020271, acc.: 100.00%] [G loss: 2.131522]\n",
      "epoch:18 step:14185 [D loss: 0.016722, acc.: 100.00%] [G loss: 0.858373]\n",
      "epoch:18 step:14186 [D loss: 0.035523, acc.: 100.00%] [G loss: 1.723884]\n",
      "epoch:18 step:14187 [D loss: 0.049006, acc.: 99.22%] [G loss: 0.516473]\n",
      "epoch:18 step:14188 [D loss: 0.082325, acc.: 98.44%] [G loss: 4.156004]\n",
      "epoch:18 step:14189 [D loss: 0.004663, acc.: 100.00%] [G loss: 3.878413]\n",
      "epoch:18 step:14190 [D loss: 0.086155, acc.: 96.88%] [G loss: 0.823686]\n",
      "epoch:18 step:14191 [D loss: 0.038545, acc.: 100.00%] [G loss: 0.707251]\n",
      "epoch:18 step:14192 [D loss: 0.014934, acc.: 100.00%] [G loss: 0.620499]\n",
      "epoch:18 step:14193 [D loss: 0.150278, acc.: 96.88%] [G loss: 5.082047]\n",
      "epoch:18 step:14194 [D loss: 0.017279, acc.: 98.44%] [G loss: 0.859244]\n",
      "epoch:18 step:14195 [D loss: 0.052465, acc.: 98.44%] [G loss: 0.701751]\n",
      "epoch:18 step:14196 [D loss: 0.004596, acc.: 100.00%] [G loss: 0.096596]\n",
      "epoch:18 step:14197 [D loss: 0.021189, acc.: 100.00%] [G loss: 0.273449]\n",
      "epoch:18 step:14198 [D loss: 0.043529, acc.: 100.00%] [G loss: 0.540472]\n",
      "epoch:18 step:14199 [D loss: 0.039519, acc.: 100.00%] [G loss: 4.629053]\n",
      "epoch:18 step:14200 [D loss: 0.006745, acc.: 100.00%] [G loss: 0.150490]\n",
      "epoch:18 step:14201 [D loss: 0.006266, acc.: 100.00%] [G loss: 0.040554]\n",
      "epoch:18 step:14202 [D loss: 0.002419, acc.: 100.00%] [G loss: 0.051221]\n",
      "epoch:18 step:14203 [D loss: 0.002024, acc.: 100.00%] [G loss: 0.142242]\n",
      "epoch:18 step:14204 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.048906]\n",
      "epoch:18 step:14205 [D loss: 0.002384, acc.: 100.00%] [G loss: 1.701011]\n",
      "epoch:18 step:14206 [D loss: 0.001685, acc.: 100.00%] [G loss: 0.020403]\n",
      "epoch:18 step:14207 [D loss: 0.004483, acc.: 100.00%] [G loss: 0.091760]\n",
      "epoch:18 step:14208 [D loss: 0.002343, acc.: 100.00%] [G loss: 0.023663]\n",
      "epoch:18 step:14209 [D loss: 0.004200, acc.: 100.00%] [G loss: 0.033245]\n",
      "epoch:18 step:14210 [D loss: 0.004566, acc.: 100.00%] [G loss: 0.009275]\n",
      "epoch:18 step:14211 [D loss: 0.011674, acc.: 100.00%] [G loss: 0.014182]\n",
      "epoch:18 step:14212 [D loss: 0.003433, acc.: 100.00%] [G loss: 0.021298]\n",
      "epoch:18 step:14213 [D loss: 0.003213, acc.: 100.00%] [G loss: 0.039590]\n",
      "epoch:18 step:14214 [D loss: 0.014043, acc.: 100.00%] [G loss: 0.055324]\n",
      "epoch:18 step:14215 [D loss: 0.015477, acc.: 100.00%] [G loss: 0.105082]\n",
      "epoch:18 step:14216 [D loss: 0.037466, acc.: 100.00%] [G loss: 0.687546]\n",
      "epoch:18 step:14217 [D loss: 0.002812, acc.: 100.00%] [G loss: 6.046797]\n",
      "epoch:18 step:14218 [D loss: 0.063848, acc.: 98.44%] [G loss: 0.573133]\n",
      "epoch:18 step:14219 [D loss: 0.027248, acc.: 100.00%] [G loss: 1.082234]\n",
      "epoch:18 step:14220 [D loss: 0.016528, acc.: 100.00%] [G loss: 2.581985]\n",
      "epoch:18 step:14221 [D loss: 0.034968, acc.: 99.22%] [G loss: 0.912214]\n",
      "epoch:18 step:14222 [D loss: 0.003074, acc.: 100.00%] [G loss: 0.698132]\n",
      "epoch:18 step:14223 [D loss: 0.004750, acc.: 100.00%] [G loss: 0.782447]\n",
      "epoch:18 step:14224 [D loss: 0.009062, acc.: 100.00%] [G loss: 0.609540]\n",
      "epoch:18 step:14225 [D loss: 0.001274, acc.: 100.00%] [G loss: 0.804622]\n",
      "epoch:18 step:14226 [D loss: 0.007322, acc.: 100.00%] [G loss: 0.082937]\n",
      "epoch:18 step:14227 [D loss: 0.003248, acc.: 100.00%] [G loss: 0.677274]\n",
      "epoch:18 step:14228 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.377059]\n",
      "epoch:18 step:14229 [D loss: 0.001753, acc.: 100.00%] [G loss: 0.229835]\n",
      "epoch:18 step:14230 [D loss: 0.003445, acc.: 100.00%] [G loss: 3.647956]\n",
      "epoch:18 step:14231 [D loss: 0.003351, acc.: 100.00%] [G loss: 0.135559]\n",
      "epoch:18 step:14232 [D loss: 0.033478, acc.: 99.22%] [G loss: 0.558634]\n",
      "epoch:18 step:14233 [D loss: 0.003438, acc.: 100.00%] [G loss: 0.574492]\n",
      "epoch:18 step:14234 [D loss: 4.832572, acc.: 24.22%] [G loss: 7.983588]\n",
      "epoch:18 step:14235 [D loss: 2.782975, acc.: 50.00%] [G loss: 4.559814]\n",
      "epoch:18 step:14236 [D loss: 1.337251, acc.: 51.56%] [G loss: 1.806223]\n",
      "epoch:18 step:14237 [D loss: 0.111845, acc.: 97.66%] [G loss: 0.628965]\n",
      "epoch:18 step:14238 [D loss: 0.085498, acc.: 99.22%] [G loss: 0.434188]\n",
      "epoch:18 step:14239 [D loss: 0.090834, acc.: 97.66%] [G loss: 0.573427]\n",
      "epoch:18 step:14240 [D loss: 0.066475, acc.: 100.00%] [G loss: 1.114843]\n",
      "epoch:18 step:14241 [D loss: 0.041355, acc.: 100.00%] [G loss: 0.562682]\n",
      "epoch:18 step:14242 [D loss: 0.055276, acc.: 100.00%] [G loss: 0.403330]\n",
      "epoch:18 step:14243 [D loss: 0.035412, acc.: 100.00%] [G loss: 0.487165]\n",
      "epoch:18 step:14244 [D loss: 0.081676, acc.: 100.00%] [G loss: 0.285222]\n",
      "epoch:18 step:14245 [D loss: 0.036115, acc.: 100.00%] [G loss: 0.204014]\n",
      "epoch:18 step:14246 [D loss: 0.028944, acc.: 100.00%] [G loss: 0.111416]\n",
      "epoch:18 step:14247 [D loss: 0.060811, acc.: 99.22%] [G loss: 0.122226]\n",
      "epoch:18 step:14248 [D loss: 0.118568, acc.: 96.88%] [G loss: 3.448690]\n",
      "epoch:18 step:14249 [D loss: 0.054503, acc.: 100.00%] [G loss: 0.309700]\n",
      "epoch:18 step:14250 [D loss: 0.077505, acc.: 99.22%] [G loss: 0.226330]\n",
      "epoch:18 step:14251 [D loss: 0.244810, acc.: 95.31%] [G loss: 1.145016]\n",
      "epoch:18 step:14252 [D loss: 0.194935, acc.: 94.53%] [G loss: 1.528684]\n",
      "epoch:18 step:14253 [D loss: 0.082005, acc.: 96.88%] [G loss: 0.499556]\n",
      "epoch:18 step:14254 [D loss: 0.202166, acc.: 92.19%] [G loss: 0.140002]\n",
      "epoch:18 step:14255 [D loss: 0.132526, acc.: 96.88%] [G loss: 1.431641]\n",
      "epoch:18 step:14256 [D loss: 0.068765, acc.: 99.22%] [G loss: 0.324680]\n",
      "epoch:18 step:14257 [D loss: 0.080698, acc.: 99.22%] [G loss: 0.264453]\n",
      "epoch:18 step:14258 [D loss: 0.273922, acc.: 89.06%] [G loss: 0.591180]\n",
      "epoch:18 step:14259 [D loss: 0.082112, acc.: 97.66%] [G loss: 0.571518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14260 [D loss: 0.209455, acc.: 93.75%] [G loss: 0.802399]\n",
      "epoch:18 step:14261 [D loss: 0.052047, acc.: 98.44%] [G loss: 1.210698]\n",
      "epoch:18 step:14262 [D loss: 0.104308, acc.: 96.88%] [G loss: 0.425030]\n",
      "epoch:18 step:14263 [D loss: 0.464835, acc.: 80.47%] [G loss: 3.526002]\n",
      "epoch:18 step:14264 [D loss: 0.574238, acc.: 73.44%] [G loss: 3.113112]\n",
      "epoch:18 step:14265 [D loss: 0.151643, acc.: 94.53%] [G loss: 2.605185]\n",
      "epoch:18 step:14266 [D loss: 0.113220, acc.: 96.88%] [G loss: 3.243992]\n",
      "epoch:18 step:14267 [D loss: 0.137601, acc.: 96.88%] [G loss: 3.313406]\n",
      "epoch:18 step:14268 [D loss: 0.063449, acc.: 97.66%] [G loss: 3.639830]\n",
      "epoch:18 step:14269 [D loss: 0.070858, acc.: 99.22%] [G loss: 4.003172]\n",
      "epoch:18 step:14270 [D loss: 0.041627, acc.: 99.22%] [G loss: 1.869634]\n",
      "epoch:18 step:14271 [D loss: 0.095819, acc.: 97.66%] [G loss: 0.433332]\n",
      "epoch:18 step:14272 [D loss: 0.055890, acc.: 97.66%] [G loss: 3.310251]\n",
      "epoch:18 step:14273 [D loss: 0.039121, acc.: 99.22%] [G loss: 4.563333]\n",
      "epoch:18 step:14274 [D loss: 0.023085, acc.: 100.00%] [G loss: 4.239860]\n",
      "epoch:18 step:14275 [D loss: 0.054410, acc.: 98.44%] [G loss: 4.283151]\n",
      "epoch:18 step:14276 [D loss: 0.036575, acc.: 99.22%] [G loss: 3.225433]\n",
      "epoch:18 step:14277 [D loss: 0.061099, acc.: 99.22%] [G loss: 0.156920]\n",
      "epoch:18 step:14278 [D loss: 0.151163, acc.: 93.75%] [G loss: 4.371193]\n",
      "epoch:18 step:14279 [D loss: 0.011609, acc.: 100.00%] [G loss: 0.892281]\n",
      "epoch:18 step:14280 [D loss: 0.113650, acc.: 97.66%] [G loss: 0.044311]\n",
      "epoch:18 step:14281 [D loss: 0.874051, acc.: 62.50%] [G loss: 7.850024]\n",
      "epoch:18 step:14282 [D loss: 1.538147, acc.: 53.91%] [G loss: 4.860690]\n",
      "epoch:18 step:14283 [D loss: 0.472129, acc.: 75.00%] [G loss: 1.736996]\n",
      "epoch:18 step:14284 [D loss: 0.019396, acc.: 100.00%] [G loss: 0.751160]\n",
      "epoch:18 step:14285 [D loss: 0.096356, acc.: 97.66%] [G loss: 0.674379]\n",
      "epoch:18 step:14286 [D loss: 0.014765, acc.: 100.00%] [G loss: 1.141240]\n",
      "epoch:18 step:14287 [D loss: 0.039714, acc.: 99.22%] [G loss: 1.525551]\n",
      "epoch:18 step:14288 [D loss: 0.013976, acc.: 100.00%] [G loss: 1.019348]\n",
      "epoch:18 step:14289 [D loss: 0.043568, acc.: 100.00%] [G loss: 0.851967]\n",
      "epoch:18 step:14290 [D loss: 0.041247, acc.: 100.00%] [G loss: 0.530812]\n",
      "epoch:18 step:14291 [D loss: 0.014735, acc.: 100.00%] [G loss: 0.605145]\n",
      "epoch:18 step:14292 [D loss: 0.065170, acc.: 100.00%] [G loss: 0.278139]\n",
      "epoch:18 step:14293 [D loss: 0.072686, acc.: 99.22%] [G loss: 2.734796]\n",
      "epoch:18 step:14294 [D loss: 0.101077, acc.: 97.66%] [G loss: 1.059959]\n",
      "epoch:18 step:14295 [D loss: 0.102564, acc.: 99.22%] [G loss: 0.974012]\n",
      "epoch:18 step:14296 [D loss: 0.045497, acc.: 99.22%] [G loss: 1.473350]\n",
      "epoch:18 step:14297 [D loss: 0.135151, acc.: 96.09%] [G loss: 2.528405]\n",
      "epoch:18 step:14298 [D loss: 0.700778, acc.: 65.62%] [G loss: 2.562458]\n",
      "epoch:18 step:14299 [D loss: 0.093789, acc.: 98.44%] [G loss: 3.128956]\n",
      "epoch:18 step:14300 [D loss: 0.032597, acc.: 99.22%] [G loss: 3.079944]\n",
      "epoch:18 step:14301 [D loss: 0.064344, acc.: 97.66%] [G loss: 0.846881]\n",
      "epoch:18 step:14302 [D loss: 0.074211, acc.: 99.22%] [G loss: 2.956619]\n",
      "epoch:18 step:14303 [D loss: 0.008414, acc.: 100.00%] [G loss: 3.263776]\n",
      "epoch:18 step:14304 [D loss: 0.072187, acc.: 98.44%] [G loss: 2.318390]\n",
      "epoch:18 step:14305 [D loss: 0.055260, acc.: 99.22%] [G loss: 2.234878]\n",
      "epoch:18 step:14306 [D loss: 0.038536, acc.: 100.00%] [G loss: 2.459894]\n",
      "epoch:18 step:14307 [D loss: 0.069528, acc.: 98.44%] [G loss: 2.467865]\n",
      "epoch:18 step:14308 [D loss: 0.129650, acc.: 96.88%] [G loss: 1.799527]\n",
      "epoch:18 step:14309 [D loss: 0.021313, acc.: 99.22%] [G loss: 2.836714]\n",
      "epoch:18 step:14310 [D loss: 0.044678, acc.: 100.00%] [G loss: 0.668993]\n",
      "epoch:18 step:14311 [D loss: 0.050877, acc.: 99.22%] [G loss: 2.359656]\n",
      "epoch:18 step:14312 [D loss: 0.092087, acc.: 98.44%] [G loss: 3.464593]\n",
      "epoch:18 step:14313 [D loss: 0.070488, acc.: 97.66%] [G loss: 2.500709]\n",
      "epoch:18 step:14314 [D loss: 0.017164, acc.: 100.00%] [G loss: 1.360537]\n",
      "epoch:18 step:14315 [D loss: 0.042877, acc.: 100.00%] [G loss: 1.248653]\n",
      "epoch:18 step:14316 [D loss: 0.036891, acc.: 100.00%] [G loss: 1.669445]\n",
      "epoch:18 step:14317 [D loss: 0.052020, acc.: 100.00%] [G loss: 0.584690]\n",
      "epoch:18 step:14318 [D loss: 0.013948, acc.: 100.00%] [G loss: 2.620181]\n",
      "epoch:18 step:14319 [D loss: 0.068530, acc.: 98.44%] [G loss: 0.266040]\n",
      "epoch:18 step:14320 [D loss: 0.062926, acc.: 98.44%] [G loss: 1.744179]\n",
      "epoch:18 step:14321 [D loss: 0.055967, acc.: 98.44%] [G loss: 0.435360]\n",
      "epoch:18 step:14322 [D loss: 0.029537, acc.: 100.00%] [G loss: 1.844279]\n",
      "epoch:18 step:14323 [D loss: 0.015672, acc.: 100.00%] [G loss: 0.288512]\n",
      "epoch:18 step:14324 [D loss: 0.027701, acc.: 100.00%] [G loss: 1.786991]\n",
      "epoch:18 step:14325 [D loss: 0.971989, acc.: 50.00%] [G loss: 7.457923]\n",
      "epoch:18 step:14326 [D loss: 0.552348, acc.: 72.66%] [G loss: 1.844634]\n",
      "epoch:18 step:14327 [D loss: 0.045770, acc.: 98.44%] [G loss: 1.281641]\n",
      "epoch:18 step:14328 [D loss: 0.022042, acc.: 100.00%] [G loss: 0.268958]\n",
      "epoch:18 step:14329 [D loss: 0.007692, acc.: 100.00%] [G loss: 0.234370]\n",
      "epoch:18 step:14330 [D loss: 0.026530, acc.: 100.00%] [G loss: 2.438726]\n",
      "epoch:18 step:14331 [D loss: 0.041663, acc.: 100.00%] [G loss: 0.229621]\n",
      "epoch:18 step:14332 [D loss: 0.016481, acc.: 100.00%] [G loss: 0.464327]\n",
      "epoch:18 step:14333 [D loss: 0.016103, acc.: 100.00%] [G loss: 0.201730]\n",
      "epoch:18 step:14334 [D loss: 0.019598, acc.: 99.22%] [G loss: 0.276014]\n",
      "epoch:18 step:14335 [D loss: 0.050766, acc.: 100.00%] [G loss: 0.321509]\n",
      "epoch:18 step:14336 [D loss: 0.036870, acc.: 100.00%] [G loss: 0.494090]\n",
      "epoch:18 step:14337 [D loss: 0.021847, acc.: 100.00%] [G loss: 1.076165]\n",
      "epoch:18 step:14338 [D loss: 0.025287, acc.: 99.22%] [G loss: 1.329599]\n",
      "epoch:18 step:14339 [D loss: 0.055944, acc.: 99.22%] [G loss: 1.229292]\n",
      "epoch:18 step:14340 [D loss: 0.022476, acc.: 100.00%] [G loss: 1.064130]\n",
      "epoch:18 step:14341 [D loss: 0.051911, acc.: 98.44%] [G loss: 4.317262]\n",
      "epoch:18 step:14342 [D loss: 0.012168, acc.: 100.00%] [G loss: 4.219031]\n",
      "epoch:18 step:14343 [D loss: 0.031397, acc.: 99.22%] [G loss: 3.260998]\n",
      "epoch:18 step:14344 [D loss: 0.033198, acc.: 98.44%] [G loss: 2.986003]\n",
      "epoch:18 step:14345 [D loss: 0.016629, acc.: 100.00%] [G loss: 2.622789]\n",
      "epoch:18 step:14346 [D loss: 0.008554, acc.: 100.00%] [G loss: 2.613371]\n",
      "epoch:18 step:14347 [D loss: 0.013474, acc.: 100.00%] [G loss: 3.012339]\n",
      "epoch:18 step:14348 [D loss: 0.010354, acc.: 100.00%] [G loss: 3.069930]\n",
      "epoch:18 step:14349 [D loss: 0.079543, acc.: 99.22%] [G loss: 3.024935]\n",
      "epoch:18 step:14350 [D loss: 0.029020, acc.: 100.00%] [G loss: 4.811651]\n",
      "epoch:18 step:14351 [D loss: 0.031545, acc.: 100.00%] [G loss: 4.795326]\n",
      "epoch:18 step:14352 [D loss: 0.021509, acc.: 100.00%] [G loss: 4.706387]\n",
      "epoch:18 step:14353 [D loss: 0.024292, acc.: 100.00%] [G loss: 4.550249]\n",
      "epoch:18 step:14354 [D loss: 0.020598, acc.: 100.00%] [G loss: 4.588303]\n",
      "epoch:18 step:14355 [D loss: 0.009800, acc.: 100.00%] [G loss: 4.595763]\n",
      "epoch:18 step:14356 [D loss: 0.012316, acc.: 100.00%] [G loss: 5.535537]\n",
      "epoch:18 step:14357 [D loss: 0.007348, acc.: 100.00%] [G loss: 2.258750]\n",
      "epoch:18 step:14358 [D loss: 0.013004, acc.: 100.00%] [G loss: 0.859749]\n",
      "epoch:18 step:14359 [D loss: 0.005354, acc.: 100.00%] [G loss: 4.035550]\n",
      "epoch:18 step:14360 [D loss: 0.012776, acc.: 100.00%] [G loss: 4.077548]\n",
      "epoch:18 step:14361 [D loss: 0.014590, acc.: 100.00%] [G loss: 4.171453]\n",
      "epoch:18 step:14362 [D loss: 0.013704, acc.: 100.00%] [G loss: 4.249679]\n",
      "epoch:18 step:14363 [D loss: 0.032722, acc.: 100.00%] [G loss: 4.696865]\n",
      "epoch:18 step:14364 [D loss: 0.060048, acc.: 100.00%] [G loss: 1.265933]\n",
      "epoch:18 step:14365 [D loss: 0.092831, acc.: 95.31%] [G loss: 3.502754]\n",
      "epoch:18 step:14366 [D loss: 0.124756, acc.: 95.31%] [G loss: 6.380360]\n",
      "epoch:18 step:14367 [D loss: 0.098634, acc.: 96.88%] [G loss: 5.463343]\n",
      "epoch:18 step:14368 [D loss: 0.086527, acc.: 97.66%] [G loss: 0.404453]\n",
      "epoch:18 step:14369 [D loss: 0.434279, acc.: 81.25%] [G loss: 4.427937]\n",
      "epoch:18 step:14370 [D loss: 1.255783, acc.: 57.81%] [G loss: 0.884413]\n",
      "epoch:18 step:14371 [D loss: 0.116900, acc.: 95.31%] [G loss: 0.413160]\n",
      "epoch:18 step:14372 [D loss: 0.006829, acc.: 100.00%] [G loss: 0.320062]\n",
      "epoch:18 step:14373 [D loss: 0.135094, acc.: 93.75%] [G loss: 0.787234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14374 [D loss: 0.002929, acc.: 100.00%] [G loss: 2.717532]\n",
      "epoch:18 step:14375 [D loss: 0.045502, acc.: 97.66%] [G loss: 0.609452]\n",
      "epoch:18 step:14376 [D loss: 0.011435, acc.: 100.00%] [G loss: 1.394321]\n",
      "epoch:18 step:14377 [D loss: 0.030685, acc.: 99.22%] [G loss: 0.159123]\n",
      "epoch:18 step:14378 [D loss: 0.005551, acc.: 100.00%] [G loss: 0.690202]\n",
      "epoch:18 step:14379 [D loss: 0.033031, acc.: 100.00%] [G loss: 0.572433]\n",
      "epoch:18 step:14380 [D loss: 0.019691, acc.: 99.22%] [G loss: 0.108452]\n",
      "epoch:18 step:14381 [D loss: 0.008831, acc.: 100.00%] [G loss: 0.318063]\n",
      "epoch:18 step:14382 [D loss: 0.003435, acc.: 100.00%] [G loss: 0.145498]\n",
      "epoch:18 step:14383 [D loss: 0.014390, acc.: 100.00%] [G loss: 0.093030]\n",
      "epoch:18 step:14384 [D loss: 0.004075, acc.: 100.00%] [G loss: 1.992374]\n",
      "epoch:18 step:14385 [D loss: 0.057266, acc.: 99.22%] [G loss: 0.171851]\n",
      "epoch:18 step:14386 [D loss: 0.038299, acc.: 100.00%] [G loss: 0.637096]\n",
      "epoch:18 step:14387 [D loss: 0.017337, acc.: 100.00%] [G loss: 0.551894]\n",
      "epoch:18 step:14388 [D loss: 0.073607, acc.: 100.00%] [G loss: 0.336883]\n",
      "epoch:18 step:14389 [D loss: 0.068633, acc.: 96.88%] [G loss: 0.970666]\n",
      "epoch:18 step:14390 [D loss: 0.022041, acc.: 99.22%] [G loss: 1.110742]\n",
      "epoch:18 step:14391 [D loss: 0.004420, acc.: 100.00%] [G loss: 1.238251]\n",
      "epoch:18 step:14392 [D loss: 0.004804, acc.: 100.00%] [G loss: 1.326599]\n",
      "epoch:18 step:14393 [D loss: 0.005003, acc.: 100.00%] [G loss: 0.170042]\n",
      "epoch:18 step:14394 [D loss: 0.015478, acc.: 100.00%] [G loss: 0.034552]\n",
      "epoch:18 step:14395 [D loss: 0.310349, acc.: 81.25%] [G loss: 4.858090]\n",
      "epoch:18 step:14396 [D loss: 0.314883, acc.: 84.38%] [G loss: 0.984989]\n",
      "epoch:18 step:14397 [D loss: 0.086443, acc.: 97.66%] [G loss: 0.111607]\n",
      "epoch:18 step:14398 [D loss: 1.228465, acc.: 60.94%] [G loss: 7.431758]\n",
      "epoch:18 step:14399 [D loss: 2.495916, acc.: 50.00%] [G loss: 6.731239]\n",
      "epoch:18 step:14400 [D loss: 0.937248, acc.: 59.38%] [G loss: 3.598880]\n",
      "epoch:18 step:14401 [D loss: 0.115964, acc.: 96.09%] [G loss: 1.843034]\n",
      "epoch:18 step:14402 [D loss: 0.056695, acc.: 99.22%] [G loss: 2.092014]\n",
      "epoch:18 step:14403 [D loss: 0.094946, acc.: 98.44%] [G loss: 0.255730]\n",
      "epoch:18 step:14404 [D loss: 0.043166, acc.: 99.22%] [G loss: 3.990436]\n",
      "epoch:18 step:14405 [D loss: 0.033514, acc.: 99.22%] [G loss: 3.719077]\n",
      "epoch:18 step:14406 [D loss: 0.026392, acc.: 100.00%] [G loss: 3.477428]\n",
      "epoch:18 step:14407 [D loss: 0.024211, acc.: 99.22%] [G loss: 3.564867]\n",
      "epoch:18 step:14408 [D loss: 0.065282, acc.: 99.22%] [G loss: 3.478462]\n",
      "epoch:18 step:14409 [D loss: 0.020624, acc.: 100.00%] [G loss: 4.020446]\n",
      "epoch:18 step:14410 [D loss: 0.022741, acc.: 100.00%] [G loss: 4.509775]\n",
      "epoch:18 step:14411 [D loss: 0.016847, acc.: 100.00%] [G loss: 4.143131]\n",
      "epoch:18 step:14412 [D loss: 0.029596, acc.: 100.00%] [G loss: 3.655488]\n",
      "epoch:18 step:14413 [D loss: 0.025339, acc.: 100.00%] [G loss: 4.046593]\n",
      "epoch:18 step:14414 [D loss: 0.049600, acc.: 100.00%] [G loss: 4.570536]\n",
      "epoch:18 step:14415 [D loss: 0.012082, acc.: 100.00%] [G loss: 4.593200]\n",
      "epoch:18 step:14416 [D loss: 0.045127, acc.: 99.22%] [G loss: 3.886567]\n",
      "epoch:18 step:14417 [D loss: 0.040436, acc.: 99.22%] [G loss: 0.345589]\n",
      "epoch:18 step:14418 [D loss: 0.013631, acc.: 100.00%] [G loss: 4.391822]\n",
      "epoch:18 step:14419 [D loss: 0.014486, acc.: 100.00%] [G loss: 4.599918]\n",
      "epoch:18 step:14420 [D loss: 0.012640, acc.: 100.00%] [G loss: 4.835585]\n",
      "epoch:18 step:14421 [D loss: 0.048998, acc.: 100.00%] [G loss: 4.184592]\n",
      "epoch:18 step:14422 [D loss: 0.018745, acc.: 100.00%] [G loss: 4.123147]\n",
      "epoch:18 step:14423 [D loss: 0.021324, acc.: 100.00%] [G loss: 4.732327]\n",
      "epoch:18 step:14424 [D loss: 0.018189, acc.: 100.00%] [G loss: 4.638907]\n",
      "epoch:18 step:14425 [D loss: 0.037587, acc.: 100.00%] [G loss: 4.672109]\n",
      "epoch:18 step:14426 [D loss: 0.003944, acc.: 100.00%] [G loss: 5.628949]\n",
      "epoch:18 step:14427 [D loss: 0.014513, acc.: 100.00%] [G loss: 0.100970]\n",
      "epoch:18 step:14428 [D loss: 0.042908, acc.: 99.22%] [G loss: 0.019305]\n",
      "epoch:18 step:14429 [D loss: 0.039814, acc.: 100.00%] [G loss: 5.260360]\n",
      "epoch:18 step:14430 [D loss: 0.046946, acc.: 100.00%] [G loss: 5.271124]\n",
      "epoch:18 step:14431 [D loss: 0.083099, acc.: 99.22%] [G loss: 5.175363]\n",
      "epoch:18 step:14432 [D loss: 0.011057, acc.: 100.00%] [G loss: 5.338774]\n",
      "epoch:18 step:14433 [D loss: 0.033818, acc.: 100.00%] [G loss: 4.619071]\n",
      "epoch:18 step:14434 [D loss: 0.016308, acc.: 100.00%] [G loss: 3.449711]\n",
      "epoch:18 step:14435 [D loss: 0.062937, acc.: 98.44%] [G loss: 0.025996]\n",
      "epoch:18 step:14436 [D loss: 0.007545, acc.: 100.00%] [G loss: 5.359918]\n",
      "epoch:18 step:14437 [D loss: 0.063224, acc.: 97.66%] [G loss: 4.089863]\n",
      "epoch:18 step:14438 [D loss: 0.041525, acc.: 99.22%] [G loss: 0.053286]\n",
      "epoch:18 step:14439 [D loss: 0.007140, acc.: 100.00%] [G loss: 0.378950]\n",
      "epoch:18 step:14440 [D loss: 0.048423, acc.: 100.00%] [G loss: 4.061293]\n",
      "epoch:18 step:14441 [D loss: 0.011097, acc.: 100.00%] [G loss: 5.189501]\n",
      "epoch:18 step:14442 [D loss: 0.007253, acc.: 100.00%] [G loss: 4.024014]\n",
      "epoch:18 step:14443 [D loss: 0.025466, acc.: 100.00%] [G loss: 2.740977]\n",
      "epoch:18 step:14444 [D loss: 0.035159, acc.: 98.44%] [G loss: 1.173214]\n",
      "epoch:18 step:14445 [D loss: 0.019084, acc.: 100.00%] [G loss: 1.970019]\n",
      "epoch:18 step:14446 [D loss: 0.013499, acc.: 100.00%] [G loss: 0.017585]\n",
      "epoch:18 step:14447 [D loss: 0.023376, acc.: 99.22%] [G loss: 1.171103]\n",
      "epoch:18 step:14448 [D loss: 0.006023, acc.: 100.00%] [G loss: 0.907230]\n",
      "epoch:18 step:14449 [D loss: 0.009183, acc.: 100.00%] [G loss: 0.543193]\n",
      "epoch:18 step:14450 [D loss: 0.009489, acc.: 100.00%] [G loss: 0.144394]\n",
      "epoch:18 step:14451 [D loss: 0.009337, acc.: 100.00%] [G loss: 0.483354]\n",
      "epoch:18 step:14452 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.250770]\n",
      "epoch:18 step:14453 [D loss: 0.046970, acc.: 99.22%] [G loss: 0.207949]\n",
      "epoch:18 step:14454 [D loss: 0.007413, acc.: 100.00%] [G loss: 1.027637]\n",
      "epoch:18 step:14455 [D loss: 0.004189, acc.: 100.00%] [G loss: 0.181871]\n",
      "epoch:18 step:14456 [D loss: 0.410457, acc.: 76.56%] [G loss: 6.937132]\n",
      "epoch:18 step:14457 [D loss: 2.284437, acc.: 50.00%] [G loss: 2.516553]\n",
      "epoch:18 step:14458 [D loss: 0.019568, acc.: 100.00%] [G loss: 0.939136]\n",
      "epoch:18 step:14459 [D loss: 0.018079, acc.: 100.00%] [G loss: 0.600232]\n",
      "epoch:18 step:14460 [D loss: 0.025268, acc.: 100.00%] [G loss: 1.378664]\n",
      "epoch:18 step:14461 [D loss: 0.071507, acc.: 100.00%] [G loss: 0.587381]\n",
      "epoch:18 step:14462 [D loss: 0.013612, acc.: 100.00%] [G loss: 1.195525]\n",
      "epoch:18 step:14463 [D loss: 0.028214, acc.: 99.22%] [G loss: 1.274709]\n",
      "epoch:18 step:14464 [D loss: 0.011344, acc.: 100.00%] [G loss: 0.425375]\n",
      "epoch:18 step:14465 [D loss: 0.005796, acc.: 100.00%] [G loss: 0.740617]\n",
      "epoch:18 step:14466 [D loss: 0.006182, acc.: 100.00%] [G loss: 0.509001]\n",
      "epoch:18 step:14467 [D loss: 0.011811, acc.: 100.00%] [G loss: 0.264894]\n",
      "epoch:18 step:14468 [D loss: 0.007987, acc.: 100.00%] [G loss: 0.393140]\n",
      "epoch:18 step:14469 [D loss: 0.035250, acc.: 100.00%] [G loss: 0.506760]\n",
      "epoch:18 step:14470 [D loss: 0.028532, acc.: 100.00%] [G loss: 0.515981]\n",
      "epoch:18 step:14471 [D loss: 0.043954, acc.: 98.44%] [G loss: 0.929724]\n",
      "epoch:18 step:14472 [D loss: 0.023500, acc.: 100.00%] [G loss: 1.329966]\n",
      "epoch:18 step:14473 [D loss: 0.077089, acc.: 99.22%] [G loss: 2.494146]\n",
      "epoch:18 step:14474 [D loss: 0.034128, acc.: 100.00%] [G loss: 2.556563]\n",
      "epoch:18 step:14475 [D loss: 0.016605, acc.: 100.00%] [G loss: 3.761732]\n",
      "epoch:18 step:14476 [D loss: 0.014141, acc.: 100.00%] [G loss: 2.938607]\n",
      "epoch:18 step:14477 [D loss: 0.026124, acc.: 100.00%] [G loss: 0.496391]\n",
      "epoch:18 step:14478 [D loss: 0.024941, acc.: 100.00%] [G loss: 3.101361]\n",
      "epoch:18 step:14479 [D loss: 0.019342, acc.: 100.00%] [G loss: 2.181826]\n",
      "epoch:18 step:14480 [D loss: 0.032414, acc.: 100.00%] [G loss: 3.921728]\n",
      "epoch:18 step:14481 [D loss: 0.010937, acc.: 100.00%] [G loss: 3.500613]\n",
      "epoch:18 step:14482 [D loss: 0.152714, acc.: 95.31%] [G loss: 1.350857]\n",
      "epoch:18 step:14483 [D loss: 0.296847, acc.: 86.72%] [G loss: 8.084702]\n",
      "epoch:18 step:14484 [D loss: 0.753251, acc.: 66.41%] [G loss: 4.071820]\n",
      "epoch:18 step:14485 [D loss: 0.040884, acc.: 98.44%] [G loss: 4.451523]\n",
      "epoch:18 step:14486 [D loss: 0.010253, acc.: 100.00%] [G loss: 5.359459]\n",
      "epoch:18 step:14487 [D loss: 0.007858, acc.: 100.00%] [G loss: 5.134490]\n",
      "epoch:18 step:14488 [D loss: 0.081240, acc.: 97.66%] [G loss: 7.360010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14489 [D loss: 0.315064, acc.: 85.94%] [G loss: 4.988030]\n",
      "epoch:18 step:14490 [D loss: 0.051496, acc.: 98.44%] [G loss: 5.118608]\n",
      "epoch:18 step:14491 [D loss: 0.018131, acc.: 100.00%] [G loss: 7.382576]\n",
      "epoch:18 step:14492 [D loss: 0.008325, acc.: 100.00%] [G loss: 0.034626]\n",
      "epoch:18 step:14493 [D loss: 0.016409, acc.: 100.00%] [G loss: 8.018213]\n",
      "epoch:18 step:14494 [D loss: 0.017900, acc.: 99.22%] [G loss: 7.679978]\n",
      "epoch:18 step:14495 [D loss: 0.015240, acc.: 100.00%] [G loss: 0.078267]\n",
      "epoch:18 step:14496 [D loss: 0.021567, acc.: 100.00%] [G loss: 7.166569]\n",
      "epoch:18 step:14497 [D loss: 0.011636, acc.: 100.00%] [G loss: 6.422587]\n",
      "epoch:18 step:14498 [D loss: 0.012907, acc.: 100.00%] [G loss: 5.447915]\n",
      "epoch:18 step:14499 [D loss: 0.019727, acc.: 100.00%] [G loss: 5.575758]\n",
      "epoch:18 step:14500 [D loss: 0.032742, acc.: 99.22%] [G loss: 6.063286]\n",
      "epoch:18 step:14501 [D loss: 0.003783, acc.: 100.00%] [G loss: 5.804442]\n",
      "epoch:18 step:14502 [D loss: 0.008705, acc.: 100.00%] [G loss: 2.778184]\n",
      "epoch:18 step:14503 [D loss: 0.007851, acc.: 100.00%] [G loss: 5.661635]\n",
      "epoch:18 step:14504 [D loss: 0.014776, acc.: 100.00%] [G loss: 4.966387]\n",
      "epoch:18 step:14505 [D loss: 0.029769, acc.: 99.22%] [G loss: 4.527594]\n",
      "epoch:18 step:14506 [D loss: 0.011770, acc.: 100.00%] [G loss: 4.047027]\n",
      "epoch:18 step:14507 [D loss: 0.046989, acc.: 100.00%] [G loss: 5.995828]\n",
      "epoch:18 step:14508 [D loss: 0.029447, acc.: 99.22%] [G loss: 0.317334]\n",
      "epoch:18 step:14509 [D loss: 0.012828, acc.: 100.00%] [G loss: 5.083226]\n",
      "epoch:18 step:14510 [D loss: 0.012727, acc.: 100.00%] [G loss: 4.419863]\n",
      "epoch:18 step:14511 [D loss: 0.081364, acc.: 97.66%] [G loss: 3.619559]\n",
      "epoch:18 step:14512 [D loss: 0.006775, acc.: 100.00%] [G loss: 2.229372]\n",
      "epoch:18 step:14513 [D loss: 0.009703, acc.: 100.00%] [G loss: 1.704210]\n",
      "epoch:18 step:14514 [D loss: 0.023551, acc.: 99.22%] [G loss: 0.064425]\n",
      "epoch:18 step:14515 [D loss: 0.006951, acc.: 100.00%] [G loss: 2.725248]\n",
      "epoch:18 step:14516 [D loss: 0.004343, acc.: 100.00%] [G loss: 2.751051]\n",
      "epoch:18 step:14517 [D loss: 0.008459, acc.: 100.00%] [G loss: 1.802616]\n",
      "epoch:18 step:14518 [D loss: 0.015141, acc.: 100.00%] [G loss: 1.549886]\n",
      "epoch:18 step:14519 [D loss: 0.008577, acc.: 100.00%] [G loss: 1.051723]\n",
      "epoch:18 step:14520 [D loss: 0.018752, acc.: 100.00%] [G loss: 1.245224]\n",
      "epoch:18 step:14521 [D loss: 0.008264, acc.: 100.00%] [G loss: 0.859748]\n",
      "epoch:18 step:14522 [D loss: 0.007486, acc.: 100.00%] [G loss: 1.075003]\n",
      "epoch:18 step:14523 [D loss: 0.008528, acc.: 100.00%] [G loss: 1.985720]\n",
      "epoch:18 step:14524 [D loss: 0.054003, acc.: 98.44%] [G loss: 2.606299]\n",
      "epoch:18 step:14525 [D loss: 0.017631, acc.: 99.22%] [G loss: 1.313289]\n",
      "epoch:18 step:14526 [D loss: 0.039365, acc.: 100.00%] [G loss: 2.949990]\n",
      "epoch:18 step:14527 [D loss: 0.003762, acc.: 100.00%] [G loss: 1.658074]\n",
      "epoch:18 step:14528 [D loss: 0.010164, acc.: 100.00%] [G loss: 1.085046]\n",
      "epoch:18 step:14529 [D loss: 0.016037, acc.: 100.00%] [G loss: 0.479038]\n",
      "epoch:18 step:14530 [D loss: 0.016113, acc.: 100.00%] [G loss: 0.793353]\n",
      "epoch:18 step:14531 [D loss: 0.046374, acc.: 99.22%] [G loss: 2.657011]\n",
      "epoch:18 step:14532 [D loss: 0.013139, acc.: 100.00%] [G loss: 3.603260]\n",
      "epoch:18 step:14533 [D loss: 0.024842, acc.: 100.00%] [G loss: 1.198521]\n",
      "epoch:18 step:14534 [D loss: 0.004345, acc.: 100.00%] [G loss: 0.784319]\n",
      "epoch:18 step:14535 [D loss: 0.007897, acc.: 100.00%] [G loss: 0.085479]\n",
      "epoch:18 step:14536 [D loss: 0.019184, acc.: 100.00%] [G loss: 1.822962]\n",
      "epoch:18 step:14537 [D loss: 0.011425, acc.: 100.00%] [G loss: 0.185749]\n",
      "epoch:18 step:14538 [D loss: 0.060603, acc.: 99.22%] [G loss: 1.848848]\n",
      "epoch:18 step:14539 [D loss: 0.004211, acc.: 100.00%] [G loss: 5.353864]\n",
      "epoch:18 step:14540 [D loss: 0.091447, acc.: 96.88%] [G loss: 3.132542]\n",
      "epoch:18 step:14541 [D loss: 0.107596, acc.: 95.31%] [G loss: 6.557471]\n",
      "epoch:18 step:14542 [D loss: 0.013507, acc.: 100.00%] [G loss: 8.164255]\n",
      "epoch:18 step:14543 [D loss: 0.274833, acc.: 88.28%] [G loss: 3.780910]\n",
      "epoch:18 step:14544 [D loss: 0.509511, acc.: 75.00%] [G loss: 9.728803]\n",
      "epoch:18 step:14545 [D loss: 0.070680, acc.: 98.44%] [G loss: 10.206526]\n",
      "epoch:18 step:14546 [D loss: 0.113983, acc.: 94.53%] [G loss: 7.088219]\n",
      "epoch:18 step:14547 [D loss: 0.015731, acc.: 100.00%] [G loss: 5.385765]\n",
      "epoch:18 step:14548 [D loss: 0.004034, acc.: 100.00%] [G loss: 5.929305]\n",
      "epoch:18 step:14549 [D loss: 0.017975, acc.: 100.00%] [G loss: 4.921047]\n",
      "epoch:18 step:14550 [D loss: 0.053464, acc.: 97.66%] [G loss: 3.023209]\n",
      "epoch:18 step:14551 [D loss: 0.005654, acc.: 100.00%] [G loss: 7.568831]\n",
      "epoch:18 step:14552 [D loss: 0.002239, acc.: 100.00%] [G loss: 7.341658]\n",
      "epoch:18 step:14553 [D loss: 0.008126, acc.: 100.00%] [G loss: 6.606209]\n",
      "epoch:18 step:14554 [D loss: 0.003140, acc.: 100.00%] [G loss: 5.149735]\n",
      "epoch:18 step:14555 [D loss: 0.014982, acc.: 100.00%] [G loss: 0.869301]\n",
      "epoch:18 step:14556 [D loss: 0.076172, acc.: 98.44%] [G loss: 6.211928]\n",
      "epoch:18 step:14557 [D loss: 0.037602, acc.: 100.00%] [G loss: 7.012045]\n",
      "epoch:18 step:14558 [D loss: 0.006507, acc.: 100.00%] [G loss: 8.581747]\n",
      "epoch:18 step:14559 [D loss: 0.051063, acc.: 98.44%] [G loss: 6.894517]\n",
      "epoch:18 step:14560 [D loss: 0.010349, acc.: 100.00%] [G loss: 6.498081]\n",
      "epoch:18 step:14561 [D loss: 0.003353, acc.: 100.00%] [G loss: 5.875951]\n",
      "epoch:18 step:14562 [D loss: 0.004766, acc.: 100.00%] [G loss: 6.185027]\n",
      "epoch:18 step:14563 [D loss: 0.034596, acc.: 99.22%] [G loss: 6.514932]\n",
      "epoch:18 step:14564 [D loss: 0.005602, acc.: 100.00%] [G loss: 6.182083]\n",
      "epoch:18 step:14565 [D loss: 0.041848, acc.: 100.00%] [G loss: 1.877856]\n",
      "epoch:18 step:14566 [D loss: 0.007919, acc.: 100.00%] [G loss: 6.033468]\n",
      "epoch:18 step:14567 [D loss: 0.046268, acc.: 98.44%] [G loss: 0.419897]\n",
      "epoch:18 step:14568 [D loss: 0.043223, acc.: 100.00%] [G loss: 7.452404]\n",
      "epoch:18 step:14569 [D loss: 0.099733, acc.: 96.09%] [G loss: 0.011394]\n",
      "epoch:18 step:14570 [D loss: 0.570748, acc.: 76.56%] [G loss: 11.475224]\n",
      "epoch:18 step:14571 [D loss: 2.870407, acc.: 50.00%] [G loss: 6.782687]\n",
      "epoch:18 step:14572 [D loss: 0.128094, acc.: 94.53%] [G loss: 4.713168]\n",
      "epoch:18 step:14573 [D loss: 0.167243, acc.: 92.97%] [G loss: 1.951670]\n",
      "epoch:18 step:14574 [D loss: 0.013369, acc.: 100.00%] [G loss: 4.918013]\n",
      "epoch:18 step:14575 [D loss: 0.048249, acc.: 99.22%] [G loss: 0.344909]\n",
      "epoch:18 step:14576 [D loss: 0.083549, acc.: 99.22%] [G loss: 2.016424]\n",
      "epoch:18 step:14577 [D loss: 0.247890, acc.: 89.06%] [G loss: 0.664354]\n",
      "epoch:18 step:14578 [D loss: 0.205862, acc.: 91.41%] [G loss: 5.899259]\n",
      "epoch:18 step:14579 [D loss: 0.101266, acc.: 97.66%] [G loss: 5.823015]\n",
      "epoch:18 step:14580 [D loss: 0.020559, acc.: 99.22%] [G loss: 4.865050]\n",
      "epoch:18 step:14581 [D loss: 0.091966, acc.: 99.22%] [G loss: 1.375175]\n",
      "epoch:18 step:14582 [D loss: 0.080278, acc.: 99.22%] [G loss: 4.708815]\n",
      "epoch:18 step:14583 [D loss: 0.035329, acc.: 99.22%] [G loss: 5.126979]\n",
      "epoch:18 step:14584 [D loss: 0.084750, acc.: 96.09%] [G loss: 0.289290]\n",
      "epoch:18 step:14585 [D loss: 0.242984, acc.: 88.28%] [G loss: 3.991878]\n",
      "epoch:18 step:14586 [D loss: 0.099818, acc.: 96.88%] [G loss: 3.574394]\n",
      "epoch:18 step:14587 [D loss: 0.057670, acc.: 97.66%] [G loss: 2.292720]\n",
      "epoch:18 step:14588 [D loss: 0.004959, acc.: 100.00%] [G loss: 1.040903]\n",
      "epoch:18 step:14589 [D loss: 0.021577, acc.: 99.22%] [G loss: 0.523547]\n",
      "epoch:18 step:14590 [D loss: 0.007257, acc.: 100.00%] [G loss: 0.480083]\n",
      "epoch:18 step:14591 [D loss: 0.072052, acc.: 97.66%] [G loss: 0.262822]\n",
      "epoch:18 step:14592 [D loss: 0.005261, acc.: 100.00%] [G loss: 0.764938]\n",
      "epoch:18 step:14593 [D loss: 0.004319, acc.: 100.00%] [G loss: 0.639498]\n",
      "epoch:18 step:14594 [D loss: 0.025949, acc.: 99.22%] [G loss: 0.047767]\n",
      "epoch:18 step:14595 [D loss: 0.087983, acc.: 97.66%] [G loss: 0.893089]\n",
      "epoch:18 step:14596 [D loss: 0.138406, acc.: 96.88%] [G loss: 2.147908]\n",
      "epoch:18 step:14597 [D loss: 0.011258, acc.: 100.00%] [G loss: 3.377335]\n",
      "epoch:18 step:14598 [D loss: 0.015543, acc.: 100.00%] [G loss: 0.230665]\n",
      "epoch:18 step:14599 [D loss: 0.054102, acc.: 98.44%] [G loss: 2.560150]\n",
      "epoch:18 step:14600 [D loss: 0.197784, acc.: 91.41%] [G loss: 2.273444]\n",
      "epoch:18 step:14601 [D loss: 0.008860, acc.: 100.00%] [G loss: 1.811283]\n",
      "epoch:18 step:14602 [D loss: 0.051388, acc.: 97.66%] [G loss: 0.596085]\n",
      "epoch:18 step:14603 [D loss: 0.251447, acc.: 90.62%] [G loss: 4.958336]\n",
      "epoch:18 step:14604 [D loss: 0.470224, acc.: 78.12%] [G loss: 0.478583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14605 [D loss: 0.209982, acc.: 87.50%] [G loss: 2.610304]\n",
      "epoch:18 step:14606 [D loss: 0.032060, acc.: 100.00%] [G loss: 4.668344]\n",
      "epoch:18 step:14607 [D loss: 0.353472, acc.: 85.94%] [G loss: 2.510546]\n",
      "epoch:18 step:14608 [D loss: 0.878128, acc.: 69.53%] [G loss: 4.753896]\n",
      "epoch:18 step:14609 [D loss: 0.923270, acc.: 64.06%] [G loss: 0.956428]\n",
      "epoch:18 step:14610 [D loss: 0.017273, acc.: 99.22%] [G loss: 0.434016]\n",
      "epoch:18 step:14611 [D loss: 0.014482, acc.: 100.00%] [G loss: 5.580931]\n",
      "epoch:18 step:14612 [D loss: 0.013935, acc.: 100.00%] [G loss: 0.090364]\n",
      "epoch:18 step:14613 [D loss: 0.014745, acc.: 100.00%] [G loss: 0.164368]\n",
      "epoch:18 step:14614 [D loss: 0.020573, acc.: 100.00%] [G loss: 0.101940]\n",
      "epoch:18 step:14615 [D loss: 0.013736, acc.: 100.00%] [G loss: 4.739404]\n",
      "epoch:18 step:14616 [D loss: 0.028557, acc.: 100.00%] [G loss: 0.125290]\n",
      "epoch:18 step:14617 [D loss: 0.050029, acc.: 99.22%] [G loss: 0.246653]\n",
      "epoch:18 step:14618 [D loss: 0.061600, acc.: 98.44%] [G loss: 4.919174]\n",
      "epoch:18 step:14619 [D loss: 0.145743, acc.: 92.97%] [G loss: 0.108225]\n",
      "epoch:18 step:14620 [D loss: 0.006148, acc.: 100.00%] [G loss: 0.016877]\n",
      "epoch:18 step:14621 [D loss: 0.001625, acc.: 100.00%] [G loss: 0.022887]\n",
      "epoch:18 step:14622 [D loss: 0.002628, acc.: 100.00%] [G loss: 0.135578]\n",
      "epoch:18 step:14623 [D loss: 0.003366, acc.: 100.00%] [G loss: 0.015094]\n",
      "epoch:18 step:14624 [D loss: 0.001911, acc.: 100.00%] [G loss: 0.001115]\n",
      "epoch:18 step:14625 [D loss: 0.005235, acc.: 100.00%] [G loss: 0.187974]\n",
      "epoch:18 step:14626 [D loss: 0.001229, acc.: 100.00%] [G loss: 0.005187]\n",
      "epoch:18 step:14627 [D loss: 0.002366, acc.: 100.00%] [G loss: 0.111666]\n",
      "epoch:18 step:14628 [D loss: 0.004397, acc.: 100.00%] [G loss: 0.043927]\n",
      "epoch:18 step:14629 [D loss: 0.004584, acc.: 100.00%] [G loss: 0.014541]\n",
      "epoch:18 step:14630 [D loss: 0.026820, acc.: 99.22%] [G loss: 0.280630]\n",
      "epoch:18 step:14631 [D loss: 0.002015, acc.: 100.00%] [G loss: 0.879381]\n",
      "epoch:18 step:14632 [D loss: 0.010413, acc.: 100.00%] [G loss: 0.209923]\n",
      "epoch:18 step:14633 [D loss: 0.018127, acc.: 99.22%] [G loss: 0.110013]\n",
      "epoch:18 step:14634 [D loss: 0.024532, acc.: 100.00%] [G loss: 0.298260]\n",
      "epoch:18 step:14635 [D loss: 0.060922, acc.: 100.00%] [G loss: 2.075159]\n",
      "epoch:18 step:14636 [D loss: 0.016147, acc.: 100.00%] [G loss: 1.981646]\n",
      "epoch:18 step:14637 [D loss: 0.026654, acc.: 99.22%] [G loss: 0.569503]\n",
      "epoch:18 step:14638 [D loss: 0.053467, acc.: 100.00%] [G loss: 0.739486]\n",
      "epoch:18 step:14639 [D loss: 0.037499, acc.: 99.22%] [G loss: 0.879086]\n",
      "epoch:18 step:14640 [D loss: 0.006961, acc.: 100.00%] [G loss: 0.163380]\n",
      "epoch:18 step:14641 [D loss: 0.006078, acc.: 100.00%] [G loss: 1.246069]\n",
      "epoch:18 step:14642 [D loss: 0.052248, acc.: 99.22%] [G loss: 0.814658]\n",
      "epoch:18 step:14643 [D loss: 0.341843, acc.: 85.94%] [G loss: 5.094069]\n",
      "epoch:18 step:14644 [D loss: 0.222629, acc.: 90.62%] [G loss: 4.218020]\n",
      "epoch:18 step:14645 [D loss: 0.016081, acc.: 100.00%] [G loss: 0.561449]\n",
      "epoch:18 step:14646 [D loss: 0.049533, acc.: 99.22%] [G loss: 5.443423]\n",
      "epoch:18 step:14647 [D loss: 0.014210, acc.: 100.00%] [G loss: 0.773956]\n",
      "epoch:18 step:14648 [D loss: 0.008961, acc.: 100.00%] [G loss: 3.960962]\n",
      "epoch:18 step:14649 [D loss: 0.023650, acc.: 98.44%] [G loss: 0.193231]\n",
      "epoch:18 step:14650 [D loss: 0.004733, acc.: 100.00%] [G loss: 0.461591]\n",
      "epoch:18 step:14651 [D loss: 0.014779, acc.: 99.22%] [G loss: 0.055059]\n",
      "epoch:18 step:14652 [D loss: 0.003761, acc.: 100.00%] [G loss: 0.009538]\n",
      "epoch:18 step:14653 [D loss: 0.006253, acc.: 100.00%] [G loss: 0.007599]\n",
      "epoch:18 step:14654 [D loss: 0.004457, acc.: 100.00%] [G loss: 0.010119]\n",
      "epoch:18 step:14655 [D loss: 0.007074, acc.: 100.00%] [G loss: 0.038721]\n",
      "epoch:18 step:14656 [D loss: 0.006928, acc.: 100.00%] [G loss: 0.043078]\n",
      "epoch:18 step:14657 [D loss: 0.021619, acc.: 100.00%] [G loss: 1.882948]\n",
      "epoch:18 step:14658 [D loss: 0.008793, acc.: 100.00%] [G loss: 0.113548]\n",
      "epoch:18 step:14659 [D loss: 0.005700, acc.: 100.00%] [G loss: 0.027599]\n",
      "epoch:18 step:14660 [D loss: 0.017641, acc.: 100.00%] [G loss: 0.053065]\n",
      "epoch:18 step:14661 [D loss: 0.006575, acc.: 100.00%] [G loss: 0.014524]\n",
      "epoch:18 step:14662 [D loss: 0.037452, acc.: 100.00%] [G loss: 0.142583]\n",
      "epoch:18 step:14663 [D loss: 0.004681, acc.: 100.00%] [G loss: 0.210154]\n",
      "epoch:18 step:14664 [D loss: 0.008458, acc.: 100.00%] [G loss: 0.058620]\n",
      "epoch:18 step:14665 [D loss: 0.145454, acc.: 96.09%] [G loss: 4.060493]\n",
      "epoch:18 step:14666 [D loss: 0.191895, acc.: 91.41%] [G loss: 0.847649]\n",
      "epoch:18 step:14667 [D loss: 0.042988, acc.: 99.22%] [G loss: 1.974790]\n",
      "epoch:18 step:14668 [D loss: 0.007192, acc.: 100.00%] [G loss: 0.012347]\n",
      "epoch:18 step:14669 [D loss: 0.004992, acc.: 100.00%] [G loss: 0.083448]\n",
      "epoch:18 step:14670 [D loss: 0.002715, acc.: 100.00%] [G loss: 0.026755]\n",
      "epoch:18 step:14671 [D loss: 0.004187, acc.: 100.00%] [G loss: 0.021372]\n",
      "epoch:18 step:14672 [D loss: 0.008099, acc.: 100.00%] [G loss: 0.385402]\n",
      "epoch:18 step:14673 [D loss: 0.009614, acc.: 100.00%] [G loss: 0.009220]\n",
      "epoch:18 step:14674 [D loss: 0.006052, acc.: 100.00%] [G loss: 0.077477]\n",
      "epoch:18 step:14675 [D loss: 0.003692, acc.: 100.00%] [G loss: 0.198291]\n",
      "epoch:18 step:14676 [D loss: 0.016961, acc.: 100.00%] [G loss: 0.058451]\n",
      "epoch:18 step:14677 [D loss: 0.014854, acc.: 100.00%] [G loss: 0.057977]\n",
      "epoch:18 step:14678 [D loss: 0.006972, acc.: 100.00%] [G loss: 0.059368]\n",
      "epoch:18 step:14679 [D loss: 0.001139, acc.: 100.00%] [G loss: 0.331258]\n",
      "epoch:18 step:14680 [D loss: 0.004802, acc.: 100.00%] [G loss: 0.012643]\n",
      "epoch:18 step:14681 [D loss: 0.004500, acc.: 100.00%] [G loss: 0.024795]\n",
      "epoch:18 step:14682 [D loss: 0.004697, acc.: 100.00%] [G loss: 0.138195]\n",
      "epoch:18 step:14683 [D loss: 0.001377, acc.: 100.00%] [G loss: 0.004290]\n",
      "epoch:18 step:14684 [D loss: 0.001378, acc.: 100.00%] [G loss: 0.171364]\n",
      "epoch:18 step:14685 [D loss: 0.046438, acc.: 100.00%] [G loss: 0.104373]\n",
      "epoch:18 step:14686 [D loss: 0.001409, acc.: 100.00%] [G loss: 1.212176]\n",
      "epoch:18 step:14687 [D loss: 0.022877, acc.: 99.22%] [G loss: 0.023057]\n",
      "epoch:18 step:14688 [D loss: 0.002539, acc.: 100.00%] [G loss: 0.240023]\n",
      "epoch:18 step:14689 [D loss: 0.003582, acc.: 100.00%] [G loss: 0.003105]\n",
      "epoch:18 step:14690 [D loss: 0.001946, acc.: 100.00%] [G loss: 2.005909]\n",
      "epoch:18 step:14691 [D loss: 0.013917, acc.: 100.00%] [G loss: 0.017478]\n",
      "epoch:18 step:14692 [D loss: 0.001688, acc.: 100.00%] [G loss: 0.002253]\n",
      "epoch:18 step:14693 [D loss: 0.011068, acc.: 100.00%] [G loss: 0.000696]\n",
      "epoch:18 step:14694 [D loss: 0.006872, acc.: 100.00%] [G loss: 0.013599]\n",
      "epoch:18 step:14695 [D loss: 0.028176, acc.: 99.22%] [G loss: 0.429968]\n",
      "epoch:18 step:14696 [D loss: 0.016049, acc.: 100.00%] [G loss: 0.002824]\n",
      "epoch:18 step:14697 [D loss: 0.006278, acc.: 100.00%] [G loss: 0.365885]\n",
      "epoch:18 step:14698 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.063420]\n",
      "epoch:18 step:14699 [D loss: 0.003661, acc.: 100.00%] [G loss: 0.015552]\n",
      "epoch:18 step:14700 [D loss: 0.006025, acc.: 100.00%] [G loss: 0.000881]\n",
      "epoch:18 step:14701 [D loss: 0.005799, acc.: 100.00%] [G loss: 0.015831]\n",
      "epoch:18 step:14702 [D loss: 0.002380, acc.: 100.00%] [G loss: 0.001786]\n",
      "epoch:18 step:14703 [D loss: 0.001465, acc.: 100.00%] [G loss: 0.011335]\n",
      "epoch:18 step:14704 [D loss: 0.013166, acc.: 100.00%] [G loss: 0.020481]\n",
      "epoch:18 step:14705 [D loss: 0.033626, acc.: 100.00%] [G loss: 0.136870]\n",
      "epoch:18 step:14706 [D loss: 0.002648, acc.: 100.00%] [G loss: 0.205933]\n",
      "epoch:18 step:14707 [D loss: 0.014769, acc.: 100.00%] [G loss: 0.017004]\n",
      "epoch:18 step:14708 [D loss: 0.026205, acc.: 99.22%] [G loss: 0.016560]\n",
      "epoch:18 step:14709 [D loss: 0.001072, acc.: 100.00%] [G loss: 0.000908]\n",
      "epoch:18 step:14710 [D loss: 0.002693, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:18 step:14711 [D loss: 0.004249, acc.: 100.00%] [G loss: 0.003934]\n",
      "epoch:18 step:14712 [D loss: 0.001352, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:18 step:14713 [D loss: 0.003505, acc.: 100.00%] [G loss: 0.288138]\n",
      "epoch:18 step:14714 [D loss: 0.001556, acc.: 100.00%] [G loss: 0.000600]\n",
      "epoch:18 step:14715 [D loss: 0.001489, acc.: 100.00%] [G loss: 0.002612]\n",
      "epoch:18 step:14716 [D loss: 0.002226, acc.: 100.00%] [G loss: 0.002873]\n",
      "epoch:18 step:14717 [D loss: 0.001764, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:18 step:14718 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.000317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14719 [D loss: 0.001335, acc.: 100.00%] [G loss: 0.022666]\n",
      "epoch:18 step:14720 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:18 step:14721 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.002452]\n",
      "epoch:18 step:14722 [D loss: 0.002452, acc.: 100.00%] [G loss: 0.090217]\n",
      "epoch:18 step:14723 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.070044]\n",
      "epoch:18 step:14724 [D loss: 0.002550, acc.: 100.00%] [G loss: 0.000546]\n",
      "epoch:18 step:14725 [D loss: 0.003046, acc.: 100.00%] [G loss: 0.002896]\n",
      "epoch:18 step:14726 [D loss: 0.002244, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:18 step:14727 [D loss: 0.002067, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:18 step:14728 [D loss: 0.002273, acc.: 100.00%] [G loss: 0.593427]\n",
      "epoch:18 step:14729 [D loss: 0.001943, acc.: 100.00%] [G loss: 0.001306]\n",
      "epoch:18 step:14730 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.190241]\n",
      "epoch:18 step:14731 [D loss: 0.027583, acc.: 100.00%] [G loss: 0.120275]\n",
      "epoch:18 step:14732 [D loss: 0.001440, acc.: 100.00%] [G loss: 0.005219]\n",
      "epoch:18 step:14733 [D loss: 0.000367, acc.: 100.00%] [G loss: 2.376034]\n",
      "epoch:18 step:14734 [D loss: 0.003331, acc.: 100.00%] [G loss: 0.645044]\n",
      "epoch:18 step:14735 [D loss: 0.244485, acc.: 88.28%] [G loss: 7.749409]\n",
      "epoch:18 step:14736 [D loss: 1.967402, acc.: 51.56%] [G loss: 0.000698]\n",
      "epoch:18 step:14737 [D loss: 1.034091, acc.: 63.28%] [G loss: 6.247674]\n",
      "epoch:18 step:14738 [D loss: 0.698674, acc.: 69.53%] [G loss: 6.649350]\n",
      "epoch:18 step:14739 [D loss: 0.315199, acc.: 88.28%] [G loss: 0.007755]\n",
      "epoch:18 step:14740 [D loss: 0.075893, acc.: 96.88%] [G loss: 5.492486]\n",
      "epoch:18 step:14741 [D loss: 0.028211, acc.: 100.00%] [G loss: 0.000736]\n",
      "epoch:18 step:14742 [D loss: 0.008843, acc.: 100.00%] [G loss: 5.447594]\n",
      "epoch:18 step:14743 [D loss: 0.035197, acc.: 99.22%] [G loss: 0.009639]\n",
      "epoch:18 step:14744 [D loss: 0.042225, acc.: 99.22%] [G loss: 0.017460]\n",
      "epoch:18 step:14745 [D loss: 0.003080, acc.: 100.00%] [G loss: 0.006514]\n",
      "epoch:18 step:14746 [D loss: 0.027894, acc.: 99.22%] [G loss: 0.000288]\n",
      "epoch:18 step:14747 [D loss: 0.029896, acc.: 99.22%] [G loss: 0.023775]\n",
      "epoch:18 step:14748 [D loss: 0.032337, acc.: 100.00%] [G loss: 0.070985]\n",
      "epoch:18 step:14749 [D loss: 0.018940, acc.: 100.00%] [G loss: 0.004400]\n",
      "epoch:18 step:14750 [D loss: 0.058996, acc.: 99.22%] [G loss: 0.241568]\n",
      "epoch:18 step:14751 [D loss: 0.022431, acc.: 100.00%] [G loss: 0.070771]\n",
      "epoch:18 step:14752 [D loss: 0.126436, acc.: 96.09%] [G loss: 2.329860]\n",
      "epoch:18 step:14753 [D loss: 0.071085, acc.: 99.22%] [G loss: 7.010839]\n",
      "epoch:18 step:14754 [D loss: 0.098018, acc.: 98.44%] [G loss: 1.429573]\n",
      "epoch:18 step:14755 [D loss: 0.637875, acc.: 67.19%] [G loss: 8.552808]\n",
      "epoch:18 step:14756 [D loss: 2.367899, acc.: 50.00%] [G loss: 6.960427]\n",
      "epoch:18 step:14757 [D loss: 0.642004, acc.: 71.88%] [G loss: 1.994918]\n",
      "epoch:18 step:14758 [D loss: 0.470961, acc.: 75.78%] [G loss: 4.547556]\n",
      "epoch:18 step:14759 [D loss: 0.011313, acc.: 100.00%] [G loss: 6.456830]\n",
      "epoch:18 step:14760 [D loss: 0.273170, acc.: 89.84%] [G loss: 4.938782]\n",
      "epoch:18 step:14761 [D loss: 0.034527, acc.: 100.00%] [G loss: 4.062712]\n",
      "epoch:18 step:14762 [D loss: 0.100853, acc.: 97.66%] [G loss: 3.397226]\n",
      "epoch:18 step:14763 [D loss: 0.092903, acc.: 97.66%] [G loss: 4.289630]\n",
      "epoch:18 step:14764 [D loss: 0.090621, acc.: 99.22%] [G loss: 4.157598]\n",
      "epoch:18 step:14765 [D loss: 0.096247, acc.: 97.66%] [G loss: 3.500260]\n",
      "epoch:18 step:14766 [D loss: 0.078604, acc.: 99.22%] [G loss: 4.132905]\n",
      "epoch:18 step:14767 [D loss: 0.053368, acc.: 97.66%] [G loss: 5.400248]\n",
      "epoch:18 step:14768 [D loss: 0.078534, acc.: 98.44%] [G loss: 5.014891]\n",
      "epoch:18 step:14769 [D loss: 0.092426, acc.: 97.66%] [G loss: 3.920678]\n",
      "epoch:18 step:14770 [D loss: 0.035546, acc.: 99.22%] [G loss: 1.742049]\n",
      "epoch:18 step:14771 [D loss: 0.085139, acc.: 99.22%] [G loss: 3.746549]\n",
      "epoch:18 step:14772 [D loss: 0.094845, acc.: 98.44%] [G loss: 5.485963]\n",
      "epoch:18 step:14773 [D loss: 0.072703, acc.: 97.66%] [G loss: 4.721495]\n",
      "epoch:18 step:14774 [D loss: 0.011388, acc.: 100.00%] [G loss: 4.829404]\n",
      "epoch:18 step:14775 [D loss: 0.009587, acc.: 100.00%] [G loss: 3.801524]\n",
      "epoch:18 step:14776 [D loss: 0.031922, acc.: 99.22%] [G loss: 3.636911]\n",
      "epoch:18 step:14777 [D loss: 0.027833, acc.: 100.00%] [G loss: 3.265731]\n",
      "epoch:18 step:14778 [D loss: 0.025980, acc.: 100.00%] [G loss: 0.193009]\n",
      "epoch:18 step:14779 [D loss: 0.026714, acc.: 100.00%] [G loss: 5.073383]\n",
      "epoch:18 step:14780 [D loss: 0.028836, acc.: 100.00%] [G loss: 0.051256]\n",
      "epoch:18 step:14781 [D loss: 0.045440, acc.: 100.00%] [G loss: 4.843891]\n",
      "epoch:18 step:14782 [D loss: 0.044864, acc.: 100.00%] [G loss: 4.379450]\n",
      "epoch:18 step:14783 [D loss: 0.098819, acc.: 99.22%] [G loss: 0.773061]\n",
      "epoch:18 step:14784 [D loss: 0.018960, acc.: 100.00%] [G loss: 5.321712]\n",
      "epoch:18 step:14785 [D loss: 0.250441, acc.: 87.50%] [G loss: 0.118954]\n",
      "epoch:18 step:14786 [D loss: 0.159839, acc.: 93.75%] [G loss: 0.483966]\n",
      "epoch:18 step:14787 [D loss: 0.014933, acc.: 100.00%] [G loss: 5.767301]\n",
      "epoch:18 step:14788 [D loss: 0.050760, acc.: 98.44%] [G loss: 4.479548]\n",
      "epoch:18 step:14789 [D loss: 0.023568, acc.: 100.00%] [G loss: 2.965733]\n",
      "epoch:18 step:14790 [D loss: 0.068485, acc.: 99.22%] [G loss: 2.242947]\n",
      "epoch:18 step:14791 [D loss: 0.131722, acc.: 95.31%] [G loss: 3.557034]\n",
      "epoch:18 step:14792 [D loss: 0.257855, acc.: 91.41%] [G loss: 1.817825]\n",
      "epoch:18 step:14793 [D loss: 0.101316, acc.: 95.31%] [G loss: 0.578572]\n",
      "epoch:18 step:14794 [D loss: 0.207483, acc.: 92.19%] [G loss: 3.215083]\n",
      "epoch:18 step:14795 [D loss: 0.140442, acc.: 95.31%] [G loss: 2.885505]\n",
      "epoch:18 step:14796 [D loss: 0.068540, acc.: 98.44%] [G loss: 0.264100]\n",
      "epoch:18 step:14797 [D loss: 0.075420, acc.: 98.44%] [G loss: 1.185517]\n",
      "epoch:18 step:14798 [D loss: 0.111731, acc.: 96.88%] [G loss: 0.168147]\n",
      "epoch:18 step:14799 [D loss: 0.107151, acc.: 96.88%] [G loss: 0.551023]\n",
      "epoch:18 step:14800 [D loss: 0.042516, acc.: 99.22%] [G loss: 1.624449]\n",
      "epoch:18 step:14801 [D loss: 0.018952, acc.: 100.00%] [G loss: 1.897854]\n",
      "epoch:18 step:14802 [D loss: 0.041245, acc.: 98.44%] [G loss: 2.899506]\n",
      "epoch:18 step:14803 [D loss: 0.004003, acc.: 100.00%] [G loss: 1.186624]\n",
      "epoch:18 step:14804 [D loss: 0.059894, acc.: 97.66%] [G loss: 0.246287]\n",
      "epoch:18 step:14805 [D loss: 0.006791, acc.: 100.00%] [G loss: 0.160480]\n",
      "epoch:18 step:14806 [D loss: 0.020007, acc.: 100.00%] [G loss: 0.052066]\n",
      "epoch:18 step:14807 [D loss: 0.031269, acc.: 100.00%] [G loss: 0.432935]\n",
      "epoch:18 step:14808 [D loss: 0.128857, acc.: 94.53%] [G loss: 0.100478]\n",
      "epoch:18 step:14809 [D loss: 0.067762, acc.: 99.22%] [G loss: 0.554458]\n",
      "epoch:18 step:14810 [D loss: 0.005244, acc.: 100.00%] [G loss: 1.180393]\n",
      "epoch:18 step:14811 [D loss: 0.029337, acc.: 99.22%] [G loss: 0.398131]\n",
      "epoch:18 step:14812 [D loss: 0.008486, acc.: 100.00%] [G loss: 0.394512]\n",
      "epoch:18 step:14813 [D loss: 0.003215, acc.: 100.00%] [G loss: 0.079725]\n",
      "epoch:18 step:14814 [D loss: 0.026510, acc.: 99.22%] [G loss: 2.713234]\n",
      "epoch:18 step:14815 [D loss: 0.040499, acc.: 100.00%] [G loss: 1.999205]\n",
      "epoch:18 step:14816 [D loss: 0.010887, acc.: 100.00%] [G loss: 0.051702]\n",
      "epoch:18 step:14817 [D loss: 0.029313, acc.: 100.00%] [G loss: 0.214489]\n",
      "epoch:18 step:14818 [D loss: 0.252184, acc.: 92.19%] [G loss: 0.860954]\n",
      "epoch:18 step:14819 [D loss: 0.069867, acc.: 96.88%] [G loss: 0.920233]\n",
      "epoch:18 step:14820 [D loss: 0.080246, acc.: 97.66%] [G loss: 0.001769]\n",
      "epoch:18 step:14821 [D loss: 0.048067, acc.: 98.44%] [G loss: 0.024881]\n",
      "epoch:18 step:14822 [D loss: 0.002697, acc.: 100.00%] [G loss: 2.057293]\n",
      "epoch:18 step:14823 [D loss: 0.003313, acc.: 100.00%] [G loss: 0.012553]\n",
      "epoch:18 step:14824 [D loss: 0.003260, acc.: 100.00%] [G loss: 0.013909]\n",
      "epoch:18 step:14825 [D loss: 0.001712, acc.: 100.00%] [G loss: 0.014526]\n",
      "epoch:18 step:14826 [D loss: 0.002347, acc.: 100.00%] [G loss: 0.012641]\n",
      "epoch:18 step:14827 [D loss: 0.003296, acc.: 100.00%] [G loss: 1.575176]\n",
      "epoch:18 step:14828 [D loss: 0.008438, acc.: 100.00%] [G loss: 0.031819]\n",
      "epoch:18 step:14829 [D loss: 0.006163, acc.: 100.00%] [G loss: 0.417749]\n",
      "epoch:18 step:14830 [D loss: 0.005282, acc.: 100.00%] [G loss: 0.018362]\n",
      "epoch:18 step:14831 [D loss: 0.003823, acc.: 100.00%] [G loss: 0.321028]\n",
      "epoch:18 step:14832 [D loss: 0.004281, acc.: 100.00%] [G loss: 0.022353]\n",
      "epoch:18 step:14833 [D loss: 0.002364, acc.: 100.00%] [G loss: 1.954471]\n",
      "epoch:18 step:14834 [D loss: 0.013362, acc.: 100.00%] [G loss: 0.056389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14835 [D loss: 0.053535, acc.: 99.22%] [G loss: 0.394029]\n",
      "epoch:18 step:14836 [D loss: 0.014577, acc.: 99.22%] [G loss: 0.836463]\n",
      "epoch:18 step:14837 [D loss: 0.109242, acc.: 96.88%] [G loss: 0.004626]\n",
      "epoch:18 step:14838 [D loss: 0.023384, acc.: 100.00%] [G loss: 0.056122]\n",
      "epoch:18 step:14839 [D loss: 0.018105, acc.: 100.00%] [G loss: 0.009520]\n",
      "epoch:19 step:14840 [D loss: 0.001633, acc.: 100.00%] [G loss: 0.166087]\n",
      "epoch:19 step:14841 [D loss: 0.002735, acc.: 100.00%] [G loss: 0.124260]\n",
      "epoch:19 step:14842 [D loss: 0.004490, acc.: 100.00%] [G loss: 0.071857]\n",
      "epoch:19 step:14843 [D loss: 0.005370, acc.: 100.00%] [G loss: 0.003693]\n",
      "epoch:19 step:14844 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.035932]\n",
      "epoch:19 step:14845 [D loss: 0.013386, acc.: 100.00%] [G loss: 0.053101]\n",
      "epoch:19 step:14846 [D loss: 0.001959, acc.: 100.00%] [G loss: 0.024290]\n",
      "epoch:19 step:14847 [D loss: 0.003615, acc.: 100.00%] [G loss: 0.038061]\n",
      "epoch:19 step:14848 [D loss: 0.021641, acc.: 100.00%] [G loss: 0.184613]\n",
      "epoch:19 step:14849 [D loss: 0.009311, acc.: 100.00%] [G loss: 0.173838]\n",
      "epoch:19 step:14850 [D loss: 0.030964, acc.: 99.22%] [G loss: 0.514601]\n",
      "epoch:19 step:14851 [D loss: 0.011235, acc.: 99.22%] [G loss: 2.600132]\n",
      "epoch:19 step:14852 [D loss: 0.001144, acc.: 100.00%] [G loss: 0.555668]\n",
      "epoch:19 step:14853 [D loss: 0.090369, acc.: 96.88%] [G loss: 0.000407]\n",
      "epoch:19 step:14854 [D loss: 0.002102, acc.: 100.00%] [G loss: 0.182454]\n",
      "epoch:19 step:14855 [D loss: 0.015620, acc.: 100.00%] [G loss: 0.001854]\n",
      "epoch:19 step:14856 [D loss: 0.011294, acc.: 100.00%] [G loss: 0.001360]\n",
      "epoch:19 step:14857 [D loss: 0.006274, acc.: 100.00%] [G loss: 0.007026]\n",
      "epoch:19 step:14858 [D loss: 0.003444, acc.: 100.00%] [G loss: 0.071769]\n",
      "epoch:19 step:14859 [D loss: 0.001031, acc.: 100.00%] [G loss: 0.030679]\n",
      "epoch:19 step:14860 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.076224]\n",
      "epoch:19 step:14861 [D loss: 0.000433, acc.: 100.00%] [G loss: 1.603393]\n",
      "epoch:19 step:14862 [D loss: 0.004739, acc.: 100.00%] [G loss: 0.174136]\n",
      "epoch:19 step:14863 [D loss: 0.003700, acc.: 100.00%] [G loss: 0.014582]\n",
      "epoch:19 step:14864 [D loss: 0.040931, acc.: 98.44%] [G loss: 0.660893]\n",
      "epoch:19 step:14865 [D loss: 0.006758, acc.: 100.00%] [G loss: 0.274607]\n",
      "epoch:19 step:14866 [D loss: 0.006483, acc.: 100.00%] [G loss: 0.143385]\n",
      "epoch:19 step:14867 [D loss: 0.004957, acc.: 100.00%] [G loss: 1.523274]\n",
      "epoch:19 step:14868 [D loss: 0.006277, acc.: 100.00%] [G loss: 0.038435]\n",
      "epoch:19 step:14869 [D loss: 0.007101, acc.: 100.00%] [G loss: 0.007893]\n",
      "epoch:19 step:14870 [D loss: 0.007621, acc.: 100.00%] [G loss: 0.045070]\n",
      "epoch:19 step:14871 [D loss: 0.012768, acc.: 99.22%] [G loss: 0.781319]\n",
      "epoch:19 step:14872 [D loss: 0.207733, acc.: 91.41%] [G loss: 1.647563]\n",
      "epoch:19 step:14873 [D loss: 0.160672, acc.: 92.97%] [G loss: 1.590047]\n",
      "epoch:19 step:14874 [D loss: 0.013755, acc.: 99.22%] [G loss: 3.780372]\n",
      "epoch:19 step:14875 [D loss: 0.028303, acc.: 99.22%] [G loss: 0.784450]\n",
      "epoch:19 step:14876 [D loss: 0.074875, acc.: 98.44%] [G loss: 2.019286]\n",
      "epoch:19 step:14877 [D loss: 0.009545, acc.: 100.00%] [G loss: 5.918338]\n",
      "epoch:19 step:14878 [D loss: 0.053299, acc.: 98.44%] [G loss: 3.597339]\n",
      "epoch:19 step:14879 [D loss: 0.114231, acc.: 96.09%] [G loss: 5.684587]\n",
      "epoch:19 step:14880 [D loss: 0.094544, acc.: 96.88%] [G loss: 2.658254]\n",
      "epoch:19 step:14881 [D loss: 0.045673, acc.: 99.22%] [G loss: 2.641494]\n",
      "epoch:19 step:14882 [D loss: 0.428242, acc.: 80.47%] [G loss: 6.025606]\n",
      "epoch:19 step:14883 [D loss: 0.060090, acc.: 96.88%] [G loss: 4.758680]\n",
      "epoch:19 step:14884 [D loss: 0.173163, acc.: 93.75%] [G loss: 1.137684]\n",
      "epoch:19 step:14885 [D loss: 0.541557, acc.: 75.78%] [G loss: 7.567999]\n",
      "epoch:19 step:14886 [D loss: 2.197023, acc.: 51.56%] [G loss: 0.111632]\n",
      "epoch:19 step:14887 [D loss: 0.440225, acc.: 85.16%] [G loss: 1.460045]\n",
      "epoch:19 step:14888 [D loss: 0.012563, acc.: 100.00%] [G loss: 7.073964]\n",
      "epoch:19 step:14889 [D loss: 0.250087, acc.: 89.06%] [G loss: 1.391223]\n",
      "epoch:19 step:14890 [D loss: 0.215020, acc.: 94.53%] [G loss: 2.480236]\n",
      "epoch:19 step:14891 [D loss: 0.005264, acc.: 100.00%] [G loss: 3.247610]\n",
      "epoch:19 step:14892 [D loss: 0.031013, acc.: 99.22%] [G loss: 2.591676]\n",
      "epoch:19 step:14893 [D loss: 0.038411, acc.: 98.44%] [G loss: 1.236670]\n",
      "epoch:19 step:14894 [D loss: 0.019407, acc.: 100.00%] [G loss: 4.458422]\n",
      "epoch:19 step:14895 [D loss: 0.275328, acc.: 88.28%] [G loss: 1.248779]\n",
      "epoch:19 step:14896 [D loss: 0.009494, acc.: 100.00%] [G loss: 2.979540]\n",
      "epoch:19 step:14897 [D loss: 0.079073, acc.: 98.44%] [G loss: 1.271709]\n",
      "epoch:19 step:14898 [D loss: 0.077611, acc.: 98.44%] [G loss: 4.616375]\n",
      "epoch:19 step:14899 [D loss: 0.004051, acc.: 100.00%] [G loss: 3.000399]\n",
      "epoch:19 step:14900 [D loss: 0.103716, acc.: 96.88%] [G loss: 2.391988]\n",
      "epoch:19 step:14901 [D loss: 0.008905, acc.: 100.00%] [G loss: 0.226768]\n",
      "epoch:19 step:14902 [D loss: 0.046361, acc.: 99.22%] [G loss: 1.693041]\n",
      "epoch:19 step:14903 [D loss: 0.004492, acc.: 100.00%] [G loss: 1.564986]\n",
      "epoch:19 step:14904 [D loss: 0.049349, acc.: 98.44%] [G loss: 0.601873]\n",
      "epoch:19 step:14905 [D loss: 0.006673, acc.: 100.00%] [G loss: 1.354962]\n",
      "epoch:19 step:14906 [D loss: 0.019875, acc.: 99.22%] [G loss: 0.617849]\n",
      "epoch:19 step:14907 [D loss: 0.019211, acc.: 100.00%] [G loss: 0.096301]\n",
      "epoch:19 step:14908 [D loss: 0.006520, acc.: 100.00%] [G loss: 0.236246]\n",
      "epoch:19 step:14909 [D loss: 0.013995, acc.: 100.00%] [G loss: 0.497771]\n",
      "epoch:19 step:14910 [D loss: 0.003866, acc.: 100.00%] [G loss: 0.451915]\n",
      "epoch:19 step:14911 [D loss: 0.017375, acc.: 99.22%] [G loss: 0.227048]\n",
      "epoch:19 step:14912 [D loss: 0.020786, acc.: 100.00%] [G loss: 2.429858]\n",
      "epoch:19 step:14913 [D loss: 0.015377, acc.: 100.00%] [G loss: 0.320555]\n",
      "epoch:19 step:14914 [D loss: 0.014969, acc.: 100.00%] [G loss: 0.120444]\n",
      "epoch:19 step:14915 [D loss: 0.124061, acc.: 95.31%] [G loss: 2.204664]\n",
      "epoch:19 step:14916 [D loss: 0.253768, acc.: 91.41%] [G loss: 1.969980]\n",
      "epoch:19 step:14917 [D loss: 0.191499, acc.: 91.41%] [G loss: 1.955240]\n",
      "epoch:19 step:14918 [D loss: 0.013641, acc.: 99.22%] [G loss: 4.138085]\n",
      "epoch:19 step:14919 [D loss: 0.141217, acc.: 92.97%] [G loss: 3.142132]\n",
      "epoch:19 step:14920 [D loss: 0.077215, acc.: 98.44%] [G loss: 1.785375]\n",
      "epoch:19 step:14921 [D loss: 0.005825, acc.: 100.00%] [G loss: 4.243218]\n",
      "epoch:19 step:14922 [D loss: 0.024677, acc.: 98.44%] [G loss: 2.779357]\n",
      "epoch:19 step:14923 [D loss: 0.014102, acc.: 100.00%] [G loss: 3.528172]\n",
      "epoch:19 step:14924 [D loss: 0.019040, acc.: 100.00%] [G loss: 1.090250]\n",
      "epoch:19 step:14925 [D loss: 0.053134, acc.: 97.66%] [G loss: 2.022918]\n",
      "epoch:19 step:14926 [D loss: 0.003860, acc.: 100.00%] [G loss: 2.142517]\n",
      "epoch:19 step:14927 [D loss: 0.322846, acc.: 82.81%] [G loss: 0.017710]\n",
      "epoch:19 step:14928 [D loss: 0.304147, acc.: 89.06%] [G loss: 2.834202]\n",
      "epoch:19 step:14929 [D loss: 0.219675, acc.: 89.06%] [G loss: 0.383240]\n",
      "epoch:19 step:14930 [D loss: 0.010842, acc.: 100.00%] [G loss: 0.443039]\n",
      "epoch:19 step:14931 [D loss: 0.010072, acc.: 100.00%] [G loss: 0.259527]\n",
      "epoch:19 step:14932 [D loss: 0.007932, acc.: 100.00%] [G loss: 0.374276]\n",
      "epoch:19 step:14933 [D loss: 0.014057, acc.: 99.22%] [G loss: 0.201985]\n",
      "epoch:19 step:14934 [D loss: 0.023489, acc.: 99.22%] [G loss: 4.845790]\n",
      "epoch:19 step:14935 [D loss: 0.004825, acc.: 100.00%] [G loss: 1.045190]\n",
      "epoch:19 step:14936 [D loss: 0.011083, acc.: 100.00%] [G loss: 0.590336]\n",
      "epoch:19 step:14937 [D loss: 0.009180, acc.: 100.00%] [G loss: 0.121726]\n",
      "epoch:19 step:14938 [D loss: 0.052084, acc.: 99.22%] [G loss: 1.110369]\n",
      "epoch:19 step:14939 [D loss: 0.005418, acc.: 100.00%] [G loss: 1.977443]\n",
      "epoch:19 step:14940 [D loss: 0.010784, acc.: 100.00%] [G loss: 0.841407]\n",
      "epoch:19 step:14941 [D loss: 0.053926, acc.: 99.22%] [G loss: 6.789384]\n",
      "epoch:19 step:14942 [D loss: 0.048374, acc.: 100.00%] [G loss: 1.402455]\n",
      "epoch:19 step:14943 [D loss: 0.024970, acc.: 100.00%] [G loss: 3.887086]\n",
      "epoch:19 step:14944 [D loss: 0.002570, acc.: 100.00%] [G loss: 3.084017]\n",
      "epoch:19 step:14945 [D loss: 0.001449, acc.: 100.00%] [G loss: 1.452241]\n",
      "epoch:19 step:14946 [D loss: 0.024811, acc.: 99.22%] [G loss: 0.871346]\n",
      "epoch:19 step:14947 [D loss: 0.002382, acc.: 100.00%] [G loss: 0.777387]\n",
      "epoch:19 step:14948 [D loss: 0.003106, acc.: 100.00%] [G loss: 0.568487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14949 [D loss: 0.008593, acc.: 100.00%] [G loss: 0.708807]\n",
      "epoch:19 step:14950 [D loss: 0.035432, acc.: 100.00%] [G loss: 0.265864]\n",
      "epoch:19 step:14951 [D loss: 0.007756, acc.: 100.00%] [G loss: 1.066642]\n",
      "epoch:19 step:14952 [D loss: 0.017844, acc.: 99.22%] [G loss: 1.229427]\n",
      "epoch:19 step:14953 [D loss: 0.011139, acc.: 100.00%] [G loss: 0.996273]\n",
      "epoch:19 step:14954 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.346079]\n",
      "epoch:19 step:14955 [D loss: 0.021872, acc.: 100.00%] [G loss: 0.087818]\n",
      "epoch:19 step:14956 [D loss: 0.001852, acc.: 100.00%] [G loss: 0.201434]\n",
      "epoch:19 step:14957 [D loss: 0.005786, acc.: 100.00%] [G loss: 0.157318]\n",
      "epoch:19 step:14958 [D loss: 0.008073, acc.: 100.00%] [G loss: 0.201231]\n",
      "epoch:19 step:14959 [D loss: 0.008368, acc.: 100.00%] [G loss: 0.302450]\n",
      "epoch:19 step:14960 [D loss: 0.002699, acc.: 100.00%] [G loss: 0.085437]\n",
      "epoch:19 step:14961 [D loss: 0.011379, acc.: 100.00%] [G loss: 4.975859]\n",
      "epoch:19 step:14962 [D loss: 0.004783, acc.: 100.00%] [G loss: 0.911259]\n",
      "epoch:19 step:14963 [D loss: 0.010865, acc.: 100.00%] [G loss: 0.494104]\n",
      "epoch:19 step:14964 [D loss: 0.015671, acc.: 100.00%] [G loss: 0.361175]\n",
      "epoch:19 step:14965 [D loss: 0.002691, acc.: 100.00%] [G loss: 1.207793]\n",
      "epoch:19 step:14966 [D loss: 0.010185, acc.: 100.00%] [G loss: 0.539335]\n",
      "epoch:19 step:14967 [D loss: 0.043838, acc.: 99.22%] [G loss: 0.589723]\n",
      "epoch:19 step:14968 [D loss: 0.434492, acc.: 78.12%] [G loss: 9.764889]\n",
      "epoch:19 step:14969 [D loss: 3.198792, acc.: 50.00%] [G loss: 0.434632]\n",
      "epoch:19 step:14970 [D loss: 0.039889, acc.: 98.44%] [G loss: 5.645687]\n",
      "epoch:19 step:14971 [D loss: 0.198874, acc.: 93.75%] [G loss: 5.060295]\n",
      "epoch:19 step:14972 [D loss: 0.016549, acc.: 100.00%] [G loss: 0.187490]\n",
      "epoch:19 step:14973 [D loss: 0.012990, acc.: 100.00%] [G loss: 0.138808]\n",
      "epoch:19 step:14974 [D loss: 0.020077, acc.: 100.00%] [G loss: 0.096228]\n",
      "epoch:19 step:14975 [D loss: 0.065815, acc.: 99.22%] [G loss: 4.394202]\n",
      "epoch:19 step:14976 [D loss: 0.013408, acc.: 100.00%] [G loss: 3.276816]\n",
      "epoch:19 step:14977 [D loss: 0.024095, acc.: 99.22%] [G loss: 2.014045]\n",
      "epoch:19 step:14978 [D loss: 0.047019, acc.: 99.22%] [G loss: 0.063568]\n",
      "epoch:19 step:14979 [D loss: 0.143401, acc.: 93.75%] [G loss: 1.308115]\n",
      "epoch:19 step:14980 [D loss: 0.030928, acc.: 100.00%] [G loss: 2.143715]\n",
      "epoch:19 step:14981 [D loss: 0.006520, acc.: 100.00%] [G loss: 1.014792]\n",
      "epoch:19 step:14982 [D loss: 0.086412, acc.: 98.44%] [G loss: 0.076494]\n",
      "epoch:19 step:14983 [D loss: 0.012982, acc.: 100.00%] [G loss: 0.097218]\n",
      "epoch:19 step:14984 [D loss: 0.013912, acc.: 100.00%] [G loss: 1.000496]\n",
      "epoch:19 step:14985 [D loss: 0.090012, acc.: 98.44%] [G loss: 0.939509]\n",
      "epoch:19 step:14986 [D loss: 0.037969, acc.: 99.22%] [G loss: 0.960329]\n",
      "epoch:19 step:14987 [D loss: 0.019209, acc.: 100.00%] [G loss: 0.312574]\n",
      "epoch:19 step:14988 [D loss: 0.022797, acc.: 100.00%] [G loss: 0.062780]\n",
      "epoch:19 step:14989 [D loss: 0.044139, acc.: 100.00%] [G loss: 1.669407]\n",
      "epoch:19 step:14990 [D loss: 0.013121, acc.: 100.00%] [G loss: 1.467121]\n",
      "epoch:19 step:14991 [D loss: 0.012407, acc.: 100.00%] [G loss: 1.547956]\n",
      "epoch:19 step:14992 [D loss: 0.180636, acc.: 94.53%] [G loss: 0.148560]\n",
      "epoch:19 step:14993 [D loss: 0.005074, acc.: 100.00%] [G loss: 0.029894]\n",
      "epoch:19 step:14994 [D loss: 0.016966, acc.: 100.00%] [G loss: 0.045607]\n",
      "epoch:19 step:14995 [D loss: 0.024789, acc.: 99.22%] [G loss: 0.070374]\n",
      "epoch:19 step:14996 [D loss: 0.005322, acc.: 100.00%] [G loss: 0.003933]\n",
      "epoch:19 step:14997 [D loss: 0.037480, acc.: 99.22%] [G loss: 0.481842]\n",
      "epoch:19 step:14998 [D loss: 0.034010, acc.: 100.00%] [G loss: 0.080744]\n",
      "epoch:19 step:14999 [D loss: 0.005588, acc.: 100.00%] [G loss: 0.026272]\n",
      "epoch:19 step:15000 [D loss: 0.018589, acc.: 100.00%] [G loss: 0.003126]\n",
      "epoch:19 step:15001 [D loss: 0.011299, acc.: 100.00%] [G loss: 0.005056]\n",
      "epoch:19 step:15002 [D loss: 0.004514, acc.: 100.00%] [G loss: 3.204178]\n",
      "epoch:19 step:15003 [D loss: 0.005727, acc.: 100.00%] [G loss: 1.622688]\n",
      "epoch:19 step:15004 [D loss: 0.017775, acc.: 100.00%] [G loss: 0.060126]\n",
      "epoch:19 step:15005 [D loss: 0.033266, acc.: 100.00%] [G loss: 0.128353]\n",
      "epoch:19 step:15006 [D loss: 0.031657, acc.: 99.22%] [G loss: 1.129107]\n",
      "epoch:19 step:15007 [D loss: 0.059087, acc.: 100.00%] [G loss: 1.513939]\n",
      "epoch:19 step:15008 [D loss: 0.066536, acc.: 97.66%] [G loss: 0.698343]\n",
      "epoch:19 step:15009 [D loss: 0.046441, acc.: 99.22%] [G loss: 0.601620]\n",
      "epoch:19 step:15010 [D loss: 0.055064, acc.: 98.44%] [G loss: 1.895627]\n",
      "epoch:19 step:15011 [D loss: 0.020563, acc.: 99.22%] [G loss: 1.765593]\n",
      "epoch:19 step:15012 [D loss: 0.024384, acc.: 100.00%] [G loss: 0.127904]\n",
      "epoch:19 step:15013 [D loss: 0.006350, acc.: 100.00%] [G loss: 2.068558]\n",
      "epoch:19 step:15014 [D loss: 0.002502, acc.: 100.00%] [G loss: 0.821504]\n",
      "epoch:19 step:15015 [D loss: 0.015064, acc.: 100.00%] [G loss: 0.107008]\n",
      "epoch:19 step:15016 [D loss: 0.001845, acc.: 100.00%] [G loss: 0.122176]\n",
      "epoch:19 step:15017 [D loss: 0.004327, acc.: 100.00%] [G loss: 0.055430]\n",
      "epoch:19 step:15018 [D loss: 0.014101, acc.: 100.00%] [G loss: 0.148061]\n",
      "epoch:19 step:15019 [D loss: 0.003541, acc.: 100.00%] [G loss: 0.004244]\n",
      "epoch:19 step:15020 [D loss: 0.004686, acc.: 100.00%] [G loss: 0.065528]\n",
      "epoch:19 step:15021 [D loss: 0.035284, acc.: 100.00%] [G loss: 0.427994]\n",
      "epoch:19 step:15022 [D loss: 0.002550, acc.: 100.00%] [G loss: 1.409138]\n",
      "epoch:19 step:15023 [D loss: 0.069469, acc.: 99.22%] [G loss: 0.006871]\n",
      "epoch:19 step:15024 [D loss: 0.039016, acc.: 99.22%] [G loss: 0.046411]\n",
      "epoch:19 step:15025 [D loss: 0.004930, acc.: 100.00%] [G loss: 0.162215]\n",
      "epoch:19 step:15026 [D loss: 0.092447, acc.: 99.22%] [G loss: 0.696728]\n",
      "epoch:19 step:15027 [D loss: 0.007537, acc.: 100.00%] [G loss: 1.506711]\n",
      "epoch:19 step:15028 [D loss: 0.063620, acc.: 99.22%] [G loss: 0.287094]\n",
      "epoch:19 step:15029 [D loss: 0.061229, acc.: 100.00%] [G loss: 4.853688]\n",
      "epoch:19 step:15030 [D loss: 0.025409, acc.: 99.22%] [G loss: 0.202840]\n",
      "epoch:19 step:15031 [D loss: 0.001232, acc.: 100.00%] [G loss: 3.461794]\n",
      "epoch:19 step:15032 [D loss: 0.082875, acc.: 97.66%] [G loss: 0.669825]\n",
      "epoch:19 step:15033 [D loss: 0.079341, acc.: 98.44%] [G loss: 1.125983]\n",
      "epoch:19 step:15034 [D loss: 0.001467, acc.: 100.00%] [G loss: 3.916699]\n",
      "epoch:19 step:15035 [D loss: 0.104515, acc.: 96.88%] [G loss: 0.016874]\n",
      "epoch:19 step:15036 [D loss: 0.002316, acc.: 100.00%] [G loss: 3.054311]\n",
      "epoch:19 step:15037 [D loss: 0.003282, acc.: 100.00%] [G loss: 2.052660]\n",
      "epoch:19 step:15038 [D loss: 0.013142, acc.: 100.00%] [G loss: 0.280898]\n",
      "epoch:19 step:15039 [D loss: 0.012720, acc.: 100.00%] [G loss: 1.572821]\n",
      "epoch:19 step:15040 [D loss: 0.020081, acc.: 100.00%] [G loss: 1.752859]\n",
      "epoch:19 step:15041 [D loss: 0.012151, acc.: 100.00%] [G loss: 1.747828]\n",
      "epoch:19 step:15042 [D loss: 0.006569, acc.: 100.00%] [G loss: 1.216953]\n",
      "epoch:19 step:15043 [D loss: 0.045189, acc.: 99.22%] [G loss: 0.166873]\n",
      "epoch:19 step:15044 [D loss: 0.056753, acc.: 98.44%] [G loss: 0.549440]\n",
      "epoch:19 step:15045 [D loss: 0.031989, acc.: 99.22%] [G loss: 0.009262]\n",
      "epoch:19 step:15046 [D loss: 0.006060, acc.: 100.00%] [G loss: 1.426469]\n",
      "epoch:19 step:15047 [D loss: 0.004103, acc.: 100.00%] [G loss: 4.098798]\n",
      "epoch:19 step:15048 [D loss: 0.013892, acc.: 100.00%] [G loss: 0.021963]\n",
      "epoch:19 step:15049 [D loss: 0.072145, acc.: 98.44%] [G loss: 1.105607]\n",
      "epoch:19 step:15050 [D loss: 0.001314, acc.: 100.00%] [G loss: 3.793016]\n",
      "epoch:19 step:15051 [D loss: 0.013858, acc.: 100.00%] [G loss: 0.941059]\n",
      "epoch:19 step:15052 [D loss: 0.046352, acc.: 98.44%] [G loss: 0.163448]\n",
      "epoch:19 step:15053 [D loss: 0.008835, acc.: 100.00%] [G loss: 0.003845]\n",
      "epoch:19 step:15054 [D loss: 0.094416, acc.: 97.66%] [G loss: 1.741268]\n",
      "epoch:19 step:15055 [D loss: 0.020525, acc.: 100.00%] [G loss: 1.265347]\n",
      "epoch:19 step:15056 [D loss: 0.013631, acc.: 100.00%] [G loss: 4.653934]\n",
      "epoch:19 step:15057 [D loss: 0.095098, acc.: 96.88%] [G loss: 0.329949]\n",
      "epoch:19 step:15058 [D loss: 0.017255, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:19 step:15059 [D loss: 0.008557, acc.: 100.00%] [G loss: 0.001090]\n",
      "epoch:19 step:15060 [D loss: 0.003518, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:19 step:15061 [D loss: 0.001157, acc.: 100.00%] [G loss: 0.000502]\n",
      "epoch:19 step:15062 [D loss: 0.007426, acc.: 100.00%] [G loss: 0.339926]\n",
      "epoch:19 step:15063 [D loss: 0.001546, acc.: 100.00%] [G loss: 0.006830]\n",
      "epoch:19 step:15064 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.018318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15065 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.000977]\n",
      "epoch:19 step:15066 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.057158]\n",
      "epoch:19 step:15067 [D loss: 0.000912, acc.: 100.00%] [G loss: 1.267868]\n",
      "epoch:19 step:15068 [D loss: 0.001645, acc.: 100.00%] [G loss: 0.001715]\n",
      "epoch:19 step:15069 [D loss: 0.002761, acc.: 100.00%] [G loss: 0.008333]\n",
      "epoch:19 step:15070 [D loss: 0.004591, acc.: 100.00%] [G loss: 0.120622]\n",
      "epoch:19 step:15071 [D loss: 0.004613, acc.: 100.00%] [G loss: 0.011695]\n",
      "epoch:19 step:15072 [D loss: 0.001697, acc.: 100.00%] [G loss: 0.018803]\n",
      "epoch:19 step:15073 [D loss: 0.034045, acc.: 99.22%] [G loss: 0.103505]\n",
      "epoch:19 step:15074 [D loss: 0.001417, acc.: 100.00%] [G loss: 0.574027]\n",
      "epoch:19 step:15075 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.496208]\n",
      "epoch:19 step:15076 [D loss: 0.000844, acc.: 100.00%] [G loss: 0.333403]\n",
      "epoch:19 step:15077 [D loss: 0.002716, acc.: 100.00%] [G loss: 0.208436]\n",
      "epoch:19 step:15078 [D loss: 0.012191, acc.: 100.00%] [G loss: 0.072449]\n",
      "epoch:19 step:15079 [D loss: 0.041055, acc.: 99.22%] [G loss: 0.056197]\n",
      "epoch:19 step:15080 [D loss: 0.056790, acc.: 98.44%] [G loss: 0.001985]\n",
      "epoch:19 step:15081 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.233056]\n",
      "epoch:19 step:15082 [D loss: 0.003643, acc.: 100.00%] [G loss: 0.042532]\n",
      "epoch:19 step:15083 [D loss: 0.024085, acc.: 99.22%] [G loss: 2.660944]\n",
      "epoch:19 step:15084 [D loss: 0.006880, acc.: 100.00%] [G loss: 0.016152]\n",
      "epoch:19 step:15085 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.001554]\n",
      "epoch:19 step:15086 [D loss: 0.006620, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:19 step:15087 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:19 step:15088 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.001709]\n",
      "epoch:19 step:15089 [D loss: 0.004031, acc.: 100.00%] [G loss: 0.006707]\n",
      "epoch:19 step:15090 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.250851]\n",
      "epoch:19 step:15091 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.010773]\n",
      "epoch:19 step:15092 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.002295]\n",
      "epoch:19 step:15093 [D loss: 0.001246, acc.: 100.00%] [G loss: 0.002897]\n",
      "epoch:19 step:15094 [D loss: 0.030116, acc.: 98.44%] [G loss: 0.950199]\n",
      "epoch:19 step:15095 [D loss: 0.000840, acc.: 100.00%] [G loss: 0.343175]\n",
      "epoch:19 step:15096 [D loss: 0.030419, acc.: 99.22%] [G loss: 0.009572]\n",
      "epoch:19 step:15097 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.130335]\n",
      "epoch:19 step:15098 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.001309]\n",
      "epoch:19 step:15099 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.010071]\n",
      "epoch:19 step:15100 [D loss: 0.001933, acc.: 100.00%] [G loss: 0.006458]\n",
      "epoch:19 step:15101 [D loss: 0.008608, acc.: 100.00%] [G loss: 0.046501]\n",
      "epoch:19 step:15102 [D loss: 0.001229, acc.: 100.00%] [G loss: 0.076525]\n",
      "epoch:19 step:15103 [D loss: 0.025525, acc.: 97.66%] [G loss: 0.000279]\n",
      "epoch:19 step:15104 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:19 step:15105 [D loss: 0.004698, acc.: 100.00%] [G loss: 0.036503]\n",
      "epoch:19 step:15106 [D loss: 0.009427, acc.: 100.00%] [G loss: 0.001292]\n",
      "epoch:19 step:15107 [D loss: 0.003246, acc.: 100.00%] [G loss: 0.002386]\n",
      "epoch:19 step:15108 [D loss: 0.002787, acc.: 100.00%] [G loss: 0.004682]\n",
      "epoch:19 step:15109 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.026871]\n",
      "epoch:19 step:15110 [D loss: 0.001480, acc.: 100.00%] [G loss: 0.010733]\n",
      "epoch:19 step:15111 [D loss: 0.001118, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:19 step:15112 [D loss: 0.000772, acc.: 100.00%] [G loss: 0.001006]\n",
      "epoch:19 step:15113 [D loss: 0.009227, acc.: 100.00%] [G loss: 0.004148]\n",
      "epoch:19 step:15114 [D loss: 0.004044, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:19 step:15115 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.004437]\n",
      "epoch:19 step:15116 [D loss: 0.012908, acc.: 100.00%] [G loss: 0.084751]\n",
      "epoch:19 step:15117 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.043076]\n",
      "epoch:19 step:15118 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.038667]\n",
      "epoch:19 step:15119 [D loss: 0.000918, acc.: 100.00%] [G loss: 5.238294]\n",
      "epoch:19 step:15120 [D loss: 0.005358, acc.: 100.00%] [G loss: 0.025692]\n",
      "epoch:19 step:15121 [D loss: 0.003419, acc.: 100.00%] [G loss: 0.005904]\n",
      "epoch:19 step:15122 [D loss: 0.037646, acc.: 100.00%] [G loss: 1.094464]\n",
      "epoch:19 step:15123 [D loss: 0.003635, acc.: 100.00%] [G loss: 1.574028]\n",
      "epoch:19 step:15124 [D loss: 0.501202, acc.: 75.78%] [G loss: 7.570211]\n",
      "epoch:19 step:15125 [D loss: 2.203559, acc.: 35.16%] [G loss: 0.199578]\n",
      "epoch:19 step:15126 [D loss: 0.031589, acc.: 99.22%] [G loss: 4.654788]\n",
      "epoch:19 step:15127 [D loss: 0.045746, acc.: 98.44%] [G loss: 1.368013]\n",
      "epoch:19 step:15128 [D loss: 0.020314, acc.: 98.44%] [G loss: 3.569371]\n",
      "epoch:19 step:15129 [D loss: 0.010691, acc.: 100.00%] [G loss: 2.906871]\n",
      "epoch:19 step:15130 [D loss: 0.055614, acc.: 99.22%] [G loss: 1.222549]\n",
      "epoch:19 step:15131 [D loss: 0.019659, acc.: 100.00%] [G loss: 1.147617]\n",
      "epoch:19 step:15132 [D loss: 0.007000, acc.: 100.00%] [G loss: 0.988165]\n",
      "epoch:19 step:15133 [D loss: 0.003696, acc.: 100.00%] [G loss: 0.651289]\n",
      "epoch:19 step:15134 [D loss: 0.237403, acc.: 89.84%] [G loss: 5.371945]\n",
      "epoch:19 step:15135 [D loss: 0.680745, acc.: 70.31%] [G loss: 0.013328]\n",
      "epoch:19 step:15136 [D loss: 1.204220, acc.: 58.59%] [G loss: 6.832489]\n",
      "epoch:19 step:15137 [D loss: 1.595711, acc.: 53.12%] [G loss: 1.926193]\n",
      "epoch:19 step:15138 [D loss: 0.203420, acc.: 89.84%] [G loss: 1.451279]\n",
      "epoch:19 step:15139 [D loss: 0.050674, acc.: 99.22%] [G loss: 4.130998]\n",
      "epoch:19 step:15140 [D loss: 0.014541, acc.: 100.00%] [G loss: 2.778247]\n",
      "epoch:19 step:15141 [D loss: 0.375205, acc.: 84.38%] [G loss: 5.558328]\n",
      "epoch:19 step:15142 [D loss: 0.045161, acc.: 97.66%] [G loss: 1.336217]\n",
      "epoch:19 step:15143 [D loss: 0.975677, acc.: 60.94%] [G loss: 0.042816]\n",
      "epoch:19 step:15144 [D loss: 0.036438, acc.: 98.44%] [G loss: 0.065361]\n",
      "epoch:19 step:15145 [D loss: 0.032779, acc.: 100.00%] [G loss: 0.020823]\n",
      "epoch:19 step:15146 [D loss: 0.123717, acc.: 96.09%] [G loss: 0.752484]\n",
      "epoch:19 step:15147 [D loss: 0.042880, acc.: 98.44%] [G loss: 0.761600]\n",
      "epoch:19 step:15148 [D loss: 0.052236, acc.: 97.66%] [G loss: 0.344401]\n",
      "epoch:19 step:15149 [D loss: 0.179481, acc.: 94.53%] [G loss: 5.104336]\n",
      "epoch:19 step:15150 [D loss: 0.018936, acc.: 98.44%] [G loss: 4.182664]\n",
      "epoch:19 step:15151 [D loss: 0.063207, acc.: 96.88%] [G loss: 0.371007]\n",
      "epoch:19 step:15152 [D loss: 0.019543, acc.: 99.22%] [G loss: 0.105340]\n",
      "epoch:19 step:15153 [D loss: 0.004588, acc.: 100.00%] [G loss: 2.235860]\n",
      "epoch:19 step:15154 [D loss: 0.045267, acc.: 100.00%] [G loss: 0.044828]\n",
      "epoch:19 step:15155 [D loss: 0.021597, acc.: 100.00%] [G loss: 0.342945]\n",
      "epoch:19 step:15156 [D loss: 0.003240, acc.: 100.00%] [G loss: 0.611484]\n",
      "epoch:19 step:15157 [D loss: 0.025711, acc.: 100.00%] [G loss: 0.056266]\n",
      "epoch:19 step:15158 [D loss: 0.002932, acc.: 100.00%] [G loss: 0.019105]\n",
      "epoch:19 step:15159 [D loss: 0.006140, acc.: 100.00%] [G loss: 0.051723]\n",
      "epoch:19 step:15160 [D loss: 0.010288, acc.: 100.00%] [G loss: 0.173008]\n",
      "epoch:19 step:15161 [D loss: 0.002670, acc.: 100.00%] [G loss: 0.037360]\n",
      "epoch:19 step:15162 [D loss: 0.007964, acc.: 100.00%] [G loss: 1.018820]\n",
      "epoch:19 step:15163 [D loss: 0.018916, acc.: 100.00%] [G loss: 0.020528]\n",
      "epoch:19 step:15164 [D loss: 0.027817, acc.: 99.22%] [G loss: 0.078485]\n",
      "epoch:19 step:15165 [D loss: 0.010043, acc.: 100.00%] [G loss: 0.016237]\n",
      "epoch:19 step:15166 [D loss: 0.018440, acc.: 100.00%] [G loss: 1.129651]\n",
      "epoch:19 step:15167 [D loss: 0.048695, acc.: 99.22%] [G loss: 0.227818]\n",
      "epoch:19 step:15168 [D loss: 0.013436, acc.: 100.00%] [G loss: 1.135774]\n",
      "epoch:19 step:15169 [D loss: 0.009679, acc.: 100.00%] [G loss: 0.990063]\n",
      "epoch:19 step:15170 [D loss: 0.023746, acc.: 99.22%] [G loss: 0.641915]\n",
      "epoch:19 step:15171 [D loss: 0.032916, acc.: 100.00%] [G loss: 0.929711]\n",
      "epoch:19 step:15172 [D loss: 0.006612, acc.: 100.00%] [G loss: 1.170983]\n",
      "epoch:19 step:15173 [D loss: 0.016602, acc.: 100.00%] [G loss: 0.221678]\n",
      "epoch:19 step:15174 [D loss: 0.009255, acc.: 100.00%] [G loss: 0.055994]\n",
      "epoch:19 step:15175 [D loss: 0.087561, acc.: 98.44%] [G loss: 0.026078]\n",
      "epoch:19 step:15176 [D loss: 0.127786, acc.: 94.53%] [G loss: 0.597364]\n",
      "epoch:19 step:15177 [D loss: 0.040245, acc.: 98.44%] [G loss: 0.823639]\n",
      "epoch:19 step:15178 [D loss: 0.099256, acc.: 97.66%] [G loss: 0.122923]\n",
      "epoch:19 step:15179 [D loss: 0.019839, acc.: 99.22%] [G loss: 0.130322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15180 [D loss: 0.011701, acc.: 100.00%] [G loss: 0.541275]\n",
      "epoch:19 step:15181 [D loss: 0.038950, acc.: 100.00%] [G loss: 0.335507]\n",
      "epoch:19 step:15182 [D loss: 0.019933, acc.: 100.00%] [G loss: 3.167228]\n",
      "epoch:19 step:15183 [D loss: 0.001543, acc.: 100.00%] [G loss: 0.388496]\n",
      "epoch:19 step:15184 [D loss: 0.002275, acc.: 100.00%] [G loss: 0.102089]\n",
      "epoch:19 step:15185 [D loss: 0.084764, acc.: 97.66%] [G loss: 3.184700]\n",
      "epoch:19 step:15186 [D loss: 0.034062, acc.: 99.22%] [G loss: 0.768759]\n",
      "epoch:19 step:15187 [D loss: 0.053983, acc.: 98.44%] [G loss: 0.183039]\n",
      "epoch:19 step:15188 [D loss: 0.004269, acc.: 100.00%] [G loss: 0.156716]\n",
      "epoch:19 step:15189 [D loss: 0.099896, acc.: 97.66%] [G loss: 0.495702]\n",
      "epoch:19 step:15190 [D loss: 0.070144, acc.: 97.66%] [G loss: 0.268422]\n",
      "epoch:19 step:15191 [D loss: 0.010665, acc.: 100.00%] [G loss: 0.212611]\n",
      "epoch:19 step:15192 [D loss: 0.025865, acc.: 100.00%] [G loss: 0.205708]\n",
      "epoch:19 step:15193 [D loss: 0.004833, acc.: 100.00%] [G loss: 0.249271]\n",
      "epoch:19 step:15194 [D loss: 0.061556, acc.: 99.22%] [G loss: 3.533557]\n",
      "epoch:19 step:15195 [D loss: 0.007647, acc.: 100.00%] [G loss: 2.704685]\n",
      "epoch:19 step:15196 [D loss: 0.025754, acc.: 100.00%] [G loss: 0.583466]\n",
      "epoch:19 step:15197 [D loss: 0.002464, acc.: 100.00%] [G loss: 0.331699]\n",
      "epoch:19 step:15198 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.363954]\n",
      "epoch:19 step:15199 [D loss: 0.002430, acc.: 100.00%] [G loss: 1.265712]\n",
      "epoch:19 step:15200 [D loss: 0.007385, acc.: 100.00%] [G loss: 0.371718]\n",
      "epoch:19 step:15201 [D loss: 0.005011, acc.: 100.00%] [G loss: 0.109152]\n",
      "epoch:19 step:15202 [D loss: 0.008989, acc.: 99.22%] [G loss: 0.398158]\n",
      "epoch:19 step:15203 [D loss: 0.006536, acc.: 100.00%] [G loss: 0.155114]\n",
      "epoch:19 step:15204 [D loss: 0.002600, acc.: 100.00%] [G loss: 0.310156]\n",
      "epoch:19 step:15205 [D loss: 0.034131, acc.: 99.22%] [G loss: 0.096527]\n",
      "epoch:19 step:15206 [D loss: 0.023104, acc.: 98.44%] [G loss: 0.001001]\n",
      "epoch:19 step:15207 [D loss: 0.003322, acc.: 100.00%] [G loss: 0.036454]\n",
      "epoch:19 step:15208 [D loss: 0.008264, acc.: 100.00%] [G loss: 0.030060]\n",
      "epoch:19 step:15209 [D loss: 0.003823, acc.: 100.00%] [G loss: 0.010167]\n",
      "epoch:19 step:15210 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.410117]\n",
      "epoch:19 step:15211 [D loss: 0.005508, acc.: 100.00%] [G loss: 0.144362]\n",
      "epoch:19 step:15212 [D loss: 0.004509, acc.: 100.00%] [G loss: 0.003210]\n",
      "epoch:19 step:15213 [D loss: 0.004327, acc.: 100.00%] [G loss: 0.024276]\n",
      "epoch:19 step:15214 [D loss: 0.002862, acc.: 100.00%] [G loss: 0.179870]\n",
      "epoch:19 step:15215 [D loss: 0.004679, acc.: 100.00%] [G loss: 0.009469]\n",
      "epoch:19 step:15216 [D loss: 0.004630, acc.: 100.00%] [G loss: 0.168361]\n",
      "epoch:19 step:15217 [D loss: 0.013319, acc.: 100.00%] [G loss: 0.055285]\n",
      "epoch:19 step:15218 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.156574]\n",
      "epoch:19 step:15219 [D loss: 0.005226, acc.: 100.00%] [G loss: 0.127488]\n",
      "epoch:19 step:15220 [D loss: 0.002144, acc.: 100.00%] [G loss: 0.012188]\n",
      "epoch:19 step:15221 [D loss: 0.014626, acc.: 99.22%] [G loss: 0.034473]\n",
      "epoch:19 step:15222 [D loss: 0.003729, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:19 step:15223 [D loss: 0.000907, acc.: 100.00%] [G loss: 0.001193]\n",
      "epoch:19 step:15224 [D loss: 0.002031, acc.: 100.00%] [G loss: 0.843117]\n",
      "epoch:19 step:15225 [D loss: 0.004284, acc.: 100.00%] [G loss: 0.082194]\n",
      "epoch:19 step:15226 [D loss: 0.011116, acc.: 100.00%] [G loss: 0.141227]\n",
      "epoch:19 step:15227 [D loss: 0.003963, acc.: 100.00%] [G loss: 0.010750]\n",
      "epoch:19 step:15228 [D loss: 0.002408, acc.: 100.00%] [G loss: 0.010838]\n",
      "epoch:19 step:15229 [D loss: 0.005385, acc.: 100.00%] [G loss: 0.003736]\n",
      "epoch:19 step:15230 [D loss: 0.001680, acc.: 100.00%] [G loss: 0.083855]\n",
      "epoch:19 step:15231 [D loss: 0.007097, acc.: 100.00%] [G loss: 0.103126]\n",
      "epoch:19 step:15232 [D loss: 0.004797, acc.: 100.00%] [G loss: 1.646155]\n",
      "epoch:19 step:15233 [D loss: 0.064026, acc.: 98.44%] [G loss: 0.027154]\n",
      "epoch:19 step:15234 [D loss: 3.527036, acc.: 21.88%] [G loss: 6.393421]\n",
      "epoch:19 step:15235 [D loss: 2.495826, acc.: 50.00%] [G loss: 4.218827]\n",
      "epoch:19 step:15236 [D loss: 1.419011, acc.: 50.78%] [G loss: 0.396202]\n",
      "epoch:19 step:15237 [D loss: 0.339462, acc.: 85.16%] [G loss: 0.691181]\n",
      "epoch:19 step:15238 [D loss: 0.082824, acc.: 100.00%] [G loss: 0.848617]\n",
      "epoch:19 step:15239 [D loss: 0.247312, acc.: 91.41%] [G loss: 2.120528]\n",
      "epoch:19 step:15240 [D loss: 0.231112, acc.: 92.97%] [G loss: 0.071499]\n",
      "epoch:19 step:15241 [D loss: 0.346950, acc.: 85.16%] [G loss: 1.426062]\n",
      "epoch:19 step:15242 [D loss: 0.361703, acc.: 80.47%] [G loss: 2.695021]\n",
      "epoch:19 step:15243 [D loss: 0.162492, acc.: 96.88%] [G loss: 0.077571]\n",
      "epoch:19 step:15244 [D loss: 0.107156, acc.: 97.66%] [G loss: 0.146528]\n",
      "epoch:19 step:15245 [D loss: 0.067631, acc.: 99.22%] [G loss: 0.294177]\n",
      "epoch:19 step:15246 [D loss: 0.087461, acc.: 96.09%] [G loss: 2.660100]\n",
      "epoch:19 step:15247 [D loss: 0.088597, acc.: 98.44%] [G loss: 0.141280]\n",
      "epoch:19 step:15248 [D loss: 0.192527, acc.: 92.19%] [G loss: 0.435102]\n",
      "epoch:19 step:15249 [D loss: 0.087827, acc.: 96.88%] [G loss: 0.658043]\n",
      "epoch:19 step:15250 [D loss: 0.078021, acc.: 96.88%] [G loss: 0.325889]\n",
      "epoch:19 step:15251 [D loss: 0.063858, acc.: 100.00%] [G loss: 0.131554]\n",
      "epoch:19 step:15252 [D loss: 0.042128, acc.: 99.22%] [G loss: 0.187389]\n",
      "epoch:19 step:15253 [D loss: 0.049121, acc.: 99.22%] [G loss: 0.311029]\n",
      "epoch:19 step:15254 [D loss: 0.113378, acc.: 96.88%] [G loss: 2.132736]\n",
      "epoch:19 step:15255 [D loss: 0.039547, acc.: 100.00%] [G loss: 0.072670]\n",
      "epoch:19 step:15256 [D loss: 0.129111, acc.: 98.44%] [G loss: 0.563508]\n",
      "epoch:19 step:15257 [D loss: 0.122827, acc.: 97.66%] [G loss: 1.555664]\n",
      "epoch:19 step:15258 [D loss: 0.064497, acc.: 100.00%] [G loss: 0.784603]\n",
      "epoch:19 step:15259 [D loss: 0.249547, acc.: 90.62%] [G loss: 0.264719]\n",
      "epoch:19 step:15260 [D loss: 0.105337, acc.: 96.88%] [G loss: 0.186047]\n",
      "epoch:19 step:15261 [D loss: 0.186532, acc.: 95.31%] [G loss: 0.111507]\n",
      "epoch:19 step:15262 [D loss: 0.047847, acc.: 99.22%] [G loss: 0.494020]\n",
      "epoch:19 step:15263 [D loss: 0.095190, acc.: 98.44%] [G loss: 1.411734]\n",
      "epoch:19 step:15264 [D loss: 0.036825, acc.: 99.22%] [G loss: 0.254718]\n",
      "epoch:19 step:15265 [D loss: 0.076391, acc.: 97.66%] [G loss: 0.128621]\n",
      "epoch:19 step:15266 [D loss: 0.019598, acc.: 99.22%] [G loss: 0.272675]\n",
      "epoch:19 step:15267 [D loss: 0.138254, acc.: 92.97%] [G loss: 0.108126]\n",
      "epoch:19 step:15268 [D loss: 0.026291, acc.: 99.22%] [G loss: 0.149072]\n",
      "epoch:19 step:15269 [D loss: 0.118331, acc.: 95.31%] [G loss: 0.065301]\n",
      "epoch:19 step:15270 [D loss: 0.156845, acc.: 96.88%] [G loss: 0.699931]\n",
      "epoch:19 step:15271 [D loss: 0.016903, acc.: 99.22%] [G loss: 0.454449]\n",
      "epoch:19 step:15272 [D loss: 0.067104, acc.: 99.22%] [G loss: 0.045069]\n",
      "epoch:19 step:15273 [D loss: 0.018977, acc.: 100.00%] [G loss: 0.028832]\n",
      "epoch:19 step:15274 [D loss: 0.003273, acc.: 100.00%] [G loss: 1.255804]\n",
      "epoch:19 step:15275 [D loss: 0.033956, acc.: 99.22%] [G loss: 0.270989]\n",
      "epoch:19 step:15276 [D loss: 0.090073, acc.: 97.66%] [G loss: 0.206857]\n",
      "epoch:19 step:15277 [D loss: 0.006117, acc.: 100.00%] [G loss: 0.233968]\n",
      "epoch:19 step:15278 [D loss: 0.004226, acc.: 100.00%] [G loss: 0.081913]\n",
      "epoch:19 step:15279 [D loss: 0.045461, acc.: 99.22%] [G loss: 0.067154]\n",
      "epoch:19 step:15280 [D loss: 0.049001, acc.: 100.00%] [G loss: 0.126732]\n",
      "epoch:19 step:15281 [D loss: 0.036369, acc.: 99.22%] [G loss: 0.107596]\n",
      "epoch:19 step:15282 [D loss: 0.026599, acc.: 100.00%] [G loss: 2.009162]\n",
      "epoch:19 step:15283 [D loss: 0.006492, acc.: 100.00%] [G loss: 0.106883]\n",
      "epoch:19 step:15284 [D loss: 0.091375, acc.: 99.22%] [G loss: 0.411163]\n",
      "epoch:19 step:15285 [D loss: 0.068872, acc.: 97.66%] [G loss: 0.071511]\n",
      "epoch:19 step:15286 [D loss: 0.074128, acc.: 98.44%] [G loss: 4.583859]\n",
      "epoch:19 step:15287 [D loss: 0.045007, acc.: 98.44%] [G loss: 0.979427]\n",
      "epoch:19 step:15288 [D loss: 0.159692, acc.: 95.31%] [G loss: 0.546448]\n",
      "epoch:19 step:15289 [D loss: 0.139906, acc.: 92.19%] [G loss: 0.064096]\n",
      "epoch:19 step:15290 [D loss: 0.038894, acc.: 99.22%] [G loss: 4.287459]\n",
      "epoch:19 step:15291 [D loss: 0.037460, acc.: 98.44%] [G loss: 0.059257]\n",
      "epoch:19 step:15292 [D loss: 0.110051, acc.: 94.53%] [G loss: 0.783730]\n",
      "epoch:19 step:15293 [D loss: 0.028313, acc.: 99.22%] [G loss: 1.982017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15294 [D loss: 0.116482, acc.: 95.31%] [G loss: 0.236922]\n",
      "epoch:19 step:15295 [D loss: 0.151940, acc.: 94.53%] [G loss: 0.935099]\n",
      "epoch:19 step:15296 [D loss: 0.081903, acc.: 99.22%] [G loss: 6.209947]\n",
      "epoch:19 step:15297 [D loss: 0.232542, acc.: 88.28%] [G loss: 0.013038]\n",
      "epoch:19 step:15298 [D loss: 0.989640, acc.: 69.53%] [G loss: 6.172787]\n",
      "epoch:19 step:15299 [D loss: 1.861181, acc.: 54.69%] [G loss: 2.378986]\n",
      "epoch:19 step:15300 [D loss: 0.541113, acc.: 72.66%] [G loss: 0.089665]\n",
      "epoch:19 step:15301 [D loss: 0.015563, acc.: 100.00%] [G loss: 0.006190]\n",
      "epoch:19 step:15302 [D loss: 0.187566, acc.: 91.41%] [G loss: 0.154579]\n",
      "epoch:19 step:15303 [D loss: 0.027920, acc.: 99.22%] [G loss: 0.455845]\n",
      "epoch:19 step:15304 [D loss: 0.068334, acc.: 96.88%] [G loss: 0.107012]\n",
      "epoch:19 step:15305 [D loss: 0.028285, acc.: 100.00%] [G loss: 0.347173]\n",
      "epoch:19 step:15306 [D loss: 0.116977, acc.: 96.09%] [G loss: 3.327540]\n",
      "epoch:19 step:15307 [D loss: 0.019597, acc.: 100.00%] [G loss: 0.097922]\n",
      "epoch:19 step:15308 [D loss: 0.008333, acc.: 100.00%] [G loss: 0.219699]\n",
      "epoch:19 step:15309 [D loss: 0.025787, acc.: 100.00%] [G loss: 1.483302]\n",
      "epoch:19 step:15310 [D loss: 0.017348, acc.: 100.00%] [G loss: 0.038458]\n",
      "epoch:19 step:15311 [D loss: 0.019437, acc.: 100.00%] [G loss: 2.407894]\n",
      "epoch:19 step:15312 [D loss: 0.087833, acc.: 98.44%] [G loss: 0.416989]\n",
      "epoch:19 step:15313 [D loss: 0.072700, acc.: 99.22%] [G loss: 0.265296]\n",
      "epoch:19 step:15314 [D loss: 0.252418, acc.: 89.84%] [G loss: 0.475771]\n",
      "epoch:19 step:15315 [D loss: 0.021199, acc.: 99.22%] [G loss: 1.970367]\n",
      "epoch:19 step:15316 [D loss: 0.018767, acc.: 100.00%] [G loss: 1.985971]\n",
      "epoch:19 step:15317 [D loss: 0.120669, acc.: 96.88%] [G loss: 4.183667]\n",
      "epoch:19 step:15318 [D loss: 0.099164, acc.: 98.44%] [G loss: 0.117864]\n",
      "epoch:19 step:15319 [D loss: 0.014906, acc.: 100.00%] [G loss: 0.092519]\n",
      "epoch:19 step:15320 [D loss: 0.025263, acc.: 99.22%] [G loss: 2.248296]\n",
      "epoch:19 step:15321 [D loss: 0.050509, acc.: 100.00%] [G loss: 0.006547]\n",
      "epoch:19 step:15322 [D loss: 0.037980, acc.: 100.00%] [G loss: 0.023403]\n",
      "epoch:19 step:15323 [D loss: 0.015459, acc.: 100.00%] [G loss: 1.259914]\n",
      "epoch:19 step:15324 [D loss: 0.045127, acc.: 98.44%] [G loss: 0.059182]\n",
      "epoch:19 step:15325 [D loss: 0.005758, acc.: 100.00%] [G loss: 1.819267]\n",
      "epoch:19 step:15326 [D loss: 0.127728, acc.: 95.31%] [G loss: 2.209975]\n",
      "epoch:19 step:15327 [D loss: 0.180640, acc.: 93.75%] [G loss: 0.067092]\n",
      "epoch:19 step:15328 [D loss: 0.057126, acc.: 99.22%] [G loss: 0.138581]\n",
      "epoch:19 step:15329 [D loss: 0.025830, acc.: 100.00%] [G loss: 0.180144]\n",
      "epoch:19 step:15330 [D loss: 0.086366, acc.: 96.88%] [G loss: 0.020326]\n",
      "epoch:19 step:15331 [D loss: 0.034849, acc.: 100.00%] [G loss: 0.014809]\n",
      "epoch:19 step:15332 [D loss: 0.010400, acc.: 100.00%] [G loss: 1.734817]\n",
      "epoch:19 step:15333 [D loss: 0.025913, acc.: 100.00%] [G loss: 0.014914]\n",
      "epoch:19 step:15334 [D loss: 0.003963, acc.: 100.00%] [G loss: 0.025191]\n",
      "epoch:19 step:15335 [D loss: 0.005125, acc.: 100.00%] [G loss: 0.020220]\n",
      "epoch:19 step:15336 [D loss: 0.006542, acc.: 100.00%] [G loss: 0.016777]\n",
      "epoch:19 step:15337 [D loss: 0.004040, acc.: 100.00%] [G loss: 1.761048]\n",
      "epoch:19 step:15338 [D loss: 0.003267, acc.: 100.00%] [G loss: 1.066979]\n",
      "epoch:19 step:15339 [D loss: 0.005411, acc.: 100.00%] [G loss: 0.004659]\n",
      "epoch:19 step:15340 [D loss: 0.012740, acc.: 100.00%] [G loss: 0.005742]\n",
      "epoch:19 step:15341 [D loss: 0.003211, acc.: 100.00%] [G loss: 2.044098]\n",
      "epoch:19 step:15342 [D loss: 0.005479, acc.: 100.00%] [G loss: 0.017581]\n",
      "epoch:19 step:15343 [D loss: 0.008079, acc.: 100.00%] [G loss: 0.077517]\n",
      "epoch:19 step:15344 [D loss: 0.018559, acc.: 100.00%] [G loss: 0.005783]\n",
      "epoch:19 step:15345 [D loss: 0.008090, acc.: 100.00%] [G loss: 0.011194]\n",
      "epoch:19 step:15346 [D loss: 0.006312, acc.: 100.00%] [G loss: 0.008821]\n",
      "epoch:19 step:15347 [D loss: 0.007633, acc.: 100.00%] [G loss: 0.009744]\n",
      "epoch:19 step:15348 [D loss: 0.038157, acc.: 98.44%] [G loss: 0.001861]\n",
      "epoch:19 step:15349 [D loss: 0.055014, acc.: 98.44%] [G loss: 1.234081]\n",
      "epoch:19 step:15350 [D loss: 0.011959, acc.: 100.00%] [G loss: 0.033404]\n",
      "epoch:19 step:15351 [D loss: 0.016541, acc.: 100.00%] [G loss: 0.021229]\n",
      "epoch:19 step:15352 [D loss: 0.074113, acc.: 98.44%] [G loss: 0.108532]\n",
      "epoch:19 step:15353 [D loss: 0.244681, acc.: 88.28%] [G loss: 2.708323]\n",
      "epoch:19 step:15354 [D loss: 0.157953, acc.: 95.31%] [G loss: 2.452115]\n",
      "epoch:19 step:15355 [D loss: 0.168163, acc.: 93.75%] [G loss: 0.065212]\n",
      "epoch:19 step:15356 [D loss: 0.003108, acc.: 100.00%] [G loss: 0.024173]\n",
      "epoch:19 step:15357 [D loss: 0.002342, acc.: 100.00%] [G loss: 0.008563]\n",
      "epoch:19 step:15358 [D loss: 0.009574, acc.: 100.00%] [G loss: 0.003369]\n",
      "epoch:19 step:15359 [D loss: 0.006878, acc.: 100.00%] [G loss: 0.009797]\n",
      "epoch:19 step:15360 [D loss: 0.005079, acc.: 100.00%] [G loss: 0.003846]\n",
      "epoch:19 step:15361 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.041401]\n",
      "epoch:19 step:15362 [D loss: 0.058991, acc.: 98.44%] [G loss: 0.019437]\n",
      "epoch:19 step:15363 [D loss: 0.005979, acc.: 100.00%] [G loss: 0.176976]\n",
      "epoch:19 step:15364 [D loss: 0.007444, acc.: 100.00%] [G loss: 0.069676]\n",
      "epoch:19 step:15365 [D loss: 0.054488, acc.: 100.00%] [G loss: 0.100859]\n",
      "epoch:19 step:15366 [D loss: 0.030747, acc.: 99.22%] [G loss: 0.164899]\n",
      "epoch:19 step:15367 [D loss: 0.011270, acc.: 100.00%] [G loss: 0.320016]\n",
      "epoch:19 step:15368 [D loss: 0.001839, acc.: 100.00%] [G loss: 0.132893]\n",
      "epoch:19 step:15369 [D loss: 0.005397, acc.: 100.00%] [G loss: 0.150389]\n",
      "epoch:19 step:15370 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.044733]\n",
      "epoch:19 step:15371 [D loss: 0.002217, acc.: 100.00%] [G loss: 0.051983]\n",
      "epoch:19 step:15372 [D loss: 0.019029, acc.: 99.22%] [G loss: 0.129988]\n",
      "epoch:19 step:15373 [D loss: 0.002508, acc.: 100.00%] [G loss: 0.028438]\n",
      "epoch:19 step:15374 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.020679]\n",
      "epoch:19 step:15375 [D loss: 0.002267, acc.: 100.00%] [G loss: 0.871736]\n",
      "epoch:19 step:15376 [D loss: 0.008067, acc.: 100.00%] [G loss: 0.011347]\n",
      "epoch:19 step:15377 [D loss: 0.004690, acc.: 100.00%] [G loss: 0.391928]\n",
      "epoch:19 step:15378 [D loss: 0.003475, acc.: 100.00%] [G loss: 0.045906]\n",
      "epoch:19 step:15379 [D loss: 0.008151, acc.: 100.00%] [G loss: 0.009979]\n",
      "epoch:19 step:15380 [D loss: 0.008874, acc.: 100.00%] [G loss: 0.673948]\n",
      "epoch:19 step:15381 [D loss: 0.058910, acc.: 98.44%] [G loss: 0.065675]\n",
      "epoch:19 step:15382 [D loss: 0.014240, acc.: 100.00%] [G loss: 0.255402]\n",
      "epoch:19 step:15383 [D loss: 0.082892, acc.: 97.66%] [G loss: 0.018112]\n",
      "epoch:19 step:15384 [D loss: 0.081440, acc.: 96.88%] [G loss: 0.266811]\n",
      "epoch:19 step:15385 [D loss: 0.004293, acc.: 100.00%] [G loss: 0.354333]\n",
      "epoch:19 step:15386 [D loss: 0.214709, acc.: 91.41%] [G loss: 0.000738]\n",
      "epoch:19 step:15387 [D loss: 0.099660, acc.: 96.09%] [G loss: 0.034268]\n",
      "epoch:19 step:15388 [D loss: 0.004174, acc.: 100.00%] [G loss: 0.050755]\n",
      "epoch:19 step:15389 [D loss: 0.020388, acc.: 100.00%] [G loss: 0.103207]\n",
      "epoch:19 step:15390 [D loss: 0.004766, acc.: 100.00%] [G loss: 0.014563]\n",
      "epoch:19 step:15391 [D loss: 0.023489, acc.: 99.22%] [G loss: 0.007448]\n",
      "epoch:19 step:15392 [D loss: 0.004921, acc.: 100.00%] [G loss: 0.007020]\n",
      "epoch:19 step:15393 [D loss: 0.047731, acc.: 98.44%] [G loss: 0.137699]\n",
      "epoch:19 step:15394 [D loss: 0.005108, acc.: 100.00%] [G loss: 0.246990]\n",
      "epoch:19 step:15395 [D loss: 0.042975, acc.: 99.22%] [G loss: 0.108948]\n",
      "epoch:19 step:15396 [D loss: 0.035445, acc.: 100.00%] [G loss: 0.331415]\n",
      "epoch:19 step:15397 [D loss: 0.002134, acc.: 100.00%] [G loss: 3.704588]\n",
      "epoch:19 step:15398 [D loss: 0.062906, acc.: 97.66%] [G loss: 0.226745]\n",
      "epoch:19 step:15399 [D loss: 0.025551, acc.: 99.22%] [G loss: 0.033321]\n",
      "epoch:19 step:15400 [D loss: 0.287660, acc.: 86.72%] [G loss: 5.132040]\n",
      "epoch:19 step:15401 [D loss: 1.100872, acc.: 57.03%] [G loss: 0.005736]\n",
      "epoch:19 step:15402 [D loss: 0.095601, acc.: 97.66%] [G loss: 2.851898]\n",
      "epoch:19 step:15403 [D loss: 0.004735, acc.: 100.00%] [G loss: 3.791459]\n",
      "epoch:19 step:15404 [D loss: 0.004650, acc.: 100.00%] [G loss: 0.343581]\n",
      "epoch:19 step:15405 [D loss: 0.014378, acc.: 100.00%] [G loss: 0.207081]\n",
      "epoch:19 step:15406 [D loss: 0.003728, acc.: 100.00%] [G loss: 2.279953]\n",
      "epoch:19 step:15407 [D loss: 0.004452, acc.: 100.00%] [G loss: 0.061426]\n",
      "epoch:19 step:15408 [D loss: 0.003468, acc.: 100.00%] [G loss: 0.059591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15409 [D loss: 0.010137, acc.: 100.00%] [G loss: 0.057796]\n",
      "epoch:19 step:15410 [D loss: 0.005150, acc.: 100.00%] [G loss: 0.195903]\n",
      "epoch:19 step:15411 [D loss: 0.007591, acc.: 100.00%] [G loss: 1.268813]\n",
      "epoch:19 step:15412 [D loss: 0.012677, acc.: 100.00%] [G loss: 1.175301]\n",
      "epoch:19 step:15413 [D loss: 0.036169, acc.: 100.00%] [G loss: 0.007560]\n",
      "epoch:19 step:15414 [D loss: 0.050901, acc.: 99.22%] [G loss: 0.002518]\n",
      "epoch:19 step:15415 [D loss: 0.004380, acc.: 100.00%] [G loss: 0.013027]\n",
      "epoch:19 step:15416 [D loss: 0.043980, acc.: 98.44%] [G loss: 0.002177]\n",
      "epoch:19 step:15417 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.860402]\n",
      "epoch:19 step:15418 [D loss: 0.007989, acc.: 100.00%] [G loss: 0.319638]\n",
      "epoch:19 step:15419 [D loss: 0.038073, acc.: 99.22%] [G loss: 0.002149]\n",
      "epoch:19 step:15420 [D loss: 0.003970, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:19 step:15421 [D loss: 0.009768, acc.: 100.00%] [G loss: 2.177498]\n",
      "epoch:19 step:15422 [D loss: 0.007977, acc.: 100.00%] [G loss: 0.017130]\n",
      "epoch:19 step:15423 [D loss: 0.005023, acc.: 100.00%] [G loss: 1.936680]\n",
      "epoch:19 step:15424 [D loss: 0.010052, acc.: 100.00%] [G loss: 0.010556]\n",
      "epoch:19 step:15425 [D loss: 0.004138, acc.: 100.00%] [G loss: 0.002511]\n",
      "epoch:19 step:15426 [D loss: 0.008154, acc.: 100.00%] [G loss: 0.002903]\n",
      "epoch:19 step:15427 [D loss: 0.008971, acc.: 100.00%] [G loss: 0.732501]\n",
      "epoch:19 step:15428 [D loss: 0.004210, acc.: 100.00%] [G loss: 0.170207]\n",
      "epoch:19 step:15429 [D loss: 0.010491, acc.: 100.00%] [G loss: 0.222031]\n",
      "epoch:19 step:15430 [D loss: 0.112437, acc.: 96.88%] [G loss: 3.276520]\n",
      "epoch:19 step:15431 [D loss: 0.018089, acc.: 99.22%] [G loss: 0.106232]\n",
      "epoch:19 step:15432 [D loss: 0.145565, acc.: 92.19%] [G loss: 0.000231]\n",
      "epoch:19 step:15433 [D loss: 0.006241, acc.: 100.00%] [G loss: 0.743034]\n",
      "epoch:19 step:15434 [D loss: 0.006896, acc.: 100.00%] [G loss: 0.494894]\n",
      "epoch:19 step:15435 [D loss: 0.015138, acc.: 100.00%] [G loss: 0.439778]\n",
      "epoch:19 step:15436 [D loss: 0.002914, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:19 step:15437 [D loss: 0.015426, acc.: 100.00%] [G loss: 0.101842]\n",
      "epoch:19 step:15438 [D loss: 0.002162, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:19 step:15439 [D loss: 0.004824, acc.: 100.00%] [G loss: 0.533839]\n",
      "epoch:19 step:15440 [D loss: 0.002053, acc.: 100.00%] [G loss: 0.871616]\n",
      "epoch:19 step:15441 [D loss: 0.003899, acc.: 100.00%] [G loss: 0.202162]\n",
      "epoch:19 step:15442 [D loss: 0.004936, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:19 step:15443 [D loss: 0.004214, acc.: 100.00%] [G loss: 0.112557]\n",
      "epoch:19 step:15444 [D loss: 0.028461, acc.: 100.00%] [G loss: 0.233509]\n",
      "epoch:19 step:15445 [D loss: 0.013523, acc.: 100.00%] [G loss: 0.759597]\n",
      "epoch:19 step:15446 [D loss: 0.005196, acc.: 100.00%] [G loss: 0.001620]\n",
      "epoch:19 step:15447 [D loss: 0.001402, acc.: 100.00%] [G loss: 0.001182]\n",
      "epoch:19 step:15448 [D loss: 0.012682, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:19 step:15449 [D loss: 0.003313, acc.: 100.00%] [G loss: 0.580972]\n",
      "epoch:19 step:15450 [D loss: 0.004472, acc.: 100.00%] [G loss: 0.149168]\n",
      "epoch:19 step:15451 [D loss: 0.003911, acc.: 100.00%] [G loss: 0.077272]\n",
      "epoch:19 step:15452 [D loss: 0.014309, acc.: 100.00%] [G loss: 0.239389]\n",
      "epoch:19 step:15453 [D loss: 0.031624, acc.: 100.00%] [G loss: 0.001969]\n",
      "epoch:19 step:15454 [D loss: 0.007543, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:19 step:15455 [D loss: 0.015608, acc.: 100.00%] [G loss: 0.005537]\n",
      "epoch:19 step:15456 [D loss: 0.004565, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:19 step:15457 [D loss: 0.003280, acc.: 100.00%] [G loss: 3.089687]\n",
      "epoch:19 step:15458 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:19 step:15459 [D loss: 0.003161, acc.: 100.00%] [G loss: 1.219715]\n",
      "epoch:19 step:15460 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.000887]\n",
      "epoch:19 step:15461 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:19 step:15462 [D loss: 0.004576, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:19 step:15463 [D loss: 0.002779, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:19 step:15464 [D loss: 0.001881, acc.: 100.00%] [G loss: 0.109434]\n",
      "epoch:19 step:15465 [D loss: 0.001488, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:19 step:15466 [D loss: 0.001887, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:19 step:15467 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.143833]\n",
      "epoch:19 step:15468 [D loss: 0.107068, acc.: 96.88%] [G loss: 3.917001]\n",
      "epoch:19 step:15469 [D loss: 0.002778, acc.: 100.00%] [G loss: 0.433936]\n",
      "epoch:19 step:15470 [D loss: 0.394405, acc.: 87.50%] [G loss: 0.213752]\n",
      "epoch:19 step:15471 [D loss: 0.002590, acc.: 100.00%] [G loss: 0.037791]\n",
      "epoch:19 step:15472 [D loss: 0.006723, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:19 step:15473 [D loss: 0.002277, acc.: 100.00%] [G loss: 0.017358]\n",
      "epoch:19 step:15474 [D loss: 0.015903, acc.: 100.00%] [G loss: 0.014119]\n",
      "epoch:19 step:15475 [D loss: 0.017905, acc.: 99.22%] [G loss: 0.003241]\n",
      "epoch:19 step:15476 [D loss: 0.001369, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:19 step:15477 [D loss: 0.001206, acc.: 100.00%] [G loss: 0.422943]\n",
      "epoch:19 step:15478 [D loss: 0.004009, acc.: 100.00%] [G loss: 0.000839]\n",
      "epoch:19 step:15479 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.145302]\n",
      "epoch:19 step:15480 [D loss: 0.004066, acc.: 100.00%] [G loss: 0.228217]\n",
      "epoch:19 step:15481 [D loss: 0.017409, acc.: 100.00%] [G loss: 0.086082]\n",
      "epoch:19 step:15482 [D loss: 0.002603, acc.: 100.00%] [G loss: 0.400954]\n",
      "epoch:19 step:15483 [D loss: 0.004397, acc.: 100.00%] [G loss: 0.155071]\n",
      "epoch:19 step:15484 [D loss: 0.002901, acc.: 100.00%] [G loss: 0.002528]\n",
      "epoch:19 step:15485 [D loss: 0.076893, acc.: 99.22%] [G loss: 0.142167]\n",
      "epoch:19 step:15486 [D loss: 0.281809, acc.: 87.50%] [G loss: 0.000004]\n",
      "epoch:19 step:15487 [D loss: 0.067667, acc.: 97.66%] [G loss: 0.000101]\n",
      "epoch:19 step:15488 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.044705]\n",
      "epoch:19 step:15489 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.029048]\n",
      "epoch:19 step:15490 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.018956]\n",
      "epoch:19 step:15491 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.057321]\n",
      "epoch:19 step:15492 [D loss: 0.000950, acc.: 100.00%] [G loss: 4.361904]\n",
      "epoch:19 step:15493 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.031177]\n",
      "epoch:19 step:15494 [D loss: 0.002639, acc.: 100.00%] [G loss: 2.066910]\n",
      "epoch:19 step:15495 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.005987]\n",
      "epoch:19 step:15496 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.343455]\n",
      "epoch:19 step:15497 [D loss: 0.028060, acc.: 99.22%] [G loss: 0.482880]\n",
      "epoch:19 step:15498 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.020817]\n",
      "epoch:19 step:15499 [D loss: 0.010861, acc.: 99.22%] [G loss: 0.043008]\n",
      "epoch:19 step:15500 [D loss: 0.007302, acc.: 100.00%] [G loss: 0.032978]\n",
      "epoch:19 step:15501 [D loss: 0.001798, acc.: 100.00%] [G loss: 0.002404]\n",
      "epoch:19 step:15502 [D loss: 0.001549, acc.: 100.00%] [G loss: 0.045918]\n",
      "epoch:19 step:15503 [D loss: 0.148836, acc.: 95.31%] [G loss: 6.498787]\n",
      "epoch:19 step:15504 [D loss: 0.359033, acc.: 81.25%] [G loss: 1.345126]\n",
      "epoch:19 step:15505 [D loss: 0.074834, acc.: 98.44%] [G loss: 0.897151]\n",
      "epoch:19 step:15506 [D loss: 0.072893, acc.: 98.44%] [G loss: 4.296249]\n",
      "epoch:19 step:15507 [D loss: 0.004512, acc.: 100.00%] [G loss: 5.047547]\n",
      "epoch:19 step:15508 [D loss: 0.016399, acc.: 99.22%] [G loss: 0.000077]\n",
      "epoch:19 step:15509 [D loss: 0.028158, acc.: 98.44%] [G loss: 0.000200]\n",
      "epoch:19 step:15510 [D loss: 0.075589, acc.: 96.88%] [G loss: 1.978266]\n",
      "epoch:19 step:15511 [D loss: 0.146084, acc.: 92.97%] [G loss: 7.127618]\n",
      "epoch:19 step:15512 [D loss: 0.106790, acc.: 96.88%] [G loss: 0.000002]\n",
      "epoch:19 step:15513 [D loss: 0.002449, acc.: 100.00%] [G loss: 0.006776]\n",
      "epoch:19 step:15514 [D loss: 0.002571, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:19 step:15515 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:19 step:15516 [D loss: 0.001338, acc.: 100.00%] [G loss: 0.001659]\n",
      "epoch:19 step:15517 [D loss: 0.001252, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:19 step:15518 [D loss: 0.001618, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:19 step:15519 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:19 step:15520 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.001937]\n",
      "epoch:19 step:15521 [D loss: 0.005798, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:19 step:15522 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:19 step:15523 [D loss: 0.000172, acc.: 100.00%] [G loss: 6.077830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15524 [D loss: 0.027415, acc.: 99.22%] [G loss: 6.236854]\n",
      "epoch:19 step:15525 [D loss: 0.000233, acc.: 100.00%] [G loss: 2.149070]\n",
      "epoch:19 step:15526 [D loss: 0.005231, acc.: 100.00%] [G loss: 0.825384]\n",
      "epoch:19 step:15527 [D loss: 0.002288, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:19 step:15528 [D loss: 0.019290, acc.: 100.00%] [G loss: 1.620534]\n",
      "epoch:19 step:15529 [D loss: 0.009056, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:19 step:15530 [D loss: 0.061579, acc.: 97.66%] [G loss: 4.173020]\n",
      "epoch:19 step:15531 [D loss: 0.188235, acc.: 92.97%] [G loss: 0.184309]\n",
      "epoch:19 step:15532 [D loss: 0.013152, acc.: 100.00%] [G loss: 10.359617]\n",
      "epoch:19 step:15533 [D loss: 0.515595, acc.: 82.03%] [G loss: 0.317549]\n",
      "epoch:19 step:15534 [D loss: 0.543980, acc.: 78.91%] [G loss: 11.711357]\n",
      "epoch:19 step:15535 [D loss: 1.406866, acc.: 59.38%] [G loss: 0.738770]\n",
      "epoch:19 step:15536 [D loss: 0.017057, acc.: 99.22%] [G loss: 4.826930]\n",
      "epoch:19 step:15537 [D loss: 0.041124, acc.: 97.66%] [G loss: 0.089984]\n",
      "epoch:19 step:15538 [D loss: 0.113854, acc.: 97.66%] [G loss: 0.110187]\n",
      "epoch:19 step:15539 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.473151]\n",
      "epoch:19 step:15540 [D loss: 0.006268, acc.: 100.00%] [G loss: 6.496204]\n",
      "epoch:19 step:15541 [D loss: 0.003295, acc.: 100.00%] [G loss: 3.397007]\n",
      "epoch:19 step:15542 [D loss: 0.036800, acc.: 99.22%] [G loss: 0.000362]\n",
      "epoch:19 step:15543 [D loss: 0.228040, acc.: 89.84%] [G loss: 0.211387]\n",
      "epoch:19 step:15544 [D loss: 0.180667, acc.: 93.75%] [G loss: 0.542214]\n",
      "epoch:19 step:15545 [D loss: 0.115563, acc.: 96.09%] [G loss: 0.017166]\n",
      "epoch:19 step:15546 [D loss: 0.038625, acc.: 98.44%] [G loss: 0.000079]\n",
      "epoch:19 step:15547 [D loss: 0.001346, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:19 step:15548 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:19 step:15549 [D loss: 0.002515, acc.: 100.00%] [G loss: 5.185549]\n",
      "epoch:19 step:15550 [D loss: 0.000733, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:19 step:15551 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:19 step:15552 [D loss: 0.002553, acc.: 100.00%] [G loss: 2.292826]\n",
      "epoch:19 step:15553 [D loss: 0.004431, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:19 step:15554 [D loss: 0.006274, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:19 step:15555 [D loss: 0.012946, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:19 step:15556 [D loss: 0.016079, acc.: 100.00%] [G loss: 0.656558]\n",
      "epoch:19 step:15557 [D loss: 0.005257, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:19 step:15558 [D loss: 0.007731, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:19 step:15559 [D loss: 0.006488, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:19 step:15560 [D loss: 0.032908, acc.: 100.00%] [G loss: 0.009271]\n",
      "epoch:19 step:15561 [D loss: 0.003876, acc.: 100.00%] [G loss: 0.003105]\n",
      "epoch:19 step:15562 [D loss: 0.044202, acc.: 98.44%] [G loss: 1.276685]\n",
      "epoch:19 step:15563 [D loss: 0.001871, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:19 step:15564 [D loss: 0.005864, acc.: 100.00%] [G loss: 0.128903]\n",
      "epoch:19 step:15565 [D loss: 0.001547, acc.: 100.00%] [G loss: 0.120042]\n",
      "epoch:19 step:15566 [D loss: 0.003943, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:19 step:15567 [D loss: 0.003821, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:19 step:15568 [D loss: 0.013792, acc.: 99.22%] [G loss: 0.001543]\n",
      "epoch:19 step:15569 [D loss: 0.004302, acc.: 100.00%] [G loss: 0.169618]\n",
      "epoch:19 step:15570 [D loss: 0.035983, acc.: 100.00%] [G loss: 0.011397]\n",
      "epoch:19 step:15571 [D loss: 0.010513, acc.: 100.00%] [G loss: 1.452087]\n",
      "epoch:19 step:15572 [D loss: 0.006290, acc.: 100.00%] [G loss: 0.022068]\n",
      "epoch:19 step:15573 [D loss: 0.060752, acc.: 99.22%] [G loss: 0.000243]\n",
      "epoch:19 step:15574 [D loss: 0.011620, acc.: 100.00%] [G loss: 0.130371]\n",
      "epoch:19 step:15575 [D loss: 0.009721, acc.: 100.00%] [G loss: 0.002394]\n",
      "epoch:19 step:15576 [D loss: 0.001789, acc.: 100.00%] [G loss: 0.002188]\n",
      "epoch:19 step:15577 [D loss: 0.182434, acc.: 92.97%] [G loss: 2.621789]\n",
      "epoch:19 step:15578 [D loss: 0.879422, acc.: 69.53%] [G loss: 0.000001]\n",
      "epoch:19 step:15579 [D loss: 0.622579, acc.: 76.56%] [G loss: 7.272215]\n",
      "epoch:19 step:15580 [D loss: 0.002446, acc.: 100.00%] [G loss: 7.353120]\n",
      "epoch:19 step:15581 [D loss: 1.057692, acc.: 62.50%] [G loss: 1.005062]\n",
      "epoch:19 step:15582 [D loss: 0.759677, acc.: 80.47%] [G loss: 1.142593]\n",
      "epoch:19 step:15583 [D loss: 0.000606, acc.: 100.00%] [G loss: 7.569644]\n",
      "epoch:19 step:15584 [D loss: 0.564154, acc.: 83.59%] [G loss: 0.072881]\n",
      "epoch:19 step:15585 [D loss: 0.031991, acc.: 99.22%] [G loss: 3.638803]\n",
      "epoch:19 step:15586 [D loss: 0.482374, acc.: 78.12%] [G loss: 1.776858]\n",
      "epoch:19 step:15587 [D loss: 0.213367, acc.: 92.97%] [G loss: 1.802169]\n",
      "epoch:19 step:15588 [D loss: 0.683795, acc.: 68.75%] [G loss: 0.000370]\n",
      "epoch:19 step:15589 [D loss: 0.090909, acc.: 96.88%] [G loss: 0.010414]\n",
      "epoch:19 step:15590 [D loss: 0.017848, acc.: 100.00%] [G loss: 0.086845]\n",
      "epoch:19 step:15591 [D loss: 0.012759, acc.: 100.00%] [G loss: 6.201580]\n",
      "epoch:19 step:15592 [D loss: 0.002500, acc.: 100.00%] [G loss: 0.061753]\n",
      "epoch:19 step:15593 [D loss: 0.089623, acc.: 97.66%] [G loss: 0.106175]\n",
      "epoch:19 step:15594 [D loss: 0.034056, acc.: 99.22%] [G loss: 0.013213]\n",
      "epoch:19 step:15595 [D loss: 0.052115, acc.: 97.66%] [G loss: 6.372355]\n",
      "epoch:19 step:15596 [D loss: 0.009943, acc.: 100.00%] [G loss: 0.001683]\n",
      "epoch:19 step:15597 [D loss: 0.022767, acc.: 100.00%] [G loss: 0.002168]\n",
      "epoch:19 step:15598 [D loss: 0.192371, acc.: 92.19%] [G loss: 0.794658]\n",
      "epoch:19 step:15599 [D loss: 0.237315, acc.: 89.06%] [G loss: 0.073598]\n",
      "epoch:19 step:15600 [D loss: 0.076711, acc.: 98.44%] [G loss: 0.357889]\n",
      "epoch:19 step:15601 [D loss: 0.044259, acc.: 99.22%] [G loss: 0.282817]\n",
      "epoch:19 step:15602 [D loss: 0.018860, acc.: 100.00%] [G loss: 0.073265]\n",
      "epoch:19 step:15603 [D loss: 0.314045, acc.: 88.28%] [G loss: 3.642427]\n",
      "epoch:19 step:15604 [D loss: 0.459164, acc.: 78.91%] [G loss: 1.137328]\n",
      "epoch:19 step:15605 [D loss: 0.085086, acc.: 97.66%] [G loss: 2.832155]\n",
      "epoch:19 step:15606 [D loss: 0.007424, acc.: 100.00%] [G loss: 3.608417]\n",
      "epoch:19 step:15607 [D loss: 0.043336, acc.: 98.44%] [G loss: 6.607561]\n",
      "epoch:19 step:15608 [D loss: 0.056788, acc.: 99.22%] [G loss: 4.189919]\n",
      "epoch:19 step:15609 [D loss: 0.038762, acc.: 99.22%] [G loss: 4.705089]\n",
      "epoch:19 step:15610 [D loss: 0.435884, acc.: 79.69%] [G loss: 6.406048]\n",
      "epoch:19 step:15611 [D loss: 0.128038, acc.: 95.31%] [G loss: 3.938654]\n",
      "epoch:19 step:15612 [D loss: 0.038758, acc.: 99.22%] [G loss: 4.507749]\n",
      "epoch:19 step:15613 [D loss: 0.026093, acc.: 100.00%] [G loss: 5.383851]\n",
      "epoch:19 step:15614 [D loss: 0.015181, acc.: 100.00%] [G loss: 4.427177]\n",
      "epoch:19 step:15615 [D loss: 0.041791, acc.: 98.44%] [G loss: 2.855160]\n",
      "epoch:19 step:15616 [D loss: 0.097967, acc.: 99.22%] [G loss: 3.961378]\n",
      "epoch:19 step:15617 [D loss: 0.021939, acc.: 100.00%] [G loss: 2.719252]\n",
      "epoch:19 step:15618 [D loss: 0.191466, acc.: 90.62%] [G loss: 4.020391]\n",
      "epoch:19 step:15619 [D loss: 0.059907, acc.: 98.44%] [G loss: 4.373691]\n",
      "epoch:19 step:15620 [D loss: 0.061095, acc.: 98.44%] [G loss: 4.793115]\n",
      "epoch:20 step:15621 [D loss: 0.053884, acc.: 98.44%] [G loss: 4.572396]\n",
      "epoch:20 step:15622 [D loss: 0.048993, acc.: 99.22%] [G loss: 1.556381]\n",
      "epoch:20 step:15623 [D loss: 0.093704, acc.: 98.44%] [G loss: 3.031287]\n",
      "epoch:20 step:15624 [D loss: 0.246885, acc.: 91.41%] [G loss: 6.925778]\n",
      "epoch:20 step:15625 [D loss: 0.297891, acc.: 85.94%] [G loss: 1.996877]\n",
      "epoch:20 step:15626 [D loss: 0.230211, acc.: 92.19%] [G loss: 6.066436]\n",
      "epoch:20 step:15627 [D loss: 0.073729, acc.: 97.66%] [G loss: 5.232748]\n",
      "epoch:20 step:15628 [D loss: 0.029470, acc.: 99.22%] [G loss: 4.315200]\n",
      "epoch:20 step:15629 [D loss: 1.593338, acc.: 47.66%] [G loss: 10.371569]\n",
      "epoch:20 step:15630 [D loss: 2.394832, acc.: 50.00%] [G loss: 8.347649]\n",
      "epoch:20 step:15631 [D loss: 0.484337, acc.: 78.91%] [G loss: 1.315180]\n",
      "epoch:20 step:15632 [D loss: 0.007613, acc.: 100.00%] [G loss: 1.221753]\n",
      "epoch:20 step:15633 [D loss: 0.011197, acc.: 100.00%] [G loss: 0.269726]\n",
      "epoch:20 step:15634 [D loss: 0.106774, acc.: 96.88%] [G loss: 3.231798]\n",
      "epoch:20 step:15635 [D loss: 0.005136, acc.: 100.00%] [G loss: 0.177404]\n",
      "epoch:20 step:15636 [D loss: 0.036792, acc.: 99.22%] [G loss: 5.222524]\n",
      "epoch:20 step:15637 [D loss: 0.010210, acc.: 100.00%] [G loss: 0.336998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15638 [D loss: 0.039717, acc.: 99.22%] [G loss: 0.184177]\n",
      "epoch:20 step:15639 [D loss: 0.056119, acc.: 100.00%] [G loss: 0.390951]\n",
      "epoch:20 step:15640 [D loss: 0.016488, acc.: 99.22%] [G loss: 0.540289]\n",
      "epoch:20 step:15641 [D loss: 0.053598, acc.: 100.00%] [G loss: 0.532592]\n",
      "epoch:20 step:15642 [D loss: 0.053901, acc.: 99.22%] [G loss: 0.286476]\n",
      "epoch:20 step:15643 [D loss: 0.128695, acc.: 95.31%] [G loss: 2.169991]\n",
      "epoch:20 step:15644 [D loss: 0.125558, acc.: 94.53%] [G loss: 0.248199]\n",
      "epoch:20 step:15645 [D loss: 0.244683, acc.: 89.84%] [G loss: 2.518288]\n",
      "epoch:20 step:15646 [D loss: 0.076102, acc.: 96.88%] [G loss: 6.560705]\n",
      "epoch:20 step:15647 [D loss: 0.056481, acc.: 99.22%] [G loss: 5.267645]\n",
      "epoch:20 step:15648 [D loss: 0.017398, acc.: 100.00%] [G loss: 0.315244]\n",
      "epoch:20 step:15649 [D loss: 0.014479, acc.: 100.00%] [G loss: 0.077294]\n",
      "epoch:20 step:15650 [D loss: 0.008265, acc.: 100.00%] [G loss: 0.129285]\n",
      "epoch:20 step:15651 [D loss: 0.021028, acc.: 100.00%] [G loss: 3.820048]\n",
      "epoch:20 step:15652 [D loss: 0.016665, acc.: 100.00%] [G loss: 0.097490]\n",
      "epoch:20 step:15653 [D loss: 0.025964, acc.: 100.00%] [G loss: 0.041193]\n",
      "epoch:20 step:15654 [D loss: 0.014715, acc.: 100.00%] [G loss: 3.322944]\n",
      "epoch:20 step:15655 [D loss: 0.073983, acc.: 97.66%] [G loss: 0.026197]\n",
      "epoch:20 step:15656 [D loss: 0.027929, acc.: 100.00%] [G loss: 2.167736]\n",
      "epoch:20 step:15657 [D loss: 0.244762, acc.: 90.62%] [G loss: 2.494675]\n",
      "epoch:20 step:15658 [D loss: 0.267356, acc.: 84.38%] [G loss: 0.152963]\n",
      "epoch:20 step:15659 [D loss: 0.063827, acc.: 97.66%] [G loss: 0.007013]\n",
      "epoch:20 step:15660 [D loss: 0.034217, acc.: 100.00%] [G loss: 0.003722]\n",
      "epoch:20 step:15661 [D loss: 0.007606, acc.: 100.00%] [G loss: 2.777084]\n",
      "epoch:20 step:15662 [D loss: 0.009808, acc.: 100.00%] [G loss: 1.701787]\n",
      "epoch:20 step:15663 [D loss: 0.021401, acc.: 100.00%] [G loss: 0.005557]\n",
      "epoch:20 step:15664 [D loss: 0.187826, acc.: 92.19%] [G loss: 0.269763]\n",
      "epoch:20 step:15665 [D loss: 0.040382, acc.: 99.22%] [G loss: 0.526340]\n",
      "epoch:20 step:15666 [D loss: 0.252204, acc.: 88.28%] [G loss: 0.000730]\n",
      "epoch:20 step:15667 [D loss: 0.097708, acc.: 97.66%] [G loss: 0.011338]\n",
      "epoch:20 step:15668 [D loss: 0.006726, acc.: 100.00%] [G loss: 0.021857]\n",
      "epoch:20 step:15669 [D loss: 0.005750, acc.: 100.00%] [G loss: 0.052664]\n",
      "epoch:20 step:15670 [D loss: 0.012675, acc.: 100.00%] [G loss: 0.032570]\n",
      "epoch:20 step:15671 [D loss: 0.004700, acc.: 100.00%] [G loss: 3.567687]\n",
      "epoch:20 step:15672 [D loss: 0.009452, acc.: 100.00%] [G loss: 0.206149]\n",
      "epoch:20 step:15673 [D loss: 0.003948, acc.: 100.00%] [G loss: 4.647023]\n",
      "epoch:20 step:15674 [D loss: 0.018281, acc.: 99.22%] [G loss: 0.026169]\n",
      "epoch:20 step:15675 [D loss: 0.008805, acc.: 100.00%] [G loss: 0.033566]\n",
      "epoch:20 step:15676 [D loss: 0.013908, acc.: 100.00%] [G loss: 0.008720]\n",
      "epoch:20 step:15677 [D loss: 0.013585, acc.: 100.00%] [G loss: 3.130154]\n",
      "epoch:20 step:15678 [D loss: 0.026831, acc.: 100.00%] [G loss: 0.090052]\n",
      "epoch:20 step:15679 [D loss: 0.081825, acc.: 99.22%] [G loss: 1.028199]\n",
      "epoch:20 step:15680 [D loss: 0.003843, acc.: 100.00%] [G loss: 1.823519]\n",
      "epoch:20 step:15681 [D loss: 0.356841, acc.: 83.59%] [G loss: 0.030839]\n",
      "epoch:20 step:15682 [D loss: 0.385558, acc.: 81.25%] [G loss: 5.517408]\n",
      "epoch:20 step:15683 [D loss: 0.423269, acc.: 79.69%] [G loss: 2.491128]\n",
      "epoch:20 step:15684 [D loss: 0.037194, acc.: 99.22%] [G loss: 3.725763]\n",
      "epoch:20 step:15685 [D loss: 0.022830, acc.: 99.22%] [G loss: 4.168701]\n",
      "epoch:20 step:15686 [D loss: 0.014351, acc.: 100.00%] [G loss: 2.780052]\n",
      "epoch:20 step:15687 [D loss: 0.051148, acc.: 98.44%] [G loss: 2.989416]\n",
      "epoch:20 step:15688 [D loss: 0.092870, acc.: 96.09%] [G loss: 2.409703]\n",
      "epoch:20 step:15689 [D loss: 0.068054, acc.: 98.44%] [G loss: 4.066278]\n",
      "epoch:20 step:15690 [D loss: 0.010946, acc.: 100.00%] [G loss: 4.341804]\n",
      "epoch:20 step:15691 [D loss: 0.010195, acc.: 100.00%] [G loss: 4.246774]\n",
      "epoch:20 step:15692 [D loss: 0.034195, acc.: 98.44%] [G loss: 3.190340]\n",
      "epoch:20 step:15693 [D loss: 0.066617, acc.: 100.00%] [G loss: 4.519745]\n",
      "epoch:20 step:15694 [D loss: 0.013511, acc.: 100.00%] [G loss: 4.808987]\n",
      "epoch:20 step:15695 [D loss: 0.030680, acc.: 99.22%] [G loss: 3.268120]\n",
      "epoch:20 step:15696 [D loss: 0.066920, acc.: 99.22%] [G loss: 2.605289]\n",
      "epoch:20 step:15697 [D loss: 0.047302, acc.: 100.00%] [G loss: 3.021278]\n",
      "epoch:20 step:15698 [D loss: 0.036254, acc.: 99.22%] [G loss: 5.616082]\n",
      "epoch:20 step:15699 [D loss: 0.024640, acc.: 99.22%] [G loss: 0.975717]\n",
      "epoch:20 step:15700 [D loss: 0.028453, acc.: 100.00%] [G loss: 5.707200]\n",
      "epoch:20 step:15701 [D loss: 0.039399, acc.: 99.22%] [G loss: 5.180259]\n",
      "epoch:20 step:15702 [D loss: 0.018753, acc.: 99.22%] [G loss: 0.208698]\n",
      "epoch:20 step:15703 [D loss: 0.289184, acc.: 84.38%] [G loss: 4.624227]\n",
      "epoch:20 step:15704 [D loss: 1.373220, acc.: 56.25%] [G loss: 3.087091]\n",
      "epoch:20 step:15705 [D loss: 0.009105, acc.: 100.00%] [G loss: 1.878754]\n",
      "epoch:20 step:15706 [D loss: 0.008105, acc.: 100.00%] [G loss: 1.173389]\n",
      "epoch:20 step:15707 [D loss: 0.003789, acc.: 100.00%] [G loss: 0.702038]\n",
      "epoch:20 step:15708 [D loss: 0.019143, acc.: 99.22%] [G loss: 0.744088]\n",
      "epoch:20 step:15709 [D loss: 0.018798, acc.: 100.00%] [G loss: 2.102158]\n",
      "epoch:20 step:15710 [D loss: 0.013970, acc.: 100.00%] [G loss: 0.416274]\n",
      "epoch:20 step:15711 [D loss: 0.005562, acc.: 100.00%] [G loss: 0.370796]\n",
      "epoch:20 step:15712 [D loss: 0.008620, acc.: 100.00%] [G loss: 0.507787]\n",
      "epoch:20 step:15713 [D loss: 0.037694, acc.: 99.22%] [G loss: 0.854266]\n",
      "epoch:20 step:15714 [D loss: 0.018030, acc.: 100.00%] [G loss: 1.810572]\n",
      "epoch:20 step:15715 [D loss: 0.007734, acc.: 100.00%] [G loss: 1.091297]\n",
      "epoch:20 step:15716 [D loss: 0.033677, acc.: 99.22%] [G loss: 1.273622]\n",
      "epoch:20 step:15717 [D loss: 0.021656, acc.: 99.22%] [G loss: 2.920876]\n",
      "epoch:20 step:15718 [D loss: 0.008576, acc.: 100.00%] [G loss: 3.881663]\n",
      "epoch:20 step:15719 [D loss: 0.056990, acc.: 99.22%] [G loss: 1.339351]\n",
      "epoch:20 step:15720 [D loss: 0.023675, acc.: 100.00%] [G loss: 0.201878]\n",
      "epoch:20 step:15721 [D loss: 0.017521, acc.: 100.00%] [G loss: 0.379797]\n",
      "epoch:20 step:15722 [D loss: 0.048972, acc.: 100.00%] [G loss: 1.733829]\n",
      "epoch:20 step:15723 [D loss: 0.092156, acc.: 99.22%] [G loss: 0.225074]\n",
      "epoch:20 step:15724 [D loss: 0.012188, acc.: 100.00%] [G loss: 0.198777]\n",
      "epoch:20 step:15725 [D loss: 0.149062, acc.: 95.31%] [G loss: 5.633282]\n",
      "epoch:20 step:15726 [D loss: 0.045750, acc.: 97.66%] [G loss: 4.445896]\n",
      "epoch:20 step:15727 [D loss: 0.225603, acc.: 87.50%] [G loss: 1.110263]\n",
      "epoch:20 step:15728 [D loss: 0.017768, acc.: 99.22%] [G loss: 0.320899]\n",
      "epoch:20 step:15729 [D loss: 0.023157, acc.: 100.00%] [G loss: 0.741700]\n",
      "epoch:20 step:15730 [D loss: 0.081235, acc.: 96.88%] [G loss: 4.568182]\n",
      "epoch:20 step:15731 [D loss: 0.012698, acc.: 100.00%] [G loss: 2.970257]\n",
      "epoch:20 step:15732 [D loss: 0.005485, acc.: 100.00%] [G loss: 2.919163]\n",
      "epoch:20 step:15733 [D loss: 0.016107, acc.: 100.00%] [G loss: 0.913761]\n",
      "epoch:20 step:15734 [D loss: 0.058651, acc.: 99.22%] [G loss: 0.833110]\n",
      "epoch:20 step:15735 [D loss: 0.012266, acc.: 100.00%] [G loss: 1.006874]\n",
      "epoch:20 step:15736 [D loss: 0.174376, acc.: 92.19%] [G loss: 5.452758]\n",
      "epoch:20 step:15737 [D loss: 0.297483, acc.: 84.38%] [G loss: 3.655202]\n",
      "epoch:20 step:15738 [D loss: 0.006367, acc.: 100.00%] [G loss: 0.946674]\n",
      "epoch:20 step:15739 [D loss: 0.023235, acc.: 99.22%] [G loss: 2.845175]\n",
      "epoch:20 step:15740 [D loss: 0.007815, acc.: 100.00%] [G loss: 4.065394]\n",
      "epoch:20 step:15741 [D loss: 0.006510, acc.: 100.00%] [G loss: 3.375213]\n",
      "epoch:20 step:15742 [D loss: 0.010142, acc.: 100.00%] [G loss: 2.867829]\n",
      "epoch:20 step:15743 [D loss: 0.012841, acc.: 100.00%] [G loss: 2.930295]\n",
      "epoch:20 step:15744 [D loss: 0.006195, acc.: 100.00%] [G loss: 2.908623]\n",
      "epoch:20 step:15745 [D loss: 0.023970, acc.: 100.00%] [G loss: 2.993032]\n",
      "epoch:20 step:15746 [D loss: 0.010677, acc.: 100.00%] [G loss: 4.497270]\n",
      "epoch:20 step:15747 [D loss: 0.007579, acc.: 100.00%] [G loss: 1.196426]\n",
      "epoch:20 step:15748 [D loss: 0.009589, acc.: 100.00%] [G loss: 6.077441]\n",
      "epoch:20 step:15749 [D loss: 0.012332, acc.: 100.00%] [G loss: 1.270080]\n",
      "epoch:20 step:15750 [D loss: 0.004752, acc.: 100.00%] [G loss: 2.157902]\n",
      "epoch:20 step:15751 [D loss: 0.071424, acc.: 97.66%] [G loss: 4.370359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15752 [D loss: 0.033538, acc.: 99.22%] [G loss: 4.559119]\n",
      "epoch:20 step:15753 [D loss: 0.007203, acc.: 100.00%] [G loss: 3.788072]\n",
      "epoch:20 step:15754 [D loss: 0.011535, acc.: 100.00%] [G loss: 1.871241]\n",
      "epoch:20 step:15755 [D loss: 0.040182, acc.: 99.22%] [G loss: 3.171058]\n",
      "epoch:20 step:15756 [D loss: 0.018172, acc.: 100.00%] [G loss: 3.481850]\n",
      "epoch:20 step:15757 [D loss: 0.017122, acc.: 99.22%] [G loss: 3.083102]\n",
      "epoch:20 step:15758 [D loss: 0.045516, acc.: 100.00%] [G loss: 4.164345]\n",
      "epoch:20 step:15759 [D loss: 0.054162, acc.: 98.44%] [G loss: 2.879741]\n",
      "epoch:20 step:15760 [D loss: 0.104106, acc.: 97.66%] [G loss: 5.125288]\n",
      "epoch:20 step:15761 [D loss: 0.005726, acc.: 100.00%] [G loss: 0.876897]\n",
      "epoch:20 step:15762 [D loss: 0.002241, acc.: 100.00%] [G loss: 4.781316]\n",
      "epoch:20 step:15763 [D loss: 0.009001, acc.: 100.00%] [G loss: 6.061224]\n",
      "epoch:20 step:15764 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.830185]\n",
      "epoch:20 step:15765 [D loss: 0.579345, acc.: 70.31%] [G loss: 8.295586]\n",
      "epoch:20 step:15766 [D loss: 2.539953, acc.: 50.00%] [G loss: 4.643276]\n",
      "epoch:20 step:15767 [D loss: 0.076433, acc.: 99.22%] [G loss: 2.312035]\n",
      "epoch:20 step:15768 [D loss: 0.005834, acc.: 100.00%] [G loss: 1.380610]\n",
      "epoch:20 step:15769 [D loss: 0.004713, acc.: 100.00%] [G loss: 1.281430]\n",
      "epoch:20 step:15770 [D loss: 0.015470, acc.: 100.00%] [G loss: 0.835302]\n",
      "epoch:20 step:15771 [D loss: 0.062117, acc.: 98.44%] [G loss: 0.943104]\n",
      "epoch:20 step:15772 [D loss: 0.007031, acc.: 100.00%] [G loss: 0.571049]\n",
      "epoch:20 step:15773 [D loss: 0.011008, acc.: 100.00%] [G loss: 2.241330]\n",
      "epoch:20 step:15774 [D loss: 0.007706, acc.: 100.00%] [G loss: 0.896431]\n",
      "epoch:20 step:15775 [D loss: 0.019950, acc.: 100.00%] [G loss: 1.038025]\n",
      "epoch:20 step:15776 [D loss: 0.105541, acc.: 97.66%] [G loss: 0.440628]\n",
      "epoch:20 step:15777 [D loss: 0.013473, acc.: 100.00%] [G loss: 2.775723]\n",
      "epoch:20 step:15778 [D loss: 0.133467, acc.: 96.09%] [G loss: 1.565249]\n",
      "epoch:20 step:15779 [D loss: 0.011760, acc.: 100.00%] [G loss: 1.745722]\n",
      "epoch:20 step:15780 [D loss: 0.191818, acc.: 89.06%] [G loss: 0.430880]\n",
      "epoch:20 step:15781 [D loss: 0.045881, acc.: 99.22%] [G loss: 0.900154]\n",
      "epoch:20 step:15782 [D loss: 0.033190, acc.: 99.22%] [G loss: 1.375362]\n",
      "epoch:20 step:15783 [D loss: 0.078658, acc.: 99.22%] [G loss: 0.886688]\n",
      "epoch:20 step:15784 [D loss: 0.009220, acc.: 100.00%] [G loss: 4.288521]\n",
      "epoch:20 step:15785 [D loss: 0.018339, acc.: 100.00%] [G loss: 1.686983]\n",
      "epoch:20 step:15786 [D loss: 0.031607, acc.: 99.22%] [G loss: 0.716595]\n",
      "epoch:20 step:15787 [D loss: 0.040826, acc.: 100.00%] [G loss: 1.360711]\n",
      "epoch:20 step:15788 [D loss: 0.009111, acc.: 100.00%] [G loss: 0.461238]\n",
      "epoch:20 step:15789 [D loss: 0.027862, acc.: 100.00%] [G loss: 0.594347]\n",
      "epoch:20 step:15790 [D loss: 0.013417, acc.: 100.00%] [G loss: 0.424196]\n",
      "epoch:20 step:15791 [D loss: 0.009921, acc.: 100.00%] [G loss: 0.369450]\n",
      "epoch:20 step:15792 [D loss: 0.014102, acc.: 100.00%] [G loss: 0.290104]\n",
      "epoch:20 step:15793 [D loss: 0.010901, acc.: 100.00%] [G loss: 0.233040]\n",
      "epoch:20 step:15794 [D loss: 0.009353, acc.: 100.00%] [G loss: 0.058094]\n",
      "epoch:20 step:15795 [D loss: 0.003840, acc.: 100.00%] [G loss: 0.107868]\n",
      "epoch:20 step:15796 [D loss: 0.043586, acc.: 99.22%] [G loss: 0.190994]\n",
      "epoch:20 step:15797 [D loss: 0.023578, acc.: 99.22%] [G loss: 0.086824]\n",
      "epoch:20 step:15798 [D loss: 0.002263, acc.: 100.00%] [G loss: 0.088295]\n",
      "epoch:20 step:15799 [D loss: 0.018487, acc.: 100.00%] [G loss: 0.029302]\n",
      "epoch:20 step:15800 [D loss: 0.003265, acc.: 100.00%] [G loss: 0.026657]\n",
      "epoch:20 step:15801 [D loss: 0.003522, acc.: 100.00%] [G loss: 0.067681]\n",
      "epoch:20 step:15802 [D loss: 0.016347, acc.: 100.00%] [G loss: 0.062377]\n",
      "epoch:20 step:15803 [D loss: 0.008505, acc.: 100.00%] [G loss: 0.109048]\n",
      "epoch:20 step:15804 [D loss: 0.002630, acc.: 100.00%] [G loss: 0.022550]\n",
      "epoch:20 step:15805 [D loss: 0.002699, acc.: 100.00%] [G loss: 1.430327]\n",
      "epoch:20 step:15806 [D loss: 0.016084, acc.: 100.00%] [G loss: 0.020269]\n",
      "epoch:20 step:15807 [D loss: 0.029702, acc.: 100.00%] [G loss: 0.027218]\n",
      "epoch:20 step:15808 [D loss: 0.003445, acc.: 100.00%] [G loss: 0.008495]\n",
      "epoch:20 step:15809 [D loss: 0.003731, acc.: 100.00%] [G loss: 0.306822]\n",
      "epoch:20 step:15810 [D loss: 0.002807, acc.: 100.00%] [G loss: 0.164156]\n",
      "epoch:20 step:15811 [D loss: 0.007629, acc.: 100.00%] [G loss: 0.217885]\n",
      "epoch:20 step:15812 [D loss: 0.006096, acc.: 100.00%] [G loss: 0.016619]\n",
      "epoch:20 step:15813 [D loss: 0.001644, acc.: 100.00%] [G loss: 0.055878]\n",
      "epoch:20 step:15814 [D loss: 0.002292, acc.: 100.00%] [G loss: 0.110873]\n",
      "epoch:20 step:15815 [D loss: 0.003488, acc.: 100.00%] [G loss: 0.136109]\n",
      "epoch:20 step:15816 [D loss: 0.007306, acc.: 100.00%] [G loss: 0.051059]\n",
      "epoch:20 step:15817 [D loss: 0.002425, acc.: 100.00%] [G loss: 0.311248]\n",
      "epoch:20 step:15818 [D loss: 0.002998, acc.: 100.00%] [G loss: 0.003262]\n",
      "epoch:20 step:15819 [D loss: 0.002773, acc.: 100.00%] [G loss: 0.017422]\n",
      "epoch:20 step:15820 [D loss: 0.002407, acc.: 100.00%] [G loss: 0.066212]\n",
      "epoch:20 step:15821 [D loss: 0.077630, acc.: 99.22%] [G loss: 1.504412]\n",
      "epoch:20 step:15822 [D loss: 0.030077, acc.: 99.22%] [G loss: 0.312836]\n",
      "epoch:20 step:15823 [D loss: 0.005320, acc.: 100.00%] [G loss: 2.048036]\n",
      "epoch:20 step:15824 [D loss: 0.015343, acc.: 100.00%] [G loss: 0.902131]\n",
      "epoch:20 step:15825 [D loss: 0.017260, acc.: 100.00%] [G loss: 0.093003]\n",
      "epoch:20 step:15826 [D loss: 0.078511, acc.: 97.66%] [G loss: 0.065641]\n",
      "epoch:20 step:15827 [D loss: 0.003561, acc.: 100.00%] [G loss: 0.258238]\n",
      "epoch:20 step:15828 [D loss: 0.026305, acc.: 99.22%] [G loss: 2.612724]\n",
      "epoch:20 step:15829 [D loss: 0.026815, acc.: 100.00%] [G loss: 0.371057]\n",
      "epoch:20 step:15830 [D loss: 0.029534, acc.: 100.00%] [G loss: 0.011872]\n",
      "epoch:20 step:15831 [D loss: 0.001395, acc.: 100.00%] [G loss: 0.008695]\n",
      "epoch:20 step:15832 [D loss: 0.004724, acc.: 100.00%] [G loss: 0.001360]\n",
      "epoch:20 step:15833 [D loss: 0.056751, acc.: 98.44%] [G loss: 0.000515]\n",
      "epoch:20 step:15834 [D loss: 0.027041, acc.: 100.00%] [G loss: 2.139774]\n",
      "epoch:20 step:15835 [D loss: 0.005436, acc.: 100.00%] [G loss: 0.046884]\n",
      "epoch:20 step:15836 [D loss: 0.002513, acc.: 100.00%] [G loss: 0.013124]\n",
      "epoch:20 step:15837 [D loss: 0.005773, acc.: 100.00%] [G loss: 0.034246]\n",
      "epoch:20 step:15838 [D loss: 0.029919, acc.: 98.44%] [G loss: 0.003723]\n",
      "epoch:20 step:15839 [D loss: 0.015595, acc.: 100.00%] [G loss: 0.032279]\n",
      "epoch:20 step:15840 [D loss: 0.004226, acc.: 100.00%] [G loss: 0.067644]\n",
      "epoch:20 step:15841 [D loss: 0.009002, acc.: 100.00%] [G loss: 1.866624]\n",
      "epoch:20 step:15842 [D loss: 0.001821, acc.: 100.00%] [G loss: 0.017615]\n",
      "epoch:20 step:15843 [D loss: 0.015695, acc.: 100.00%] [G loss: 0.399671]\n",
      "epoch:20 step:15844 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.213846]\n",
      "epoch:20 step:15845 [D loss: 0.002535, acc.: 100.00%] [G loss: 0.002508]\n",
      "epoch:20 step:15846 [D loss: 0.001864, acc.: 100.00%] [G loss: 0.118264]\n",
      "epoch:20 step:15847 [D loss: 0.009325, acc.: 100.00%] [G loss: 0.173410]\n",
      "epoch:20 step:15848 [D loss: 0.013140, acc.: 100.00%] [G loss: 0.002943]\n",
      "epoch:20 step:15849 [D loss: 0.010748, acc.: 100.00%] [G loss: 0.316498]\n",
      "epoch:20 step:15850 [D loss: 0.008446, acc.: 100.00%] [G loss: 0.037980]\n",
      "epoch:20 step:15851 [D loss: 0.001272, acc.: 100.00%] [G loss: 0.015138]\n",
      "epoch:20 step:15852 [D loss: 0.032287, acc.: 99.22%] [G loss: 0.000164]\n",
      "epoch:20 step:15853 [D loss: 0.009796, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:20 step:15854 [D loss: 0.000994, acc.: 100.00%] [G loss: 0.083736]\n",
      "epoch:20 step:15855 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.004416]\n",
      "epoch:20 step:15856 [D loss: 0.002506, acc.: 100.00%] [G loss: 0.002395]\n",
      "epoch:20 step:15857 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.089730]\n",
      "epoch:20 step:15858 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:20 step:15859 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.051488]\n",
      "epoch:20 step:15860 [D loss: 0.013943, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:20 step:15861 [D loss: 0.002213, acc.: 100.00%] [G loss: 0.143355]\n",
      "epoch:20 step:15862 [D loss: 0.004547, acc.: 100.00%] [G loss: 0.003019]\n",
      "epoch:20 step:15863 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.066325]\n",
      "epoch:20 step:15864 [D loss: 0.002282, acc.: 100.00%] [G loss: 0.003691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15865 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:20 step:15866 [D loss: 0.001336, acc.: 100.00%] [G loss: 0.001932]\n",
      "epoch:20 step:15867 [D loss: 0.005771, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:20 step:15868 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.000704]\n",
      "epoch:20 step:15869 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:20 step:15870 [D loss: 0.002531, acc.: 100.00%] [G loss: 0.042660]\n",
      "epoch:20 step:15871 [D loss: 0.001078, acc.: 100.00%] [G loss: 0.041444]\n",
      "epoch:20 step:15872 [D loss: 0.001609, acc.: 100.00%] [G loss: 0.052663]\n",
      "epoch:20 step:15873 [D loss: 0.001079, acc.: 100.00%] [G loss: 0.002578]\n",
      "epoch:20 step:15874 [D loss: 0.003996, acc.: 100.00%] [G loss: 0.407438]\n",
      "epoch:20 step:15875 [D loss: 0.001592, acc.: 100.00%] [G loss: 0.270667]\n",
      "epoch:20 step:15876 [D loss: 0.025860, acc.: 100.00%] [G loss: 0.001588]\n",
      "epoch:20 step:15877 [D loss: 0.004182, acc.: 100.00%] [G loss: 1.115068]\n",
      "epoch:20 step:15878 [D loss: 0.013283, acc.: 99.22%] [G loss: 0.036442]\n",
      "epoch:20 step:15879 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.006765]\n",
      "epoch:20 step:15880 [D loss: 0.003531, acc.: 100.00%] [G loss: 0.004295]\n",
      "epoch:20 step:15881 [D loss: 0.048846, acc.: 98.44%] [G loss: 0.000131]\n",
      "epoch:20 step:15882 [D loss: 0.027568, acc.: 100.00%] [G loss: 0.055635]\n",
      "epoch:20 step:15883 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.003326]\n",
      "epoch:20 step:15884 [D loss: 0.008211, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:20 step:15885 [D loss: 0.001338, acc.: 100.00%] [G loss: 0.119450]\n",
      "epoch:20 step:15886 [D loss: 0.001958, acc.: 100.00%] [G loss: 0.001307]\n",
      "epoch:20 step:15887 [D loss: 0.012231, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:20 step:15888 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.037822]\n",
      "epoch:20 step:15889 [D loss: 0.009182, acc.: 100.00%] [G loss: 0.020246]\n",
      "epoch:20 step:15890 [D loss: 0.004736, acc.: 100.00%] [G loss: 0.350711]\n",
      "epoch:20 step:15891 [D loss: 0.004639, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:20 step:15892 [D loss: 0.002588, acc.: 100.00%] [G loss: 0.000878]\n",
      "epoch:20 step:15893 [D loss: 0.004055, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:20 step:15894 [D loss: 0.004834, acc.: 100.00%] [G loss: 0.095727]\n",
      "epoch:20 step:15895 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.188878]\n",
      "epoch:20 step:15896 [D loss: 0.001345, acc.: 100.00%] [G loss: 0.248047]\n",
      "epoch:20 step:15897 [D loss: 0.002685, acc.: 100.00%] [G loss: 0.613951]\n",
      "epoch:20 step:15898 [D loss: 0.008755, acc.: 100.00%] [G loss: 0.012754]\n",
      "epoch:20 step:15899 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.295564]\n",
      "epoch:20 step:15900 [D loss: 0.019506, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:20 step:15901 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.004973]\n",
      "epoch:20 step:15902 [D loss: 0.002094, acc.: 100.00%] [G loss: 0.260673]\n",
      "epoch:20 step:15903 [D loss: 0.002558, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:20 step:15904 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.005124]\n",
      "epoch:20 step:15905 [D loss: 0.001130, acc.: 100.00%] [G loss: 0.021850]\n",
      "epoch:20 step:15906 [D loss: 0.002399, acc.: 100.00%] [G loss: 0.446763]\n",
      "epoch:20 step:15907 [D loss: 0.012294, acc.: 100.00%] [G loss: 1.328620]\n",
      "epoch:20 step:15908 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.002991]\n",
      "epoch:20 step:15909 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.180455]\n",
      "epoch:20 step:15910 [D loss: 0.006679, acc.: 100.00%] [G loss: 0.002104]\n",
      "epoch:20 step:15911 [D loss: 0.025370, acc.: 99.22%] [G loss: 0.014144]\n",
      "epoch:20 step:15912 [D loss: 0.044636, acc.: 100.00%] [G loss: 0.006207]\n",
      "epoch:20 step:15913 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.037168]\n",
      "epoch:20 step:15914 [D loss: 0.026432, acc.: 99.22%] [G loss: 0.004415]\n",
      "epoch:20 step:15915 [D loss: 0.002456, acc.: 100.00%] [G loss: 0.268031]\n",
      "epoch:20 step:15916 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.001480]\n",
      "epoch:20 step:15917 [D loss: 0.004431, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:20 step:15918 [D loss: 0.004939, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:20 step:15919 [D loss: 0.117754, acc.: 95.31%] [G loss: 3.847052]\n",
      "epoch:20 step:15920 [D loss: 5.003347, acc.: 2.34%] [G loss: 9.706999]\n",
      "epoch:20 step:15921 [D loss: 4.033226, acc.: 50.00%] [G loss: 7.713428]\n",
      "epoch:20 step:15922 [D loss: 3.082595, acc.: 50.00%] [G loss: 4.744197]\n",
      "epoch:20 step:15923 [D loss: 2.182711, acc.: 49.22%] [G loss: 2.502069]\n",
      "epoch:20 step:15924 [D loss: 0.967398, acc.: 53.91%] [G loss: 1.556717]\n",
      "epoch:20 step:15925 [D loss: 0.600247, acc.: 63.28%] [G loss: 0.948806]\n",
      "epoch:20 step:15926 [D loss: 0.592535, acc.: 65.62%] [G loss: 1.055322]\n",
      "epoch:20 step:15927 [D loss: 0.530987, acc.: 75.00%] [G loss: 0.912394]\n",
      "epoch:20 step:15928 [D loss: 0.297684, acc.: 94.53%] [G loss: 0.753223]\n",
      "epoch:20 step:15929 [D loss: 0.346951, acc.: 90.62%] [G loss: 0.681536]\n",
      "epoch:20 step:15930 [D loss: 0.477309, acc.: 78.91%] [G loss: 0.502255]\n",
      "epoch:20 step:15931 [D loss: 0.171683, acc.: 99.22%] [G loss: 0.560408]\n",
      "epoch:20 step:15932 [D loss: 0.242546, acc.: 99.22%] [G loss: 1.141203]\n",
      "epoch:20 step:15933 [D loss: 0.180964, acc.: 98.44%] [G loss: 0.483954]\n",
      "epoch:20 step:15934 [D loss: 0.286389, acc.: 92.19%] [G loss: 1.390170]\n",
      "epoch:20 step:15935 [D loss: 0.172265, acc.: 98.44%] [G loss: 0.746791]\n",
      "epoch:20 step:15936 [D loss: 0.233264, acc.: 95.31%] [G loss: 0.203318]\n",
      "epoch:20 step:15937 [D loss: 0.152450, acc.: 97.66%] [G loss: 0.627856]\n",
      "epoch:20 step:15938 [D loss: 0.268015, acc.: 86.72%] [G loss: 0.444137]\n",
      "epoch:20 step:15939 [D loss: 0.374027, acc.: 79.69%] [G loss: 0.263906]\n",
      "epoch:20 step:15940 [D loss: 0.232031, acc.: 92.19%] [G loss: 0.072998]\n",
      "epoch:20 step:15941 [D loss: 0.208180, acc.: 91.41%] [G loss: 2.124565]\n",
      "epoch:20 step:15942 [D loss: 0.126900, acc.: 97.66%] [G loss: 0.044815]\n",
      "epoch:20 step:15943 [D loss: 0.192108, acc.: 93.75%] [G loss: 1.522017]\n",
      "epoch:20 step:15944 [D loss: 0.328522, acc.: 85.16%] [G loss: 0.086988]\n",
      "epoch:20 step:15945 [D loss: 0.058074, acc.: 98.44%] [G loss: 1.509651]\n",
      "epoch:20 step:15946 [D loss: 0.109212, acc.: 97.66%] [G loss: 0.037918]\n",
      "epoch:20 step:15947 [D loss: 0.077832, acc.: 99.22%] [G loss: 0.040138]\n",
      "epoch:20 step:15948 [D loss: 0.044076, acc.: 100.00%] [G loss: 0.029014]\n",
      "epoch:20 step:15949 [D loss: 0.095677, acc.: 98.44%] [G loss: 2.384377]\n",
      "epoch:20 step:15950 [D loss: 0.064387, acc.: 99.22%] [G loss: 0.032247]\n",
      "epoch:20 step:15951 [D loss: 0.054945, acc.: 100.00%] [G loss: 0.073535]\n",
      "epoch:20 step:15952 [D loss: 0.066606, acc.: 98.44%] [G loss: 0.060768]\n",
      "epoch:20 step:15953 [D loss: 0.122563, acc.: 97.66%] [G loss: 1.591356]\n",
      "epoch:20 step:15954 [D loss: 0.099670, acc.: 97.66%] [G loss: 0.052967]\n",
      "epoch:20 step:15955 [D loss: 0.060558, acc.: 99.22%] [G loss: 0.006914]\n",
      "epoch:20 step:15956 [D loss: 0.155680, acc.: 96.09%] [G loss: 0.048423]\n",
      "epoch:20 step:15957 [D loss: 0.047498, acc.: 100.00%] [G loss: 0.376215]\n",
      "epoch:20 step:15958 [D loss: 0.048461, acc.: 98.44%] [G loss: 0.027617]\n",
      "epoch:20 step:15959 [D loss: 0.122603, acc.: 96.09%] [G loss: 1.844999]\n",
      "epoch:20 step:15960 [D loss: 0.029482, acc.: 100.00%] [G loss: 1.798921]\n",
      "epoch:20 step:15961 [D loss: 0.048954, acc.: 100.00%] [G loss: 0.003831]\n",
      "epoch:20 step:15962 [D loss: 0.079760, acc.: 97.66%] [G loss: 0.693241]\n",
      "epoch:20 step:15963 [D loss: 0.077078, acc.: 96.88%] [G loss: 0.277876]\n",
      "epoch:20 step:15964 [D loss: 0.046396, acc.: 100.00%] [G loss: 0.583552]\n",
      "epoch:20 step:15965 [D loss: 0.089342, acc.: 97.66%] [G loss: 0.024063]\n",
      "epoch:20 step:15966 [D loss: 0.029650, acc.: 100.00%] [G loss: 0.265280]\n",
      "epoch:20 step:15967 [D loss: 0.062732, acc.: 99.22%] [G loss: 0.007143]\n",
      "epoch:20 step:15968 [D loss: 0.014665, acc.: 100.00%] [G loss: 0.006437]\n",
      "epoch:20 step:15969 [D loss: 0.016204, acc.: 100.00%] [G loss: 0.005744]\n",
      "epoch:20 step:15970 [D loss: 0.058666, acc.: 99.22%] [G loss: 0.497755]\n",
      "epoch:20 step:15971 [D loss: 0.090594, acc.: 98.44%] [G loss: 0.850100]\n",
      "epoch:20 step:15972 [D loss: 0.020644, acc.: 100.00%] [G loss: 0.054776]\n",
      "epoch:20 step:15973 [D loss: 0.184888, acc.: 96.88%] [G loss: 0.027192]\n",
      "epoch:20 step:15974 [D loss: 0.062143, acc.: 98.44%] [G loss: 1.632547]\n",
      "epoch:20 step:15975 [D loss: 0.016012, acc.: 100.00%] [G loss: 0.083755]\n",
      "epoch:20 step:15976 [D loss: 0.182759, acc.: 95.31%] [G loss: 1.771441]\n",
      "epoch:20 step:15977 [D loss: 0.055553, acc.: 99.22%] [G loss: 0.139962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15978 [D loss: 0.157280, acc.: 93.75%] [G loss: 0.857129]\n",
      "epoch:20 step:15979 [D loss: 0.022099, acc.: 100.00%] [G loss: 0.274202]\n",
      "epoch:20 step:15980 [D loss: 0.052474, acc.: 98.44%] [G loss: 0.036017]\n",
      "epoch:20 step:15981 [D loss: 0.727338, acc.: 65.62%] [G loss: 7.323744]\n",
      "epoch:20 step:15982 [D loss: 0.420498, acc.: 85.16%] [G loss: 5.520527]\n",
      "epoch:20 step:15983 [D loss: 0.300760, acc.: 89.06%] [G loss: 2.835558]\n",
      "epoch:20 step:15984 [D loss: 0.313056, acc.: 88.28%] [G loss: 0.144429]\n",
      "epoch:20 step:15985 [D loss: 0.035547, acc.: 99.22%] [G loss: 0.456374]\n",
      "epoch:20 step:15986 [D loss: 0.174681, acc.: 92.97%] [G loss: 0.067269]\n",
      "epoch:20 step:15987 [D loss: 0.055703, acc.: 98.44%] [G loss: 0.039163]\n",
      "epoch:20 step:15988 [D loss: 0.025835, acc.: 100.00%] [G loss: 0.013736]\n",
      "epoch:20 step:15989 [D loss: 0.017486, acc.: 100.00%] [G loss: 0.013445]\n",
      "epoch:20 step:15990 [D loss: 0.031753, acc.: 100.00%] [G loss: 0.016007]\n",
      "epoch:20 step:15991 [D loss: 0.048401, acc.: 100.00%] [G loss: 0.015774]\n",
      "epoch:20 step:15992 [D loss: 0.094412, acc.: 97.66%] [G loss: 4.839917]\n",
      "epoch:20 step:15993 [D loss: 0.047062, acc.: 100.00%] [G loss: 0.125820]\n",
      "epoch:20 step:15994 [D loss: 0.054977, acc.: 96.88%] [G loss: 0.013402]\n",
      "epoch:20 step:15995 [D loss: 0.043535, acc.: 99.22%] [G loss: 0.002907]\n",
      "epoch:20 step:15996 [D loss: 0.087609, acc.: 97.66%] [G loss: 0.044839]\n",
      "epoch:20 step:15997 [D loss: 0.014310, acc.: 100.00%] [G loss: 5.575792]\n",
      "epoch:20 step:15998 [D loss: 0.070597, acc.: 98.44%] [G loss: 4.127574]\n",
      "epoch:20 step:15999 [D loss: 0.016269, acc.: 100.00%] [G loss: 0.025712]\n",
      "epoch:20 step:16000 [D loss: 0.035762, acc.: 100.00%] [G loss: 0.020247]\n",
      "epoch:20 step:16001 [D loss: 0.059354, acc.: 99.22%] [G loss: 0.054528]\n",
      "epoch:20 step:16002 [D loss: 0.017639, acc.: 100.00%] [G loss: 0.123161]\n",
      "epoch:20 step:16003 [D loss: 0.094653, acc.: 98.44%] [G loss: 0.055135]\n",
      "epoch:20 step:16004 [D loss: 0.037818, acc.: 99.22%] [G loss: 4.439598]\n",
      "epoch:20 step:16005 [D loss: 0.021714, acc.: 100.00%] [G loss: 0.030150]\n",
      "epoch:20 step:16006 [D loss: 0.078018, acc.: 98.44%] [G loss: 3.356458]\n",
      "epoch:20 step:16007 [D loss: 0.028681, acc.: 100.00%] [G loss: 1.698090]\n",
      "epoch:20 step:16008 [D loss: 0.511104, acc.: 78.12%] [G loss: 6.479434]\n",
      "epoch:20 step:16009 [D loss: 1.735355, acc.: 50.78%] [G loss: 1.976532]\n",
      "epoch:20 step:16010 [D loss: 0.436161, acc.: 78.12%] [G loss: 0.345532]\n",
      "epoch:20 step:16011 [D loss: 0.125277, acc.: 93.75%] [G loss: 0.503206]\n",
      "epoch:20 step:16012 [D loss: 0.074548, acc.: 99.22%] [G loss: 0.894053]\n",
      "epoch:20 step:16013 [D loss: 0.098644, acc.: 97.66%] [G loss: 2.176896]\n",
      "epoch:20 step:16014 [D loss: 0.040088, acc.: 100.00%] [G loss: 0.253890]\n",
      "epoch:20 step:16015 [D loss: 0.058111, acc.: 99.22%] [G loss: 0.750888]\n",
      "epoch:20 step:16016 [D loss: 0.036223, acc.: 99.22%] [G loss: 1.429484]\n",
      "epoch:20 step:16017 [D loss: 0.104327, acc.: 96.09%] [G loss: 0.495086]\n",
      "epoch:20 step:16018 [D loss: 0.063523, acc.: 99.22%] [G loss: 2.889536]\n",
      "epoch:20 step:16019 [D loss: 0.080001, acc.: 98.44%] [G loss: 1.173539]\n",
      "epoch:20 step:16020 [D loss: 0.247948, acc.: 89.84%] [G loss: 1.230597]\n",
      "epoch:20 step:16021 [D loss: 0.584618, acc.: 69.53%] [G loss: 0.012856]\n",
      "epoch:20 step:16022 [D loss: 0.039628, acc.: 100.00%] [G loss: 0.003211]\n",
      "epoch:20 step:16023 [D loss: 0.040811, acc.: 97.66%] [G loss: 0.029153]\n",
      "epoch:20 step:16024 [D loss: 0.006676, acc.: 100.00%] [G loss: 0.018175]\n",
      "epoch:20 step:16025 [D loss: 0.009510, acc.: 100.00%] [G loss: 0.029721]\n",
      "epoch:20 step:16026 [D loss: 0.049434, acc.: 99.22%] [G loss: 0.021775]\n",
      "epoch:20 step:16027 [D loss: 0.012326, acc.: 100.00%] [G loss: 0.012456]\n",
      "epoch:20 step:16028 [D loss: 0.008058, acc.: 100.00%] [G loss: 0.009193]\n",
      "epoch:20 step:16029 [D loss: 0.044234, acc.: 98.44%] [G loss: 2.428461]\n",
      "epoch:20 step:16030 [D loss: 0.006980, acc.: 100.00%] [G loss: 3.358119]\n",
      "epoch:20 step:16031 [D loss: 0.028475, acc.: 98.44%] [G loss: 1.244607]\n",
      "epoch:20 step:16032 [D loss: 0.042239, acc.: 100.00%] [G loss: 0.195025]\n",
      "epoch:20 step:16033 [D loss: 0.065733, acc.: 98.44%] [G loss: 0.301524]\n",
      "epoch:20 step:16034 [D loss: 0.043795, acc.: 99.22%] [G loss: 0.039010]\n",
      "epoch:20 step:16035 [D loss: 0.038843, acc.: 99.22%] [G loss: 0.011819]\n",
      "epoch:20 step:16036 [D loss: 0.022463, acc.: 100.00%] [G loss: 0.210907]\n",
      "epoch:20 step:16037 [D loss: 0.073811, acc.: 99.22%] [G loss: 0.028378]\n",
      "epoch:20 step:16038 [D loss: 0.019776, acc.: 100.00%] [G loss: 0.114646]\n",
      "epoch:20 step:16039 [D loss: 0.041672, acc.: 98.44%] [G loss: 0.320455]\n",
      "epoch:20 step:16040 [D loss: 0.038277, acc.: 100.00%] [G loss: 0.012145]\n",
      "epoch:20 step:16041 [D loss: 0.027729, acc.: 100.00%] [G loss: 0.005548]\n",
      "epoch:20 step:16042 [D loss: 0.012653, acc.: 100.00%] [G loss: 0.001633]\n",
      "epoch:20 step:16043 [D loss: 0.005517, acc.: 100.00%] [G loss: 0.020966]\n",
      "epoch:20 step:16044 [D loss: 0.058808, acc.: 98.44%] [G loss: 0.000356]\n",
      "epoch:20 step:16045 [D loss: 0.041505, acc.: 98.44%] [G loss: 0.050189]\n",
      "epoch:20 step:16046 [D loss: 0.012342, acc.: 100.00%] [G loss: 0.005879]\n",
      "epoch:20 step:16047 [D loss: 0.001821, acc.: 100.00%] [G loss: 0.030637]\n",
      "epoch:20 step:16048 [D loss: 0.020745, acc.: 99.22%] [G loss: 0.009144]\n",
      "epoch:20 step:16049 [D loss: 0.004241, acc.: 100.00%] [G loss: 0.024951]\n",
      "epoch:20 step:16050 [D loss: 0.007957, acc.: 100.00%] [G loss: 0.264724]\n",
      "epoch:20 step:16051 [D loss: 0.010183, acc.: 100.00%] [G loss: 0.002438]\n",
      "epoch:20 step:16052 [D loss: 0.003037, acc.: 100.00%] [G loss: 0.002051]\n",
      "epoch:20 step:16053 [D loss: 0.010468, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:20 step:16054 [D loss: 0.007042, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:20 step:16055 [D loss: 0.002674, acc.: 100.00%] [G loss: 0.001520]\n",
      "epoch:20 step:16056 [D loss: 0.044088, acc.: 99.22%] [G loss: 0.028041]\n",
      "epoch:20 step:16057 [D loss: 0.017160, acc.: 100.00%] [G loss: 0.172207]\n",
      "epoch:20 step:16058 [D loss: 0.006473, acc.: 100.00%] [G loss: 0.035836]\n",
      "epoch:20 step:16059 [D loss: 0.019324, acc.: 100.00%] [G loss: 0.115561]\n",
      "epoch:20 step:16060 [D loss: 0.021474, acc.: 99.22%] [G loss: 0.136439]\n",
      "epoch:20 step:16061 [D loss: 0.073856, acc.: 99.22%] [G loss: 0.150215]\n",
      "epoch:20 step:16062 [D loss: 0.035245, acc.: 100.00%] [G loss: 0.332491]\n",
      "epoch:20 step:16063 [D loss: 0.027989, acc.: 100.00%] [G loss: 0.992349]\n",
      "epoch:20 step:16064 [D loss: 0.216794, acc.: 90.62%] [G loss: 0.162605]\n",
      "epoch:20 step:16065 [D loss: 0.085649, acc.: 96.09%] [G loss: 4.190159]\n",
      "epoch:20 step:16066 [D loss: 0.161828, acc.: 89.84%] [G loss: 0.770475]\n",
      "epoch:20 step:16067 [D loss: 0.097569, acc.: 96.09%] [G loss: 0.153663]\n",
      "epoch:20 step:16068 [D loss: 0.003783, acc.: 100.00%] [G loss: 2.394691]\n",
      "epoch:20 step:16069 [D loss: 0.041996, acc.: 98.44%] [G loss: 1.377478]\n",
      "epoch:20 step:16070 [D loss: 0.003702, acc.: 100.00%] [G loss: 0.198285]\n",
      "epoch:20 step:16071 [D loss: 0.023734, acc.: 99.22%] [G loss: 0.075956]\n",
      "epoch:20 step:16072 [D loss: 0.026940, acc.: 98.44%] [G loss: 0.092306]\n",
      "epoch:20 step:16073 [D loss: 0.008142, acc.: 100.00%] [G loss: 0.067749]\n",
      "epoch:20 step:16074 [D loss: 0.016249, acc.: 99.22%] [G loss: 0.022070]\n",
      "epoch:20 step:16075 [D loss: 0.003287, acc.: 100.00%] [G loss: 0.054558]\n",
      "epoch:20 step:16076 [D loss: 0.004458, acc.: 100.00%] [G loss: 0.008510]\n",
      "epoch:20 step:16077 [D loss: 0.013036, acc.: 100.00%] [G loss: 0.075280]\n",
      "epoch:20 step:16078 [D loss: 0.009714, acc.: 100.00%] [G loss: 0.014146]\n",
      "epoch:20 step:16079 [D loss: 0.016848, acc.: 100.00%] [G loss: 0.011664]\n",
      "epoch:20 step:16080 [D loss: 0.004938, acc.: 100.00%] [G loss: 0.005247]\n",
      "epoch:20 step:16081 [D loss: 0.003746, acc.: 100.00%] [G loss: 4.145486]\n",
      "epoch:20 step:16082 [D loss: 0.002490, acc.: 100.00%] [G loss: 0.034037]\n",
      "epoch:20 step:16083 [D loss: 0.003987, acc.: 100.00%] [G loss: 0.058007]\n",
      "epoch:20 step:16084 [D loss: 0.005050, acc.: 100.00%] [G loss: 0.307395]\n",
      "epoch:20 step:16085 [D loss: 0.007336, acc.: 100.00%] [G loss: 0.008971]\n",
      "epoch:20 step:16086 [D loss: 0.046739, acc.: 99.22%] [G loss: 0.038441]\n",
      "epoch:20 step:16087 [D loss: 0.026656, acc.: 99.22%] [G loss: 0.150361]\n",
      "epoch:20 step:16088 [D loss: 0.032453, acc.: 99.22%] [G loss: 0.015349]\n",
      "epoch:20 step:16089 [D loss: 0.001491, acc.: 100.00%] [G loss: 0.029906]\n",
      "epoch:20 step:16090 [D loss: 0.003216, acc.: 100.00%] [G loss: 0.113127]\n",
      "epoch:20 step:16091 [D loss: 0.012475, acc.: 100.00%] [G loss: 0.005579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16092 [D loss: 0.005416, acc.: 100.00%] [G loss: 0.030891]\n",
      "epoch:20 step:16093 [D loss: 0.001433, acc.: 100.00%] [G loss: 0.656340]\n",
      "epoch:20 step:16094 [D loss: 0.008225, acc.: 100.00%] [G loss: 0.015838]\n",
      "epoch:20 step:16095 [D loss: 0.013970, acc.: 100.00%] [G loss: 0.003300]\n",
      "epoch:20 step:16096 [D loss: 0.011367, acc.: 100.00%] [G loss: 0.005480]\n",
      "epoch:20 step:16097 [D loss: 0.047718, acc.: 99.22%] [G loss: 0.116544]\n",
      "epoch:20 step:16098 [D loss: 0.127274, acc.: 95.31%] [G loss: 0.000122]\n",
      "epoch:20 step:16099 [D loss: 0.043519, acc.: 98.44%] [G loss: 0.000811]\n",
      "epoch:20 step:16100 [D loss: 0.003235, acc.: 100.00%] [G loss: 0.010522]\n",
      "epoch:20 step:16101 [D loss: 0.022950, acc.: 99.22%] [G loss: 0.641445]\n",
      "epoch:20 step:16102 [D loss: 0.012808, acc.: 100.00%] [G loss: 1.018142]\n",
      "epoch:20 step:16103 [D loss: 0.011651, acc.: 100.00%] [G loss: 0.020392]\n",
      "epoch:20 step:16104 [D loss: 0.057053, acc.: 100.00%] [G loss: 0.879541]\n",
      "epoch:20 step:16105 [D loss: 0.012954, acc.: 100.00%] [G loss: 1.034543]\n",
      "epoch:20 step:16106 [D loss: 0.005788, acc.: 100.00%] [G loss: 0.092894]\n",
      "epoch:20 step:16107 [D loss: 0.018459, acc.: 100.00%] [G loss: 0.044769]\n",
      "epoch:20 step:16108 [D loss: 0.034429, acc.: 99.22%] [G loss: 0.015073]\n",
      "epoch:20 step:16109 [D loss: 0.009984, acc.: 100.00%] [G loss: 0.150688]\n",
      "epoch:20 step:16110 [D loss: 0.006758, acc.: 100.00%] [G loss: 0.142088]\n",
      "epoch:20 step:16111 [D loss: 0.112236, acc.: 96.09%] [G loss: 1.583369]\n",
      "epoch:20 step:16112 [D loss: 1.655256, acc.: 32.81%] [G loss: 7.888481]\n",
      "epoch:20 step:16113 [D loss: 0.368766, acc.: 85.16%] [G loss: 6.560850]\n",
      "epoch:20 step:16114 [D loss: 0.041669, acc.: 98.44%] [G loss: 6.906180]\n",
      "epoch:20 step:16115 [D loss: 0.033315, acc.: 98.44%] [G loss: 3.267337]\n",
      "epoch:20 step:16116 [D loss: 0.007626, acc.: 100.00%] [G loss: 0.779524]\n",
      "epoch:20 step:16117 [D loss: 0.043334, acc.: 100.00%] [G loss: 2.205122]\n",
      "epoch:20 step:16118 [D loss: 0.022489, acc.: 100.00%] [G loss: 1.642031]\n",
      "epoch:20 step:16119 [D loss: 0.023083, acc.: 100.00%] [G loss: 0.523697]\n",
      "epoch:20 step:16120 [D loss: 0.007337, acc.: 100.00%] [G loss: 0.314260]\n",
      "epoch:20 step:16121 [D loss: 0.010471, acc.: 100.00%] [G loss: 0.412653]\n",
      "epoch:20 step:16122 [D loss: 0.011287, acc.: 100.00%] [G loss: 0.527890]\n",
      "epoch:20 step:16123 [D loss: 0.008924, acc.: 100.00%] [G loss: 0.013392]\n",
      "epoch:20 step:16124 [D loss: 0.010737, acc.: 100.00%] [G loss: 0.134637]\n",
      "epoch:20 step:16125 [D loss: 0.019998, acc.: 99.22%] [G loss: 0.100729]\n",
      "epoch:20 step:16126 [D loss: 0.018996, acc.: 100.00%] [G loss: 0.299504]\n",
      "epoch:20 step:16127 [D loss: 0.023057, acc.: 99.22%] [G loss: 0.104517]\n",
      "epoch:20 step:16128 [D loss: 0.005914, acc.: 100.00%] [G loss: 0.134213]\n",
      "epoch:20 step:16129 [D loss: 0.013533, acc.: 100.00%] [G loss: 0.046888]\n",
      "epoch:20 step:16130 [D loss: 0.004891, acc.: 100.00%] [G loss: 0.036725]\n",
      "epoch:20 step:16131 [D loss: 0.007000, acc.: 100.00%] [G loss: 0.040621]\n",
      "epoch:20 step:16132 [D loss: 0.014311, acc.: 100.00%] [G loss: 0.058901]\n",
      "epoch:20 step:16133 [D loss: 0.037869, acc.: 99.22%] [G loss: 0.078577]\n",
      "epoch:20 step:16134 [D loss: 0.018908, acc.: 100.00%] [G loss: 0.038526]\n",
      "epoch:20 step:16135 [D loss: 0.025087, acc.: 100.00%] [G loss: 0.047021]\n",
      "epoch:20 step:16136 [D loss: 0.007071, acc.: 100.00%] [G loss: 0.062890]\n",
      "epoch:20 step:16137 [D loss: 0.032314, acc.: 100.00%] [G loss: 0.234218]\n",
      "epoch:20 step:16138 [D loss: 0.007813, acc.: 100.00%] [G loss: 0.176457]\n",
      "epoch:20 step:16139 [D loss: 0.013153, acc.: 100.00%] [G loss: 0.021803]\n",
      "epoch:20 step:16140 [D loss: 0.002717, acc.: 100.00%] [G loss: 0.185711]\n",
      "epoch:20 step:16141 [D loss: 0.002846, acc.: 100.00%] [G loss: 0.200554]\n",
      "epoch:20 step:16142 [D loss: 0.007363, acc.: 100.00%] [G loss: 0.123725]\n",
      "epoch:20 step:16143 [D loss: 0.003083, acc.: 100.00%] [G loss: 0.045728]\n",
      "epoch:20 step:16144 [D loss: 0.013486, acc.: 100.00%] [G loss: 0.263053]\n",
      "epoch:20 step:16145 [D loss: 0.005414, acc.: 100.00%] [G loss: 0.107256]\n",
      "epoch:20 step:16146 [D loss: 0.008058, acc.: 100.00%] [G loss: 0.068247]\n",
      "epoch:20 step:16147 [D loss: 0.005240, acc.: 100.00%] [G loss: 0.058668]\n",
      "epoch:20 step:16148 [D loss: 0.032700, acc.: 100.00%] [G loss: 0.075559]\n",
      "epoch:20 step:16149 [D loss: 0.149969, acc.: 92.97%] [G loss: 3.900771]\n",
      "epoch:20 step:16150 [D loss: 0.056993, acc.: 98.44%] [G loss: 1.752911]\n",
      "epoch:20 step:16151 [D loss: 0.018862, acc.: 99.22%] [G loss: 2.954934]\n",
      "epoch:20 step:16152 [D loss: 0.094562, acc.: 96.09%] [G loss: 0.716179]\n",
      "epoch:20 step:16153 [D loss: 0.003417, acc.: 100.00%] [G loss: 0.290061]\n",
      "epoch:20 step:16154 [D loss: 0.014288, acc.: 100.00%] [G loss: 0.141966]\n",
      "epoch:20 step:16155 [D loss: 0.077077, acc.: 97.66%] [G loss: 0.077553]\n",
      "epoch:20 step:16156 [D loss: 0.002678, acc.: 100.00%] [G loss: 2.555369]\n",
      "epoch:20 step:16157 [D loss: 0.032574, acc.: 99.22%] [G loss: 0.071450]\n",
      "epoch:20 step:16158 [D loss: 0.001971, acc.: 100.00%] [G loss: 0.738159]\n",
      "epoch:20 step:16159 [D loss: 0.010172, acc.: 99.22%] [G loss: 0.609280]\n",
      "epoch:20 step:16160 [D loss: 0.004245, acc.: 100.00%] [G loss: 0.028127]\n",
      "epoch:20 step:16161 [D loss: 0.002436, acc.: 100.00%] [G loss: 0.351690]\n",
      "epoch:20 step:16162 [D loss: 0.013706, acc.: 100.00%] [G loss: 0.153894]\n",
      "epoch:20 step:16163 [D loss: 0.002542, acc.: 100.00%] [G loss: 0.161091]\n",
      "epoch:20 step:16164 [D loss: 0.014760, acc.: 100.00%] [G loss: 0.211219]\n",
      "epoch:20 step:16165 [D loss: 0.007660, acc.: 100.00%] [G loss: 0.745895]\n",
      "epoch:20 step:16166 [D loss: 0.065794, acc.: 99.22%] [G loss: 1.708929]\n",
      "epoch:20 step:16167 [D loss: 0.016182, acc.: 99.22%] [G loss: 0.231833]\n",
      "epoch:20 step:16168 [D loss: 0.056974, acc.: 98.44%] [G loss: 1.583789]\n",
      "epoch:20 step:16169 [D loss: 0.070680, acc.: 98.44%] [G loss: 0.487785]\n",
      "epoch:20 step:16170 [D loss: 0.033416, acc.: 99.22%] [G loss: 0.408570]\n",
      "epoch:20 step:16171 [D loss: 0.029883, acc.: 100.00%] [G loss: 0.432827]\n",
      "epoch:20 step:16172 [D loss: 0.295980, acc.: 89.84%] [G loss: 2.782014]\n",
      "epoch:20 step:16173 [D loss: 0.400742, acc.: 86.72%] [G loss: 0.722107]\n",
      "epoch:20 step:16174 [D loss: 0.199483, acc.: 91.41%] [G loss: 4.309321]\n",
      "epoch:20 step:16175 [D loss: 0.012731, acc.: 99.22%] [G loss: 9.254025]\n",
      "epoch:20 step:16176 [D loss: 0.073226, acc.: 96.88%] [G loss: 4.097209]\n",
      "epoch:20 step:16177 [D loss: 0.044405, acc.: 97.66%] [G loss: 1.437259]\n",
      "epoch:20 step:16178 [D loss: 0.001855, acc.: 100.00%] [G loss: 1.209508]\n",
      "epoch:20 step:16179 [D loss: 0.087229, acc.: 96.09%] [G loss: 1.991946]\n",
      "epoch:20 step:16180 [D loss: 0.006309, acc.: 100.00%] [G loss: 4.794525]\n",
      "epoch:20 step:16181 [D loss: 0.012253, acc.: 100.00%] [G loss: 1.290284]\n",
      "epoch:20 step:16182 [D loss: 0.042611, acc.: 98.44%] [G loss: 1.049718]\n",
      "epoch:20 step:16183 [D loss: 0.009594, acc.: 100.00%] [G loss: 0.052470]\n",
      "epoch:20 step:16184 [D loss: 0.005972, acc.: 100.00%] [G loss: 0.139265]\n",
      "epoch:20 step:16185 [D loss: 0.011821, acc.: 100.00%] [G loss: 0.174959]\n",
      "epoch:20 step:16186 [D loss: 0.021863, acc.: 100.00%] [G loss: 3.644102]\n",
      "epoch:20 step:16187 [D loss: 0.146309, acc.: 92.19%] [G loss: 1.033895]\n",
      "epoch:20 step:16188 [D loss: 0.061061, acc.: 97.66%] [G loss: 0.830077]\n",
      "epoch:20 step:16189 [D loss: 0.015958, acc.: 100.00%] [G loss: 1.795516]\n",
      "epoch:20 step:16190 [D loss: 0.022811, acc.: 99.22%] [G loss: 1.233292]\n",
      "epoch:20 step:16191 [D loss: 0.131732, acc.: 95.31%] [G loss: 0.992404]\n",
      "epoch:20 step:16192 [D loss: 0.016062, acc.: 100.00%] [G loss: 0.410381]\n",
      "epoch:20 step:16193 [D loss: 0.007211, acc.: 100.00%] [G loss: 0.921144]\n",
      "epoch:20 step:16194 [D loss: 0.002455, acc.: 100.00%] [G loss: 0.151287]\n",
      "epoch:20 step:16195 [D loss: 0.020648, acc.: 100.00%] [G loss: 0.013447]\n",
      "epoch:20 step:16196 [D loss: 0.018053, acc.: 100.00%] [G loss: 0.075314]\n",
      "epoch:20 step:16197 [D loss: 0.039665, acc.: 99.22%] [G loss: 0.061839]\n",
      "epoch:20 step:16198 [D loss: 0.017640, acc.: 99.22%] [G loss: 0.058482]\n",
      "epoch:20 step:16199 [D loss: 0.003391, acc.: 100.00%] [G loss: 0.238049]\n",
      "epoch:20 step:16200 [D loss: 0.019275, acc.: 100.00%] [G loss: 0.051996]\n",
      "epoch:20 step:16201 [D loss: 0.003264, acc.: 100.00%] [G loss: 0.017887]\n",
      "epoch:20 step:16202 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.088380]\n",
      "epoch:20 step:16203 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.035590]\n",
      "epoch:20 step:16204 [D loss: 0.002984, acc.: 100.00%] [G loss: 0.042620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16205 [D loss: 0.004454, acc.: 100.00%] [G loss: 0.095858]\n",
      "epoch:20 step:16206 [D loss: 0.012772, acc.: 100.00%] [G loss: 0.120358]\n",
      "epoch:20 step:16207 [D loss: 0.014945, acc.: 100.00%] [G loss: 0.018514]\n",
      "epoch:20 step:16208 [D loss: 0.005062, acc.: 100.00%] [G loss: 0.309123]\n",
      "epoch:20 step:16209 [D loss: 0.004214, acc.: 100.00%] [G loss: 0.128470]\n",
      "epoch:20 step:16210 [D loss: 0.002436, acc.: 100.00%] [G loss: 0.292029]\n",
      "epoch:20 step:16211 [D loss: 0.005253, acc.: 100.00%] [G loss: 0.192324]\n",
      "epoch:20 step:16212 [D loss: 0.004444, acc.: 100.00%] [G loss: 0.370679]\n",
      "epoch:20 step:16213 [D loss: 0.054346, acc.: 98.44%] [G loss: 3.065296]\n",
      "epoch:20 step:16214 [D loss: 0.020094, acc.: 99.22%] [G loss: 0.184508]\n",
      "epoch:20 step:16215 [D loss: 0.016305, acc.: 100.00%] [G loss: 0.878509]\n",
      "epoch:20 step:16216 [D loss: 0.004701, acc.: 100.00%] [G loss: 0.487376]\n",
      "epoch:20 step:16217 [D loss: 0.008013, acc.: 99.22%] [G loss: 0.051588]\n",
      "epoch:20 step:16218 [D loss: 0.002766, acc.: 100.00%] [G loss: 0.284049]\n",
      "epoch:20 step:16219 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.072837]\n",
      "epoch:20 step:16220 [D loss: 0.003396, acc.: 100.00%] [G loss: 0.326973]\n",
      "epoch:20 step:16221 [D loss: 0.001972, acc.: 100.00%] [G loss: 0.211009]\n",
      "epoch:20 step:16222 [D loss: 0.001847, acc.: 100.00%] [G loss: 0.031255]\n",
      "epoch:20 step:16223 [D loss: 0.002578, acc.: 100.00%] [G loss: 0.885839]\n",
      "epoch:20 step:16224 [D loss: 0.007851, acc.: 100.00%] [G loss: 0.041764]\n",
      "epoch:20 step:16225 [D loss: 0.008336, acc.: 100.00%] [G loss: 4.704177]\n",
      "epoch:20 step:16226 [D loss: 0.012728, acc.: 100.00%] [G loss: 0.527631]\n",
      "epoch:20 step:16227 [D loss: 0.056686, acc.: 99.22%] [G loss: 2.541087]\n",
      "epoch:20 step:16228 [D loss: 0.009230, acc.: 100.00%] [G loss: 4.600900]\n",
      "epoch:20 step:16229 [D loss: 0.537903, acc.: 77.34%] [G loss: 1.335414]\n",
      "epoch:20 step:16230 [D loss: 0.002038, acc.: 100.00%] [G loss: 8.871401]\n",
      "epoch:20 step:16231 [D loss: 0.077013, acc.: 96.88%] [G loss: 7.546682]\n",
      "epoch:20 step:16232 [D loss: 0.028143, acc.: 99.22%] [G loss: 5.769704]\n",
      "epoch:20 step:16233 [D loss: 0.063952, acc.: 97.66%] [G loss: 6.036175]\n",
      "epoch:20 step:16234 [D loss: 0.000563, acc.: 100.00%] [G loss: 6.273155]\n",
      "epoch:20 step:16235 [D loss: 0.005655, acc.: 100.00%] [G loss: 4.872748]\n",
      "epoch:20 step:16236 [D loss: 0.021412, acc.: 99.22%] [G loss: 4.360638]\n",
      "epoch:20 step:16237 [D loss: 0.007437, acc.: 100.00%] [G loss: 0.102066]\n",
      "epoch:20 step:16238 [D loss: 0.004939, acc.: 100.00%] [G loss: 5.177071]\n",
      "epoch:20 step:16239 [D loss: 0.004486, acc.: 100.00%] [G loss: 2.675982]\n",
      "epoch:20 step:16240 [D loss: 0.018726, acc.: 99.22%] [G loss: 3.064925]\n",
      "epoch:20 step:16241 [D loss: 0.010369, acc.: 100.00%] [G loss: 4.409208]\n",
      "epoch:20 step:16242 [D loss: 0.015720, acc.: 100.00%] [G loss: 3.607662]\n",
      "epoch:20 step:16243 [D loss: 0.135686, acc.: 95.31%] [G loss: 3.059154]\n",
      "epoch:20 step:16244 [D loss: 0.002694, acc.: 100.00%] [G loss: 3.577240]\n",
      "epoch:20 step:16245 [D loss: 0.008248, acc.: 100.00%] [G loss: 4.424298]\n",
      "epoch:20 step:16246 [D loss: 0.016283, acc.: 100.00%] [G loss: 4.598470]\n",
      "epoch:20 step:16247 [D loss: 0.056473, acc.: 99.22%] [G loss: 3.320063]\n",
      "epoch:20 step:16248 [D loss: 0.034571, acc.: 99.22%] [G loss: 5.962216]\n",
      "epoch:20 step:16249 [D loss: 0.006225, acc.: 100.00%] [G loss: 6.274431]\n",
      "epoch:20 step:16250 [D loss: 0.003116, acc.: 100.00%] [G loss: 6.586945]\n",
      "epoch:20 step:16251 [D loss: 0.032344, acc.: 99.22%] [G loss: 5.155859]\n",
      "epoch:20 step:16252 [D loss: 0.005874, acc.: 100.00%] [G loss: 7.171626]\n",
      "epoch:20 step:16253 [D loss: 0.002753, acc.: 100.00%] [G loss: 5.961087]\n",
      "epoch:20 step:16254 [D loss: 0.004426, acc.: 100.00%] [G loss: 1.615640]\n",
      "epoch:20 step:16255 [D loss: 0.003373, acc.: 100.00%] [G loss: 6.000120]\n",
      "epoch:20 step:16256 [D loss: 0.003156, acc.: 100.00%] [G loss: 2.644984]\n",
      "epoch:20 step:16257 [D loss: 0.002168, acc.: 100.00%] [G loss: 0.228422]\n",
      "epoch:20 step:16258 [D loss: 0.014514, acc.: 100.00%] [G loss: 4.220254]\n",
      "epoch:20 step:16259 [D loss: 0.109815, acc.: 96.88%] [G loss: 0.699574]\n",
      "epoch:20 step:16260 [D loss: 0.022583, acc.: 99.22%] [G loss: 10.164940]\n",
      "epoch:20 step:16261 [D loss: 0.070216, acc.: 97.66%] [G loss: 7.796219]\n",
      "epoch:20 step:16262 [D loss: 0.487918, acc.: 78.91%] [G loss: 11.524268]\n",
      "epoch:20 step:16263 [D loss: 4.055517, acc.: 46.88%] [G loss: 2.120700]\n",
      "epoch:20 step:16264 [D loss: 0.197305, acc.: 89.84%] [G loss: 4.604373]\n",
      "epoch:20 step:16265 [D loss: 0.074088, acc.: 96.88%] [G loss: 3.528311]\n",
      "epoch:20 step:16266 [D loss: 0.110392, acc.: 95.31%] [G loss: 4.256294]\n",
      "epoch:20 step:16267 [D loss: 0.066281, acc.: 98.44%] [G loss: 2.923621]\n",
      "epoch:20 step:16268 [D loss: 0.012446, acc.: 100.00%] [G loss: 2.442402]\n",
      "epoch:20 step:16269 [D loss: 0.027006, acc.: 100.00%] [G loss: 0.052619]\n",
      "epoch:20 step:16270 [D loss: 0.007312, acc.: 100.00%] [G loss: 0.130502]\n",
      "epoch:20 step:16271 [D loss: 0.014362, acc.: 100.00%] [G loss: 1.837036]\n",
      "epoch:20 step:16272 [D loss: 0.046509, acc.: 99.22%] [G loss: 1.875298]\n",
      "epoch:20 step:16273 [D loss: 0.029291, acc.: 100.00%] [G loss: 0.351726]\n",
      "epoch:20 step:16274 [D loss: 0.034277, acc.: 99.22%] [G loss: 1.544306]\n",
      "epoch:20 step:16275 [D loss: 0.006095, acc.: 100.00%] [G loss: 2.241964]\n",
      "epoch:20 step:16276 [D loss: 1.492006, acc.: 41.41%] [G loss: 2.701170]\n",
      "epoch:20 step:16277 [D loss: 1.036987, acc.: 57.03%] [G loss: 5.445164]\n",
      "epoch:20 step:16278 [D loss: 0.042850, acc.: 99.22%] [G loss: 0.093922]\n",
      "epoch:20 step:16279 [D loss: 0.226038, acc.: 90.62%] [G loss: 0.022089]\n",
      "epoch:20 step:16280 [D loss: 0.004526, acc.: 100.00%] [G loss: 1.861547]\n",
      "epoch:20 step:16281 [D loss: 0.003703, acc.: 100.00%] [G loss: 1.203194]\n",
      "epoch:20 step:16282 [D loss: 0.051634, acc.: 98.44%] [G loss: 0.025113]\n",
      "epoch:20 step:16283 [D loss: 0.122079, acc.: 94.53%] [G loss: 0.371179]\n",
      "epoch:20 step:16284 [D loss: 0.045263, acc.: 98.44%] [G loss: 2.805967]\n",
      "epoch:20 step:16285 [D loss: 0.235550, acc.: 89.84%] [G loss: 0.002595]\n",
      "epoch:20 step:16286 [D loss: 0.116584, acc.: 94.53%] [G loss: 0.876334]\n",
      "epoch:20 step:16287 [D loss: 0.015962, acc.: 99.22%] [G loss: 0.144293]\n",
      "epoch:20 step:16288 [D loss: 0.024646, acc.: 100.00%] [G loss: 0.092026]\n",
      "epoch:20 step:16289 [D loss: 0.012165, acc.: 100.00%] [G loss: 0.020853]\n",
      "epoch:20 step:16290 [D loss: 0.033783, acc.: 100.00%] [G loss: 0.895428]\n",
      "epoch:20 step:16291 [D loss: 0.110873, acc.: 97.66%] [G loss: 0.000565]\n",
      "epoch:20 step:16292 [D loss: 0.003918, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:20 step:16293 [D loss: 0.047453, acc.: 99.22%] [G loss: 0.276542]\n",
      "epoch:20 step:16294 [D loss: 0.009553, acc.: 100.00%] [G loss: 0.001724]\n",
      "epoch:20 step:16295 [D loss: 0.003029, acc.: 100.00%] [G loss: 0.005003]\n",
      "epoch:20 step:16296 [D loss: 0.015091, acc.: 100.00%] [G loss: 0.002393]\n",
      "epoch:20 step:16297 [D loss: 0.004682, acc.: 100.00%] [G loss: 0.000587]\n",
      "epoch:20 step:16298 [D loss: 0.003318, acc.: 100.00%] [G loss: 0.003303]\n",
      "epoch:20 step:16299 [D loss: 0.006975, acc.: 100.00%] [G loss: 0.002476]\n",
      "epoch:20 step:16300 [D loss: 0.021752, acc.: 99.22%] [G loss: 0.009687]\n",
      "epoch:20 step:16301 [D loss: 0.002829, acc.: 100.00%] [G loss: 0.698008]\n",
      "epoch:20 step:16302 [D loss: 0.002937, acc.: 100.00%] [G loss: 0.009113]\n",
      "epoch:20 step:16303 [D loss: 0.009788, acc.: 100.00%] [G loss: 0.014392]\n",
      "epoch:20 step:16304 [D loss: 0.011222, acc.: 100.00%] [G loss: 0.002166]\n",
      "epoch:20 step:16305 [D loss: 0.010960, acc.: 100.00%] [G loss: 0.293877]\n",
      "epoch:20 step:16306 [D loss: 0.006607, acc.: 100.00%] [G loss: 0.010793]\n",
      "epoch:20 step:16307 [D loss: 0.004931, acc.: 100.00%] [G loss: 0.005458]\n",
      "epoch:20 step:16308 [D loss: 0.018719, acc.: 100.00%] [G loss: 0.990532]\n",
      "epoch:20 step:16309 [D loss: 0.256335, acc.: 89.84%] [G loss: 3.899296]\n",
      "epoch:20 step:16310 [D loss: 1.586716, acc.: 50.78%] [G loss: 0.538461]\n",
      "epoch:20 step:16311 [D loss: 0.197569, acc.: 92.19%] [G loss: 3.404958]\n",
      "epoch:20 step:16312 [D loss: 0.059219, acc.: 97.66%] [G loss: 0.569939]\n",
      "epoch:20 step:16313 [D loss: 0.004760, acc.: 100.00%] [G loss: 0.562849]\n",
      "epoch:20 step:16314 [D loss: 0.022090, acc.: 99.22%] [G loss: 0.459047]\n",
      "epoch:20 step:16315 [D loss: 0.044095, acc.: 100.00%] [G loss: 2.253514]\n",
      "epoch:20 step:16316 [D loss: 0.047603, acc.: 98.44%] [G loss: 0.644476]\n",
      "epoch:20 step:16317 [D loss: 0.133059, acc.: 96.09%] [G loss: 1.029775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16318 [D loss: 0.017898, acc.: 100.00%] [G loss: 1.636271]\n",
      "epoch:20 step:16319 [D loss: 0.079360, acc.: 99.22%] [G loss: 2.223831]\n",
      "epoch:20 step:16320 [D loss: 0.105271, acc.: 95.31%] [G loss: 2.244485]\n",
      "epoch:20 step:16321 [D loss: 0.051814, acc.: 99.22%] [G loss: 3.148508]\n",
      "epoch:20 step:16322 [D loss: 0.063561, acc.: 98.44%] [G loss: 1.496610]\n",
      "epoch:20 step:16323 [D loss: 0.073412, acc.: 98.44%] [G loss: 1.075860]\n",
      "epoch:20 step:16324 [D loss: 0.057352, acc.: 99.22%] [G loss: 2.097929]\n",
      "epoch:20 step:16325 [D loss: 0.029145, acc.: 100.00%] [G loss: 3.491933]\n",
      "epoch:20 step:16326 [D loss: 0.034925, acc.: 100.00%] [G loss: 3.059539]\n",
      "epoch:20 step:16327 [D loss: 0.033999, acc.: 100.00%] [G loss: 3.708714]\n",
      "epoch:20 step:16328 [D loss: 0.031257, acc.: 100.00%] [G loss: 4.290390]\n",
      "epoch:20 step:16329 [D loss: 0.026068, acc.: 100.00%] [G loss: 4.877462]\n",
      "epoch:20 step:16330 [D loss: 0.071119, acc.: 96.88%] [G loss: 3.806744]\n",
      "epoch:20 step:16331 [D loss: 0.053118, acc.: 99.22%] [G loss: 3.278167]\n",
      "epoch:20 step:16332 [D loss: 0.013725, acc.: 100.00%] [G loss: 1.451390]\n",
      "epoch:20 step:16333 [D loss: 0.184928, acc.: 92.97%] [G loss: 0.341164]\n",
      "epoch:20 step:16334 [D loss: 0.033243, acc.: 100.00%] [G loss: 5.672704]\n",
      "epoch:20 step:16335 [D loss: 0.120617, acc.: 95.31%] [G loss: 0.996820]\n",
      "epoch:20 step:16336 [D loss: 0.009304, acc.: 100.00%] [G loss: 6.018383]\n",
      "epoch:20 step:16337 [D loss: 0.005123, acc.: 100.00%] [G loss: 6.261948]\n",
      "epoch:20 step:16338 [D loss: 0.015506, acc.: 100.00%] [G loss: 5.711935]\n",
      "epoch:20 step:16339 [D loss: 0.011237, acc.: 100.00%] [G loss: 4.453504]\n",
      "epoch:20 step:16340 [D loss: 0.004718, acc.: 100.00%] [G loss: 0.584164]\n",
      "epoch:20 step:16341 [D loss: 0.024262, acc.: 99.22%] [G loss: 3.799502]\n",
      "epoch:20 step:16342 [D loss: 0.007889, acc.: 100.00%] [G loss: 3.656907]\n",
      "epoch:20 step:16343 [D loss: 0.004666, acc.: 100.00%] [G loss: 2.955245]\n",
      "epoch:20 step:16344 [D loss: 0.052812, acc.: 99.22%] [G loss: 2.180222]\n",
      "epoch:20 step:16345 [D loss: 0.103754, acc.: 96.88%] [G loss: 0.298857]\n",
      "epoch:20 step:16346 [D loss: 0.400097, acc.: 80.47%] [G loss: 6.297094]\n",
      "epoch:20 step:16347 [D loss: 1.360977, acc.: 53.12%] [G loss: 0.668229]\n",
      "epoch:20 step:16348 [D loss: 0.579523, acc.: 79.69%] [G loss: 2.625732]\n",
      "epoch:20 step:16349 [D loss: 0.098823, acc.: 96.09%] [G loss: 6.750661]\n",
      "epoch:20 step:16350 [D loss: 0.161450, acc.: 93.75%] [G loss: 6.370714]\n",
      "epoch:20 step:16351 [D loss: 0.014021, acc.: 99.22%] [G loss: 4.766706]\n",
      "epoch:20 step:16352 [D loss: 0.003603, acc.: 100.00%] [G loss: 3.927887]\n",
      "epoch:20 step:16353 [D loss: 0.004087, acc.: 100.00%] [G loss: 3.750587]\n",
      "epoch:20 step:16354 [D loss: 0.002977, acc.: 100.00%] [G loss: 3.033132]\n",
      "epoch:20 step:16355 [D loss: 0.013668, acc.: 100.00%] [G loss: 5.049274]\n",
      "epoch:20 step:16356 [D loss: 0.052432, acc.: 97.66%] [G loss: 0.103585]\n",
      "epoch:20 step:16357 [D loss: 0.000992, acc.: 100.00%] [G loss: 2.245518]\n",
      "epoch:20 step:16358 [D loss: 0.001898, acc.: 100.00%] [G loss: 0.507877]\n",
      "epoch:20 step:16359 [D loss: 0.003229, acc.: 100.00%] [G loss: 2.213860]\n",
      "epoch:20 step:16360 [D loss: 0.006473, acc.: 100.00%] [G loss: 1.730089]\n",
      "epoch:20 step:16361 [D loss: 0.004672, acc.: 100.00%] [G loss: 1.130707]\n",
      "epoch:20 step:16362 [D loss: 0.013817, acc.: 100.00%] [G loss: 2.348224]\n",
      "epoch:20 step:16363 [D loss: 0.007795, acc.: 100.00%] [G loss: 0.352445]\n",
      "epoch:20 step:16364 [D loss: 0.073072, acc.: 99.22%] [G loss: 1.072111]\n",
      "epoch:20 step:16365 [D loss: 0.004270, acc.: 100.00%] [G loss: 0.475981]\n",
      "epoch:20 step:16366 [D loss: 0.055270, acc.: 99.22%] [G loss: 0.053707]\n",
      "epoch:20 step:16367 [D loss: 0.012601, acc.: 100.00%] [G loss: 0.774549]\n",
      "epoch:20 step:16368 [D loss: 0.004039, acc.: 100.00%] [G loss: 0.520030]\n",
      "epoch:20 step:16369 [D loss: 0.002395, acc.: 100.00%] [G loss: 0.004259]\n",
      "epoch:20 step:16370 [D loss: 0.020888, acc.: 100.00%] [G loss: 0.999804]\n",
      "epoch:20 step:16371 [D loss: 0.005106, acc.: 100.00%] [G loss: 0.641392]\n",
      "epoch:20 step:16372 [D loss: 0.002085, acc.: 100.00%] [G loss: 0.268654]\n",
      "epoch:20 step:16373 [D loss: 0.003245, acc.: 100.00%] [G loss: 0.008246]\n",
      "epoch:20 step:16374 [D loss: 0.007211, acc.: 100.00%] [G loss: 0.060663]\n",
      "epoch:20 step:16375 [D loss: 0.006673, acc.: 100.00%] [G loss: 0.056148]\n",
      "epoch:20 step:16376 [D loss: 0.005293, acc.: 100.00%] [G loss: 1.184428]\n",
      "epoch:20 step:16377 [D loss: 0.003171, acc.: 100.00%] [G loss: 0.364236]\n",
      "epoch:20 step:16378 [D loss: 0.001270, acc.: 100.00%] [G loss: 0.611433]\n",
      "epoch:20 step:16379 [D loss: 0.003802, acc.: 100.00%] [G loss: 0.312346]\n",
      "epoch:20 step:16380 [D loss: 0.006957, acc.: 100.00%] [G loss: 0.029932]\n",
      "epoch:20 step:16381 [D loss: 0.049236, acc.: 99.22%] [G loss: 0.294959]\n",
      "epoch:20 step:16382 [D loss: 0.098335, acc.: 97.66%] [G loss: 0.060731]\n",
      "epoch:20 step:16383 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.008404]\n",
      "epoch:20 step:16384 [D loss: 0.005963, acc.: 100.00%] [G loss: 0.051667]\n",
      "epoch:20 step:16385 [D loss: 0.015270, acc.: 99.22%] [G loss: 0.469296]\n",
      "epoch:20 step:16386 [D loss: 0.004615, acc.: 100.00%] [G loss: 0.286024]\n",
      "epoch:20 step:16387 [D loss: 0.004917, acc.: 100.00%] [G loss: 0.050360]\n",
      "epoch:20 step:16388 [D loss: 0.004296, acc.: 100.00%] [G loss: 0.084835]\n",
      "epoch:20 step:16389 [D loss: 0.005726, acc.: 100.00%] [G loss: 0.089105]\n",
      "epoch:20 step:16390 [D loss: 0.120670, acc.: 97.66%] [G loss: 2.329319]\n",
      "epoch:20 step:16391 [D loss: 0.087378, acc.: 95.31%] [G loss: 0.109312]\n",
      "epoch:20 step:16392 [D loss: 0.000697, acc.: 100.00%] [G loss: 2.077777]\n",
      "epoch:20 step:16393 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.007131]\n",
      "epoch:20 step:16394 [D loss: 0.000770, acc.: 100.00%] [G loss: 5.049311]\n",
      "epoch:20 step:16395 [D loss: 0.001189, acc.: 100.00%] [G loss: 0.018671]\n",
      "epoch:20 step:16396 [D loss: 0.002310, acc.: 100.00%] [G loss: 0.019497]\n",
      "epoch:20 step:16397 [D loss: 0.002174, acc.: 100.00%] [G loss: 0.322572]\n",
      "epoch:20 step:16398 [D loss: 0.002368, acc.: 100.00%] [G loss: 0.937286]\n",
      "epoch:20 step:16399 [D loss: 0.015063, acc.: 100.00%] [G loss: 0.088588]\n",
      "epoch:20 step:16400 [D loss: 0.006451, acc.: 100.00%] [G loss: 0.343091]\n",
      "epoch:20 step:16401 [D loss: 0.017354, acc.: 100.00%] [G loss: 0.019090]\n",
      "epoch:21 step:16402 [D loss: 0.004742, acc.: 100.00%] [G loss: 0.056452]\n",
      "epoch:21 step:16403 [D loss: 0.003610, acc.: 100.00%] [G loss: 1.207515]\n",
      "epoch:21 step:16404 [D loss: 0.005323, acc.: 100.00%] [G loss: 0.593218]\n",
      "epoch:21 step:16405 [D loss: 0.020513, acc.: 100.00%] [G loss: 0.032064]\n",
      "epoch:21 step:16406 [D loss: 0.004045, acc.: 100.00%] [G loss: 0.047328]\n",
      "epoch:21 step:16407 [D loss: 0.003367, acc.: 100.00%] [G loss: 0.648088]\n",
      "epoch:21 step:16408 [D loss: 0.005744, acc.: 100.00%] [G loss: 0.058448]\n",
      "epoch:21 step:16409 [D loss: 0.002723, acc.: 100.00%] [G loss: 0.365609]\n",
      "epoch:21 step:16410 [D loss: 0.016636, acc.: 100.00%] [G loss: 0.044436]\n",
      "epoch:21 step:16411 [D loss: 0.002393, acc.: 100.00%] [G loss: 0.148516]\n",
      "epoch:21 step:16412 [D loss: 0.005455, acc.: 100.00%] [G loss: 0.358811]\n",
      "epoch:21 step:16413 [D loss: 0.011954, acc.: 100.00%] [G loss: 0.011733]\n",
      "epoch:21 step:16414 [D loss: 0.001374, acc.: 100.00%] [G loss: 1.576888]\n",
      "epoch:21 step:16415 [D loss: 0.111576, acc.: 96.09%] [G loss: 0.403964]\n",
      "epoch:21 step:16416 [D loss: 0.088101, acc.: 96.09%] [G loss: 0.148030]\n",
      "epoch:21 step:16417 [D loss: 0.008597, acc.: 100.00%] [G loss: 1.945036]\n",
      "epoch:21 step:16418 [D loss: 0.003462, acc.: 100.00%] [G loss: 0.015073]\n",
      "epoch:21 step:16419 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.245259]\n",
      "epoch:21 step:16420 [D loss: 0.001248, acc.: 100.00%] [G loss: 0.020183]\n",
      "epoch:21 step:16421 [D loss: 0.001913, acc.: 100.00%] [G loss: 0.003149]\n",
      "epoch:21 step:16422 [D loss: 0.002459, acc.: 100.00%] [G loss: 0.008426]\n",
      "epoch:21 step:16423 [D loss: 0.002826, acc.: 100.00%] [G loss: 1.034183]\n",
      "epoch:21 step:16424 [D loss: 0.001668, acc.: 100.00%] [G loss: 0.512108]\n",
      "epoch:21 step:16425 [D loss: 0.002023, acc.: 100.00%] [G loss: 0.041926]\n",
      "epoch:21 step:16426 [D loss: 0.002272, acc.: 100.00%] [G loss: 0.078909]\n",
      "epoch:21 step:16427 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.015964]\n",
      "epoch:21 step:16428 [D loss: 0.001746, acc.: 100.00%] [G loss: 0.001999]\n",
      "epoch:21 step:16429 [D loss: 0.002365, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:21 step:16430 [D loss: 0.001159, acc.: 100.00%] [G loss: 0.013094]\n",
      "epoch:21 step:16431 [D loss: 0.005121, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:21 step:16432 [D loss: 0.000920, acc.: 100.00%] [G loss: 0.654869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16433 [D loss: 0.004187, acc.: 100.00%] [G loss: 0.049930]\n",
      "epoch:21 step:16434 [D loss: 0.006198, acc.: 100.00%] [G loss: 1.689859]\n",
      "epoch:21 step:16435 [D loss: 0.007681, acc.: 100.00%] [G loss: 0.051197]\n",
      "epoch:21 step:16436 [D loss: 0.004475, acc.: 100.00%] [G loss: 0.003380]\n",
      "epoch:21 step:16437 [D loss: 0.015190, acc.: 100.00%] [G loss: 0.058898]\n",
      "epoch:21 step:16438 [D loss: 0.002162, acc.: 100.00%] [G loss: 0.331385]\n",
      "epoch:21 step:16439 [D loss: 0.007047, acc.: 100.00%] [G loss: 0.068686]\n",
      "epoch:21 step:16440 [D loss: 0.001717, acc.: 100.00%] [G loss: 0.010619]\n",
      "epoch:21 step:16441 [D loss: 0.004691, acc.: 100.00%] [G loss: 0.242275]\n",
      "epoch:21 step:16442 [D loss: 0.005004, acc.: 100.00%] [G loss: 0.113598]\n",
      "epoch:21 step:16443 [D loss: 0.002325, acc.: 100.00%] [G loss: 0.031511]\n",
      "epoch:21 step:16444 [D loss: 0.001791, acc.: 100.00%] [G loss: 0.062436]\n",
      "epoch:21 step:16445 [D loss: 0.010793, acc.: 100.00%] [G loss: 0.050538]\n",
      "epoch:21 step:16446 [D loss: 0.006928, acc.: 100.00%] [G loss: 0.244112]\n",
      "epoch:21 step:16447 [D loss: 0.005257, acc.: 100.00%] [G loss: 0.161939]\n",
      "epoch:21 step:16448 [D loss: 0.002021, acc.: 100.00%] [G loss: 0.170085]\n",
      "epoch:21 step:16449 [D loss: 0.025005, acc.: 100.00%] [G loss: 0.459035]\n",
      "epoch:21 step:16450 [D loss: 0.002384, acc.: 100.00%] [G loss: 0.205063]\n",
      "epoch:21 step:16451 [D loss: 0.114223, acc.: 96.88%] [G loss: 0.238494]\n",
      "epoch:21 step:16452 [D loss: 0.001062, acc.: 100.00%] [G loss: 0.006307]\n",
      "epoch:21 step:16453 [D loss: 0.002549, acc.: 100.00%] [G loss: 0.186369]\n",
      "epoch:21 step:16454 [D loss: 0.002505, acc.: 100.00%] [G loss: 0.000875]\n",
      "epoch:21 step:16455 [D loss: 0.003937, acc.: 100.00%] [G loss: 0.120936]\n",
      "epoch:21 step:16456 [D loss: 0.004562, acc.: 100.00%] [G loss: 0.008589]\n",
      "epoch:21 step:16457 [D loss: 0.002476, acc.: 100.00%] [G loss: 0.296639]\n",
      "epoch:21 step:16458 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.018652]\n",
      "epoch:21 step:16459 [D loss: 0.004041, acc.: 100.00%] [G loss: 0.004010]\n",
      "epoch:21 step:16460 [D loss: 0.001758, acc.: 100.00%] [G loss: 0.056511]\n",
      "epoch:21 step:16461 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.001272]\n",
      "epoch:21 step:16462 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.001743]\n",
      "epoch:21 step:16463 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.350266]\n",
      "epoch:21 step:16464 [D loss: 0.001631, acc.: 100.00%] [G loss: 1.626599]\n",
      "epoch:21 step:16465 [D loss: 0.001198, acc.: 100.00%] [G loss: 0.060979]\n",
      "epoch:21 step:16466 [D loss: 0.002802, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:21 step:16467 [D loss: 0.005720, acc.: 100.00%] [G loss: 0.370572]\n",
      "epoch:21 step:16468 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.229761]\n",
      "epoch:21 step:16469 [D loss: 0.002246, acc.: 100.00%] [G loss: 0.140062]\n",
      "epoch:21 step:16470 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.005411]\n",
      "epoch:21 step:16471 [D loss: 0.001018, acc.: 100.00%] [G loss: 0.236289]\n",
      "epoch:21 step:16472 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.067811]\n",
      "epoch:21 step:16473 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.046071]\n",
      "epoch:21 step:16474 [D loss: 0.041349, acc.: 99.22%] [G loss: 2.667329]\n",
      "epoch:21 step:16475 [D loss: 0.004661, acc.: 100.00%] [G loss: 1.679047]\n",
      "epoch:21 step:16476 [D loss: 0.053389, acc.: 97.66%] [G loss: 0.005129]\n",
      "epoch:21 step:16477 [D loss: 0.001126, acc.: 100.00%] [G loss: 0.000546]\n",
      "epoch:21 step:16478 [D loss: 0.039686, acc.: 100.00%] [G loss: 0.133395]\n",
      "epoch:21 step:16479 [D loss: 0.001177, acc.: 100.00%] [G loss: 4.070362]\n",
      "epoch:21 step:16480 [D loss: 0.053977, acc.: 98.44%] [G loss: 0.110766]\n",
      "epoch:21 step:16481 [D loss: 0.004605, acc.: 100.00%] [G loss: 0.051224]\n",
      "epoch:21 step:16482 [D loss: 0.001323, acc.: 100.00%] [G loss: 2.449189]\n",
      "epoch:21 step:16483 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.018488]\n",
      "epoch:21 step:16484 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.009304]\n",
      "epoch:21 step:16485 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.014565]\n",
      "epoch:21 step:16486 [D loss: 0.001677, acc.: 100.00%] [G loss: 0.434396]\n",
      "epoch:21 step:16487 [D loss: 0.006572, acc.: 100.00%] [G loss: 0.101431]\n",
      "epoch:21 step:16488 [D loss: 0.000801, acc.: 100.00%] [G loss: 0.067940]\n",
      "epoch:21 step:16489 [D loss: 0.051626, acc.: 100.00%] [G loss: 0.358719]\n",
      "epoch:21 step:16490 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.227433]\n",
      "epoch:21 step:16491 [D loss: 5.014735, acc.: 24.22%] [G loss: 8.535076]\n",
      "epoch:21 step:16492 [D loss: 2.888132, acc.: 50.00%] [G loss: 8.312231]\n",
      "epoch:21 step:16493 [D loss: 1.331523, acc.: 55.47%] [G loss: 0.000761]\n",
      "epoch:21 step:16494 [D loss: 0.146839, acc.: 95.31%] [G loss: 0.001950]\n",
      "epoch:21 step:16495 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.011930]\n",
      "epoch:21 step:16496 [D loss: 0.014032, acc.: 100.00%] [G loss: 0.007518]\n",
      "epoch:21 step:16497 [D loss: 0.018849, acc.: 100.00%] [G loss: 0.011085]\n",
      "epoch:21 step:16498 [D loss: 0.024797, acc.: 100.00%] [G loss: 0.009043]\n",
      "epoch:21 step:16499 [D loss: 0.009692, acc.: 100.00%] [G loss: 0.004868]\n",
      "epoch:21 step:16500 [D loss: 0.105818, acc.: 96.09%] [G loss: 0.025287]\n",
      "epoch:21 step:16501 [D loss: 0.003271, acc.: 100.00%] [G loss: 5.757371]\n",
      "epoch:21 step:16502 [D loss: 0.007639, acc.: 100.00%] [G loss: 0.137454]\n",
      "epoch:21 step:16503 [D loss: 0.025860, acc.: 98.44%] [G loss: 5.925684]\n",
      "epoch:21 step:16504 [D loss: 0.010182, acc.: 100.00%] [G loss: 4.849115]\n",
      "epoch:21 step:16505 [D loss: 0.011209, acc.: 100.00%] [G loss: 0.007059]\n",
      "epoch:21 step:16506 [D loss: 0.063482, acc.: 99.22%] [G loss: 0.108080]\n",
      "epoch:21 step:16507 [D loss: 0.034653, acc.: 100.00%] [G loss: 0.031435]\n",
      "epoch:21 step:16508 [D loss: 0.052689, acc.: 99.22%] [G loss: 0.139576]\n",
      "epoch:21 step:16509 [D loss: 0.049509, acc.: 98.44%] [G loss: 3.804730]\n",
      "epoch:21 step:16510 [D loss: 0.013063, acc.: 100.00%] [G loss: 0.005302]\n",
      "epoch:21 step:16511 [D loss: 0.009284, acc.: 100.00%] [G loss: 0.016236]\n",
      "epoch:21 step:16512 [D loss: 0.093676, acc.: 99.22%] [G loss: 0.011050]\n",
      "epoch:21 step:16513 [D loss: 0.012103, acc.: 100.00%] [G loss: 0.013510]\n",
      "epoch:21 step:16514 [D loss: 0.098588, acc.: 97.66%] [G loss: 0.586623]\n",
      "epoch:21 step:16515 [D loss: 0.024682, acc.: 100.00%] [G loss: 4.215709]\n",
      "epoch:21 step:16516 [D loss: 0.040260, acc.: 100.00%] [G loss: 0.041287]\n",
      "epoch:21 step:16517 [D loss: 0.047988, acc.: 99.22%] [G loss: 0.987620]\n",
      "epoch:21 step:16518 [D loss: 0.007507, acc.: 100.00%] [G loss: 0.584236]\n",
      "epoch:21 step:16519 [D loss: 0.031403, acc.: 100.00%] [G loss: 0.019623]\n",
      "epoch:21 step:16520 [D loss: 0.002669, acc.: 100.00%] [G loss: 1.877120]\n",
      "epoch:21 step:16521 [D loss: 0.117431, acc.: 93.75%] [G loss: 2.797541]\n",
      "epoch:21 step:16522 [D loss: 0.083713, acc.: 96.88%] [G loss: 0.251022]\n",
      "epoch:21 step:16523 [D loss: 0.055220, acc.: 99.22%] [G loss: 0.007843]\n",
      "epoch:21 step:16524 [D loss: 0.039529, acc.: 100.00%] [G loss: 0.329453]\n",
      "epoch:21 step:16525 [D loss: 0.380317, acc.: 83.59%] [G loss: 8.154392]\n",
      "epoch:21 step:16526 [D loss: 0.080344, acc.: 97.66%] [G loss: 2.842403]\n",
      "epoch:21 step:16527 [D loss: 0.508621, acc.: 72.66%] [G loss: 0.005341]\n",
      "epoch:21 step:16528 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.003559]\n",
      "epoch:21 step:16529 [D loss: 0.001687, acc.: 100.00%] [G loss: 5.178630]\n",
      "epoch:21 step:16530 [D loss: 0.031416, acc.: 99.22%] [G loss: 5.139583]\n",
      "epoch:21 step:16531 [D loss: 0.005646, acc.: 100.00%] [G loss: 4.453742]\n",
      "epoch:21 step:16532 [D loss: 0.035214, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:21 step:16533 [D loss: 0.019820, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:21 step:16534 [D loss: 0.020049, acc.: 100.00%] [G loss: 0.001978]\n",
      "epoch:21 step:16535 [D loss: 0.004579, acc.: 100.00%] [G loss: 0.002686]\n",
      "epoch:21 step:16536 [D loss: 0.036797, acc.: 98.44%] [G loss: 0.001263]\n",
      "epoch:21 step:16537 [D loss: 0.003361, acc.: 100.00%] [G loss: 0.013188]\n",
      "epoch:21 step:16538 [D loss: 0.004025, acc.: 100.00%] [G loss: 0.018035]\n",
      "epoch:21 step:16539 [D loss: 0.008402, acc.: 100.00%] [G loss: 0.102489]\n",
      "epoch:21 step:16540 [D loss: 0.001912, acc.: 100.00%] [G loss: 0.028362]\n",
      "epoch:21 step:16541 [D loss: 0.025291, acc.: 99.22%] [G loss: 5.246646]\n",
      "epoch:21 step:16542 [D loss: 0.003569, acc.: 100.00%] [G loss: 0.009294]\n",
      "epoch:21 step:16543 [D loss: 0.005137, acc.: 100.00%] [G loss: 4.435170]\n",
      "epoch:21 step:16544 [D loss: 0.005046, acc.: 100.00%] [G loss: 0.010581]\n",
      "epoch:21 step:16545 [D loss: 0.009316, acc.: 100.00%] [G loss: 0.017859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16546 [D loss: 0.016050, acc.: 100.00%] [G loss: 0.026766]\n",
      "epoch:21 step:16547 [D loss: 0.012371, acc.: 100.00%] [G loss: 0.008306]\n",
      "epoch:21 step:16548 [D loss: 0.015959, acc.: 100.00%] [G loss: 2.347738]\n",
      "epoch:21 step:16549 [D loss: 0.004917, acc.: 100.00%] [G loss: 0.072519]\n",
      "epoch:21 step:16550 [D loss: 0.003567, acc.: 100.00%] [G loss: 1.755883]\n",
      "epoch:21 step:16551 [D loss: 0.033064, acc.: 99.22%] [G loss: 0.144762]\n",
      "epoch:21 step:16552 [D loss: 0.046052, acc.: 99.22%] [G loss: 0.155736]\n",
      "epoch:21 step:16553 [D loss: 0.010845, acc.: 100.00%] [G loss: 2.091413]\n",
      "epoch:21 step:16554 [D loss: 0.120948, acc.: 97.66%] [G loss: 0.004116]\n",
      "epoch:21 step:16555 [D loss: 0.012603, acc.: 100.00%] [G loss: 0.195134]\n",
      "epoch:21 step:16556 [D loss: 0.075375, acc.: 99.22%] [G loss: 2.736258]\n",
      "epoch:21 step:16557 [D loss: 0.045217, acc.: 99.22%] [G loss: 2.684216]\n",
      "epoch:21 step:16558 [D loss: 0.019942, acc.: 100.00%] [G loss: 0.021071]\n",
      "epoch:21 step:16559 [D loss: 0.120586, acc.: 96.09%] [G loss: 0.681248]\n",
      "epoch:21 step:16560 [D loss: 0.034253, acc.: 100.00%] [G loss: 0.002212]\n",
      "epoch:21 step:16561 [D loss: 0.003589, acc.: 100.00%] [G loss: 2.381980]\n",
      "epoch:21 step:16562 [D loss: 0.009765, acc.: 100.00%] [G loss: 2.654838]\n",
      "epoch:21 step:16563 [D loss: 0.065826, acc.: 99.22%] [G loss: 0.054792]\n",
      "epoch:21 step:16564 [D loss: 0.020802, acc.: 100.00%] [G loss: 0.112067]\n",
      "epoch:21 step:16565 [D loss: 0.010419, acc.: 100.00%] [G loss: 0.053084]\n",
      "epoch:21 step:16566 [D loss: 0.004781, acc.: 100.00%] [G loss: 3.272830]\n",
      "epoch:21 step:16567 [D loss: 0.005888, acc.: 100.00%] [G loss: 2.243552]\n",
      "epoch:21 step:16568 [D loss: 0.010075, acc.: 100.00%] [G loss: 1.268094]\n",
      "epoch:21 step:16569 [D loss: 0.155025, acc.: 95.31%] [G loss: 2.099151]\n",
      "epoch:21 step:16570 [D loss: 0.034924, acc.: 100.00%] [G loss: 6.859371]\n",
      "epoch:21 step:16571 [D loss: 0.783556, acc.: 65.62%] [G loss: 0.000033]\n",
      "epoch:21 step:16572 [D loss: 0.696942, acc.: 69.53%] [G loss: 8.269413]\n",
      "epoch:21 step:16573 [D loss: 0.694745, acc.: 66.41%] [G loss: 6.149327]\n",
      "epoch:21 step:16574 [D loss: 0.004063, acc.: 100.00%] [G loss: 4.853294]\n",
      "epoch:21 step:16575 [D loss: 0.007111, acc.: 100.00%] [G loss: 0.083810]\n",
      "epoch:21 step:16576 [D loss: 0.013395, acc.: 100.00%] [G loss: 3.975624]\n",
      "epoch:21 step:16577 [D loss: 0.030292, acc.: 100.00%] [G loss: 0.192319]\n",
      "epoch:21 step:16578 [D loss: 0.021345, acc.: 100.00%] [G loss: 0.189978]\n",
      "epoch:21 step:16579 [D loss: 0.005156, acc.: 100.00%] [G loss: 3.640047]\n",
      "epoch:21 step:16580 [D loss: 0.124004, acc.: 96.88%] [G loss: 0.020264]\n",
      "epoch:21 step:16581 [D loss: 0.026444, acc.: 99.22%] [G loss: 0.076355]\n",
      "epoch:21 step:16582 [D loss: 0.001977, acc.: 100.00%] [G loss: 2.269110]\n",
      "epoch:21 step:16583 [D loss: 0.045599, acc.: 100.00%] [G loss: 0.032129]\n",
      "epoch:21 step:16584 [D loss: 0.005369, acc.: 100.00%] [G loss: 0.123202]\n",
      "epoch:21 step:16585 [D loss: 0.007205, acc.: 100.00%] [G loss: 0.025758]\n",
      "epoch:21 step:16586 [D loss: 0.039121, acc.: 98.44%] [G loss: 0.025860]\n",
      "epoch:21 step:16587 [D loss: 0.015574, acc.: 100.00%] [G loss: 0.813385]\n",
      "epoch:21 step:16588 [D loss: 0.051328, acc.: 100.00%] [G loss: 0.013593]\n",
      "epoch:21 step:16589 [D loss: 0.018702, acc.: 99.22%] [G loss: 0.019052]\n",
      "epoch:21 step:16590 [D loss: 0.017424, acc.: 100.00%] [G loss: 0.006711]\n",
      "epoch:21 step:16591 [D loss: 0.027323, acc.: 100.00%] [G loss: 0.025312]\n",
      "epoch:21 step:16592 [D loss: 0.016233, acc.: 100.00%] [G loss: 0.009235]\n",
      "epoch:21 step:16593 [D loss: 0.005410, acc.: 100.00%] [G loss: 0.020273]\n",
      "epoch:21 step:16594 [D loss: 0.019350, acc.: 99.22%] [G loss: 0.010146]\n",
      "epoch:21 step:16595 [D loss: 0.011808, acc.: 100.00%] [G loss: 0.007812]\n",
      "epoch:21 step:16596 [D loss: 0.003926, acc.: 100.00%] [G loss: 0.028468]\n",
      "epoch:21 step:16597 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.150970]\n",
      "epoch:21 step:16598 [D loss: 0.006925, acc.: 100.00%] [G loss: 0.014365]\n",
      "epoch:21 step:16599 [D loss: 0.009979, acc.: 100.00%] [G loss: 2.338108]\n",
      "epoch:21 step:16600 [D loss: 0.208945, acc.: 92.19%] [G loss: 2.590642]\n",
      "epoch:21 step:16601 [D loss: 0.611203, acc.: 71.88%] [G loss: 1.276984]\n",
      "epoch:21 step:16602 [D loss: 0.030145, acc.: 99.22%] [G loss: 4.222754]\n",
      "epoch:21 step:16603 [D loss: 0.021997, acc.: 100.00%] [G loss: 0.904486]\n",
      "epoch:21 step:16604 [D loss: 0.076282, acc.: 96.09%] [G loss: 2.153381]\n",
      "epoch:21 step:16605 [D loss: 0.011912, acc.: 100.00%] [G loss: 4.503847]\n",
      "epoch:21 step:16606 [D loss: 0.125122, acc.: 93.75%] [G loss: 0.124557]\n",
      "epoch:21 step:16607 [D loss: 0.029916, acc.: 100.00%] [G loss: 0.654066]\n",
      "epoch:21 step:16608 [D loss: 0.005929, acc.: 100.00%] [G loss: 0.159744]\n",
      "epoch:21 step:16609 [D loss: 0.020372, acc.: 100.00%] [G loss: 0.478231]\n",
      "epoch:21 step:16610 [D loss: 0.009793, acc.: 100.00%] [G loss: 0.023644]\n",
      "epoch:21 step:16611 [D loss: 0.016175, acc.: 100.00%] [G loss: 0.021028]\n",
      "epoch:21 step:16612 [D loss: 0.023521, acc.: 100.00%] [G loss: 0.057960]\n",
      "epoch:21 step:16613 [D loss: 0.014419, acc.: 100.00%] [G loss: 2.048731]\n",
      "epoch:21 step:16614 [D loss: 0.004613, acc.: 100.00%] [G loss: 0.195731]\n",
      "epoch:21 step:16615 [D loss: 0.141728, acc.: 95.31%] [G loss: 0.127416]\n",
      "epoch:21 step:16616 [D loss: 0.098207, acc.: 96.88%] [G loss: 0.012151]\n",
      "epoch:21 step:16617 [D loss: 0.009373, acc.: 100.00%] [G loss: 1.739285]\n",
      "epoch:21 step:16618 [D loss: 0.008146, acc.: 100.00%] [G loss: 0.068651]\n",
      "epoch:21 step:16619 [D loss: 0.004665, acc.: 100.00%] [G loss: 0.719869]\n",
      "epoch:21 step:16620 [D loss: 0.020765, acc.: 100.00%] [G loss: 0.007166]\n",
      "epoch:21 step:16621 [D loss: 0.011001, acc.: 100.00%] [G loss: 0.002220]\n",
      "epoch:21 step:16622 [D loss: 0.002371, acc.: 100.00%] [G loss: 0.018010]\n",
      "epoch:21 step:16623 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.010361]\n",
      "epoch:21 step:16624 [D loss: 0.010555, acc.: 100.00%] [G loss: 0.003195]\n",
      "epoch:21 step:16625 [D loss: 0.003651, acc.: 100.00%] [G loss: 0.003988]\n",
      "epoch:21 step:16626 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.003553]\n",
      "epoch:21 step:16627 [D loss: 0.009178, acc.: 100.00%] [G loss: 0.012872]\n",
      "epoch:21 step:16628 [D loss: 0.006036, acc.: 100.00%] [G loss: 0.021255]\n",
      "epoch:21 step:16629 [D loss: 0.013810, acc.: 100.00%] [G loss: 0.018578]\n",
      "epoch:21 step:16630 [D loss: 0.013465, acc.: 100.00%] [G loss: 0.459736]\n",
      "epoch:21 step:16631 [D loss: 0.018755, acc.: 99.22%] [G loss: 0.026112]\n",
      "epoch:21 step:16632 [D loss: 0.012833, acc.: 100.00%] [G loss: 0.012129]\n",
      "epoch:21 step:16633 [D loss: 0.040621, acc.: 99.22%] [G loss: 0.062689]\n",
      "epoch:21 step:16634 [D loss: 0.008591, acc.: 100.00%] [G loss: 1.235640]\n",
      "epoch:21 step:16635 [D loss: 0.108822, acc.: 93.75%] [G loss: 0.013295]\n",
      "epoch:21 step:16636 [D loss: 0.068051, acc.: 97.66%] [G loss: 0.099728]\n",
      "epoch:21 step:16637 [D loss: 0.001764, acc.: 100.00%] [G loss: 0.392278]\n",
      "epoch:21 step:16638 [D loss: 0.001832, acc.: 100.00%] [G loss: 0.162917]\n",
      "epoch:21 step:16639 [D loss: 0.003811, acc.: 100.00%] [G loss: 0.234014]\n",
      "epoch:21 step:16640 [D loss: 0.011525, acc.: 100.00%] [G loss: 0.378778]\n",
      "epoch:21 step:16641 [D loss: 0.180589, acc.: 92.19%] [G loss: 0.131659]\n",
      "epoch:21 step:16642 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.347196]\n",
      "epoch:21 step:16643 [D loss: 0.000582, acc.: 100.00%] [G loss: 0.598102]\n",
      "epoch:21 step:16644 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.209159]\n",
      "epoch:21 step:16645 [D loss: 0.002068, acc.: 100.00%] [G loss: 0.300780]\n",
      "epoch:21 step:16646 [D loss: 0.002553, acc.: 100.00%] [G loss: 0.133788]\n",
      "epoch:21 step:16647 [D loss: 0.009774, acc.: 99.22%] [G loss: 0.071919]\n",
      "epoch:21 step:16648 [D loss: 0.004620, acc.: 100.00%] [G loss: 0.020282]\n",
      "epoch:21 step:16649 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.009557]\n",
      "epoch:21 step:16650 [D loss: 0.004854, acc.: 100.00%] [G loss: 0.046600]\n",
      "epoch:21 step:16651 [D loss: 0.010255, acc.: 99.22%] [G loss: 0.015866]\n",
      "epoch:21 step:16652 [D loss: 0.004927, acc.: 100.00%] [G loss: 0.007000]\n",
      "epoch:21 step:16653 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.009444]\n",
      "epoch:21 step:16654 [D loss: 0.049374, acc.: 97.66%] [G loss: 0.265373]\n",
      "epoch:21 step:16655 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.457397]\n",
      "epoch:21 step:16656 [D loss: 0.015568, acc.: 100.00%] [G loss: 1.139613]\n",
      "epoch:21 step:16657 [D loss: 0.008592, acc.: 100.00%] [G loss: 0.303672]\n",
      "epoch:21 step:16658 [D loss: 0.007757, acc.: 100.00%] [G loss: 0.133467]\n",
      "epoch:21 step:16659 [D loss: 0.003060, acc.: 100.00%] [G loss: 0.027071]\n",
      "epoch:21 step:16660 [D loss: 0.013444, acc.: 100.00%] [G loss: 0.071037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16661 [D loss: 0.004797, acc.: 100.00%] [G loss: 0.264518]\n",
      "epoch:21 step:16662 [D loss: 0.016030, acc.: 99.22%] [G loss: 0.127216]\n",
      "epoch:21 step:16663 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.250922]\n",
      "epoch:21 step:16664 [D loss: 0.010898, acc.: 100.00%] [G loss: 2.878149]\n",
      "epoch:21 step:16665 [D loss: 0.090599, acc.: 96.09%] [G loss: 0.087567]\n",
      "epoch:21 step:16666 [D loss: 0.025814, acc.: 100.00%] [G loss: 0.062265]\n",
      "epoch:21 step:16667 [D loss: 0.020982, acc.: 100.00%] [G loss: 0.197909]\n",
      "epoch:21 step:16668 [D loss: 0.032000, acc.: 100.00%] [G loss: 0.012851]\n",
      "epoch:21 step:16669 [D loss: 0.015075, acc.: 100.00%] [G loss: 0.008726]\n",
      "epoch:21 step:16670 [D loss: 0.008296, acc.: 100.00%] [G loss: 0.030320]\n",
      "epoch:21 step:16671 [D loss: 0.020877, acc.: 99.22%] [G loss: 0.079020]\n",
      "epoch:21 step:16672 [D loss: 0.003696, acc.: 100.00%] [G loss: 1.555980]\n",
      "epoch:21 step:16673 [D loss: 0.013301, acc.: 100.00%] [G loss: 1.359102]\n",
      "epoch:21 step:16674 [D loss: 0.094147, acc.: 98.44%] [G loss: 0.231191]\n",
      "epoch:21 step:16675 [D loss: 0.247781, acc.: 89.06%] [G loss: 0.035226]\n",
      "epoch:21 step:16676 [D loss: 0.003515, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:21 step:16677 [D loss: 0.003110, acc.: 100.00%] [G loss: 0.688230]\n",
      "epoch:21 step:16678 [D loss: 0.499354, acc.: 80.47%] [G loss: 4.154587]\n",
      "epoch:21 step:16679 [D loss: 2.161937, acc.: 53.12%] [G loss: 2.087121]\n",
      "epoch:21 step:16680 [D loss: 0.632787, acc.: 80.47%] [G loss: 0.278846]\n",
      "epoch:21 step:16681 [D loss: 0.067469, acc.: 98.44%] [G loss: 0.718979]\n",
      "epoch:21 step:16682 [D loss: 0.524635, acc.: 84.38%] [G loss: 4.791481]\n",
      "epoch:21 step:16683 [D loss: 0.017992, acc.: 100.00%] [G loss: 3.324643]\n",
      "epoch:21 step:16684 [D loss: 0.332068, acc.: 87.50%] [G loss: 0.038554]\n",
      "epoch:21 step:16685 [D loss: 0.017243, acc.: 100.00%] [G loss: 0.099933]\n",
      "epoch:21 step:16686 [D loss: 0.067606, acc.: 96.09%] [G loss: 0.068509]\n",
      "epoch:21 step:16687 [D loss: 0.072404, acc.: 95.31%] [G loss: 5.051763]\n",
      "epoch:21 step:16688 [D loss: 0.007664, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:21 step:16689 [D loss: 0.030379, acc.: 99.22%] [G loss: 0.001200]\n",
      "epoch:21 step:16690 [D loss: 0.001812, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:21 step:16691 [D loss: 0.014303, acc.: 100.00%] [G loss: 3.866850]\n",
      "epoch:21 step:16692 [D loss: 0.010696, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:21 step:16693 [D loss: 0.121034, acc.: 96.88%] [G loss: 0.003760]\n",
      "epoch:21 step:16694 [D loss: 0.047244, acc.: 100.00%] [G loss: 0.013586]\n",
      "epoch:21 step:16695 [D loss: 0.033618, acc.: 100.00%] [G loss: 4.395307]\n",
      "epoch:21 step:16696 [D loss: 0.075001, acc.: 97.66%] [G loss: 0.000253]\n",
      "epoch:21 step:16697 [D loss: 0.020105, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:21 step:16698 [D loss: 0.036707, acc.: 100.00%] [G loss: 0.000492]\n",
      "epoch:21 step:16699 [D loss: 0.001488, acc.: 100.00%] [G loss: 0.006134]\n",
      "epoch:21 step:16700 [D loss: 0.014617, acc.: 100.00%] [G loss: 2.878240]\n",
      "epoch:21 step:16701 [D loss: 0.033867, acc.: 100.00%] [G loss: 0.006330]\n",
      "epoch:21 step:16702 [D loss: 0.028234, acc.: 99.22%] [G loss: 0.003831]\n",
      "epoch:21 step:16703 [D loss: 0.020419, acc.: 100.00%] [G loss: 0.065109]\n",
      "epoch:21 step:16704 [D loss: 0.010945, acc.: 100.00%] [G loss: 4.672744]\n",
      "epoch:21 step:16705 [D loss: 0.003188, acc.: 100.00%] [G loss: 3.879766]\n",
      "epoch:21 step:16706 [D loss: 0.043122, acc.: 99.22%] [G loss: 0.007195]\n",
      "epoch:21 step:16707 [D loss: 0.031337, acc.: 99.22%] [G loss: 0.057870]\n",
      "epoch:21 step:16708 [D loss: 0.008598, acc.: 100.00%] [G loss: 0.121384]\n",
      "epoch:21 step:16709 [D loss: 0.017396, acc.: 100.00%] [G loss: 2.689266]\n",
      "epoch:21 step:16710 [D loss: 0.018749, acc.: 100.00%] [G loss: 2.580156]\n",
      "epoch:21 step:16711 [D loss: 0.031478, acc.: 100.00%] [G loss: 0.035128]\n",
      "epoch:21 step:16712 [D loss: 0.033603, acc.: 100.00%] [G loss: 0.061933]\n",
      "epoch:21 step:16713 [D loss: 0.044391, acc.: 99.22%] [G loss: 1.648403]\n",
      "epoch:21 step:16714 [D loss: 0.038492, acc.: 100.00%] [G loss: 0.005918]\n",
      "epoch:21 step:16715 [D loss: 0.015058, acc.: 100.00%] [G loss: 0.034584]\n",
      "epoch:21 step:16716 [D loss: 0.206744, acc.: 91.41%] [G loss: 0.246342]\n",
      "epoch:21 step:16717 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.392795]\n",
      "epoch:21 step:16718 [D loss: 0.029304, acc.: 99.22%] [G loss: 0.204297]\n",
      "epoch:21 step:16719 [D loss: 0.000691, acc.: 100.00%] [G loss: 0.370313]\n",
      "epoch:21 step:16720 [D loss: 0.001441, acc.: 100.00%] [G loss: 3.640164]\n",
      "epoch:21 step:16721 [D loss: 0.038859, acc.: 97.66%] [G loss: 0.038012]\n",
      "epoch:21 step:16722 [D loss: 0.005942, acc.: 100.00%] [G loss: 0.267114]\n",
      "epoch:21 step:16723 [D loss: 0.001188, acc.: 100.00%] [G loss: 0.004928]\n",
      "epoch:21 step:16724 [D loss: 0.001961, acc.: 100.00%] [G loss: 0.044796]\n",
      "epoch:21 step:16725 [D loss: 0.009962, acc.: 100.00%] [G loss: 0.301730]\n",
      "epoch:21 step:16726 [D loss: 0.036428, acc.: 100.00%] [G loss: 0.508691]\n",
      "epoch:21 step:16727 [D loss: 0.003000, acc.: 100.00%] [G loss: 1.095036]\n",
      "epoch:21 step:16728 [D loss: 0.005310, acc.: 100.00%] [G loss: 3.054559]\n",
      "epoch:21 step:16729 [D loss: 0.023607, acc.: 100.00%] [G loss: 0.067050]\n",
      "epoch:21 step:16730 [D loss: 0.022397, acc.: 100.00%] [G loss: 0.078223]\n",
      "epoch:21 step:16731 [D loss: 0.117485, acc.: 96.09%] [G loss: 2.767148]\n",
      "epoch:21 step:16732 [D loss: 0.099004, acc.: 96.88%] [G loss: 0.013230]\n",
      "epoch:21 step:16733 [D loss: 0.033370, acc.: 99.22%] [G loss: 3.421469]\n",
      "epoch:21 step:16734 [D loss: 0.005679, acc.: 100.00%] [G loss: 0.170712]\n",
      "epoch:21 step:16735 [D loss: 0.022402, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:21 step:16736 [D loss: 0.009298, acc.: 100.00%] [G loss: 1.743151]\n",
      "epoch:21 step:16737 [D loss: 0.384031, acc.: 81.25%] [G loss: 8.430891]\n",
      "epoch:21 step:16738 [D loss: 0.612801, acc.: 76.56%] [G loss: 4.016771]\n",
      "epoch:21 step:16739 [D loss: 0.339393, acc.: 86.72%] [G loss: 7.926181]\n",
      "epoch:21 step:16740 [D loss: 0.066620, acc.: 95.31%] [G loss: 0.023658]\n",
      "epoch:21 step:16741 [D loss: 0.606498, acc.: 74.22%] [G loss: 0.000000]\n",
      "epoch:21 step:16742 [D loss: 3.015106, acc.: 51.56%] [G loss: 2.229981]\n",
      "epoch:21 step:16743 [D loss: 1.110673, acc.: 57.03%] [G loss: 0.345822]\n",
      "epoch:21 step:16744 [D loss: 0.267110, acc.: 92.97%] [G loss: 0.050905]\n",
      "epoch:21 step:16745 [D loss: 0.047003, acc.: 98.44%] [G loss: 4.493248]\n",
      "epoch:21 step:16746 [D loss: 0.098485, acc.: 96.09%] [G loss: 0.020990]\n",
      "epoch:21 step:16747 [D loss: 0.075558, acc.: 98.44%] [G loss: 3.790205]\n",
      "epoch:21 step:16748 [D loss: 0.042496, acc.: 99.22%] [G loss: 0.009071]\n",
      "epoch:21 step:16749 [D loss: 0.095720, acc.: 97.66%] [G loss: 0.016768]\n",
      "epoch:21 step:16750 [D loss: 0.023862, acc.: 100.00%] [G loss: 0.002079]\n",
      "epoch:21 step:16751 [D loss: 0.101618, acc.: 96.09%] [G loss: 0.004048]\n",
      "epoch:21 step:16752 [D loss: 0.035338, acc.: 100.00%] [G loss: 2.061167]\n",
      "epoch:21 step:16753 [D loss: 0.059752, acc.: 99.22%] [G loss: 2.845233]\n",
      "epoch:21 step:16754 [D loss: 0.086978, acc.: 98.44%] [G loss: 0.018182]\n",
      "epoch:21 step:16755 [D loss: 0.059467, acc.: 98.44%] [G loss: 0.014243]\n",
      "epoch:21 step:16756 [D loss: 0.028372, acc.: 99.22%] [G loss: 0.001834]\n",
      "epoch:21 step:16757 [D loss: 0.030712, acc.: 100.00%] [G loss: 0.916142]\n",
      "epoch:21 step:16758 [D loss: 0.116734, acc.: 99.22%] [G loss: 0.007641]\n",
      "epoch:21 step:16759 [D loss: 0.044369, acc.: 99.22%] [G loss: 0.006089]\n",
      "epoch:21 step:16760 [D loss: 0.033540, acc.: 99.22%] [G loss: 0.008543]\n",
      "epoch:21 step:16761 [D loss: 0.003716, acc.: 100.00%] [G loss: 0.901038]\n",
      "epoch:21 step:16762 [D loss: 0.116376, acc.: 95.31%] [G loss: 0.001641]\n",
      "epoch:21 step:16763 [D loss: 0.006681, acc.: 100.00%] [G loss: 0.001559]\n",
      "epoch:21 step:16764 [D loss: 0.005895, acc.: 100.00%] [G loss: 1.255643]\n",
      "epoch:21 step:16765 [D loss: 0.032797, acc.: 99.22%] [G loss: 0.000447]\n",
      "epoch:21 step:16766 [D loss: 0.040630, acc.: 99.22%] [G loss: 0.001283]\n",
      "epoch:21 step:16767 [D loss: 0.006192, acc.: 100.00%] [G loss: 0.007820]\n",
      "epoch:21 step:16768 [D loss: 0.023823, acc.: 100.00%] [G loss: 0.250338]\n",
      "epoch:21 step:16769 [D loss: 0.074197, acc.: 98.44%] [G loss: 0.000386]\n",
      "epoch:21 step:16770 [D loss: 0.028784, acc.: 98.44%] [G loss: 0.000099]\n",
      "epoch:21 step:16771 [D loss: 0.119003, acc.: 96.88%] [G loss: 0.028655]\n",
      "epoch:21 step:16772 [D loss: 0.094533, acc.: 96.88%] [G loss: 0.216613]\n",
      "epoch:21 step:16773 [D loss: 0.135737, acc.: 92.19%] [G loss: 0.000053]\n",
      "epoch:21 step:16774 [D loss: 0.219123, acc.: 89.84%] [G loss: 0.026675]\n",
      "epoch:21 step:16775 [D loss: 0.070611, acc.: 97.66%] [G loss: 4.228351]\n",
      "epoch:21 step:16776 [D loss: 0.143652, acc.: 93.75%] [G loss: 0.269249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16777 [D loss: 0.008792, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:21 step:16778 [D loss: 0.083588, acc.: 98.44%] [G loss: 0.025564]\n",
      "epoch:21 step:16779 [D loss: 0.012606, acc.: 100.00%] [G loss: 1.364111]\n",
      "epoch:21 step:16780 [D loss: 0.010722, acc.: 100.00%] [G loss: 0.038515]\n",
      "epoch:21 step:16781 [D loss: 0.040567, acc.: 99.22%] [G loss: 0.242489]\n",
      "epoch:21 step:16782 [D loss: 0.063452, acc.: 97.66%] [G loss: 0.043418]\n",
      "epoch:21 step:16783 [D loss: 0.017519, acc.: 99.22%] [G loss: 1.130966]\n",
      "epoch:21 step:16784 [D loss: 0.161595, acc.: 94.53%] [G loss: 1.127649]\n",
      "epoch:21 step:16785 [D loss: 0.328036, acc.: 85.94%] [G loss: 0.000201]\n",
      "epoch:21 step:16786 [D loss: 0.144409, acc.: 92.19%] [G loss: 0.044912]\n",
      "epoch:21 step:16787 [D loss: 0.073650, acc.: 96.09%] [G loss: 6.763017]\n",
      "epoch:21 step:16788 [D loss: 0.040153, acc.: 98.44%] [G loss: 0.005140]\n",
      "epoch:21 step:16789 [D loss: 0.030779, acc.: 99.22%] [G loss: 0.002607]\n",
      "epoch:21 step:16790 [D loss: 0.082498, acc.: 96.88%] [G loss: 0.075756]\n",
      "epoch:21 step:16791 [D loss: 0.019959, acc.: 100.00%] [G loss: 0.145712]\n",
      "epoch:21 step:16792 [D loss: 0.122082, acc.: 96.88%] [G loss: 4.556449]\n",
      "epoch:21 step:16793 [D loss: 0.004677, acc.: 100.00%] [G loss: 0.008224]\n",
      "epoch:21 step:16794 [D loss: 0.004503, acc.: 100.00%] [G loss: 2.521382]\n",
      "epoch:21 step:16795 [D loss: 0.005876, acc.: 100.00%] [G loss: 0.005076]\n",
      "epoch:21 step:16796 [D loss: 0.018786, acc.: 100.00%] [G loss: 0.026516]\n",
      "epoch:21 step:16797 [D loss: 0.015114, acc.: 100.00%] [G loss: 1.188701]\n",
      "epoch:21 step:16798 [D loss: 0.032718, acc.: 100.00%] [G loss: 2.147649]\n",
      "epoch:21 step:16799 [D loss: 0.080077, acc.: 98.44%] [G loss: 1.807155]\n",
      "epoch:21 step:16800 [D loss: 0.087370, acc.: 98.44%] [G loss: 1.390657]\n",
      "epoch:21 step:16801 [D loss: 0.226830, acc.: 90.62%] [G loss: 2.936314]\n",
      "epoch:21 step:16802 [D loss: 0.799359, acc.: 67.97%] [G loss: 0.000605]\n",
      "epoch:21 step:16803 [D loss: 0.079067, acc.: 98.44%] [G loss: 0.005053]\n",
      "epoch:21 step:16804 [D loss: 0.006695, acc.: 100.00%] [G loss: 2.899230]\n",
      "epoch:21 step:16805 [D loss: 0.040926, acc.: 99.22%] [G loss: 4.182736]\n",
      "epoch:21 step:16806 [D loss: 0.022368, acc.: 100.00%] [G loss: 0.039624]\n",
      "epoch:21 step:16807 [D loss: 0.095209, acc.: 98.44%] [G loss: 0.004329]\n",
      "epoch:21 step:16808 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.045070]\n",
      "epoch:21 step:16809 [D loss: 0.008910, acc.: 100.00%] [G loss: 0.031191]\n",
      "epoch:21 step:16810 [D loss: 0.028432, acc.: 99.22%] [G loss: 2.901770]\n",
      "epoch:21 step:16811 [D loss: 0.038233, acc.: 99.22%] [G loss: 0.009161]\n",
      "epoch:21 step:16812 [D loss: 0.028130, acc.: 100.00%] [G loss: 2.547933]\n",
      "epoch:21 step:16813 [D loss: 0.005090, acc.: 100.00%] [G loss: 2.256927]\n",
      "epoch:21 step:16814 [D loss: 0.132795, acc.: 94.53%] [G loss: 4.003667]\n",
      "epoch:21 step:16815 [D loss: 0.011257, acc.: 100.00%] [G loss: 4.682921]\n",
      "epoch:21 step:16816 [D loss: 0.119130, acc.: 96.88%] [G loss: 0.056727]\n",
      "epoch:21 step:16817 [D loss: 0.002525, acc.: 100.00%] [G loss: 0.011467]\n",
      "epoch:21 step:16818 [D loss: 0.025577, acc.: 99.22%] [G loss: 2.560126]\n",
      "epoch:21 step:16819 [D loss: 0.007137, acc.: 100.00%] [G loss: 0.007989]\n",
      "epoch:21 step:16820 [D loss: 0.031038, acc.: 100.00%] [G loss: 0.028876]\n",
      "epoch:21 step:16821 [D loss: 0.056482, acc.: 98.44%] [G loss: 1.791620]\n",
      "epoch:21 step:16822 [D loss: 0.122630, acc.: 94.53%] [G loss: 0.316915]\n",
      "epoch:21 step:16823 [D loss: 0.130756, acc.: 94.53%] [G loss: 0.002559]\n",
      "epoch:21 step:16824 [D loss: 0.010198, acc.: 100.00%] [G loss: 0.013074]\n",
      "epoch:21 step:16825 [D loss: 0.011963, acc.: 99.22%] [G loss: 0.000422]\n",
      "epoch:21 step:16826 [D loss: 0.008219, acc.: 100.00%] [G loss: 3.297283]\n",
      "epoch:21 step:16827 [D loss: 0.002687, acc.: 100.00%] [G loss: 2.194082]\n",
      "epoch:21 step:16828 [D loss: 0.013636, acc.: 100.00%] [G loss: 0.123500]\n",
      "epoch:21 step:16829 [D loss: 0.008225, acc.: 100.00%] [G loss: 0.029334]\n",
      "epoch:21 step:16830 [D loss: 0.006120, acc.: 100.00%] [G loss: 0.008919]\n",
      "epoch:21 step:16831 [D loss: 0.069739, acc.: 99.22%] [G loss: 0.021476]\n",
      "epoch:21 step:16832 [D loss: 0.021051, acc.: 99.22%] [G loss: 3.749056]\n",
      "epoch:21 step:16833 [D loss: 0.018680, acc.: 99.22%] [G loss: 0.010947]\n",
      "epoch:21 step:16834 [D loss: 0.009005, acc.: 100.00%] [G loss: 1.441150]\n",
      "epoch:21 step:16835 [D loss: 0.028341, acc.: 100.00%] [G loss: 1.797079]\n",
      "epoch:21 step:16836 [D loss: 0.045926, acc.: 99.22%] [G loss: 4.123562]\n",
      "epoch:21 step:16837 [D loss: 0.180442, acc.: 93.75%] [G loss: 0.001645]\n",
      "epoch:21 step:16838 [D loss: 0.032453, acc.: 100.00%] [G loss: 0.019590]\n",
      "epoch:21 step:16839 [D loss: 0.001420, acc.: 100.00%] [G loss: 0.025596]\n",
      "epoch:21 step:16840 [D loss: 0.002076, acc.: 100.00%] [G loss: 0.155054]\n",
      "epoch:21 step:16841 [D loss: 0.007214, acc.: 100.00%] [G loss: 0.126023]\n",
      "epoch:21 step:16842 [D loss: 0.009646, acc.: 100.00%] [G loss: 0.059918]\n",
      "epoch:21 step:16843 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.007447]\n",
      "epoch:21 step:16844 [D loss: 0.002553, acc.: 100.00%] [G loss: 0.013015]\n",
      "epoch:21 step:16845 [D loss: 0.007053, acc.: 100.00%] [G loss: 0.036681]\n",
      "epoch:21 step:16846 [D loss: 0.019391, acc.: 100.00%] [G loss: 0.231083]\n",
      "epoch:21 step:16847 [D loss: 0.004820, acc.: 100.00%] [G loss: 5.644434]\n",
      "epoch:21 step:16848 [D loss: 0.010946, acc.: 100.00%] [G loss: 0.007706]\n",
      "epoch:21 step:16849 [D loss: 0.042389, acc.: 99.22%] [G loss: 0.002604]\n",
      "epoch:21 step:16850 [D loss: 0.058866, acc.: 98.44%] [G loss: 0.130446]\n",
      "epoch:21 step:16851 [D loss: 0.002304, acc.: 100.00%] [G loss: 0.675500]\n",
      "epoch:21 step:16852 [D loss: 0.015558, acc.: 99.22%] [G loss: 0.339736]\n",
      "epoch:21 step:16853 [D loss: 0.031363, acc.: 100.00%] [G loss: 0.430032]\n",
      "epoch:21 step:16854 [D loss: 0.084061, acc.: 97.66%] [G loss: 0.400450]\n",
      "epoch:21 step:16855 [D loss: 0.033893, acc.: 100.00%] [G loss: 0.625223]\n",
      "epoch:21 step:16856 [D loss: 0.044848, acc.: 98.44%] [G loss: 6.915785]\n",
      "epoch:21 step:16857 [D loss: 0.034372, acc.: 98.44%] [G loss: 0.200837]\n",
      "epoch:21 step:16858 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.193438]\n",
      "epoch:21 step:16859 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.097970]\n",
      "epoch:21 step:16860 [D loss: 0.018958, acc.: 100.00%] [G loss: 0.165631]\n",
      "epoch:21 step:16861 [D loss: 0.209790, acc.: 92.19%] [G loss: 3.285726]\n",
      "epoch:21 step:16862 [D loss: 0.116710, acc.: 93.75%] [G loss: 2.478042]\n",
      "epoch:21 step:16863 [D loss: 0.006863, acc.: 100.00%] [G loss: 1.051666]\n",
      "epoch:21 step:16864 [D loss: 0.011300, acc.: 100.00%] [G loss: 0.407251]\n",
      "epoch:21 step:16865 [D loss: 0.007280, acc.: 100.00%] [G loss: 0.416072]\n",
      "epoch:21 step:16866 [D loss: 0.082419, acc.: 97.66%] [G loss: 7.171435]\n",
      "epoch:21 step:16867 [D loss: 0.032464, acc.: 98.44%] [G loss: 3.103749]\n",
      "epoch:21 step:16868 [D loss: 0.171222, acc.: 93.75%] [G loss: 0.024143]\n",
      "epoch:21 step:16869 [D loss: 0.004018, acc.: 100.00%] [G loss: 0.007491]\n",
      "epoch:21 step:16870 [D loss: 0.660635, acc.: 73.44%] [G loss: 11.305914]\n",
      "epoch:21 step:16871 [D loss: 0.948498, acc.: 64.06%] [G loss: 7.762984]\n",
      "epoch:21 step:16872 [D loss: 0.089890, acc.: 98.44%] [G loss: 5.233826]\n",
      "epoch:21 step:16873 [D loss: 0.005270, acc.: 100.00%] [G loss: 2.128828]\n",
      "epoch:21 step:16874 [D loss: 0.018927, acc.: 100.00%] [G loss: 0.618384]\n",
      "epoch:21 step:16875 [D loss: 0.067444, acc.: 96.88%] [G loss: 4.033033]\n",
      "epoch:21 step:16876 [D loss: 0.010447, acc.: 100.00%] [G loss: 4.438960]\n",
      "epoch:21 step:16877 [D loss: 0.003723, acc.: 100.00%] [G loss: 1.158211]\n",
      "epoch:21 step:16878 [D loss: 0.001333, acc.: 100.00%] [G loss: 3.141622]\n",
      "epoch:21 step:16879 [D loss: 0.026938, acc.: 100.00%] [G loss: 0.338242]\n",
      "epoch:21 step:16880 [D loss: 0.076552, acc.: 97.66%] [G loss: 3.773187]\n",
      "epoch:21 step:16881 [D loss: 0.043788, acc.: 98.44%] [G loss: 3.578374]\n",
      "epoch:21 step:16882 [D loss: 0.025043, acc.: 100.00%] [G loss: 2.120660]\n",
      "epoch:21 step:16883 [D loss: 0.062013, acc.: 98.44%] [G loss: 2.806552]\n",
      "epoch:21 step:16884 [D loss: 0.026169, acc.: 99.22%] [G loss: 1.315533]\n",
      "epoch:21 step:16885 [D loss: 0.328861, acc.: 83.59%] [G loss: 7.009689]\n",
      "epoch:21 step:16886 [D loss: 1.320764, acc.: 58.59%] [G loss: 1.817410]\n",
      "epoch:21 step:16887 [D loss: 0.040944, acc.: 98.44%] [G loss: 0.518164]\n",
      "epoch:21 step:16888 [D loss: 0.013774, acc.: 100.00%] [G loss: 0.586413]\n",
      "epoch:21 step:16889 [D loss: 0.004813, acc.: 100.00%] [G loss: 1.137056]\n",
      "epoch:21 step:16890 [D loss: 0.009418, acc.: 100.00%] [G loss: 0.322540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16891 [D loss: 0.027290, acc.: 99.22%] [G loss: 0.258050]\n",
      "epoch:21 step:16892 [D loss: 0.010890, acc.: 100.00%] [G loss: 0.305928]\n",
      "epoch:21 step:16893 [D loss: 0.008494, acc.: 100.00%] [G loss: 0.526589]\n",
      "epoch:21 step:16894 [D loss: 0.007708, acc.: 100.00%] [G loss: 0.146137]\n",
      "epoch:21 step:16895 [D loss: 0.087126, acc.: 97.66%] [G loss: 6.616716]\n",
      "epoch:21 step:16896 [D loss: 0.007348, acc.: 100.00%] [G loss: 2.823809]\n",
      "epoch:21 step:16897 [D loss: 0.034241, acc.: 100.00%] [G loss: 1.735459]\n",
      "epoch:21 step:16898 [D loss: 0.009830, acc.: 100.00%] [G loss: 5.489543]\n",
      "epoch:21 step:16899 [D loss: 0.106891, acc.: 96.88%] [G loss: 5.703689]\n",
      "epoch:21 step:16900 [D loss: 0.018230, acc.: 100.00%] [G loss: 3.161696]\n",
      "epoch:21 step:16901 [D loss: 0.262463, acc.: 91.41%] [G loss: 1.932319]\n",
      "epoch:21 step:16902 [D loss: 0.113209, acc.: 96.88%] [G loss: 0.970020]\n",
      "epoch:21 step:16903 [D loss: 0.010575, acc.: 100.00%] [G loss: 3.682706]\n",
      "epoch:21 step:16904 [D loss: 0.053363, acc.: 100.00%] [G loss: 4.127620]\n",
      "epoch:21 step:16905 [D loss: 0.014199, acc.: 100.00%] [G loss: 1.756036]\n",
      "epoch:21 step:16906 [D loss: 0.022595, acc.: 100.00%] [G loss: 1.939638]\n",
      "epoch:21 step:16907 [D loss: 0.011617, acc.: 100.00%] [G loss: 0.708483]\n",
      "epoch:21 step:16908 [D loss: 0.321026, acc.: 86.72%] [G loss: 5.669917]\n",
      "epoch:21 step:16909 [D loss: 0.892069, acc.: 63.28%] [G loss: 1.218773]\n",
      "epoch:21 step:16910 [D loss: 0.436895, acc.: 81.25%] [G loss: 4.913159]\n",
      "epoch:21 step:16911 [D loss: 0.008392, acc.: 100.00%] [G loss: 6.934748]\n",
      "epoch:21 step:16912 [D loss: 0.474094, acc.: 82.81%] [G loss: 3.308417]\n",
      "epoch:21 step:16913 [D loss: 0.163211, acc.: 91.41%] [G loss: 6.196372]\n",
      "epoch:21 step:16914 [D loss: 0.052713, acc.: 98.44%] [G loss: 6.331770]\n",
      "epoch:21 step:16915 [D loss: 0.020499, acc.: 100.00%] [G loss: 3.557415]\n",
      "epoch:21 step:16916 [D loss: 0.025133, acc.: 99.22%] [G loss: 5.572394]\n",
      "epoch:21 step:16917 [D loss: 0.040060, acc.: 99.22%] [G loss: 5.144501]\n",
      "epoch:21 step:16918 [D loss: 0.009399, acc.: 100.00%] [G loss: 5.255227]\n",
      "epoch:21 step:16919 [D loss: 0.013126, acc.: 100.00%] [G loss: 0.501808]\n",
      "epoch:21 step:16920 [D loss: 0.050412, acc.: 100.00%] [G loss: 5.166856]\n",
      "epoch:21 step:16921 [D loss: 0.037446, acc.: 99.22%] [G loss: 0.926624]\n",
      "epoch:21 step:16922 [D loss: 0.006622, acc.: 100.00%] [G loss: 6.714107]\n",
      "epoch:21 step:16923 [D loss: 0.081634, acc.: 97.66%] [G loss: 0.211161]\n",
      "epoch:21 step:16924 [D loss: 0.333427, acc.: 88.28%] [G loss: 5.283628]\n",
      "epoch:21 step:16925 [D loss: 1.069796, acc.: 60.16%] [G loss: 1.297594]\n",
      "epoch:21 step:16926 [D loss: 0.052779, acc.: 98.44%] [G loss: 0.968292]\n",
      "epoch:21 step:16927 [D loss: 0.046311, acc.: 98.44%] [G loss: 0.864394]\n",
      "epoch:21 step:16928 [D loss: 0.004154, acc.: 100.00%] [G loss: 1.039651]\n",
      "epoch:21 step:16929 [D loss: 0.029758, acc.: 99.22%] [G loss: 1.519914]\n",
      "epoch:21 step:16930 [D loss: 0.007082, acc.: 100.00%] [G loss: 4.254958]\n",
      "epoch:21 step:16931 [D loss: 0.007804, acc.: 100.00%] [G loss: 1.882972]\n",
      "epoch:21 step:16932 [D loss: 0.003107, acc.: 100.00%] [G loss: 0.540916]\n",
      "epoch:21 step:16933 [D loss: 0.015745, acc.: 100.00%] [G loss: 0.363845]\n",
      "epoch:21 step:16934 [D loss: 0.004423, acc.: 100.00%] [G loss: 0.281367]\n",
      "epoch:21 step:16935 [D loss: 0.004878, acc.: 100.00%] [G loss: 0.343769]\n",
      "epoch:21 step:16936 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.133130]\n",
      "epoch:21 step:16937 [D loss: 0.014005, acc.: 99.22%] [G loss: 0.126722]\n",
      "epoch:21 step:16938 [D loss: 0.076441, acc.: 98.44%] [G loss: 0.302431]\n",
      "epoch:21 step:16939 [D loss: 0.004746, acc.: 100.00%] [G loss: 0.487219]\n",
      "epoch:21 step:16940 [D loss: 0.011993, acc.: 100.00%] [G loss: 0.224495]\n",
      "epoch:21 step:16941 [D loss: 0.002321, acc.: 100.00%] [G loss: 0.468570]\n",
      "epoch:21 step:16942 [D loss: 0.012553, acc.: 100.00%] [G loss: 1.235554]\n",
      "epoch:21 step:16943 [D loss: 0.044922, acc.: 98.44%] [G loss: 0.153791]\n",
      "epoch:21 step:16944 [D loss: 0.011590, acc.: 100.00%] [G loss: 0.171179]\n",
      "epoch:21 step:16945 [D loss: 0.003995, acc.: 100.00%] [G loss: 0.197028]\n",
      "epoch:21 step:16946 [D loss: 0.002717, acc.: 100.00%] [G loss: 3.586254]\n",
      "epoch:21 step:16947 [D loss: 0.093949, acc.: 98.44%] [G loss: 0.420395]\n",
      "epoch:21 step:16948 [D loss: 0.067366, acc.: 96.88%] [G loss: 1.259520]\n",
      "epoch:21 step:16949 [D loss: 0.021879, acc.: 100.00%] [G loss: 1.165775]\n",
      "epoch:21 step:16950 [D loss: 0.007747, acc.: 100.00%] [G loss: 1.166087]\n",
      "epoch:21 step:16951 [D loss: 0.009989, acc.: 100.00%] [G loss: 0.539882]\n",
      "epoch:21 step:16952 [D loss: 0.000869, acc.: 100.00%] [G loss: 1.634720]\n",
      "epoch:21 step:16953 [D loss: 0.003443, acc.: 100.00%] [G loss: 2.697496]\n",
      "epoch:21 step:16954 [D loss: 0.008680, acc.: 100.00%] [G loss: 0.266814]\n",
      "epoch:21 step:16955 [D loss: 0.066952, acc.: 99.22%] [G loss: 0.806541]\n",
      "epoch:21 step:16956 [D loss: 0.006959, acc.: 100.00%] [G loss: 1.246813]\n",
      "epoch:21 step:16957 [D loss: 0.008972, acc.: 100.00%] [G loss: 0.899333]\n",
      "epoch:21 step:16958 [D loss: 0.088679, acc.: 97.66%] [G loss: 0.212165]\n",
      "epoch:21 step:16959 [D loss: 0.098467, acc.: 97.66%] [G loss: 0.970023]\n",
      "epoch:21 step:16960 [D loss: 0.005789, acc.: 100.00%] [G loss: 1.464560]\n",
      "epoch:21 step:16961 [D loss: 0.023870, acc.: 100.00%] [G loss: 0.671852]\n",
      "epoch:21 step:16962 [D loss: 0.020220, acc.: 99.22%] [G loss: 0.213679]\n",
      "epoch:21 step:16963 [D loss: 0.012443, acc.: 100.00%] [G loss: 0.287566]\n",
      "epoch:21 step:16964 [D loss: 0.001459, acc.: 100.00%] [G loss: 0.404629]\n",
      "epoch:21 step:16965 [D loss: 0.000655, acc.: 100.00%] [G loss: 3.785554]\n",
      "epoch:21 step:16966 [D loss: 0.036441, acc.: 99.22%] [G loss: 0.349330]\n",
      "epoch:21 step:16967 [D loss: 0.029661, acc.: 100.00%] [G loss: 0.327165]\n",
      "epoch:21 step:16968 [D loss: 0.012911, acc.: 100.00%] [G loss: 0.285063]\n",
      "epoch:21 step:16969 [D loss: 0.005855, acc.: 100.00%] [G loss: 0.626533]\n",
      "epoch:21 step:16970 [D loss: 0.004233, acc.: 100.00%] [G loss: 0.673964]\n",
      "epoch:21 step:16971 [D loss: 0.007792, acc.: 100.00%] [G loss: 1.036887]\n",
      "epoch:21 step:16972 [D loss: 0.037573, acc.: 100.00%] [G loss: 0.768979]\n",
      "epoch:21 step:16973 [D loss: 0.016978, acc.: 100.00%] [G loss: 1.484194]\n",
      "epoch:21 step:16974 [D loss: 0.014998, acc.: 100.00%] [G loss: 1.740041]\n",
      "epoch:21 step:16975 [D loss: 0.014653, acc.: 99.22%] [G loss: 0.312132]\n",
      "epoch:21 step:16976 [D loss: 0.042777, acc.: 98.44%] [G loss: 0.175003]\n",
      "epoch:21 step:16977 [D loss: 0.019554, acc.: 99.22%] [G loss: 0.619751]\n",
      "epoch:21 step:16978 [D loss: 0.016858, acc.: 100.00%] [G loss: 0.824411]\n",
      "epoch:21 step:16979 [D loss: 0.013115, acc.: 100.00%] [G loss: 0.610080]\n",
      "epoch:21 step:16980 [D loss: 0.004396, acc.: 100.00%] [G loss: 0.353907]\n",
      "epoch:21 step:16981 [D loss: 0.009862, acc.: 100.00%] [G loss: 0.538734]\n",
      "epoch:21 step:16982 [D loss: 0.006095, acc.: 100.00%] [G loss: 0.313286]\n",
      "epoch:21 step:16983 [D loss: 0.005086, acc.: 100.00%] [G loss: 0.465866]\n",
      "epoch:21 step:16984 [D loss: 0.006115, acc.: 100.00%] [G loss: 3.519718]\n",
      "epoch:21 step:16985 [D loss: 0.067076, acc.: 98.44%] [G loss: 0.930821]\n",
      "epoch:21 step:16986 [D loss: 0.009521, acc.: 100.00%] [G loss: 2.914946]\n",
      "epoch:21 step:16987 [D loss: 0.054144, acc.: 98.44%] [G loss: 1.526410]\n",
      "epoch:21 step:16988 [D loss: 0.049827, acc.: 97.66%] [G loss: 1.873436]\n",
      "epoch:21 step:16989 [D loss: 0.022103, acc.: 98.44%] [G loss: 1.194608]\n",
      "epoch:21 step:16990 [D loss: 0.023442, acc.: 99.22%] [G loss: 3.855458]\n",
      "epoch:21 step:16991 [D loss: 0.022964, acc.: 100.00%] [G loss: 0.756437]\n",
      "epoch:21 step:16992 [D loss: 0.063685, acc.: 98.44%] [G loss: 1.536900]\n",
      "epoch:21 step:16993 [D loss: 0.031537, acc.: 98.44%] [G loss: 1.638812]\n",
      "epoch:21 step:16994 [D loss: 0.030214, acc.: 99.22%] [G loss: 0.843311]\n",
      "epoch:21 step:16995 [D loss: 0.015268, acc.: 100.00%] [G loss: 0.536058]\n",
      "epoch:21 step:16996 [D loss: 0.002512, acc.: 100.00%] [G loss: 0.420185]\n",
      "epoch:21 step:16997 [D loss: 0.013354, acc.: 100.00%] [G loss: 0.321768]\n",
      "epoch:21 step:16998 [D loss: 0.012027, acc.: 100.00%] [G loss: 0.249548]\n",
      "epoch:21 step:16999 [D loss: 0.002180, acc.: 100.00%] [G loss: 0.247988]\n",
      "epoch:21 step:17000 [D loss: 0.030096, acc.: 99.22%] [G loss: 0.480496]\n",
      "epoch:21 step:17001 [D loss: 0.025996, acc.: 100.00%] [G loss: 0.224706]\n",
      "epoch:21 step:17002 [D loss: 0.020363, acc.: 100.00%] [G loss: 4.829465]\n",
      "epoch:21 step:17003 [D loss: 0.011040, acc.: 100.00%] [G loss: 2.277312]\n",
      "epoch:21 step:17004 [D loss: 0.002725, acc.: 100.00%] [G loss: 1.330772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17005 [D loss: 0.170625, acc.: 92.97%] [G loss: 0.079034]\n",
      "epoch:21 step:17006 [D loss: 0.071777, acc.: 99.22%] [G loss: 0.068465]\n",
      "epoch:21 step:17007 [D loss: 0.000447, acc.: 100.00%] [G loss: 6.650807]\n",
      "epoch:21 step:17008 [D loss: 0.001537, acc.: 100.00%] [G loss: 0.197875]\n",
      "epoch:21 step:17009 [D loss: 0.001294, acc.: 100.00%] [G loss: 2.876833]\n",
      "epoch:21 step:17010 [D loss: 0.011989, acc.: 100.00%] [G loss: 0.759347]\n",
      "epoch:21 step:17011 [D loss: 0.026159, acc.: 100.00%] [G loss: 0.065371]\n",
      "epoch:21 step:17012 [D loss: 0.001779, acc.: 100.00%] [G loss: 0.063217]\n",
      "epoch:21 step:17013 [D loss: 0.001969, acc.: 100.00%] [G loss: 0.049276]\n",
      "epoch:21 step:17014 [D loss: 0.022693, acc.: 100.00%] [G loss: 0.038343]\n",
      "epoch:21 step:17015 [D loss: 0.027724, acc.: 100.00%] [G loss: 0.098684]\n",
      "epoch:21 step:17016 [D loss: 0.004879, acc.: 100.00%] [G loss: 0.180440]\n",
      "epoch:21 step:17017 [D loss: 0.001760, acc.: 100.00%] [G loss: 1.320859]\n",
      "epoch:21 step:17018 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.078607]\n",
      "epoch:21 step:17019 [D loss: 0.002112, acc.: 100.00%] [G loss: 0.106174]\n",
      "epoch:21 step:17020 [D loss: 0.003276, acc.: 100.00%] [G loss: 0.116075]\n",
      "epoch:21 step:17021 [D loss: 0.005002, acc.: 100.00%] [G loss: 0.122024]\n",
      "epoch:21 step:17022 [D loss: 0.003481, acc.: 100.00%] [G loss: 0.240063]\n",
      "epoch:21 step:17023 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.179687]\n",
      "epoch:21 step:17024 [D loss: 0.025345, acc.: 99.22%] [G loss: 0.017562]\n",
      "epoch:21 step:17025 [D loss: 0.004117, acc.: 100.00%] [G loss: 0.176139]\n",
      "epoch:21 step:17026 [D loss: 0.009196, acc.: 100.00%] [G loss: 0.390548]\n",
      "epoch:21 step:17027 [D loss: 0.001093, acc.: 100.00%] [G loss: 0.235685]\n",
      "epoch:21 step:17028 [D loss: 0.001291, acc.: 100.00%] [G loss: 5.458589]\n",
      "epoch:21 step:17029 [D loss: 0.007813, acc.: 100.00%] [G loss: 0.109091]\n",
      "epoch:21 step:17030 [D loss: 0.001286, acc.: 100.00%] [G loss: 4.819819]\n",
      "epoch:21 step:17031 [D loss: 0.017212, acc.: 100.00%] [G loss: 0.215351]\n",
      "epoch:21 step:17032 [D loss: 0.004879, acc.: 100.00%] [G loss: 1.037175]\n",
      "epoch:21 step:17033 [D loss: 0.005345, acc.: 100.00%] [G loss: 0.302499]\n",
      "epoch:21 step:17034 [D loss: 0.015069, acc.: 100.00%] [G loss: 0.507528]\n",
      "epoch:21 step:17035 [D loss: 0.010647, acc.: 100.00%] [G loss: 0.504172]\n",
      "epoch:21 step:17036 [D loss: 0.033606, acc.: 99.22%] [G loss: 2.317441]\n",
      "epoch:21 step:17037 [D loss: 0.022566, acc.: 100.00%] [G loss: 0.135463]\n",
      "epoch:21 step:17038 [D loss: 0.028283, acc.: 98.44%] [G loss: 0.219975]\n",
      "epoch:21 step:17039 [D loss: 0.010088, acc.: 100.00%] [G loss: 0.358869]\n",
      "epoch:21 step:17040 [D loss: 0.019930, acc.: 100.00%] [G loss: 1.863544]\n",
      "epoch:21 step:17041 [D loss: 0.018950, acc.: 100.00%] [G loss: 1.394048]\n",
      "epoch:21 step:17042 [D loss: 0.001740, acc.: 100.00%] [G loss: 1.208022]\n",
      "epoch:21 step:17043 [D loss: 0.332011, acc.: 85.16%] [G loss: 1.560688]\n",
      "epoch:21 step:17044 [D loss: 0.215929, acc.: 90.62%] [G loss: 1.320888]\n",
      "epoch:21 step:17045 [D loss: 0.000633, acc.: 100.00%] [G loss: 1.784455]\n",
      "epoch:21 step:17046 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.616831]\n",
      "epoch:21 step:17047 [D loss: 0.013633, acc.: 100.00%] [G loss: 5.334230]\n",
      "epoch:21 step:17048 [D loss: 0.028843, acc.: 100.00%] [G loss: 1.514566]\n",
      "epoch:21 step:17049 [D loss: 0.008350, acc.: 100.00%] [G loss: 1.679641]\n",
      "epoch:21 step:17050 [D loss: 0.086822, acc.: 96.88%] [G loss: 0.174213]\n",
      "epoch:21 step:17051 [D loss: 0.245469, acc.: 89.06%] [G loss: 9.937580]\n",
      "epoch:21 step:17052 [D loss: 0.470688, acc.: 78.91%] [G loss: 2.137895]\n",
      "epoch:21 step:17053 [D loss: 0.310742, acc.: 86.72%] [G loss: 1.851820]\n",
      "epoch:21 step:17054 [D loss: 0.015786, acc.: 99.22%] [G loss: 3.759851]\n",
      "epoch:21 step:17055 [D loss: 0.666197, acc.: 75.78%] [G loss: 0.033879]\n",
      "epoch:21 step:17056 [D loss: 0.006634, acc.: 100.00%] [G loss: 0.008942]\n",
      "epoch:21 step:17057 [D loss: 0.943636, acc.: 68.75%] [G loss: 5.456057]\n",
      "epoch:21 step:17058 [D loss: 1.942205, acc.: 53.91%] [G loss: 7.323799]\n",
      "epoch:21 step:17059 [D loss: 0.004280, acc.: 100.00%] [G loss: 0.535266]\n",
      "epoch:21 step:17060 [D loss: 0.013877, acc.: 100.00%] [G loss: 2.989092]\n",
      "epoch:21 step:17061 [D loss: 0.002108, acc.: 100.00%] [G loss: 2.042667]\n",
      "epoch:21 step:17062 [D loss: 0.055500, acc.: 97.66%] [G loss: 0.494118]\n",
      "epoch:21 step:17063 [D loss: 0.010348, acc.: 100.00%] [G loss: 0.562699]\n",
      "epoch:21 step:17064 [D loss: 0.002118, acc.: 100.00%] [G loss: 1.985314]\n",
      "epoch:21 step:17065 [D loss: 0.045762, acc.: 99.22%] [G loss: 0.364398]\n",
      "epoch:21 step:17066 [D loss: 0.009779, acc.: 100.00%] [G loss: 0.840905]\n",
      "epoch:21 step:17067 [D loss: 0.004228, acc.: 100.00%] [G loss: 0.457553]\n",
      "epoch:21 step:17068 [D loss: 0.003538, acc.: 100.00%] [G loss: 1.932805]\n",
      "epoch:21 step:17069 [D loss: 0.023574, acc.: 99.22%] [G loss: 0.087412]\n",
      "epoch:21 step:17070 [D loss: 0.018404, acc.: 100.00%] [G loss: 0.903468]\n",
      "epoch:21 step:17071 [D loss: 0.076634, acc.: 99.22%] [G loss: 0.126114]\n",
      "epoch:21 step:17072 [D loss: 0.002810, acc.: 100.00%] [G loss: 0.262587]\n",
      "epoch:21 step:17073 [D loss: 0.002527, acc.: 100.00%] [G loss: 5.003780]\n",
      "epoch:21 step:17074 [D loss: 0.028504, acc.: 99.22%] [G loss: 0.089046]\n",
      "epoch:21 step:17075 [D loss: 0.007785, acc.: 100.00%] [G loss: 0.038153]\n",
      "epoch:21 step:17076 [D loss: 0.004570, acc.: 100.00%] [G loss: 0.029264]\n",
      "epoch:21 step:17077 [D loss: 0.008852, acc.: 100.00%] [G loss: 2.579158]\n",
      "epoch:21 step:17078 [D loss: 0.059685, acc.: 99.22%] [G loss: 0.242534]\n",
      "epoch:21 step:17079 [D loss: 0.015038, acc.: 100.00%] [G loss: 0.455097]\n",
      "epoch:21 step:17080 [D loss: 0.015548, acc.: 100.00%] [G loss: 0.232021]\n",
      "epoch:21 step:17081 [D loss: 0.024719, acc.: 100.00%] [G loss: 0.761513]\n",
      "epoch:21 step:17082 [D loss: 0.023440, acc.: 99.22%] [G loss: 4.003103]\n",
      "epoch:21 step:17083 [D loss: 0.023442, acc.: 100.00%] [G loss: 1.738947]\n",
      "epoch:21 step:17084 [D loss: 0.005495, acc.: 100.00%] [G loss: 0.362853]\n",
      "epoch:21 step:17085 [D loss: 0.019553, acc.: 100.00%] [G loss: 0.176975]\n",
      "epoch:21 step:17086 [D loss: 0.036944, acc.: 99.22%] [G loss: 0.082127]\n",
      "epoch:21 step:17087 [D loss: 0.057177, acc.: 99.22%] [G loss: 0.114811]\n",
      "epoch:21 step:17088 [D loss: 0.081396, acc.: 95.31%] [G loss: 0.083368]\n",
      "epoch:21 step:17089 [D loss: 0.003058, acc.: 100.00%] [G loss: 0.694907]\n",
      "epoch:21 step:17090 [D loss: 0.006468, acc.: 100.00%] [G loss: 0.041930]\n",
      "epoch:21 step:17091 [D loss: 0.006184, acc.: 100.00%] [G loss: 0.047725]\n",
      "epoch:21 step:17092 [D loss: 0.003359, acc.: 100.00%] [G loss: 0.026712]\n",
      "epoch:21 step:17093 [D loss: 0.004914, acc.: 100.00%] [G loss: 0.033821]\n",
      "epoch:21 step:17094 [D loss: 0.028830, acc.: 100.00%] [G loss: 0.415924]\n",
      "epoch:21 step:17095 [D loss: 0.002408, acc.: 100.00%] [G loss: 0.153607]\n",
      "epoch:21 step:17096 [D loss: 0.000652, acc.: 100.00%] [G loss: 1.321886]\n",
      "epoch:21 step:17097 [D loss: 0.001402, acc.: 100.00%] [G loss: 0.443714]\n",
      "epoch:21 step:17098 [D loss: 0.004237, acc.: 100.00%] [G loss: 0.324364]\n",
      "epoch:21 step:17099 [D loss: 0.003757, acc.: 100.00%] [G loss: 0.165379]\n",
      "epoch:21 step:17100 [D loss: 0.001707, acc.: 100.00%] [G loss: 0.347377]\n",
      "epoch:21 step:17101 [D loss: 0.034667, acc.: 100.00%] [G loss: 0.792190]\n",
      "epoch:21 step:17102 [D loss: 0.007204, acc.: 100.00%] [G loss: 1.559839]\n",
      "epoch:21 step:17103 [D loss: 0.012088, acc.: 99.22%] [G loss: 1.212341]\n",
      "epoch:21 step:17104 [D loss: 0.012858, acc.: 100.00%] [G loss: 1.437181]\n",
      "epoch:21 step:17105 [D loss: 0.053824, acc.: 98.44%] [G loss: 1.148518]\n",
      "epoch:21 step:17106 [D loss: 0.037892, acc.: 99.22%] [G loss: 3.390772]\n",
      "epoch:21 step:17107 [D loss: 0.007010, acc.: 100.00%] [G loss: 3.202373]\n",
      "epoch:21 step:17108 [D loss: 0.070448, acc.: 99.22%] [G loss: 3.678820]\n",
      "epoch:21 step:17109 [D loss: 0.046544, acc.: 98.44%] [G loss: 4.912521]\n",
      "epoch:21 step:17110 [D loss: 0.009482, acc.: 100.00%] [G loss: 5.967301]\n",
      "epoch:21 step:17111 [D loss: 0.017148, acc.: 100.00%] [G loss: 5.235635]\n",
      "epoch:21 step:17112 [D loss: 0.024394, acc.: 99.22%] [G loss: 3.212787]\n",
      "epoch:21 step:17113 [D loss: 0.014099, acc.: 100.00%] [G loss: 1.267960]\n",
      "epoch:21 step:17114 [D loss: 0.191854, acc.: 91.41%] [G loss: 5.196971]\n",
      "epoch:21 step:17115 [D loss: 0.156372, acc.: 91.41%] [G loss: 5.642774]\n",
      "epoch:21 step:17116 [D loss: 0.030405, acc.: 99.22%] [G loss: 6.303889]\n",
      "epoch:21 step:17117 [D loss: 0.011400, acc.: 100.00%] [G loss: 5.337875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17118 [D loss: 0.016024, acc.: 99.22%] [G loss: 3.996818]\n",
      "epoch:21 step:17119 [D loss: 0.020567, acc.: 100.00%] [G loss: 3.069260]\n",
      "epoch:21 step:17120 [D loss: 0.034541, acc.: 99.22%] [G loss: 3.935761]\n",
      "epoch:21 step:17121 [D loss: 0.010588, acc.: 100.00%] [G loss: 4.678547]\n",
      "epoch:21 step:17122 [D loss: 0.020348, acc.: 99.22%] [G loss: 7.179401]\n",
      "epoch:21 step:17123 [D loss: 0.004698, acc.: 100.00%] [G loss: 5.448145]\n",
      "epoch:21 step:17124 [D loss: 0.003607, acc.: 100.00%] [G loss: 5.326020]\n",
      "epoch:21 step:17125 [D loss: 0.014379, acc.: 100.00%] [G loss: 5.248335]\n",
      "epoch:21 step:17126 [D loss: 0.090477, acc.: 97.66%] [G loss: 4.325164]\n",
      "epoch:21 step:17127 [D loss: 0.002728, acc.: 100.00%] [G loss: 6.597383]\n",
      "epoch:21 step:17128 [D loss: 0.015201, acc.: 100.00%] [G loss: 0.034841]\n",
      "epoch:21 step:17129 [D loss: 0.192885, acc.: 92.19%] [G loss: 7.627909]\n",
      "epoch:21 step:17130 [D loss: 0.092541, acc.: 96.09%] [G loss: 6.538151]\n",
      "epoch:21 step:17131 [D loss: 0.089268, acc.: 97.66%] [G loss: 4.167178]\n",
      "epoch:21 step:17132 [D loss: 0.008174, acc.: 100.00%] [G loss: 3.414541]\n",
      "epoch:21 step:17133 [D loss: 0.001889, acc.: 100.00%] [G loss: 2.709817]\n",
      "epoch:21 step:17134 [D loss: 0.004815, acc.: 100.00%] [G loss: 5.336818]\n",
      "epoch:21 step:17135 [D loss: 0.009949, acc.: 100.00%] [G loss: 2.654790]\n",
      "epoch:21 step:17136 [D loss: 0.012050, acc.: 100.00%] [G loss: 3.073935]\n",
      "epoch:21 step:17137 [D loss: 0.008820, acc.: 100.00%] [G loss: 3.496806]\n",
      "epoch:21 step:17138 [D loss: 0.045884, acc.: 98.44%] [G loss: 3.253338]\n",
      "epoch:21 step:17139 [D loss: 0.001835, acc.: 100.00%] [G loss: 4.119671]\n",
      "epoch:21 step:17140 [D loss: 0.045943, acc.: 99.22%] [G loss: 2.998803]\n",
      "epoch:21 step:17141 [D loss: 0.007872, acc.: 100.00%] [G loss: 0.907809]\n",
      "epoch:21 step:17142 [D loss: 0.012182, acc.: 100.00%] [G loss: 3.510194]\n",
      "epoch:21 step:17143 [D loss: 0.011439, acc.: 100.00%] [G loss: 4.131740]\n",
      "epoch:21 step:17144 [D loss: 0.004776, acc.: 100.00%] [G loss: 3.873989]\n",
      "epoch:21 step:17145 [D loss: 0.013521, acc.: 100.00%] [G loss: 4.011975]\n",
      "epoch:21 step:17146 [D loss: 0.002306, acc.: 100.00%] [G loss: 3.667031]\n",
      "epoch:21 step:17147 [D loss: 0.004338, acc.: 100.00%] [G loss: 0.061693]\n",
      "epoch:21 step:17148 [D loss: 0.015970, acc.: 100.00%] [G loss: 2.939451]\n",
      "epoch:21 step:17149 [D loss: 0.003515, acc.: 100.00%] [G loss: 4.450407]\n",
      "epoch:21 step:17150 [D loss: 0.036112, acc.: 100.00%] [G loss: 2.973111]\n",
      "epoch:21 step:17151 [D loss: 0.005149, acc.: 100.00%] [G loss: 3.763708]\n",
      "epoch:21 step:17152 [D loss: 0.011535, acc.: 100.00%] [G loss: 2.449693]\n",
      "epoch:21 step:17153 [D loss: 0.003710, acc.: 100.00%] [G loss: 2.790962]\n",
      "epoch:21 step:17154 [D loss: 0.011144, acc.: 100.00%] [G loss: 1.579638]\n",
      "epoch:21 step:17155 [D loss: 0.012547, acc.: 100.00%] [G loss: 2.358168]\n",
      "epoch:21 step:17156 [D loss: 0.034323, acc.: 100.00%] [G loss: 5.299177]\n",
      "epoch:21 step:17157 [D loss: 0.013160, acc.: 100.00%] [G loss: 0.562762]\n",
      "epoch:21 step:17158 [D loss: 0.023934, acc.: 98.44%] [G loss: 4.897249]\n",
      "epoch:21 step:17159 [D loss: 0.023395, acc.: 100.00%] [G loss: 5.228527]\n",
      "epoch:21 step:17160 [D loss: 0.004909, acc.: 100.00%] [G loss: 5.207543]\n",
      "epoch:21 step:17161 [D loss: 0.007505, acc.: 100.00%] [G loss: 3.732806]\n",
      "epoch:21 step:17162 [D loss: 0.002396, acc.: 100.00%] [G loss: 4.109281]\n",
      "epoch:21 step:17163 [D loss: 0.020204, acc.: 100.00%] [G loss: 0.061288]\n",
      "epoch:21 step:17164 [D loss: 0.007046, acc.: 100.00%] [G loss: 0.004870]\n",
      "epoch:21 step:17165 [D loss: 0.184471, acc.: 92.97%] [G loss: 2.880935]\n",
      "epoch:21 step:17166 [D loss: 0.020026, acc.: 99.22%] [G loss: 8.562545]\n",
      "epoch:21 step:17167 [D loss: 3.058809, acc.: 15.62%] [G loss: 9.075453]\n",
      "epoch:21 step:17168 [D loss: 3.262958, acc.: 50.00%] [G loss: 4.954934]\n",
      "epoch:21 step:17169 [D loss: 0.189056, acc.: 91.41%] [G loss: 1.546574]\n",
      "epoch:21 step:17170 [D loss: 0.020799, acc.: 100.00%] [G loss: 1.117902]\n",
      "epoch:21 step:17171 [D loss: 0.011246, acc.: 100.00%] [G loss: 0.896978]\n",
      "epoch:21 step:17172 [D loss: 0.041527, acc.: 99.22%] [G loss: 0.443366]\n",
      "epoch:21 step:17173 [D loss: 0.053655, acc.: 99.22%] [G loss: 0.809339]\n",
      "epoch:21 step:17174 [D loss: 0.005710, acc.: 100.00%] [G loss: 0.718694]\n",
      "epoch:21 step:17175 [D loss: 0.045218, acc.: 99.22%] [G loss: 1.848457]\n",
      "epoch:21 step:17176 [D loss: 0.015226, acc.: 100.00%] [G loss: 4.291179]\n",
      "epoch:21 step:17177 [D loss: 0.009649, acc.: 100.00%] [G loss: 1.721532]\n",
      "epoch:21 step:17178 [D loss: 0.009535, acc.: 100.00%] [G loss: 1.699364]\n",
      "epoch:21 step:17179 [D loss: 0.011529, acc.: 100.00%] [G loss: 1.963279]\n",
      "epoch:21 step:17180 [D loss: 0.098888, acc.: 95.31%] [G loss: 1.732920]\n",
      "epoch:21 step:17181 [D loss: 0.023013, acc.: 99.22%] [G loss: 1.539889]\n",
      "epoch:21 step:17182 [D loss: 0.032310, acc.: 99.22%] [G loss: 0.758363]\n",
      "epoch:22 step:17183 [D loss: 0.022627, acc.: 100.00%] [G loss: 0.606322]\n",
      "epoch:22 step:17184 [D loss: 0.026349, acc.: 99.22%] [G loss: 0.103826]\n",
      "epoch:22 step:17185 [D loss: 0.031399, acc.: 100.00%] [G loss: 0.261160]\n",
      "epoch:22 step:17186 [D loss: 0.021590, acc.: 100.00%] [G loss: 0.208796]\n",
      "epoch:22 step:17187 [D loss: 0.006102, acc.: 100.00%] [G loss: 0.982859]\n",
      "epoch:22 step:17188 [D loss: 0.021322, acc.: 100.00%] [G loss: 0.710222]\n",
      "epoch:22 step:17189 [D loss: 0.067701, acc.: 98.44%] [G loss: 0.476234]\n",
      "epoch:22 step:17190 [D loss: 0.028349, acc.: 100.00%] [G loss: 3.674916]\n",
      "epoch:22 step:17191 [D loss: 0.251148, acc.: 89.06%] [G loss: 3.427677]\n",
      "epoch:22 step:17192 [D loss: 0.870796, acc.: 65.62%] [G loss: 0.228511]\n",
      "epoch:22 step:17193 [D loss: 0.005341, acc.: 100.00%] [G loss: 0.183953]\n",
      "epoch:22 step:17194 [D loss: 0.039982, acc.: 99.22%] [G loss: 0.753491]\n",
      "epoch:22 step:17195 [D loss: 0.016111, acc.: 100.00%] [G loss: 0.467248]\n",
      "epoch:22 step:17196 [D loss: 0.027083, acc.: 100.00%] [G loss: 2.413224]\n",
      "epoch:22 step:17197 [D loss: 0.005618, acc.: 100.00%] [G loss: 1.062586]\n",
      "epoch:22 step:17198 [D loss: 0.016486, acc.: 100.00%] [G loss: 0.700703]\n",
      "epoch:22 step:17199 [D loss: 0.031476, acc.: 100.00%] [G loss: 0.689027]\n",
      "epoch:22 step:17200 [D loss: 0.004082, acc.: 100.00%] [G loss: 0.742476]\n",
      "epoch:22 step:17201 [D loss: 0.003516, acc.: 100.00%] [G loss: 0.216698]\n",
      "epoch:22 step:17202 [D loss: 0.003664, acc.: 100.00%] [G loss: 0.596675]\n",
      "epoch:22 step:17203 [D loss: 0.001359, acc.: 100.00%] [G loss: 1.120136]\n",
      "epoch:22 step:17204 [D loss: 0.011113, acc.: 100.00%] [G loss: 0.837572]\n",
      "epoch:22 step:17205 [D loss: 0.016113, acc.: 100.00%] [G loss: 0.128791]\n",
      "epoch:22 step:17206 [D loss: 0.008197, acc.: 100.00%] [G loss: 0.115004]\n",
      "epoch:22 step:17207 [D loss: 0.020856, acc.: 99.22%] [G loss: 3.104472]\n",
      "epoch:22 step:17208 [D loss: 0.022698, acc.: 100.00%] [G loss: 0.228827]\n",
      "epoch:22 step:17209 [D loss: 0.079420, acc.: 99.22%] [G loss: 1.896171]\n",
      "epoch:22 step:17210 [D loss: 0.155937, acc.: 94.53%] [G loss: 1.294569]\n",
      "epoch:22 step:17211 [D loss: 0.075526, acc.: 97.66%] [G loss: 3.285101]\n",
      "epoch:22 step:17212 [D loss: 0.032326, acc.: 99.22%] [G loss: 0.251058]\n",
      "epoch:22 step:17213 [D loss: 0.017181, acc.: 100.00%] [G loss: 3.292481]\n",
      "epoch:22 step:17214 [D loss: 0.028447, acc.: 100.00%] [G loss: 0.388391]\n",
      "epoch:22 step:17215 [D loss: 0.021782, acc.: 100.00%] [G loss: 0.293610]\n",
      "epoch:22 step:17216 [D loss: 0.157435, acc.: 94.53%] [G loss: 2.033285]\n",
      "epoch:22 step:17217 [D loss: 0.224331, acc.: 88.28%] [G loss: 3.662127]\n",
      "epoch:22 step:17218 [D loss: 0.124720, acc.: 94.53%] [G loss: 0.956236]\n",
      "epoch:22 step:17219 [D loss: 0.002819, acc.: 100.00%] [G loss: 1.440365]\n",
      "epoch:22 step:17220 [D loss: 0.023896, acc.: 100.00%] [G loss: 1.124267]\n",
      "epoch:22 step:17221 [D loss: 0.006732, acc.: 100.00%] [G loss: 0.759993]\n",
      "epoch:22 step:17222 [D loss: 0.008209, acc.: 100.00%] [G loss: 0.929588]\n",
      "epoch:22 step:17223 [D loss: 0.035119, acc.: 98.44%] [G loss: 6.256413]\n",
      "epoch:22 step:17224 [D loss: 0.006062, acc.: 100.00%] [G loss: 5.905445]\n",
      "epoch:22 step:17225 [D loss: 0.005027, acc.: 100.00%] [G loss: 0.520306]\n",
      "epoch:22 step:17226 [D loss: 0.012994, acc.: 100.00%] [G loss: 0.485227]\n",
      "epoch:22 step:17227 [D loss: 0.010699, acc.: 100.00%] [G loss: 0.604735]\n",
      "epoch:22 step:17228 [D loss: 0.221972, acc.: 89.06%] [G loss: 8.212403]\n",
      "epoch:22 step:17229 [D loss: 0.501963, acc.: 75.00%] [G loss: 4.505949]\n",
      "epoch:22 step:17230 [D loss: 0.030634, acc.: 100.00%] [G loss: 3.442132]\n",
      "epoch:22 step:17231 [D loss: 0.002780, acc.: 100.00%] [G loss: 4.490417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17232 [D loss: 0.005697, acc.: 100.00%] [G loss: 3.724792]\n",
      "epoch:22 step:17233 [D loss: 0.010744, acc.: 100.00%] [G loss: 2.767630]\n",
      "epoch:22 step:17234 [D loss: 0.012731, acc.: 100.00%] [G loss: 3.479561]\n",
      "epoch:22 step:17235 [D loss: 0.010257, acc.: 100.00%] [G loss: 4.313523]\n",
      "epoch:22 step:17236 [D loss: 0.030962, acc.: 100.00%] [G loss: 1.014938]\n",
      "epoch:22 step:17237 [D loss: 0.002103, acc.: 100.00%] [G loss: 5.123868]\n",
      "epoch:22 step:17238 [D loss: 0.012217, acc.: 100.00%] [G loss: 6.122728]\n",
      "epoch:22 step:17239 [D loss: 0.052464, acc.: 97.66%] [G loss: 4.684728]\n",
      "epoch:22 step:17240 [D loss: 0.001685, acc.: 100.00%] [G loss: 3.569103]\n",
      "epoch:22 step:17241 [D loss: 0.007875, acc.: 100.00%] [G loss: 2.778350]\n",
      "epoch:22 step:17242 [D loss: 0.007282, acc.: 100.00%] [G loss: 2.462829]\n",
      "epoch:22 step:17243 [D loss: 0.069550, acc.: 98.44%] [G loss: 1.380265]\n",
      "epoch:22 step:17244 [D loss: 0.012057, acc.: 100.00%] [G loss: 2.533499]\n",
      "epoch:22 step:17245 [D loss: 0.066512, acc.: 98.44%] [G loss: 3.886453]\n",
      "epoch:22 step:17246 [D loss: 0.010271, acc.: 100.00%] [G loss: 2.543809]\n",
      "epoch:22 step:17247 [D loss: 0.098560, acc.: 96.09%] [G loss: 0.047591]\n",
      "epoch:22 step:17248 [D loss: 0.004485, acc.: 100.00%] [G loss: 1.593995]\n",
      "epoch:22 step:17249 [D loss: 0.005762, acc.: 100.00%] [G loss: 0.769080]\n",
      "epoch:22 step:17250 [D loss: 0.027521, acc.: 99.22%] [G loss: 0.623599]\n",
      "epoch:22 step:17251 [D loss: 0.001910, acc.: 100.00%] [G loss: 1.034737]\n",
      "epoch:22 step:17252 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.823559]\n",
      "epoch:22 step:17253 [D loss: 0.004137, acc.: 100.00%] [G loss: 0.596115]\n",
      "epoch:22 step:17254 [D loss: 0.025306, acc.: 100.00%] [G loss: 0.877002]\n",
      "epoch:22 step:17255 [D loss: 0.023726, acc.: 100.00%] [G loss: 1.509070]\n",
      "epoch:22 step:17256 [D loss: 0.006730, acc.: 100.00%] [G loss: 0.977988]\n",
      "epoch:22 step:17257 [D loss: 0.002933, acc.: 100.00%] [G loss: 2.770035]\n",
      "epoch:22 step:17258 [D loss: 0.001185, acc.: 100.00%] [G loss: 1.937773]\n",
      "epoch:22 step:17259 [D loss: 0.010888, acc.: 100.00%] [G loss: 0.320306]\n",
      "epoch:22 step:17260 [D loss: 0.001625, acc.: 100.00%] [G loss: 1.541113]\n",
      "epoch:22 step:17261 [D loss: 0.002366, acc.: 100.00%] [G loss: 1.812152]\n",
      "epoch:22 step:17262 [D loss: 0.002592, acc.: 100.00%] [G loss: 0.889164]\n",
      "epoch:22 step:17263 [D loss: 0.001633, acc.: 100.00%] [G loss: 0.038754]\n",
      "epoch:22 step:17264 [D loss: 0.005140, acc.: 100.00%] [G loss: 0.845234]\n",
      "epoch:22 step:17265 [D loss: 0.004801, acc.: 100.00%] [G loss: 0.766755]\n",
      "epoch:22 step:17266 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.379214]\n",
      "epoch:22 step:17267 [D loss: 0.006518, acc.: 100.00%] [G loss: 0.500657]\n",
      "epoch:22 step:17268 [D loss: 0.008070, acc.: 100.00%] [G loss: 4.868752]\n",
      "epoch:22 step:17269 [D loss: 0.018869, acc.: 99.22%] [G loss: 0.938803]\n",
      "epoch:22 step:17270 [D loss: 0.016586, acc.: 100.00%] [G loss: 1.112902]\n",
      "epoch:22 step:17271 [D loss: 0.002768, acc.: 100.00%] [G loss: 1.465701]\n",
      "epoch:22 step:17272 [D loss: 0.086091, acc.: 98.44%] [G loss: 0.592164]\n",
      "epoch:22 step:17273 [D loss: 0.010423, acc.: 100.00%] [G loss: 1.161032]\n",
      "epoch:22 step:17274 [D loss: 0.005425, acc.: 100.00%] [G loss: 0.680867]\n",
      "epoch:22 step:17275 [D loss: 0.012604, acc.: 100.00%] [G loss: 2.029991]\n",
      "epoch:22 step:17276 [D loss: 0.007742, acc.: 100.00%] [G loss: 1.136286]\n",
      "epoch:22 step:17277 [D loss: 0.002697, acc.: 100.00%] [G loss: 1.911821]\n",
      "epoch:22 step:17278 [D loss: 0.012594, acc.: 99.22%] [G loss: 1.872458]\n",
      "epoch:22 step:17279 [D loss: 0.007341, acc.: 100.00%] [G loss: 2.273305]\n",
      "epoch:22 step:17280 [D loss: 0.014126, acc.: 100.00%] [G loss: 1.121025]\n",
      "epoch:22 step:17281 [D loss: 0.108580, acc.: 98.44%] [G loss: 0.389724]\n",
      "epoch:22 step:17282 [D loss: 0.004935, acc.: 100.00%] [G loss: 8.287088]\n",
      "epoch:22 step:17283 [D loss: 0.004688, acc.: 100.00%] [G loss: 7.956821]\n",
      "epoch:22 step:17284 [D loss: 0.010544, acc.: 100.00%] [G loss: 7.825733]\n",
      "epoch:22 step:17285 [D loss: 0.014474, acc.: 100.00%] [G loss: 6.241142]\n",
      "epoch:22 step:17286 [D loss: 0.032604, acc.: 99.22%] [G loss: 6.479106]\n",
      "epoch:22 step:17287 [D loss: 0.004406, acc.: 100.00%] [G loss: 5.947666]\n",
      "epoch:22 step:17288 [D loss: 0.002570, acc.: 100.00%] [G loss: 5.627818]\n",
      "epoch:22 step:17289 [D loss: 0.025961, acc.: 98.44%] [G loss: 4.589026]\n",
      "epoch:22 step:17290 [D loss: 0.010397, acc.: 100.00%] [G loss: 1.024525]\n",
      "epoch:22 step:17291 [D loss: 0.077764, acc.: 96.88%] [G loss: 6.443796]\n",
      "epoch:22 step:17292 [D loss: 0.152409, acc.: 93.75%] [G loss: 5.278935]\n",
      "epoch:22 step:17293 [D loss: 0.002829, acc.: 100.00%] [G loss: 0.001336]\n",
      "epoch:22 step:17294 [D loss: 0.000625, acc.: 100.00%] [G loss: 4.841162]\n",
      "epoch:22 step:17295 [D loss: 0.028604, acc.: 97.66%] [G loss: 3.981685]\n",
      "epoch:22 step:17296 [D loss: 0.000531, acc.: 100.00%] [G loss: 4.243795]\n",
      "epoch:22 step:17297 [D loss: 0.000640, acc.: 100.00%] [G loss: 6.152625]\n",
      "epoch:22 step:17298 [D loss: 0.034097, acc.: 99.22%] [G loss: 3.487971]\n",
      "epoch:22 step:17299 [D loss: 0.002373, acc.: 100.00%] [G loss: 4.089533]\n",
      "epoch:22 step:17300 [D loss: 0.012570, acc.: 100.00%] [G loss: 3.970266]\n",
      "epoch:22 step:17301 [D loss: 0.004203, acc.: 100.00%] [G loss: 2.867549]\n",
      "epoch:22 step:17302 [D loss: 0.000538, acc.: 100.00%] [G loss: 2.071026]\n",
      "epoch:22 step:17303 [D loss: 0.007217, acc.: 100.00%] [G loss: 4.275981]\n",
      "epoch:22 step:17304 [D loss: 0.003944, acc.: 100.00%] [G loss: 2.527914]\n",
      "epoch:22 step:17305 [D loss: 0.008125, acc.: 100.00%] [G loss: 4.286305]\n",
      "epoch:22 step:17306 [D loss: 0.004591, acc.: 100.00%] [G loss: 2.588307]\n",
      "epoch:22 step:17307 [D loss: 0.003480, acc.: 100.00%] [G loss: 4.167780]\n",
      "epoch:22 step:17308 [D loss: 0.009289, acc.: 100.00%] [G loss: 2.851911]\n",
      "epoch:22 step:17309 [D loss: 0.025283, acc.: 100.00%] [G loss: 3.827054]\n",
      "epoch:22 step:17310 [D loss: 0.030064, acc.: 100.00%] [G loss: 3.327940]\n",
      "epoch:22 step:17311 [D loss: 0.002595, acc.: 100.00%] [G loss: 4.976954]\n",
      "epoch:22 step:17312 [D loss: 0.003499, acc.: 100.00%] [G loss: 4.950160]\n",
      "epoch:22 step:17313 [D loss: 0.000918, acc.: 100.00%] [G loss: 3.305507]\n",
      "epoch:22 step:17314 [D loss: 0.005843, acc.: 100.00%] [G loss: 4.726284]\n",
      "epoch:22 step:17315 [D loss: 0.001752, acc.: 100.00%] [G loss: 4.658426]\n",
      "epoch:22 step:17316 [D loss: 0.018859, acc.: 100.00%] [G loss: 8.448992]\n",
      "epoch:22 step:17317 [D loss: 0.002992, acc.: 100.00%] [G loss: 0.234286]\n",
      "epoch:22 step:17318 [D loss: 0.010736, acc.: 100.00%] [G loss: 6.556907]\n",
      "epoch:22 step:17319 [D loss: 0.000399, acc.: 100.00%] [G loss: 5.739800]\n",
      "epoch:22 step:17320 [D loss: 0.000633, acc.: 100.00%] [G loss: 5.840225]\n",
      "epoch:22 step:17321 [D loss: 0.006309, acc.: 100.00%] [G loss: 0.101001]\n",
      "epoch:22 step:17322 [D loss: 0.014287, acc.: 100.00%] [G loss: 0.138363]\n",
      "epoch:22 step:17323 [D loss: 0.007236, acc.: 100.00%] [G loss: 6.261324]\n",
      "epoch:22 step:17324 [D loss: 0.000926, acc.: 100.00%] [G loss: 4.610350]\n",
      "epoch:22 step:17325 [D loss: 0.028021, acc.: 100.00%] [G loss: 5.206903]\n",
      "epoch:22 step:17326 [D loss: 0.001400, acc.: 100.00%] [G loss: 4.934010]\n",
      "epoch:22 step:17327 [D loss: 0.023367, acc.: 99.22%] [G loss: 4.116751]\n",
      "epoch:22 step:17328 [D loss: 0.001808, acc.: 100.00%] [G loss: 4.876212]\n",
      "epoch:22 step:17329 [D loss: 0.000698, acc.: 100.00%] [G loss: 3.257726]\n",
      "epoch:22 step:17330 [D loss: 0.007970, acc.: 100.00%] [G loss: 2.574372]\n",
      "epoch:22 step:17331 [D loss: 0.001114, acc.: 100.00%] [G loss: 2.742820]\n",
      "epoch:22 step:17332 [D loss: 0.001760, acc.: 100.00%] [G loss: 2.841086]\n",
      "epoch:22 step:17333 [D loss: 0.002287, acc.: 100.00%] [G loss: 2.296911]\n",
      "epoch:22 step:17334 [D loss: 0.008598, acc.: 100.00%] [G loss: 0.033358]\n",
      "epoch:22 step:17335 [D loss: 0.106518, acc.: 95.31%] [G loss: 1.876958]\n",
      "epoch:22 step:17336 [D loss: 0.002755, acc.: 100.00%] [G loss: 2.103162]\n",
      "epoch:22 step:17337 [D loss: 0.006148, acc.: 100.00%] [G loss: 1.512727]\n",
      "epoch:22 step:17338 [D loss: 0.101263, acc.: 97.66%] [G loss: 0.013522]\n",
      "epoch:22 step:17339 [D loss: 0.000101, acc.: 100.00%] [G loss: 5.377877]\n",
      "epoch:22 step:17340 [D loss: 0.048074, acc.: 98.44%] [G loss: 5.686865]\n",
      "epoch:22 step:17341 [D loss: 0.001365, acc.: 100.00%] [G loss: 6.835932]\n",
      "epoch:22 step:17342 [D loss: 0.002776, acc.: 100.00%] [G loss: 3.771986]\n",
      "epoch:22 step:17343 [D loss: 0.007318, acc.: 100.00%] [G loss: 0.019889]\n",
      "epoch:22 step:17344 [D loss: 0.002842, acc.: 100.00%] [G loss: 2.976013]\n",
      "epoch:22 step:17345 [D loss: 0.000412, acc.: 100.00%] [G loss: 3.236188]\n",
      "epoch:22 step:17346 [D loss: 0.005375, acc.: 100.00%] [G loss: 1.941073]\n",
      "epoch:22 step:17347 [D loss: 0.004615, acc.: 100.00%] [G loss: 0.025767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17348 [D loss: 0.000358, acc.: 100.00%] [G loss: 2.569928]\n",
      "epoch:22 step:17349 [D loss: 0.000701, acc.: 100.00%] [G loss: 2.205121]\n",
      "epoch:22 step:17350 [D loss: 0.000423, acc.: 100.00%] [G loss: 1.589854]\n",
      "epoch:22 step:17351 [D loss: 0.000438, acc.: 100.00%] [G loss: 2.778401]\n",
      "epoch:22 step:17352 [D loss: 0.000401, acc.: 100.00%] [G loss: 2.461396]\n",
      "epoch:22 step:17353 [D loss: 0.000086, acc.: 100.00%] [G loss: 3.479350]\n",
      "epoch:22 step:17354 [D loss: 0.008280, acc.: 100.00%] [G loss: 0.032574]\n",
      "epoch:22 step:17355 [D loss: 0.004956, acc.: 100.00%] [G loss: 1.542010]\n",
      "epoch:22 step:17356 [D loss: 0.004528, acc.: 100.00%] [G loss: 3.047150]\n",
      "epoch:22 step:17357 [D loss: 0.055747, acc.: 97.66%] [G loss: 2.279419]\n",
      "epoch:22 step:17358 [D loss: 0.260415, acc.: 91.41%] [G loss: 0.572575]\n",
      "epoch:22 step:17359 [D loss: 0.181718, acc.: 89.84%] [G loss: 1.156498]\n",
      "epoch:22 step:17360 [D loss: 0.000500, acc.: 100.00%] [G loss: 3.731026]\n",
      "epoch:22 step:17361 [D loss: 0.929615, acc.: 67.97%] [G loss: 0.196201]\n",
      "epoch:22 step:17362 [D loss: 0.297467, acc.: 86.72%] [G loss: 8.409602]\n",
      "epoch:22 step:17363 [D loss: 0.000179, acc.: 100.00%] [G loss: 4.681237]\n",
      "epoch:22 step:17364 [D loss: 0.044888, acc.: 99.22%] [G loss: 3.454672]\n",
      "epoch:22 step:17365 [D loss: 0.008545, acc.: 100.00%] [G loss: 2.419179]\n",
      "epoch:22 step:17366 [D loss: 0.001666, acc.: 100.00%] [G loss: 1.556831]\n",
      "epoch:22 step:17367 [D loss: 0.007809, acc.: 100.00%] [G loss: 0.709279]\n",
      "epoch:22 step:17368 [D loss: 0.003746, acc.: 100.00%] [G loss: 3.811845]\n",
      "epoch:22 step:17369 [D loss: 0.047654, acc.: 99.22%] [G loss: 5.776104]\n",
      "epoch:22 step:17370 [D loss: 0.006412, acc.: 100.00%] [G loss: 3.415375]\n",
      "epoch:22 step:17371 [D loss: 0.034198, acc.: 99.22%] [G loss: 2.912916]\n",
      "epoch:22 step:17372 [D loss: 0.039889, acc.: 99.22%] [G loss: 2.398516]\n",
      "epoch:22 step:17373 [D loss: 0.058009, acc.: 99.22%] [G loss: 3.412715]\n",
      "epoch:22 step:17374 [D loss: 0.026970, acc.: 100.00%] [G loss: 6.275573]\n",
      "epoch:22 step:17375 [D loss: 0.040368, acc.: 98.44%] [G loss: 5.505433]\n",
      "epoch:22 step:17376 [D loss: 0.039174, acc.: 100.00%] [G loss: 0.212151]\n",
      "epoch:22 step:17377 [D loss: 0.009511, acc.: 100.00%] [G loss: 4.172370]\n",
      "epoch:22 step:17378 [D loss: 0.008612, acc.: 100.00%] [G loss: 4.308388]\n",
      "epoch:22 step:17379 [D loss: 0.005400, acc.: 100.00%] [G loss: 0.004196]\n",
      "epoch:22 step:17380 [D loss: 0.011315, acc.: 100.00%] [G loss: 2.843353]\n",
      "epoch:22 step:17381 [D loss: 0.003757, acc.: 100.00%] [G loss: 0.027907]\n",
      "epoch:22 step:17382 [D loss: 0.009932, acc.: 100.00%] [G loss: 4.702513]\n",
      "epoch:22 step:17383 [D loss: 0.024863, acc.: 100.00%] [G loss: 0.368805]\n",
      "epoch:22 step:17384 [D loss: 0.121247, acc.: 96.09%] [G loss: 0.016293]\n",
      "epoch:22 step:17385 [D loss: 0.006249, acc.: 100.00%] [G loss: 2.446804]\n",
      "epoch:22 step:17386 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.020433]\n",
      "epoch:22 step:17387 [D loss: 0.001185, acc.: 100.00%] [G loss: 1.227535]\n",
      "epoch:22 step:17388 [D loss: 0.003558, acc.: 100.00%] [G loss: 0.003862]\n",
      "epoch:22 step:17389 [D loss: 0.006041, acc.: 100.00%] [G loss: 0.334955]\n",
      "epoch:22 step:17390 [D loss: 0.000273, acc.: 100.00%] [G loss: 1.184435]\n",
      "epoch:22 step:17391 [D loss: 0.003074, acc.: 100.00%] [G loss: 1.658328]\n",
      "epoch:22 step:17392 [D loss: 0.007501, acc.: 100.00%] [G loss: 0.679928]\n",
      "epoch:22 step:17393 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.417376]\n",
      "epoch:22 step:17394 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.032572]\n",
      "epoch:22 step:17395 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.349388]\n",
      "epoch:22 step:17396 [D loss: 0.003179, acc.: 100.00%] [G loss: 0.321127]\n",
      "epoch:22 step:17397 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.005324]\n",
      "epoch:22 step:17398 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.112150]\n",
      "epoch:22 step:17399 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.038154]\n",
      "epoch:22 step:17400 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.090680]\n",
      "epoch:22 step:17401 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.439295]\n",
      "epoch:22 step:17402 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.002893]\n",
      "epoch:22 step:17403 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.006725]\n",
      "epoch:22 step:17404 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.049783]\n",
      "epoch:22 step:17405 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.018193]\n",
      "epoch:22 step:17406 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.014003]\n",
      "epoch:22 step:17407 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.012508]\n",
      "epoch:22 step:17408 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.082345]\n",
      "epoch:22 step:17409 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.013583]\n",
      "epoch:22 step:17410 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.007011]\n",
      "epoch:22 step:17411 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.924717]\n",
      "epoch:22 step:17412 [D loss: 0.003594, acc.: 100.00%] [G loss: 0.141955]\n",
      "epoch:22 step:17413 [D loss: 0.004787, acc.: 100.00%] [G loss: 0.012841]\n",
      "epoch:22 step:17414 [D loss: 0.010145, acc.: 100.00%] [G loss: 0.017055]\n",
      "epoch:22 step:17415 [D loss: 0.006315, acc.: 100.00%] [G loss: 0.048701]\n",
      "epoch:22 step:17416 [D loss: 0.010642, acc.: 100.00%] [G loss: 0.013885]\n",
      "epoch:22 step:17417 [D loss: 0.001517, acc.: 100.00%] [G loss: 0.069872]\n",
      "epoch:22 step:17418 [D loss: 0.002223, acc.: 100.00%] [G loss: 0.088349]\n",
      "epoch:22 step:17419 [D loss: 0.005394, acc.: 100.00%] [G loss: 0.127144]\n",
      "epoch:22 step:17420 [D loss: 0.002539, acc.: 100.00%] [G loss: 0.115238]\n",
      "epoch:22 step:17421 [D loss: 0.002720, acc.: 100.00%] [G loss: 0.106011]\n",
      "epoch:22 step:17422 [D loss: 0.104998, acc.: 99.22%] [G loss: 0.088784]\n",
      "epoch:22 step:17423 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.586259]\n",
      "epoch:22 step:17424 [D loss: 0.000530, acc.: 100.00%] [G loss: 1.054986]\n",
      "epoch:22 step:17425 [D loss: 0.000930, acc.: 100.00%] [G loss: 0.271029]\n",
      "epoch:22 step:17426 [D loss: 0.006834, acc.: 100.00%] [G loss: 0.226232]\n",
      "epoch:22 step:17427 [D loss: 0.014472, acc.: 100.00%] [G loss: 1.284745]\n",
      "epoch:22 step:17428 [D loss: 0.011540, acc.: 100.00%] [G loss: 2.151807]\n",
      "epoch:22 step:17429 [D loss: 0.033405, acc.: 98.44%] [G loss: 0.373746]\n",
      "epoch:22 step:17430 [D loss: 0.000476, acc.: 100.00%] [G loss: 1.392853]\n",
      "epoch:22 step:17431 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.988891]\n",
      "epoch:22 step:17432 [D loss: 0.004701, acc.: 100.00%] [G loss: 0.642964]\n",
      "epoch:22 step:17433 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:22 step:17434 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.576011]\n",
      "epoch:22 step:17435 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.615853]\n",
      "epoch:22 step:17436 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.692111]\n",
      "epoch:22 step:17437 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.188095]\n",
      "epoch:22 step:17438 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.122726]\n",
      "epoch:22 step:17439 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.019765]\n",
      "epoch:22 step:17440 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.099451]\n",
      "epoch:22 step:17441 [D loss: 0.000878, acc.: 100.00%] [G loss: 0.051100]\n",
      "epoch:22 step:17442 [D loss: 0.001294, acc.: 100.00%] [G loss: 0.143838]\n",
      "epoch:22 step:17443 [D loss: 0.043882, acc.: 98.44%] [G loss: 0.666489]\n",
      "epoch:22 step:17444 [D loss: 0.000483, acc.: 100.00%] [G loss: 3.140298]\n",
      "epoch:22 step:17445 [D loss: 0.008610, acc.: 100.00%] [G loss: 0.077938]\n",
      "epoch:22 step:17446 [D loss: 0.044829, acc.: 97.66%] [G loss: 0.000031]\n",
      "epoch:22 step:17447 [D loss: 0.002102, acc.: 100.00%] [G loss: 0.305003]\n",
      "epoch:22 step:17448 [D loss: 0.002574, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:22 step:17449 [D loss: 0.000553, acc.: 100.00%] [G loss: 1.532305]\n",
      "epoch:22 step:17450 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.032535]\n",
      "epoch:22 step:17451 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.216343]\n",
      "epoch:22 step:17452 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.077183]\n",
      "epoch:22 step:17453 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.024338]\n",
      "epoch:22 step:17454 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.015824]\n",
      "epoch:22 step:17455 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.026261]\n",
      "epoch:22 step:17456 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.026607]\n",
      "epoch:22 step:17457 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:22 step:17458 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.012445]\n",
      "epoch:22 step:17459 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.007298]\n",
      "epoch:22 step:17460 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.002951]\n",
      "epoch:22 step:17461 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.010812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17462 [D loss: 0.001284, acc.: 100.00%] [G loss: 0.038092]\n",
      "epoch:22 step:17463 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.007673]\n",
      "epoch:22 step:17464 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.991944]\n",
      "epoch:22 step:17465 [D loss: 0.073481, acc.: 98.44%] [G loss: 2.120902]\n",
      "epoch:22 step:17466 [D loss: 0.002396, acc.: 100.00%] [G loss: 4.638811]\n",
      "epoch:22 step:17467 [D loss: 0.005697, acc.: 100.00%] [G loss: 2.837994]\n",
      "epoch:22 step:17468 [D loss: 0.094121, acc.: 96.09%] [G loss: 5.232602]\n",
      "epoch:22 step:17469 [D loss: 0.136974, acc.: 94.53%] [G loss: 4.946149]\n",
      "epoch:22 step:17470 [D loss: 0.001693, acc.: 100.00%] [G loss: 7.888144]\n",
      "epoch:22 step:17471 [D loss: 0.400548, acc.: 85.94%] [G loss: 0.000467]\n",
      "epoch:22 step:17472 [D loss: 0.254394, acc.: 92.97%] [G loss: 9.499783]\n",
      "epoch:22 step:17473 [D loss: 0.048687, acc.: 98.44%] [G loss: 10.459717]\n",
      "epoch:22 step:17474 [D loss: 0.110564, acc.: 96.09%] [G loss: 3.034774]\n",
      "epoch:22 step:17475 [D loss: 0.030277, acc.: 98.44%] [G loss: 2.200462]\n",
      "epoch:22 step:17476 [D loss: 0.000434, acc.: 100.00%] [G loss: 2.994188]\n",
      "epoch:22 step:17477 [D loss: 0.003123, acc.: 100.00%] [G loss: 1.737649]\n",
      "epoch:22 step:17478 [D loss: 0.002408, acc.: 100.00%] [G loss: 0.191569]\n",
      "epoch:22 step:17479 [D loss: 0.000084, acc.: 100.00%] [G loss: 1.310332]\n",
      "epoch:22 step:17480 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.186322]\n",
      "epoch:22 step:17481 [D loss: 0.000242, acc.: 100.00%] [G loss: 1.853337]\n",
      "epoch:22 step:17482 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.668317]\n",
      "epoch:22 step:17483 [D loss: 0.000145, acc.: 100.00%] [G loss: 1.327106]\n",
      "epoch:22 step:17484 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.577292]\n",
      "epoch:22 step:17485 [D loss: 0.000204, acc.: 100.00%] [G loss: 1.343632]\n",
      "epoch:22 step:17486 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.442678]\n",
      "epoch:22 step:17487 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.543730]\n",
      "epoch:22 step:17488 [D loss: 0.001135, acc.: 100.00%] [G loss: 0.598616]\n",
      "epoch:22 step:17489 [D loss: 0.002678, acc.: 100.00%] [G loss: 0.965947]\n",
      "epoch:22 step:17490 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.742032]\n",
      "epoch:22 step:17491 [D loss: 0.019007, acc.: 100.00%] [G loss: 1.111328]\n",
      "epoch:22 step:17492 [D loss: 0.004345, acc.: 100.00%] [G loss: 0.958892]\n",
      "epoch:22 step:17493 [D loss: 0.007996, acc.: 100.00%] [G loss: 1.618706]\n",
      "epoch:22 step:17494 [D loss: 0.001854, acc.: 100.00%] [G loss: 1.867147]\n",
      "epoch:22 step:17495 [D loss: 0.002682, acc.: 100.00%] [G loss: 2.049631]\n",
      "epoch:22 step:17496 [D loss: 0.013876, acc.: 100.00%] [G loss: 4.072220]\n",
      "epoch:22 step:17497 [D loss: 0.483478, acc.: 77.34%] [G loss: 9.838857]\n",
      "epoch:22 step:17498 [D loss: 0.000087, acc.: 100.00%] [G loss: 11.426888]\n",
      "epoch:22 step:17499 [D loss: 0.001808, acc.: 100.00%] [G loss: 6.072341]\n",
      "epoch:22 step:17500 [D loss: 0.005596, acc.: 100.00%] [G loss: 0.017312]\n",
      "epoch:22 step:17501 [D loss: 0.080893, acc.: 96.88%] [G loss: 12.193453]\n",
      "epoch:22 step:17502 [D loss: 0.891863, acc.: 64.06%] [G loss: 1.475662]\n",
      "epoch:22 step:17503 [D loss: 0.003147, acc.: 100.00%] [G loss: 9.392572]\n",
      "epoch:22 step:17504 [D loss: 0.070997, acc.: 95.31%] [G loss: 9.322668]\n",
      "epoch:22 step:17505 [D loss: 0.007019, acc.: 100.00%] [G loss: 9.316357]\n",
      "epoch:22 step:17506 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.003904]\n",
      "epoch:22 step:17507 [D loss: 0.001901, acc.: 100.00%] [G loss: 7.512880]\n",
      "epoch:22 step:17508 [D loss: 0.000151, acc.: 100.00%] [G loss: 6.512542]\n",
      "epoch:22 step:17509 [D loss: 0.001157, acc.: 100.00%] [G loss: 6.564210]\n",
      "epoch:22 step:17510 [D loss: 0.010975, acc.: 100.00%] [G loss: 6.360709]\n",
      "epoch:22 step:17511 [D loss: 0.055455, acc.: 99.22%] [G loss: 1.134731]\n",
      "epoch:22 step:17512 [D loss: 0.000173, acc.: 100.00%] [G loss: 6.308311]\n",
      "epoch:22 step:17513 [D loss: 0.002542, acc.: 100.00%] [G loss: 6.998141]\n",
      "epoch:22 step:17514 [D loss: 0.001689, acc.: 100.00%] [G loss: 6.688695]\n",
      "epoch:22 step:17515 [D loss: 0.005138, acc.: 100.00%] [G loss: 6.144210]\n",
      "epoch:22 step:17516 [D loss: 0.005279, acc.: 100.00%] [G loss: 5.456236]\n",
      "epoch:22 step:17517 [D loss: 0.001154, acc.: 100.00%] [G loss: 0.034407]\n",
      "epoch:22 step:17518 [D loss: 0.026923, acc.: 99.22%] [G loss: 3.631176]\n",
      "epoch:22 step:17519 [D loss: 0.000837, acc.: 100.00%] [G loss: 2.692660]\n",
      "epoch:22 step:17520 [D loss: 0.008619, acc.: 100.00%] [G loss: 2.630893]\n",
      "epoch:22 step:17521 [D loss: 0.092493, acc.: 96.88%] [G loss: 0.147727]\n",
      "epoch:22 step:17522 [D loss: 0.011520, acc.: 99.22%] [G loss: 7.145432]\n",
      "epoch:22 step:17523 [D loss: 0.012308, acc.: 100.00%] [G loss: 7.753652]\n",
      "epoch:22 step:17524 [D loss: 0.057330, acc.: 98.44%] [G loss: 4.974295]\n",
      "epoch:22 step:17525 [D loss: 0.012138, acc.: 99.22%] [G loss: 4.984490]\n",
      "epoch:22 step:17526 [D loss: 0.000730, acc.: 100.00%] [G loss: 1.309261]\n",
      "epoch:22 step:17527 [D loss: 0.002771, acc.: 100.00%] [G loss: 3.306214]\n",
      "epoch:22 step:17528 [D loss: 0.451491, acc.: 82.03%] [G loss: 10.532344]\n",
      "epoch:22 step:17529 [D loss: 2.154347, acc.: 51.56%] [G loss: 5.569684]\n",
      "epoch:22 step:17530 [D loss: 0.063974, acc.: 96.88%] [G loss: 3.064541]\n",
      "epoch:22 step:17531 [D loss: 0.118172, acc.: 93.75%] [G loss: 2.763878]\n",
      "epoch:22 step:17532 [D loss: 0.001034, acc.: 100.00%] [G loss: 0.890202]\n",
      "epoch:22 step:17533 [D loss: 0.023240, acc.: 99.22%] [G loss: 7.205476]\n",
      "epoch:22 step:17534 [D loss: 0.009910, acc.: 100.00%] [G loss: 0.000652]\n",
      "epoch:22 step:17535 [D loss: 0.320415, acc.: 85.94%] [G loss: 8.700970]\n",
      "epoch:22 step:17536 [D loss: 0.245709, acc.: 90.62%] [G loss: 1.521879]\n",
      "epoch:22 step:17537 [D loss: 0.004023, acc.: 100.00%] [G loss: 6.986786]\n",
      "epoch:22 step:17538 [D loss: 0.017502, acc.: 99.22%] [G loss: 6.579909]\n",
      "epoch:22 step:17539 [D loss: 0.000654, acc.: 100.00%] [G loss: 5.228638]\n",
      "epoch:22 step:17540 [D loss: 0.001468, acc.: 100.00%] [G loss: 1.397211]\n",
      "epoch:22 step:17541 [D loss: 0.002640, acc.: 100.00%] [G loss: 5.098332]\n",
      "epoch:22 step:17542 [D loss: 0.024109, acc.: 99.22%] [G loss: 0.113870]\n",
      "epoch:22 step:17543 [D loss: 0.003618, acc.: 100.00%] [G loss: 4.406697]\n",
      "epoch:22 step:17544 [D loss: 0.000761, acc.: 100.00%] [G loss: 4.048311]\n",
      "epoch:22 step:17545 [D loss: 0.006295, acc.: 100.00%] [G loss: 0.035475]\n",
      "epoch:22 step:17546 [D loss: 0.012472, acc.: 100.00%] [G loss: 3.839701]\n",
      "epoch:22 step:17547 [D loss: 0.000445, acc.: 100.00%] [G loss: 2.862257]\n",
      "epoch:22 step:17548 [D loss: 0.001663, acc.: 100.00%] [G loss: 1.955073]\n",
      "epoch:22 step:17549 [D loss: 0.001313, acc.: 100.00%] [G loss: 1.077413]\n",
      "epoch:22 step:17550 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.705807]\n",
      "epoch:22 step:17551 [D loss: 0.003839, acc.: 100.00%] [G loss: 1.863755]\n",
      "epoch:22 step:17552 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.689093]\n",
      "epoch:22 step:17553 [D loss: 0.003128, acc.: 100.00%] [G loss: 0.173469]\n",
      "epoch:22 step:17554 [D loss: 0.002855, acc.: 100.00%] [G loss: 0.004748]\n",
      "epoch:22 step:17555 [D loss: 0.005762, acc.: 100.00%] [G loss: 0.478083]\n",
      "epoch:22 step:17556 [D loss: 0.002492, acc.: 100.00%] [G loss: 0.175720]\n",
      "epoch:22 step:17557 [D loss: 0.004073, acc.: 100.00%] [G loss: 0.167940]\n",
      "epoch:22 step:17558 [D loss: 0.002939, acc.: 100.00%] [G loss: 0.165006]\n",
      "epoch:22 step:17559 [D loss: 0.002011, acc.: 100.00%] [G loss: 0.144292]\n",
      "epoch:22 step:17560 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.361856]\n",
      "epoch:22 step:17561 [D loss: 0.002107, acc.: 100.00%] [G loss: 0.182638]\n",
      "epoch:22 step:17562 [D loss: 0.004121, acc.: 100.00%] [G loss: 0.102592]\n",
      "epoch:22 step:17563 [D loss: 0.066530, acc.: 99.22%] [G loss: 0.566876]\n",
      "epoch:22 step:17564 [D loss: 0.068205, acc.: 98.44%] [G loss: 1.250396]\n",
      "epoch:22 step:17565 [D loss: 0.001503, acc.: 100.00%] [G loss: 0.322215]\n",
      "epoch:22 step:17566 [D loss: 0.003877, acc.: 100.00%] [G loss: 0.191558]\n",
      "epoch:22 step:17567 [D loss: 0.016320, acc.: 100.00%] [G loss: 0.105503]\n",
      "epoch:22 step:17568 [D loss: 0.044599, acc.: 98.44%] [G loss: 0.101958]\n",
      "epoch:22 step:17569 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.032662]\n",
      "epoch:22 step:17570 [D loss: 0.004822, acc.: 100.00%] [G loss: 0.016894]\n",
      "epoch:22 step:17571 [D loss: 0.001816, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:22 step:17572 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.008225]\n",
      "epoch:22 step:17573 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.006737]\n",
      "epoch:22 step:17574 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.015336]\n",
      "epoch:22 step:17575 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.005756]\n",
      "epoch:22 step:17576 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.004148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17577 [D loss: 0.004912, acc.: 100.00%] [G loss: 0.033331]\n",
      "epoch:22 step:17578 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.001296]\n",
      "epoch:22 step:17579 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.003381]\n",
      "epoch:22 step:17580 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.000602]\n",
      "epoch:22 step:17581 [D loss: 0.001452, acc.: 100.00%] [G loss: 0.006973]\n",
      "epoch:22 step:17582 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.001025]\n",
      "epoch:22 step:17583 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.000626]\n",
      "epoch:22 step:17584 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.008477]\n",
      "epoch:22 step:17585 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.001687]\n",
      "epoch:22 step:17586 [D loss: 0.008825, acc.: 100.00%] [G loss: 0.006283]\n",
      "epoch:22 step:17587 [D loss: 0.003803, acc.: 100.00%] [G loss: 0.028958]\n",
      "epoch:22 step:17588 [D loss: 0.000824, acc.: 100.00%] [G loss: 0.019677]\n",
      "epoch:22 step:17589 [D loss: 0.001144, acc.: 100.00%] [G loss: 0.110130]\n",
      "epoch:22 step:17590 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.033999]\n",
      "epoch:22 step:17591 [D loss: 0.001307, acc.: 100.00%] [G loss: 0.030172]\n",
      "epoch:22 step:17592 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.015513]\n",
      "epoch:22 step:17593 [D loss: 0.015348, acc.: 99.22%] [G loss: 0.015764]\n",
      "epoch:22 step:17594 [D loss: 0.003663, acc.: 100.00%] [G loss: 0.023471]\n",
      "epoch:22 step:17595 [D loss: 0.002637, acc.: 100.00%] [G loss: 0.058679]\n",
      "epoch:22 step:17596 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.082507]\n",
      "epoch:22 step:17597 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.005241]\n",
      "epoch:22 step:17598 [D loss: 0.002200, acc.: 100.00%] [G loss: 0.029690]\n",
      "epoch:22 step:17599 [D loss: 0.002262, acc.: 100.00%] [G loss: 0.006374]\n",
      "epoch:22 step:17600 [D loss: 0.002356, acc.: 100.00%] [G loss: 0.378329]\n",
      "epoch:22 step:17601 [D loss: 0.298040, acc.: 84.38%] [G loss: 9.643532]\n",
      "epoch:22 step:17602 [D loss: 2.118506, acc.: 51.56%] [G loss: 1.246367]\n",
      "epoch:22 step:17603 [D loss: 0.895999, acc.: 68.75%] [G loss: 5.025799]\n",
      "epoch:22 step:17604 [D loss: 0.166429, acc.: 89.06%] [G loss: 5.356144]\n",
      "epoch:22 step:17605 [D loss: 0.307202, acc.: 89.06%] [G loss: 1.340687]\n",
      "epoch:22 step:17606 [D loss: 0.049936, acc.: 99.22%] [G loss: 0.210697]\n",
      "epoch:22 step:17607 [D loss: 0.004647, acc.: 100.00%] [G loss: 0.059159]\n",
      "epoch:22 step:17608 [D loss: 0.010923, acc.: 100.00%] [G loss: 5.936113]\n",
      "epoch:22 step:17609 [D loss: 0.014747, acc.: 100.00%] [G loss: 5.547031]\n",
      "epoch:22 step:17610 [D loss: 0.091217, acc.: 96.09%] [G loss: 0.839491]\n",
      "epoch:22 step:17611 [D loss: 0.019471, acc.: 100.00%] [G loss: 1.353370]\n",
      "epoch:22 step:17612 [D loss: 0.143681, acc.: 96.88%] [G loss: 3.529451]\n",
      "epoch:22 step:17613 [D loss: 0.135451, acc.: 95.31%] [G loss: 0.167939]\n",
      "epoch:22 step:17614 [D loss: 0.043578, acc.: 99.22%] [G loss: 0.692072]\n",
      "epoch:22 step:17615 [D loss: 0.001917, acc.: 100.00%] [G loss: 0.501382]\n",
      "epoch:22 step:17616 [D loss: 0.023750, acc.: 100.00%] [G loss: 5.172195]\n",
      "epoch:22 step:17617 [D loss: 0.019201, acc.: 100.00%] [G loss: 0.780454]\n",
      "epoch:22 step:17618 [D loss: 0.007007, acc.: 100.00%] [G loss: 0.083700]\n",
      "epoch:22 step:17619 [D loss: 0.009731, acc.: 100.00%] [G loss: 0.204394]\n",
      "epoch:22 step:17620 [D loss: 0.013172, acc.: 100.00%] [G loss: 0.212600]\n",
      "epoch:22 step:17621 [D loss: 0.005133, acc.: 100.00%] [G loss: 0.476290]\n",
      "epoch:22 step:17622 [D loss: 0.002874, acc.: 100.00%] [G loss: 0.849195]\n",
      "epoch:22 step:17623 [D loss: 0.008726, acc.: 100.00%] [G loss: 0.081186]\n",
      "epoch:22 step:17624 [D loss: 0.007150, acc.: 100.00%] [G loss: 0.112904]\n",
      "epoch:22 step:17625 [D loss: 0.010207, acc.: 100.00%] [G loss: 0.019886]\n",
      "epoch:22 step:17626 [D loss: 0.007562, acc.: 100.00%] [G loss: 0.092131]\n",
      "epoch:22 step:17627 [D loss: 0.042239, acc.: 98.44%] [G loss: 0.079536]\n",
      "epoch:22 step:17628 [D loss: 0.009345, acc.: 100.00%] [G loss: 0.012294]\n",
      "epoch:22 step:17629 [D loss: 0.005897, acc.: 100.00%] [G loss: 0.051806]\n",
      "epoch:22 step:17630 [D loss: 0.055445, acc.: 99.22%] [G loss: 0.224882]\n",
      "epoch:22 step:17631 [D loss: 0.007543, acc.: 100.00%] [G loss: 0.212777]\n",
      "epoch:22 step:17632 [D loss: 0.009605, acc.: 100.00%] [G loss: 0.313092]\n",
      "epoch:22 step:17633 [D loss: 0.003346, acc.: 100.00%] [G loss: 0.062974]\n",
      "epoch:22 step:17634 [D loss: 0.001525, acc.: 100.00%] [G loss: 1.581663]\n",
      "epoch:22 step:17635 [D loss: 0.024891, acc.: 100.00%] [G loss: 0.010440]\n",
      "epoch:22 step:17636 [D loss: 0.000868, acc.: 100.00%] [G loss: 0.016681]\n",
      "epoch:22 step:17637 [D loss: 0.008079, acc.: 100.00%] [G loss: 0.014723]\n",
      "epoch:22 step:17638 [D loss: 0.014563, acc.: 100.00%] [G loss: 0.012572]\n",
      "epoch:22 step:17639 [D loss: 0.021547, acc.: 100.00%] [G loss: 0.169977]\n",
      "epoch:22 step:17640 [D loss: 0.004948, acc.: 100.00%] [G loss: 0.094979]\n",
      "epoch:22 step:17641 [D loss: 0.033837, acc.: 99.22%] [G loss: 0.015182]\n",
      "epoch:22 step:17642 [D loss: 0.015458, acc.: 100.00%] [G loss: 0.028353]\n",
      "epoch:22 step:17643 [D loss: 0.001714, acc.: 100.00%] [G loss: 0.027582]\n",
      "epoch:22 step:17644 [D loss: 0.009349, acc.: 100.00%] [G loss: 0.099295]\n",
      "epoch:22 step:17645 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.009193]\n",
      "epoch:22 step:17646 [D loss: 0.001113, acc.: 100.00%] [G loss: 1.676969]\n",
      "epoch:22 step:17647 [D loss: 0.002682, acc.: 100.00%] [G loss: 0.192942]\n",
      "epoch:22 step:17648 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.002577]\n",
      "epoch:22 step:17649 [D loss: 0.001245, acc.: 100.00%] [G loss: 0.002548]\n",
      "epoch:22 step:17650 [D loss: 0.001641, acc.: 100.00%] [G loss: 0.001753]\n",
      "epoch:22 step:17651 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.004028]\n",
      "epoch:22 step:17652 [D loss: 0.002691, acc.: 100.00%] [G loss: 0.004367]\n",
      "epoch:22 step:17653 [D loss: 0.002391, acc.: 100.00%] [G loss: 0.004833]\n",
      "epoch:22 step:17654 [D loss: 0.002109, acc.: 100.00%] [G loss: 0.013968]\n",
      "epoch:22 step:17655 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:22 step:17656 [D loss: 0.001634, acc.: 100.00%] [G loss: 0.005238]\n",
      "epoch:22 step:17657 [D loss: 0.012075, acc.: 100.00%] [G loss: 0.002138]\n",
      "epoch:22 step:17658 [D loss: 0.012643, acc.: 100.00%] [G loss: 0.000896]\n",
      "epoch:22 step:17659 [D loss: 0.001463, acc.: 100.00%] [G loss: 0.000751]\n",
      "epoch:22 step:17660 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.003111]\n",
      "epoch:22 step:17661 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.003017]\n",
      "epoch:22 step:17662 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.001109]\n",
      "epoch:22 step:17663 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.000677]\n",
      "epoch:22 step:17664 [D loss: 0.001628, acc.: 100.00%] [G loss: 0.001380]\n",
      "epoch:22 step:17665 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.002224]\n",
      "epoch:22 step:17666 [D loss: 0.001592, acc.: 100.00%] [G loss: 0.408809]\n",
      "epoch:22 step:17667 [D loss: 0.002348, acc.: 100.00%] [G loss: 0.004749]\n",
      "epoch:22 step:17668 [D loss: 0.003089, acc.: 100.00%] [G loss: 0.004073]\n",
      "epoch:22 step:17669 [D loss: 0.010552, acc.: 100.00%] [G loss: 0.000922]\n",
      "epoch:22 step:17670 [D loss: 0.002122, acc.: 100.00%] [G loss: 0.010429]\n",
      "epoch:22 step:17671 [D loss: 0.004145, acc.: 100.00%] [G loss: 0.007821]\n",
      "epoch:22 step:17672 [D loss: 0.004111, acc.: 100.00%] [G loss: 0.045446]\n",
      "epoch:22 step:17673 [D loss: 0.002562, acc.: 100.00%] [G loss: 0.004415]\n",
      "epoch:22 step:17674 [D loss: 0.018390, acc.: 99.22%] [G loss: 0.000982]\n",
      "epoch:22 step:17675 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.003866]\n",
      "epoch:22 step:17676 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:22 step:17677 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:22 step:17678 [D loss: 0.000595, acc.: 100.00%] [G loss: 0.003522]\n",
      "epoch:22 step:17679 [D loss: 0.001189, acc.: 100.00%] [G loss: 0.001306]\n",
      "epoch:22 step:17680 [D loss: 0.001838, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:22 step:17681 [D loss: 0.022973, acc.: 100.00%] [G loss: 0.047865]\n",
      "epoch:22 step:17682 [D loss: 0.001532, acc.: 100.00%] [G loss: 0.007650]\n",
      "epoch:22 step:17683 [D loss: 0.018476, acc.: 100.00%] [G loss: 0.001266]\n",
      "epoch:22 step:17684 [D loss: 0.000962, acc.: 100.00%] [G loss: 0.003213]\n",
      "epoch:22 step:17685 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:22 step:17686 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:22 step:17687 [D loss: 0.001505, acc.: 100.00%] [G loss: 0.006009]\n",
      "epoch:22 step:17688 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.002158]\n",
      "epoch:22 step:17689 [D loss: 0.006451, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:22 step:17690 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.000201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17691 [D loss: 0.001299, acc.: 100.00%] [G loss: 0.517929]\n",
      "epoch:22 step:17692 [D loss: 0.001115, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:22 step:17693 [D loss: 0.013127, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:22 step:17694 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.079186]\n",
      "epoch:22 step:17695 [D loss: 0.006246, acc.: 100.00%] [G loss: 0.013543]\n",
      "epoch:22 step:17696 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:22 step:17697 [D loss: 0.002187, acc.: 100.00%] [G loss: 0.014142]\n",
      "epoch:22 step:17698 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.017601]\n",
      "epoch:22 step:17699 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:22 step:17700 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.003784]\n",
      "epoch:22 step:17701 [D loss: 0.000816, acc.: 100.00%] [G loss: 0.005272]\n",
      "epoch:22 step:17702 [D loss: 0.001584, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:22 step:17703 [D loss: 0.000734, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:22 step:17704 [D loss: 0.003371, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:22 step:17705 [D loss: 0.001181, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:22 step:17706 [D loss: 0.006491, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:22 step:17707 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.000659]\n",
      "epoch:22 step:17708 [D loss: 0.087260, acc.: 98.44%] [G loss: 0.000000]\n",
      "epoch:22 step:17709 [D loss: 0.008429, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:22 step:17710 [D loss: 0.004079, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:22 step:17711 [D loss: 0.002414, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:22 step:17712 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:22 step:17713 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:22 step:17714 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:22 step:17715 [D loss: 0.001408, acc.: 100.00%] [G loss: 0.002250]\n",
      "epoch:22 step:17716 [D loss: 0.001002, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:22 step:17717 [D loss: 0.001250, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:22 step:17718 [D loss: 0.012303, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:22 step:17719 [D loss: 0.002268, acc.: 100.00%] [G loss: 0.002955]\n",
      "epoch:22 step:17720 [D loss: 0.005847, acc.: 100.00%] [G loss: 0.001106]\n",
      "epoch:22 step:17721 [D loss: 0.003719, acc.: 100.00%] [G loss: 0.003730]\n",
      "epoch:22 step:17722 [D loss: 0.005932, acc.: 100.00%] [G loss: 0.953360]\n",
      "epoch:22 step:17723 [D loss: 0.006785, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:22 step:17724 [D loss: 0.018829, acc.: 100.00%] [G loss: 0.108089]\n",
      "epoch:22 step:17725 [D loss: 0.001406, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:22 step:17726 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:22 step:17727 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:22 step:17728 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.020235]\n",
      "epoch:22 step:17729 [D loss: 0.001011, acc.: 100.00%] [G loss: 0.031705]\n",
      "epoch:22 step:17730 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:22 step:17731 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.003268]\n",
      "epoch:22 step:17732 [D loss: 0.001339, acc.: 100.00%] [G loss: 0.012163]\n",
      "epoch:22 step:17733 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:22 step:17734 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:22 step:17735 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.006440]\n",
      "epoch:22 step:17736 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.001405]\n",
      "epoch:22 step:17737 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:22 step:17738 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:22 step:17739 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.012546]\n",
      "epoch:22 step:17740 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:22 step:17741 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:22 step:17742 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.063889]\n",
      "epoch:22 step:17743 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:22 step:17744 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.001638]\n",
      "epoch:22 step:17745 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.020625]\n",
      "epoch:22 step:17746 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:22 step:17747 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:22 step:17748 [D loss: 0.002487, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:22 step:17749 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:22 step:17750 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.002829]\n",
      "epoch:22 step:17751 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:22 step:17752 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002927]\n",
      "epoch:22 step:17753 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.021400]\n",
      "epoch:22 step:17754 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.004036]\n",
      "epoch:22 step:17755 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.001525]\n",
      "epoch:22 step:17756 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000637]\n",
      "epoch:22 step:17757 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.020548]\n",
      "epoch:22 step:17758 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:22 step:17759 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.045359]\n",
      "epoch:22 step:17760 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:22 step:17761 [D loss: 0.006499, acc.: 100.00%] [G loss: 0.013645]\n",
      "epoch:22 step:17762 [D loss: 0.001919, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:22 step:17763 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.043907]\n",
      "epoch:22 step:17764 [D loss: 0.001557, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:22 step:17765 [D loss: 0.007019, acc.: 100.00%] [G loss: 0.001620]\n",
      "epoch:22 step:17766 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:22 step:17767 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:22 step:17768 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.006886]\n",
      "epoch:22 step:17769 [D loss: 0.001436, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:22 step:17770 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.010288]\n",
      "epoch:22 step:17771 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000791]\n",
      "epoch:22 step:17772 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:22 step:17773 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.018852]\n",
      "epoch:22 step:17774 [D loss: 0.000924, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:22 step:17775 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.000632]\n",
      "epoch:22 step:17776 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:22 step:17777 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:22 step:17778 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:22 step:17779 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:22 step:17780 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:22 step:17781 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.055752]\n",
      "epoch:22 step:17782 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000617]\n",
      "epoch:22 step:17783 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:22 step:17784 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:22 step:17785 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.002569]\n",
      "epoch:22 step:17786 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:22 step:17787 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:22 step:17788 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.001078]\n",
      "epoch:22 step:17789 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.005295]\n",
      "epoch:22 step:17790 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.001388]\n",
      "epoch:22 step:17791 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:22 step:17792 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:22 step:17793 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:22 step:17794 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:22 step:17795 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:22 step:17796 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:22 step:17797 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:22 step:17798 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:22 step:17799 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:22 step:17800 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:22 step:17801 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:22 step:17802 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:22 step:17803 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17804 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:22 step:17805 [D loss: 0.007707, acc.: 100.00%] [G loss: 0.000816]\n",
      "epoch:22 step:17806 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:22 step:17807 [D loss: 0.004045, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:22 step:17808 [D loss: 0.003603, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:22 step:17809 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:22 step:17810 [D loss: 0.003552, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:22 step:17811 [D loss: 0.152992, acc.: 94.53%] [G loss: 6.204361]\n",
      "epoch:22 step:17812 [D loss: 0.092843, acc.: 98.44%] [G loss: 4.244309]\n",
      "epoch:22 step:17813 [D loss: 0.092311, acc.: 96.88%] [G loss: 0.466427]\n",
      "epoch:22 step:17814 [D loss: 0.015160, acc.: 100.00%] [G loss: 0.032907]\n",
      "epoch:22 step:17815 [D loss: 0.016392, acc.: 100.00%] [G loss: 0.017291]\n",
      "epoch:22 step:17816 [D loss: 0.001595, acc.: 100.00%] [G loss: 0.020554]\n",
      "epoch:22 step:17817 [D loss: 0.005241, acc.: 100.00%] [G loss: 0.046357]\n",
      "epoch:22 step:17818 [D loss: 0.002063, acc.: 100.00%] [G loss: 3.290195]\n",
      "epoch:22 step:17819 [D loss: 0.489215, acc.: 76.56%] [G loss: 11.097187]\n",
      "epoch:22 step:17820 [D loss: 3.273880, acc.: 50.78%] [G loss: 2.667689]\n",
      "epoch:22 step:17821 [D loss: 0.031079, acc.: 99.22%] [G loss: 0.550398]\n",
      "epoch:22 step:17822 [D loss: 0.003408, acc.: 100.00%] [G loss: 0.590329]\n",
      "epoch:22 step:17823 [D loss: 0.023728, acc.: 99.22%] [G loss: 3.273318]\n",
      "epoch:22 step:17824 [D loss: 0.016701, acc.: 100.00%] [G loss: 0.064626]\n",
      "epoch:22 step:17825 [D loss: 0.042170, acc.: 98.44%] [G loss: 0.101135]\n",
      "epoch:22 step:17826 [D loss: 0.005264, acc.: 100.00%] [G loss: 4.378993]\n",
      "epoch:22 step:17827 [D loss: 0.045890, acc.: 98.44%] [G loss: 0.095424]\n",
      "epoch:22 step:17828 [D loss: 0.009690, acc.: 100.00%] [G loss: 0.338969]\n",
      "epoch:22 step:17829 [D loss: 0.023959, acc.: 99.22%] [G loss: 1.163964]\n",
      "epoch:22 step:17830 [D loss: 0.235701, acc.: 90.62%] [G loss: 6.991773]\n",
      "epoch:22 step:17831 [D loss: 1.552212, acc.: 57.03%] [G loss: 0.000136]\n",
      "epoch:22 step:17832 [D loss: 0.197229, acc.: 92.97%] [G loss: 2.981050]\n",
      "epoch:22 step:17833 [D loss: 0.005153, acc.: 100.00%] [G loss: 3.143812]\n",
      "epoch:22 step:17834 [D loss: 0.012149, acc.: 99.22%] [G loss: 2.942874]\n",
      "epoch:22 step:17835 [D loss: 0.099986, acc.: 96.88%] [G loss: 0.587816]\n",
      "epoch:22 step:17836 [D loss: 0.004232, acc.: 100.00%] [G loss: 0.151195]\n",
      "epoch:22 step:17837 [D loss: 0.006417, acc.: 100.00%] [G loss: 1.544502]\n",
      "epoch:22 step:17838 [D loss: 0.017969, acc.: 99.22%] [G loss: 0.008044]\n",
      "epoch:22 step:17839 [D loss: 0.016572, acc.: 99.22%] [G loss: 0.361039]\n",
      "epoch:22 step:17840 [D loss: 0.003732, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:22 step:17841 [D loss: 0.008754, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:22 step:17842 [D loss: 0.003659, acc.: 100.00%] [G loss: 0.001581]\n",
      "epoch:22 step:17843 [D loss: 0.275150, acc.: 85.16%] [G loss: 0.769335]\n",
      "epoch:22 step:17844 [D loss: 0.385790, acc.: 82.81%] [G loss: 0.005920]\n",
      "epoch:22 step:17845 [D loss: 0.000379, acc.: 100.00%] [G loss: 2.137585]\n",
      "epoch:22 step:17846 [D loss: 0.000946, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:22 step:17847 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:22 step:17848 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:22 step:17849 [D loss: 0.000239, acc.: 100.00%] [G loss: 1.179956]\n",
      "epoch:22 step:17850 [D loss: 0.004488, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:22 step:17851 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.874671]\n",
      "epoch:22 step:17852 [D loss: 0.001367, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:22 step:17853 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.385979]\n",
      "epoch:22 step:17854 [D loss: 0.001125, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:22 step:17855 [D loss: 0.001403, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:22 step:17856 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:22 step:17857 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.521543]\n",
      "epoch:22 step:17858 [D loss: 0.001014, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:22 step:17859 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.053414]\n",
      "epoch:22 step:17860 [D loss: 0.004604, acc.: 100.00%] [G loss: 0.060834]\n",
      "epoch:22 step:17861 [D loss: 0.003823, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:22 step:17862 [D loss: 0.000984, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:22 step:17863 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:22 step:17864 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:22 step:17865 [D loss: 0.012673, acc.: 99.22%] [G loss: 0.000128]\n",
      "epoch:22 step:17866 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.139075]\n",
      "epoch:22 step:17867 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:22 step:17868 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.143834]\n",
      "epoch:22 step:17869 [D loss: 0.021362, acc.: 100.00%] [G loss: 0.054718]\n",
      "epoch:22 step:17870 [D loss: 0.012411, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:22 step:17871 [D loss: 0.003222, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:22 step:17872 [D loss: 0.002198, acc.: 100.00%] [G loss: 0.040850]\n",
      "epoch:22 step:17873 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:22 step:17874 [D loss: 0.003843, acc.: 100.00%] [G loss: 0.036798]\n",
      "epoch:22 step:17875 [D loss: 0.005182, acc.: 100.00%] [G loss: 0.000557]\n",
      "epoch:22 step:17876 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:22 step:17877 [D loss: 0.001225, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:22 step:17878 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.001391]\n",
      "epoch:22 step:17879 [D loss: 0.016440, acc.: 99.22%] [G loss: 0.005085]\n",
      "epoch:22 step:17880 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.394988]\n",
      "epoch:22 step:17881 [D loss: 0.003898, acc.: 100.00%] [G loss: 0.033706]\n",
      "epoch:22 step:17882 [D loss: 0.010948, acc.: 99.22%] [G loss: 0.006084]\n",
      "epoch:22 step:17883 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.020093]\n",
      "epoch:22 step:17884 [D loss: 0.001843, acc.: 100.00%] [G loss: 0.002534]\n",
      "epoch:22 step:17885 [D loss: 0.047529, acc.: 99.22%] [G loss: 2.431955]\n",
      "epoch:22 step:17886 [D loss: 0.066978, acc.: 96.88%] [G loss: 0.169833]\n",
      "epoch:22 step:17887 [D loss: 0.007816, acc.: 99.22%] [G loss: 0.006558]\n",
      "epoch:22 step:17888 [D loss: 0.010567, acc.: 100.00%] [G loss: 0.200606]\n",
      "epoch:22 step:17889 [D loss: 0.422472, acc.: 78.91%] [G loss: 7.921266]\n",
      "epoch:22 step:17890 [D loss: 2.376333, acc.: 51.56%] [G loss: 2.974527]\n",
      "epoch:22 step:17891 [D loss: 0.113364, acc.: 97.66%] [G loss: 1.120295]\n",
      "epoch:22 step:17892 [D loss: 0.016012, acc.: 100.00%] [G loss: 0.499320]\n",
      "epoch:22 step:17893 [D loss: 0.008645, acc.: 100.00%] [G loss: 3.961454]\n",
      "epoch:22 step:17894 [D loss: 0.037200, acc.: 98.44%] [G loss: 0.690944]\n",
      "epoch:22 step:17895 [D loss: 0.217074, acc.: 90.62%] [G loss: 0.572437]\n",
      "epoch:22 step:17896 [D loss: 0.008998, acc.: 100.00%] [G loss: 1.900717]\n",
      "epoch:22 step:17897 [D loss: 0.559991, acc.: 74.22%] [G loss: 0.232746]\n",
      "epoch:22 step:17898 [D loss: 0.013543, acc.: 100.00%] [G loss: 1.017510]\n",
      "epoch:22 step:17899 [D loss: 0.018736, acc.: 100.00%] [G loss: 1.637181]\n",
      "epoch:22 step:17900 [D loss: 0.042062, acc.: 99.22%] [G loss: 1.479374]\n",
      "epoch:22 step:17901 [D loss: 0.033073, acc.: 99.22%] [G loss: 0.434171]\n",
      "epoch:22 step:17902 [D loss: 0.049670, acc.: 98.44%] [G loss: 0.359509]\n",
      "epoch:22 step:17903 [D loss: 0.000809, acc.: 100.00%] [G loss: 1.699755]\n",
      "epoch:22 step:17904 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.126059]\n",
      "epoch:22 step:17905 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.142843]\n",
      "epoch:22 step:17906 [D loss: 0.001163, acc.: 100.00%] [G loss: 0.044517]\n",
      "epoch:22 step:17907 [D loss: 0.000925, acc.: 100.00%] [G loss: 0.005154]\n",
      "epoch:22 step:17908 [D loss: 0.003742, acc.: 100.00%] [G loss: 0.216106]\n",
      "epoch:22 step:17909 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.041177]\n",
      "epoch:22 step:17910 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.037418]\n",
      "epoch:22 step:17911 [D loss: 0.001006, acc.: 100.00%] [G loss: 0.019762]\n",
      "epoch:22 step:17912 [D loss: 0.001506, acc.: 100.00%] [G loss: 0.004061]\n",
      "epoch:22 step:17913 [D loss: 0.002140, acc.: 100.00%] [G loss: 0.344330]\n",
      "epoch:22 step:17914 [D loss: 0.001683, acc.: 100.00%] [G loss: 0.014841]\n",
      "epoch:22 step:17915 [D loss: 0.001018, acc.: 100.00%] [G loss: 0.007320]\n",
      "epoch:22 step:17916 [D loss: 0.008102, acc.: 100.00%] [G loss: 0.039549]\n",
      "epoch:22 step:17917 [D loss: 0.001573, acc.: 100.00%] [G loss: 0.021025]\n",
      "epoch:22 step:17918 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17919 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.002505]\n",
      "epoch:22 step:17920 [D loss: 0.010087, acc.: 100.00%] [G loss: 0.339619]\n",
      "epoch:22 step:17921 [D loss: 0.010048, acc.: 100.00%] [G loss: 0.037197]\n",
      "epoch:22 step:17922 [D loss: 0.001741, acc.: 100.00%] [G loss: 0.017368]\n",
      "epoch:22 step:17923 [D loss: 0.011140, acc.: 100.00%] [G loss: 0.010943]\n",
      "epoch:22 step:17924 [D loss: 0.001063, acc.: 100.00%] [G loss: 0.005252]\n",
      "epoch:22 step:17925 [D loss: 0.004948, acc.: 100.00%] [G loss: 0.048888]\n",
      "epoch:22 step:17926 [D loss: 0.006028, acc.: 100.00%] [G loss: 0.090952]\n",
      "epoch:22 step:17927 [D loss: 0.014248, acc.: 100.00%] [G loss: 0.036701]\n",
      "epoch:22 step:17928 [D loss: 0.003379, acc.: 100.00%] [G loss: 0.006498]\n",
      "epoch:22 step:17929 [D loss: 0.004247, acc.: 100.00%] [G loss: 0.020930]\n",
      "epoch:22 step:17930 [D loss: 0.001362, acc.: 100.00%] [G loss: 0.066127]\n",
      "epoch:22 step:17931 [D loss: 0.002590, acc.: 100.00%] [G loss: 0.019932]\n",
      "epoch:22 step:17932 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.030776]\n",
      "epoch:22 step:17933 [D loss: 0.002404, acc.: 100.00%] [G loss: 0.008994]\n",
      "epoch:22 step:17934 [D loss: 0.006118, acc.: 100.00%] [G loss: 0.004502]\n",
      "epoch:22 step:17935 [D loss: 0.001637, acc.: 100.00%] [G loss: 0.010356]\n",
      "epoch:22 step:17936 [D loss: 0.001284, acc.: 100.00%] [G loss: 0.024451]\n",
      "epoch:22 step:17937 [D loss: 0.002551, acc.: 100.00%] [G loss: 0.011906]\n",
      "epoch:22 step:17938 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.001925]\n",
      "epoch:22 step:17939 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.013238]\n",
      "epoch:22 step:17940 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.001772]\n",
      "epoch:22 step:17941 [D loss: 0.005694, acc.: 100.00%] [G loss: 0.003210]\n",
      "epoch:22 step:17942 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.003146]\n",
      "epoch:22 step:17943 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.000500]\n",
      "epoch:22 step:17944 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.009297]\n",
      "epoch:22 step:17945 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.005696]\n",
      "epoch:22 step:17946 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000745]\n",
      "epoch:22 step:17947 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.005016]\n",
      "epoch:22 step:17948 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.009439]\n",
      "epoch:22 step:17949 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.004239]\n",
      "epoch:22 step:17950 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.012722]\n",
      "epoch:22 step:17951 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.023605]\n",
      "epoch:22 step:17952 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.150332]\n",
      "epoch:22 step:17953 [D loss: 0.001272, acc.: 100.00%] [G loss: 0.001412]\n",
      "epoch:22 step:17954 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.004140]\n",
      "epoch:22 step:17955 [D loss: 0.000949, acc.: 100.00%] [G loss: 0.018354]\n",
      "epoch:22 step:17956 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:22 step:17957 [D loss: 0.013988, acc.: 100.00%] [G loss: 0.031976]\n",
      "epoch:22 step:17958 [D loss: 0.010770, acc.: 100.00%] [G loss: 0.009842]\n",
      "epoch:22 step:17959 [D loss: 0.001690, acc.: 100.00%] [G loss: 0.016249]\n",
      "epoch:22 step:17960 [D loss: 0.003162, acc.: 100.00%] [G loss: 0.017131]\n",
      "epoch:22 step:17961 [D loss: 0.003422, acc.: 100.00%] [G loss: 0.007529]\n",
      "epoch:22 step:17962 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.033478]\n",
      "epoch:22 step:17963 [D loss: 0.001223, acc.: 100.00%] [G loss: 0.008966]\n",
      "epoch:23 step:17964 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.007527]\n",
      "epoch:23 step:17965 [D loss: 0.001319, acc.: 100.00%] [G loss: 0.016031]\n",
      "epoch:23 step:17966 [D loss: 0.005584, acc.: 100.00%] [G loss: 0.008861]\n",
      "epoch:23 step:17967 [D loss: 0.002027, acc.: 100.00%] [G loss: 0.190162]\n",
      "epoch:23 step:17968 [D loss: 0.000713, acc.: 100.00%] [G loss: 0.003086]\n",
      "epoch:23 step:17969 [D loss: 0.001421, acc.: 100.00%] [G loss: 0.006167]\n",
      "epoch:23 step:17970 [D loss: 0.002394, acc.: 100.00%] [G loss: 0.001842]\n",
      "epoch:23 step:17971 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.007272]\n",
      "epoch:23 step:17972 [D loss: 0.004626, acc.: 100.00%] [G loss: 0.003364]\n",
      "epoch:23 step:17973 [D loss: 0.002070, acc.: 100.00%] [G loss: 0.004795]\n",
      "epoch:23 step:17974 [D loss: 0.024323, acc.: 100.00%] [G loss: 0.087246]\n",
      "epoch:23 step:17975 [D loss: 0.002379, acc.: 100.00%] [G loss: 0.139055]\n",
      "epoch:23 step:17976 [D loss: 0.011729, acc.: 100.00%] [G loss: 0.260420]\n",
      "epoch:23 step:17977 [D loss: 0.014098, acc.: 100.00%] [G loss: 0.170300]\n",
      "epoch:23 step:17978 [D loss: 0.004184, acc.: 100.00%] [G loss: 0.006617]\n",
      "epoch:23 step:17979 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.108636]\n",
      "epoch:23 step:17980 [D loss: 0.001232, acc.: 100.00%] [G loss: 0.009624]\n",
      "epoch:23 step:17981 [D loss: 0.006534, acc.: 100.00%] [G loss: 0.003866]\n",
      "epoch:23 step:17982 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.020248]\n",
      "epoch:23 step:17983 [D loss: 0.005596, acc.: 100.00%] [G loss: 0.057083]\n",
      "epoch:23 step:17984 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.016275]\n",
      "epoch:23 step:17985 [D loss: 0.001945, acc.: 100.00%] [G loss: 0.002616]\n",
      "epoch:23 step:17986 [D loss: 0.002619, acc.: 100.00%] [G loss: 0.007373]\n",
      "epoch:23 step:17987 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.019116]\n",
      "epoch:23 step:17988 [D loss: 0.004693, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:23 step:17989 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.000985]\n",
      "epoch:23 step:17990 [D loss: 0.001254, acc.: 100.00%] [G loss: 0.080095]\n",
      "epoch:23 step:17991 [D loss: 0.000874, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:23 step:17992 [D loss: 0.001107, acc.: 100.00%] [G loss: 0.001204]\n",
      "epoch:23 step:17993 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.005066]\n",
      "epoch:23 step:17994 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.001538]\n",
      "epoch:23 step:17995 [D loss: 0.001273, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:23 step:17996 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.005994]\n",
      "epoch:23 step:17997 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000860]\n",
      "epoch:23 step:17998 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.000947]\n",
      "epoch:23 step:17999 [D loss: 0.001251, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:23 step:18000 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.003008]\n",
      "epoch:23 step:18001 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.001699]\n",
      "epoch:23 step:18002 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.054882]\n",
      "epoch:23 step:18003 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.000982]\n",
      "epoch:23 step:18004 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:23 step:18005 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:23 step:18006 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.001008]\n",
      "epoch:23 step:18007 [D loss: 0.002175, acc.: 100.00%] [G loss: 0.001316]\n",
      "epoch:23 step:18008 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.002845]\n",
      "epoch:23 step:18009 [D loss: 0.005061, acc.: 100.00%] [G loss: 0.003205]\n",
      "epoch:23 step:18010 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:23 step:18011 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.003779]\n",
      "epoch:23 step:18012 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:23 step:18013 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.006943]\n",
      "epoch:23 step:18014 [D loss: 0.002116, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:23 step:18015 [D loss: 0.001449, acc.: 100.00%] [G loss: 0.042080]\n",
      "epoch:23 step:18016 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:23 step:18017 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.010577]\n",
      "epoch:23 step:18018 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.003298]\n",
      "epoch:23 step:18019 [D loss: 0.006043, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:23 step:18020 [D loss: 0.002869, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:23 step:18021 [D loss: 0.022471, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:23 step:18022 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.002807]\n",
      "epoch:23 step:18023 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.004535]\n",
      "epoch:23 step:18024 [D loss: 0.007009, acc.: 100.00%] [G loss: 0.000546]\n",
      "epoch:23 step:18025 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:23 step:18026 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.002614]\n",
      "epoch:23 step:18027 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:23 step:18028 [D loss: 0.013069, acc.: 100.00%] [G loss: 0.001662]\n",
      "epoch:23 step:18029 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.008020]\n",
      "epoch:23 step:18030 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:23 step:18031 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:23 step:18032 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.003128]\n",
      "epoch:23 step:18033 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18034 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.001543]\n",
      "epoch:23 step:18035 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:23 step:18036 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.020556]\n",
      "epoch:23 step:18037 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:23 step:18038 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:23 step:18039 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000905]\n",
      "epoch:23 step:18040 [D loss: 0.001557, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:23 step:18041 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.080312]\n",
      "epoch:23 step:18042 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.007757]\n",
      "epoch:23 step:18043 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000590]\n",
      "epoch:23 step:18044 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.010742]\n",
      "epoch:23 step:18045 [D loss: 0.001241, acc.: 100.00%] [G loss: 0.000696]\n",
      "epoch:23 step:18046 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:23 step:18047 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:23 step:18048 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000996]\n",
      "epoch:23 step:18049 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.001512]\n",
      "epoch:23 step:18050 [D loss: 0.001487, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:23 step:18051 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.002961]\n",
      "epoch:23 step:18052 [D loss: 0.000873, acc.: 100.00%] [G loss: 0.012822]\n",
      "epoch:23 step:18053 [D loss: 0.001433, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:23 step:18054 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:23 step:18055 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:23 step:18056 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.003969]\n",
      "epoch:23 step:18057 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:23 step:18058 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002907]\n",
      "epoch:23 step:18059 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.007316]\n",
      "epoch:23 step:18060 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.003313]\n",
      "epoch:23 step:18061 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:23 step:18062 [D loss: 0.000766, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:23 step:18063 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:23 step:18064 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:23 step:18065 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:23 step:18066 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:23 step:18067 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002698]\n",
      "epoch:23 step:18068 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:23 step:18069 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:23 step:18070 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.001916]\n",
      "epoch:23 step:18071 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:23 step:18072 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:23 step:18073 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:23 step:18074 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:23 step:18075 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:23 step:18076 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:23 step:18077 [D loss: 0.000952, acc.: 100.00%] [G loss: 0.001869]\n",
      "epoch:23 step:18078 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:23 step:18079 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:23 step:18080 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:23 step:18081 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:23 step:18082 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:23 step:18083 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:23 step:18084 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.019976]\n",
      "epoch:23 step:18085 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.004054]\n",
      "epoch:23 step:18086 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:23 step:18087 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:23 step:18088 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.002370]\n",
      "epoch:23 step:18089 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001505]\n",
      "epoch:23 step:18090 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:23 step:18091 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:23 step:18092 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:23 step:18093 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:23 step:18094 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.003471]\n",
      "epoch:23 step:18095 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001317]\n",
      "epoch:23 step:18096 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:23 step:18097 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:23 step:18098 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:23 step:18099 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:23 step:18100 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:23 step:18101 [D loss: 0.000685, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:23 step:18102 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000874]\n",
      "epoch:23 step:18103 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.009410]\n",
      "epoch:23 step:18104 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:23 step:18105 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:23 step:18106 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:23 step:18107 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.004697]\n",
      "epoch:23 step:18108 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.005339]\n",
      "epoch:23 step:18109 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:23 step:18110 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:23 step:18111 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:23 step:18112 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.002930]\n",
      "epoch:23 step:18113 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.030746]\n",
      "epoch:23 step:18114 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.000676]\n",
      "epoch:23 step:18115 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:23 step:18116 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:23 step:18117 [D loss: 0.001550, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:23 step:18118 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.017440]\n",
      "epoch:23 step:18119 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:23 step:18120 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.002823]\n",
      "epoch:23 step:18121 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.029957]\n",
      "epoch:23 step:18122 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:23 step:18123 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:23 step:18124 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.002932]\n",
      "epoch:23 step:18125 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.043874]\n",
      "epoch:23 step:18126 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.002207]\n",
      "epoch:23 step:18127 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:23 step:18128 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:23 step:18129 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:23 step:18130 [D loss: 0.016107, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:23 step:18131 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:23 step:18132 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.032996]\n",
      "epoch:23 step:18133 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.001133]\n",
      "epoch:23 step:18134 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:23 step:18135 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.024625]\n",
      "epoch:23 step:18136 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.009562]\n",
      "epoch:23 step:18137 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:23 step:18138 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.002838]\n",
      "epoch:23 step:18139 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:23 step:18140 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.009958]\n",
      "epoch:23 step:18141 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.001720]\n",
      "epoch:23 step:18142 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.076703]\n",
      "epoch:23 step:18143 [D loss: 0.000997, acc.: 100.00%] [G loss: 0.002119]\n",
      "epoch:23 step:18144 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.001088]\n",
      "epoch:23 step:18145 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.002408]\n",
      "epoch:23 step:18146 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.004030]\n",
      "epoch:23 step:18147 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.001588]\n",
      "epoch:23 step:18148 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.025724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18149 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:23 step:18150 [D loss: 0.001367, acc.: 100.00%] [G loss: 0.000843]\n",
      "epoch:23 step:18151 [D loss: 0.000773, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:23 step:18152 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:23 step:18153 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:23 step:18154 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.019579]\n",
      "epoch:23 step:18155 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.008695]\n",
      "epoch:23 step:18156 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.002792]\n",
      "epoch:23 step:18157 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:23 step:18158 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000686]\n",
      "epoch:23 step:18159 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:23 step:18160 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.004525]\n",
      "epoch:23 step:18161 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:23 step:18162 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:23 step:18163 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.001533]\n",
      "epoch:23 step:18164 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.006683]\n",
      "epoch:23 step:18165 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.001215]\n",
      "epoch:23 step:18166 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000597]\n",
      "epoch:23 step:18167 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.002959]\n",
      "epoch:23 step:18168 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:23 step:18169 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:23 step:18170 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.002168]\n",
      "epoch:23 step:18171 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:23 step:18172 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.005345]\n",
      "epoch:23 step:18173 [D loss: 0.002114, acc.: 100.00%] [G loss: 0.010607]\n",
      "epoch:23 step:18174 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.010864]\n",
      "epoch:23 step:18175 [D loss: 0.002435, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:23 step:18176 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.001136]\n",
      "epoch:23 step:18177 [D loss: 0.007325, acc.: 100.00%] [G loss: 0.009751]\n",
      "epoch:23 step:18178 [D loss: 0.106084, acc.: 97.66%] [G loss: 0.642075]\n",
      "epoch:23 step:18179 [D loss: 0.147560, acc.: 94.53%] [G loss: 0.056244]\n",
      "epoch:23 step:18180 [D loss: 0.000253, acc.: 100.00%] [G loss: 3.298023]\n",
      "epoch:23 step:18181 [D loss: 0.001691, acc.: 100.00%] [G loss: 2.654456]\n",
      "epoch:23 step:18182 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.006663]\n",
      "epoch:23 step:18183 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.007788]\n",
      "epoch:23 step:18184 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.054674]\n",
      "epoch:23 step:18185 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.016166]\n",
      "epoch:23 step:18186 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.011600]\n",
      "epoch:23 step:18187 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.694275]\n",
      "epoch:23 step:18188 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.547674]\n",
      "epoch:23 step:18189 [D loss: 0.004356, acc.: 100.00%] [G loss: 0.016790]\n",
      "epoch:23 step:18190 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:23 step:18191 [D loss: 0.003554, acc.: 100.00%] [G loss: 0.077296]\n",
      "epoch:23 step:18192 [D loss: 0.002106, acc.: 100.00%] [G loss: 0.005244]\n",
      "epoch:23 step:18193 [D loss: 0.005472, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:23 step:18194 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:23 step:18195 [D loss: 0.001862, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:23 step:18196 [D loss: 0.001252, acc.: 100.00%] [G loss: 0.210135]\n",
      "epoch:23 step:18197 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:23 step:18198 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:23 step:18199 [D loss: 0.007222, acc.: 100.00%] [G loss: 0.055440]\n",
      "epoch:23 step:18200 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:23 step:18201 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:23 step:18202 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:23 step:18203 [D loss: 0.011269, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:23 step:18204 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:23 step:18205 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:23 step:18206 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:23 step:18207 [D loss: 0.001441, acc.: 100.00%] [G loss: 0.020038]\n",
      "epoch:23 step:18208 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:23 step:18209 [D loss: 0.002302, acc.: 100.00%] [G loss: 0.001060]\n",
      "epoch:23 step:18210 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:23 step:18211 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:23 step:18212 [D loss: 0.001571, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:23 step:18213 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:23 step:18214 [D loss: 0.012757, acc.: 100.00%] [G loss: 0.028690]\n",
      "epoch:23 step:18215 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.012906]\n",
      "epoch:23 step:18216 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.001647]\n",
      "epoch:23 step:18217 [D loss: 0.001102, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:23 step:18218 [D loss: 0.000910, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:23 step:18219 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:23 step:18220 [D loss: 0.002379, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:23 step:18221 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.016577]\n",
      "epoch:23 step:18222 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:23 step:18223 [D loss: 0.000885, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:23 step:18224 [D loss: 0.012888, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:23 step:18225 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.007246]\n",
      "epoch:23 step:18226 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:23 step:18227 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.003848]\n",
      "epoch:23 step:18228 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:23 step:18229 [D loss: 0.021535, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:23 step:18230 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000696]\n",
      "epoch:23 step:18231 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.001179]\n",
      "epoch:23 step:18232 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:23 step:18233 [D loss: 0.001776, acc.: 100.00%] [G loss: 0.577447]\n",
      "epoch:23 step:18234 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:23 step:18235 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:23 step:18236 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.077306]\n",
      "epoch:23 step:18237 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:23 step:18238 [D loss: 0.010254, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:23 step:18239 [D loss: 0.021800, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:23 step:18240 [D loss: 0.052945, acc.: 98.44%] [G loss: 0.000049]\n",
      "epoch:23 step:18241 [D loss: 0.778266, acc.: 63.28%] [G loss: 10.247215]\n",
      "epoch:23 step:18242 [D loss: 4.106530, acc.: 50.00%] [G loss: 5.466562]\n",
      "epoch:23 step:18243 [D loss: 1.157773, acc.: 60.94%] [G loss: 0.098324]\n",
      "epoch:23 step:18244 [D loss: 0.454682, acc.: 88.28%] [G loss: 0.183529]\n",
      "epoch:23 step:18245 [D loss: 0.046128, acc.: 98.44%] [G loss: 0.195269]\n",
      "epoch:23 step:18246 [D loss: 0.037976, acc.: 99.22%] [G loss: 0.313749]\n",
      "epoch:23 step:18247 [D loss: 0.022995, acc.: 99.22%] [G loss: 0.088260]\n",
      "epoch:23 step:18248 [D loss: 0.008103, acc.: 100.00%] [G loss: 2.731698]\n",
      "epoch:23 step:18249 [D loss: 0.041505, acc.: 99.22%] [G loss: 0.037555]\n",
      "epoch:23 step:18250 [D loss: 0.032507, acc.: 99.22%] [G loss: 0.037511]\n",
      "epoch:23 step:18251 [D loss: 0.054394, acc.: 99.22%] [G loss: 0.024573]\n",
      "epoch:23 step:18252 [D loss: 0.014809, acc.: 100.00%] [G loss: 0.009093]\n",
      "epoch:23 step:18253 [D loss: 0.011570, acc.: 100.00%] [G loss: 0.011064]\n",
      "epoch:23 step:18254 [D loss: 0.016462, acc.: 100.00%] [G loss: 0.027438]\n",
      "epoch:23 step:18255 [D loss: 0.037318, acc.: 100.00%] [G loss: 0.043801]\n",
      "epoch:23 step:18256 [D loss: 0.003428, acc.: 100.00%] [G loss: 0.001749]\n",
      "epoch:23 step:18257 [D loss: 0.005520, acc.: 100.00%] [G loss: 0.004266]\n",
      "epoch:23 step:18258 [D loss: 0.020300, acc.: 100.00%] [G loss: 0.011864]\n",
      "epoch:23 step:18259 [D loss: 0.002993, acc.: 100.00%] [G loss: 0.026824]\n",
      "epoch:23 step:18260 [D loss: 0.017576, acc.: 100.00%] [G loss: 0.025797]\n",
      "epoch:23 step:18261 [D loss: 0.025386, acc.: 99.22%] [G loss: 0.004956]\n",
      "epoch:23 step:18262 [D loss: 0.008583, acc.: 100.00%] [G loss: 0.016510]\n",
      "epoch:23 step:18263 [D loss: 0.010427, acc.: 100.00%] [G loss: 0.064205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18264 [D loss: 0.015244, acc.: 100.00%] [G loss: 0.170550]\n",
      "epoch:23 step:18265 [D loss: 0.002324, acc.: 100.00%] [G loss: 0.039347]\n",
      "epoch:23 step:18266 [D loss: 0.005561, acc.: 100.00%] [G loss: 0.162481]\n",
      "epoch:23 step:18267 [D loss: 0.074068, acc.: 98.44%] [G loss: 1.047715]\n",
      "epoch:23 step:18268 [D loss: 0.025376, acc.: 99.22%] [G loss: 0.013011]\n",
      "epoch:23 step:18269 [D loss: 0.018032, acc.: 99.22%] [G loss: 0.039222]\n",
      "epoch:23 step:18270 [D loss: 0.022483, acc.: 99.22%] [G loss: 0.033055]\n",
      "epoch:23 step:18271 [D loss: 0.033082, acc.: 99.22%] [G loss: 0.012209]\n",
      "epoch:23 step:18272 [D loss: 0.005221, acc.: 100.00%] [G loss: 0.008479]\n",
      "epoch:23 step:18273 [D loss: 0.005753, acc.: 100.00%] [G loss: 0.011309]\n",
      "epoch:23 step:18274 [D loss: 0.035560, acc.: 99.22%] [G loss: 0.026260]\n",
      "epoch:23 step:18275 [D loss: 0.014748, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:23 step:18276 [D loss: 0.010221, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:23 step:18277 [D loss: 0.003725, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:23 step:18278 [D loss: 0.103270, acc.: 95.31%] [G loss: 0.002688]\n",
      "epoch:23 step:18279 [D loss: 0.018226, acc.: 100.00%] [G loss: 0.002697]\n",
      "epoch:23 step:18280 [D loss: 0.028858, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:23 step:18281 [D loss: 0.004413, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:23 step:18282 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:23 step:18283 [D loss: 0.003008, acc.: 100.00%] [G loss: 0.017210]\n",
      "epoch:23 step:18284 [D loss: 0.005152, acc.: 100.00%] [G loss: 0.008700]\n",
      "epoch:23 step:18285 [D loss: 0.009384, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:23 step:18286 [D loss: 0.003203, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:23 step:18287 [D loss: 0.018754, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:23 step:18288 [D loss: 0.001162, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:23 step:18289 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.005729]\n",
      "epoch:23 step:18290 [D loss: 0.004748, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:23 step:18291 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.156869]\n",
      "epoch:23 step:18292 [D loss: 0.001705, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:23 step:18293 [D loss: 0.001637, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:23 step:18294 [D loss: 0.001825, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:23 step:18295 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.056479]\n",
      "epoch:23 step:18296 [D loss: 0.001490, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:23 step:18297 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.126623]\n",
      "epoch:23 step:18298 [D loss: 0.004505, acc.: 100.00%] [G loss: 0.001137]\n",
      "epoch:23 step:18299 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:23 step:18300 [D loss: 0.001943, acc.: 100.00%] [G loss: 0.020596]\n",
      "epoch:23 step:18301 [D loss: 0.023104, acc.: 100.00%] [G loss: 0.004202]\n",
      "epoch:23 step:18302 [D loss: 0.008775, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:23 step:18303 [D loss: 0.009520, acc.: 100.00%] [G loss: 0.006557]\n",
      "epoch:23 step:18304 [D loss: 0.002949, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:23 step:18305 [D loss: 0.033725, acc.: 98.44%] [G loss: 0.077231]\n",
      "epoch:23 step:18306 [D loss: 0.006449, acc.: 100.00%] [G loss: 0.002384]\n",
      "epoch:23 step:18307 [D loss: 0.002117, acc.: 100.00%] [G loss: 0.004549]\n",
      "epoch:23 step:18308 [D loss: 0.002315, acc.: 100.00%] [G loss: 0.140086]\n",
      "epoch:23 step:18309 [D loss: 0.002280, acc.: 100.00%] [G loss: 0.001994]\n",
      "epoch:23 step:18310 [D loss: 0.020183, acc.: 99.22%] [G loss: 0.000751]\n",
      "epoch:23 step:18311 [D loss: 0.001777, acc.: 100.00%] [G loss: 0.049281]\n",
      "epoch:23 step:18312 [D loss: 0.042866, acc.: 98.44%] [G loss: 0.001912]\n",
      "epoch:23 step:18313 [D loss: 0.010689, acc.: 100.00%] [G loss: 1.711691]\n",
      "epoch:23 step:18314 [D loss: 0.014686, acc.: 100.00%] [G loss: 0.278537]\n",
      "epoch:23 step:18315 [D loss: 0.060221, acc.: 98.44%] [G loss: 0.002006]\n",
      "epoch:23 step:18316 [D loss: 0.068919, acc.: 99.22%] [G loss: 0.006222]\n",
      "epoch:23 step:18317 [D loss: 0.003474, acc.: 100.00%] [G loss: 0.071839]\n",
      "epoch:23 step:18318 [D loss: 0.015894, acc.: 100.00%] [G loss: 0.710205]\n",
      "epoch:23 step:18319 [D loss: 0.004427, acc.: 100.00%] [G loss: 0.261306]\n",
      "epoch:23 step:18320 [D loss: 0.056844, acc.: 99.22%] [G loss: 0.129970]\n",
      "epoch:23 step:18321 [D loss: 0.081056, acc.: 97.66%] [G loss: 0.029792]\n",
      "epoch:23 step:18322 [D loss: 0.001553, acc.: 100.00%] [G loss: 0.590996]\n",
      "epoch:23 step:18323 [D loss: 0.005589, acc.: 100.00%] [G loss: 0.002558]\n",
      "epoch:23 step:18324 [D loss: 0.017123, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:23 step:18325 [D loss: 0.011738, acc.: 100.00%] [G loss: 0.127503]\n",
      "epoch:23 step:18326 [D loss: 0.047389, acc.: 100.00%] [G loss: 0.068541]\n",
      "epoch:23 step:18327 [D loss: 0.034126, acc.: 98.44%] [G loss: 1.471874]\n",
      "epoch:23 step:18328 [D loss: 0.014343, acc.: 100.00%] [G loss: 0.121697]\n",
      "epoch:23 step:18329 [D loss: 0.005256, acc.: 100.00%] [G loss: 0.866998]\n",
      "epoch:23 step:18330 [D loss: 0.024545, acc.: 100.00%] [G loss: 0.198335]\n",
      "epoch:23 step:18331 [D loss: 0.068515, acc.: 96.88%] [G loss: 0.108660]\n",
      "epoch:23 step:18332 [D loss: 0.042652, acc.: 98.44%] [G loss: 1.291073]\n",
      "epoch:23 step:18333 [D loss: 0.041340, acc.: 99.22%] [G loss: 0.001808]\n",
      "epoch:23 step:18334 [D loss: 0.003966, acc.: 100.00%] [G loss: 0.004897]\n",
      "epoch:23 step:18335 [D loss: 0.001695, acc.: 100.00%] [G loss: 0.005426]\n",
      "epoch:23 step:18336 [D loss: 0.056629, acc.: 98.44%] [G loss: 0.284035]\n",
      "epoch:23 step:18337 [D loss: 0.063607, acc.: 97.66%] [G loss: 0.106346]\n",
      "epoch:23 step:18338 [D loss: 0.013480, acc.: 99.22%] [G loss: 0.081792]\n",
      "epoch:23 step:18339 [D loss: 0.040218, acc.: 100.00%] [G loss: 0.019682]\n",
      "epoch:23 step:18340 [D loss: 0.007936, acc.: 100.00%] [G loss: 2.141585]\n",
      "epoch:23 step:18341 [D loss: 0.006435, acc.: 100.00%] [G loss: 0.065236]\n",
      "epoch:23 step:18342 [D loss: 0.004314, acc.: 100.00%] [G loss: 0.052858]\n",
      "epoch:23 step:18343 [D loss: 0.004127, acc.: 100.00%] [G loss: 0.311268]\n",
      "epoch:23 step:18344 [D loss: 0.005292, acc.: 100.00%] [G loss: 0.100575]\n",
      "epoch:23 step:18345 [D loss: 0.038601, acc.: 99.22%] [G loss: 0.898308]\n",
      "epoch:23 step:18346 [D loss: 0.006426, acc.: 100.00%] [G loss: 0.560293]\n",
      "epoch:23 step:18347 [D loss: 0.051387, acc.: 99.22%] [G loss: 2.012515]\n",
      "epoch:23 step:18348 [D loss: 0.067701, acc.: 97.66%] [G loss: 0.105637]\n",
      "epoch:23 step:18349 [D loss: 0.173513, acc.: 92.97%] [G loss: 0.668179]\n",
      "epoch:23 step:18350 [D loss: 0.031860, acc.: 99.22%] [G loss: 0.073265]\n",
      "epoch:23 step:18351 [D loss: 0.044013, acc.: 98.44%] [G loss: 0.000706]\n",
      "epoch:23 step:18352 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.761309]\n",
      "epoch:23 step:18353 [D loss: 0.024030, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:23 step:18354 [D loss: 0.006721, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:23 step:18355 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.000847]\n",
      "epoch:23 step:18356 [D loss: 0.004656, acc.: 100.00%] [G loss: 1.402978]\n",
      "epoch:23 step:18357 [D loss: 0.016883, acc.: 100.00%] [G loss: 0.009838]\n",
      "epoch:23 step:18358 [D loss: 0.091240, acc.: 96.09%] [G loss: 0.002428]\n",
      "epoch:23 step:18359 [D loss: 0.010697, acc.: 100.00%] [G loss: 0.010548]\n",
      "epoch:23 step:18360 [D loss: 0.009689, acc.: 100.00%] [G loss: 0.008549]\n",
      "epoch:23 step:18361 [D loss: 0.002384, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:23 step:18362 [D loss: 0.001689, acc.: 100.00%] [G loss: 0.015706]\n",
      "epoch:23 step:18363 [D loss: 0.010749, acc.: 100.00%] [G loss: 0.049486]\n",
      "epoch:23 step:18364 [D loss: 0.010665, acc.: 99.22%] [G loss: 0.004554]\n",
      "epoch:23 step:18365 [D loss: 0.001478, acc.: 100.00%] [G loss: 0.006106]\n",
      "epoch:23 step:18366 [D loss: 0.074599, acc.: 96.88%] [G loss: 0.114301]\n",
      "epoch:23 step:18367 [D loss: 0.006416, acc.: 100.00%] [G loss: 1.019175]\n",
      "epoch:23 step:18368 [D loss: 0.197006, acc.: 94.53%] [G loss: 4.490339]\n",
      "epoch:23 step:18369 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:23 step:18370 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:23 step:18371 [D loss: 0.001149, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:23 step:18372 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:23 step:18373 [D loss: 0.003417, acc.: 100.00%] [G loss: 0.090237]\n",
      "epoch:23 step:18374 [D loss: 0.001929, acc.: 100.00%] [G loss: 0.058953]\n",
      "epoch:23 step:18375 [D loss: 0.003815, acc.: 100.00%] [G loss: 0.139011]\n",
      "epoch:23 step:18376 [D loss: 0.045184, acc.: 99.22%] [G loss: 0.000006]\n",
      "epoch:23 step:18377 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:23 step:18378 [D loss: 0.000356, acc.: 100.00%] [G loss: 3.297601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18379 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.027437]\n",
      "epoch:23 step:18380 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.000661]\n",
      "epoch:23 step:18381 [D loss: 0.000497, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:23 step:18382 [D loss: 0.004417, acc.: 100.00%] [G loss: 0.347071]\n",
      "epoch:23 step:18383 [D loss: 0.006786, acc.: 100.00%] [G loss: 0.076503]\n",
      "epoch:23 step:18384 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.008626]\n",
      "epoch:23 step:18385 [D loss: 0.002029, acc.: 100.00%] [G loss: 0.211827]\n",
      "epoch:23 step:18386 [D loss: 0.008585, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:23 step:18387 [D loss: 0.009187, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:23 step:18388 [D loss: 0.004787, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:23 step:18389 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:23 step:18390 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:23 step:18391 [D loss: 0.023003, acc.: 98.44%] [G loss: 0.026257]\n",
      "epoch:23 step:18392 [D loss: 0.048100, acc.: 99.22%] [G loss: 1.023117]\n",
      "epoch:23 step:18393 [D loss: 0.097299, acc.: 96.88%] [G loss: 0.082586]\n",
      "epoch:23 step:18394 [D loss: 0.327129, acc.: 85.94%] [G loss: 9.627669]\n",
      "epoch:23 step:18395 [D loss: 2.783501, acc.: 49.22%] [G loss: 0.123359]\n",
      "epoch:23 step:18396 [D loss: 0.029036, acc.: 99.22%] [G loss: 1.765765]\n",
      "epoch:23 step:18397 [D loss: 0.047591, acc.: 99.22%] [G loss: 0.048873]\n",
      "epoch:23 step:18398 [D loss: 0.009931, acc.: 100.00%] [G loss: 0.029447]\n",
      "epoch:23 step:18399 [D loss: 0.016166, acc.: 100.00%] [G loss: 0.034225]\n",
      "epoch:23 step:18400 [D loss: 0.027466, acc.: 99.22%] [G loss: 0.003644]\n",
      "epoch:23 step:18401 [D loss: 0.014712, acc.: 100.00%] [G loss: 0.022392]\n",
      "epoch:23 step:18402 [D loss: 0.045785, acc.: 100.00%] [G loss: 5.403289]\n",
      "epoch:23 step:18403 [D loss: 0.016374, acc.: 99.22%] [G loss: 0.052900]\n",
      "epoch:23 step:18404 [D loss: 0.007630, acc.: 100.00%] [G loss: 4.192568]\n",
      "epoch:23 step:18405 [D loss: 0.005656, acc.: 100.00%] [G loss: 0.014260]\n",
      "epoch:23 step:18406 [D loss: 0.009432, acc.: 100.00%] [G loss: 3.332757]\n",
      "epoch:23 step:18407 [D loss: 0.021679, acc.: 99.22%] [G loss: 0.016812]\n",
      "epoch:23 step:18408 [D loss: 0.023433, acc.: 100.00%] [G loss: 0.023233]\n",
      "epoch:23 step:18409 [D loss: 0.024075, acc.: 100.00%] [G loss: 0.019554]\n",
      "epoch:23 step:18410 [D loss: 0.036604, acc.: 99.22%] [G loss: 0.014654]\n",
      "epoch:23 step:18411 [D loss: 0.008206, acc.: 100.00%] [G loss: 1.967682]\n",
      "epoch:23 step:18412 [D loss: 0.008489, acc.: 100.00%] [G loss: 0.006463]\n",
      "epoch:23 step:18413 [D loss: 0.006231, acc.: 100.00%] [G loss: 0.001573]\n",
      "epoch:23 step:18414 [D loss: 0.015928, acc.: 100.00%] [G loss: 0.025250]\n",
      "epoch:23 step:18415 [D loss: 0.006010, acc.: 100.00%] [G loss: 0.008895]\n",
      "epoch:23 step:18416 [D loss: 0.021301, acc.: 99.22%] [G loss: 1.078425]\n",
      "epoch:23 step:18417 [D loss: 0.015500, acc.: 100.00%] [G loss: 0.001933]\n",
      "epoch:23 step:18418 [D loss: 0.010760, acc.: 100.00%] [G loss: 0.314928]\n",
      "epoch:23 step:18419 [D loss: 0.020828, acc.: 100.00%] [G loss: 0.026716]\n",
      "epoch:23 step:18420 [D loss: 0.017458, acc.: 100.00%] [G loss: 0.009343]\n",
      "epoch:23 step:18421 [D loss: 0.010426, acc.: 100.00%] [G loss: 0.015748]\n",
      "epoch:23 step:18422 [D loss: 0.023407, acc.: 100.00%] [G loss: 0.001416]\n",
      "epoch:23 step:18423 [D loss: 0.026987, acc.: 99.22%] [G loss: 0.000292]\n",
      "epoch:23 step:18424 [D loss: 0.022049, acc.: 99.22%] [G loss: 0.001563]\n",
      "epoch:23 step:18425 [D loss: 0.005631, acc.: 100.00%] [G loss: 0.012869]\n",
      "epoch:23 step:18426 [D loss: 0.007119, acc.: 100.00%] [G loss: 1.201199]\n",
      "epoch:23 step:18427 [D loss: 0.002866, acc.: 100.00%] [G loss: 1.159394]\n",
      "epoch:23 step:18428 [D loss: 0.018673, acc.: 100.00%] [G loss: 0.155403]\n",
      "epoch:23 step:18429 [D loss: 0.003257, acc.: 100.00%] [G loss: 0.101135]\n",
      "epoch:23 step:18430 [D loss: 0.030969, acc.: 98.44%] [G loss: 0.001205]\n",
      "epoch:23 step:18431 [D loss: 0.014594, acc.: 100.00%] [G loss: 0.292194]\n",
      "epoch:23 step:18432 [D loss: 0.005022, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:23 step:18433 [D loss: 0.004725, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:23 step:18434 [D loss: 0.004283, acc.: 100.00%] [G loss: 0.003765]\n",
      "epoch:23 step:18435 [D loss: 0.007104, acc.: 100.00%] [G loss: 0.002464]\n",
      "epoch:23 step:18436 [D loss: 0.011443, acc.: 99.22%] [G loss: 2.767776]\n",
      "epoch:23 step:18437 [D loss: 0.111915, acc.: 97.66%] [G loss: 1.830915]\n",
      "epoch:23 step:18438 [D loss: 0.428244, acc.: 82.03%] [G loss: 0.507759]\n",
      "epoch:23 step:18439 [D loss: 0.003581, acc.: 100.00%] [G loss: 1.473122]\n",
      "epoch:23 step:18440 [D loss: 0.100564, acc.: 96.88%] [G loss: 0.334071]\n",
      "epoch:23 step:18441 [D loss: 0.112366, acc.: 96.09%] [G loss: 6.028884]\n",
      "epoch:23 step:18442 [D loss: 0.057219, acc.: 97.66%] [G loss: 6.943299]\n",
      "epoch:23 step:18443 [D loss: 0.003933, acc.: 100.00%] [G loss: 4.449480]\n",
      "epoch:23 step:18444 [D loss: 0.004860, acc.: 100.00%] [G loss: 0.371226]\n",
      "epoch:23 step:18445 [D loss: 0.005653, acc.: 100.00%] [G loss: 1.646699]\n",
      "epoch:23 step:18446 [D loss: 0.017089, acc.: 99.22%] [G loss: 0.083426]\n",
      "epoch:23 step:18447 [D loss: 0.019969, acc.: 100.00%] [G loss: 1.088512]\n",
      "epoch:23 step:18448 [D loss: 0.005294, acc.: 100.00%] [G loss: 0.426707]\n",
      "epoch:23 step:18449 [D loss: 0.019324, acc.: 100.00%] [G loss: 0.027697]\n",
      "epoch:23 step:18450 [D loss: 0.010562, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:23 step:18451 [D loss: 0.002821, acc.: 100.00%] [G loss: 0.006597]\n",
      "epoch:23 step:18452 [D loss: 0.001868, acc.: 100.00%] [G loss: 0.009152]\n",
      "epoch:23 step:18453 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.003614]\n",
      "epoch:23 step:18454 [D loss: 0.005271, acc.: 100.00%] [G loss: 0.001068]\n",
      "epoch:23 step:18455 [D loss: 0.001471, acc.: 100.00%] [G loss: 0.008102]\n",
      "epoch:23 step:18456 [D loss: 0.000995, acc.: 100.00%] [G loss: 0.003032]\n",
      "epoch:23 step:18457 [D loss: 0.002603, acc.: 100.00%] [G loss: 0.349952]\n",
      "epoch:23 step:18458 [D loss: 0.008954, acc.: 100.00%] [G loss: 0.015745]\n",
      "epoch:23 step:18459 [D loss: 0.007223, acc.: 100.00%] [G loss: 0.326460]\n",
      "epoch:23 step:18460 [D loss: 0.003416, acc.: 100.00%] [G loss: 0.065645]\n",
      "epoch:23 step:18461 [D loss: 0.019558, acc.: 99.22%] [G loss: 0.049070]\n",
      "epoch:23 step:18462 [D loss: 0.040787, acc.: 99.22%] [G loss: 0.007088]\n",
      "epoch:23 step:18463 [D loss: 0.001689, acc.: 100.00%] [G loss: 0.001575]\n",
      "epoch:23 step:18464 [D loss: 0.012775, acc.: 100.00%] [G loss: 0.020135]\n",
      "epoch:23 step:18465 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.038582]\n",
      "epoch:23 step:18466 [D loss: 0.002441, acc.: 100.00%] [G loss: 0.036664]\n",
      "epoch:23 step:18467 [D loss: 0.032716, acc.: 99.22%] [G loss: 0.203203]\n",
      "epoch:23 step:18468 [D loss: 0.025670, acc.: 100.00%] [G loss: 2.119949]\n",
      "epoch:23 step:18469 [D loss: 0.033015, acc.: 100.00%] [G loss: 3.027903]\n",
      "epoch:23 step:18470 [D loss: 0.109550, acc.: 96.09%] [G loss: 0.042255]\n",
      "epoch:23 step:18471 [D loss: 0.037298, acc.: 99.22%] [G loss: 0.007447]\n",
      "epoch:23 step:18472 [D loss: 0.003448, acc.: 100.00%] [G loss: 0.056545]\n",
      "epoch:23 step:18473 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.025689]\n",
      "epoch:23 step:18474 [D loss: 0.055878, acc.: 97.66%] [G loss: 1.784903]\n",
      "epoch:23 step:18475 [D loss: 0.002150, acc.: 100.00%] [G loss: 4.469271]\n",
      "epoch:23 step:18476 [D loss: 1.265847, acc.: 46.88%] [G loss: 3.756793]\n",
      "epoch:23 step:18477 [D loss: 0.232683, acc.: 90.62%] [G loss: 1.191453]\n",
      "epoch:23 step:18478 [D loss: 0.007791, acc.: 100.00%] [G loss: 1.962356]\n",
      "epoch:23 step:18479 [D loss: 0.009238, acc.: 100.00%] [G loss: 0.021296]\n",
      "epoch:23 step:18480 [D loss: 0.021792, acc.: 99.22%] [G loss: 0.011452]\n",
      "epoch:23 step:18481 [D loss: 0.011684, acc.: 100.00%] [G loss: 0.015962]\n",
      "epoch:23 step:18482 [D loss: 0.188387, acc.: 91.41%] [G loss: 4.117999]\n",
      "epoch:23 step:18483 [D loss: 0.520001, acc.: 77.34%] [G loss: 3.434597]\n",
      "epoch:23 step:18484 [D loss: 0.644561, acc.: 77.34%] [G loss: 5.094335]\n",
      "epoch:23 step:18485 [D loss: 0.629448, acc.: 78.12%] [G loss: 4.896432]\n",
      "epoch:23 step:18486 [D loss: 0.347032, acc.: 85.94%] [G loss: 3.829026]\n",
      "epoch:23 step:18487 [D loss: 0.131255, acc.: 93.75%] [G loss: 2.574183]\n",
      "epoch:23 step:18488 [D loss: 0.054176, acc.: 96.88%] [G loss: 2.584311]\n",
      "epoch:23 step:18489 [D loss: 0.183588, acc.: 92.97%] [G loss: 0.908020]\n",
      "epoch:23 step:18490 [D loss: 0.117480, acc.: 93.75%] [G loss: 1.324950]\n",
      "epoch:23 step:18491 [D loss: 0.002953, acc.: 100.00%] [G loss: 5.077144]\n",
      "epoch:23 step:18492 [D loss: 0.005454, acc.: 100.00%] [G loss: 0.636703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18493 [D loss: 0.129219, acc.: 96.09%] [G loss: 3.491881]\n",
      "epoch:23 step:18494 [D loss: 0.032685, acc.: 99.22%] [G loss: 2.023055]\n",
      "epoch:23 step:18495 [D loss: 0.134296, acc.: 94.53%] [G loss: 0.293529]\n",
      "epoch:23 step:18496 [D loss: 0.085955, acc.: 98.44%] [G loss: 1.558071]\n",
      "epoch:23 step:18497 [D loss: 0.016778, acc.: 100.00%] [G loss: 1.934484]\n",
      "epoch:23 step:18498 [D loss: 0.035518, acc.: 98.44%] [G loss: 1.589916]\n",
      "epoch:23 step:18499 [D loss: 0.006028, acc.: 100.00%] [G loss: 1.645901]\n",
      "epoch:23 step:18500 [D loss: 0.035432, acc.: 99.22%] [G loss: 0.942446]\n",
      "epoch:23 step:18501 [D loss: 0.010923, acc.: 100.00%] [G loss: 0.120307]\n",
      "epoch:23 step:18502 [D loss: 0.014299, acc.: 100.00%] [G loss: 5.108355]\n",
      "epoch:23 step:18503 [D loss: 0.036367, acc.: 100.00%] [G loss: 0.263435]\n",
      "epoch:23 step:18504 [D loss: 0.025307, acc.: 99.22%] [G loss: 0.732081]\n",
      "epoch:23 step:18505 [D loss: 0.566698, acc.: 73.44%] [G loss: 5.445793]\n",
      "epoch:23 step:18506 [D loss: 1.514566, acc.: 56.25%] [G loss: 0.513364]\n",
      "epoch:23 step:18507 [D loss: 0.167730, acc.: 92.97%] [G loss: 2.384272]\n",
      "epoch:23 step:18508 [D loss: 0.064743, acc.: 97.66%] [G loss: 0.559781]\n",
      "epoch:23 step:18509 [D loss: 0.061185, acc.: 98.44%] [G loss: 4.588026]\n",
      "epoch:23 step:18510 [D loss: 0.010322, acc.: 100.00%] [G loss: 0.083552]\n",
      "epoch:23 step:18511 [D loss: 0.021093, acc.: 100.00%] [G loss: 0.026592]\n",
      "epoch:23 step:18512 [D loss: 0.003246, acc.: 100.00%] [G loss: 0.156391]\n",
      "epoch:23 step:18513 [D loss: 0.021424, acc.: 99.22%] [G loss: 0.058149]\n",
      "epoch:23 step:18514 [D loss: 0.008626, acc.: 100.00%] [G loss: 4.026614]\n",
      "epoch:23 step:18515 [D loss: 0.028378, acc.: 99.22%] [G loss: 0.130170]\n",
      "epoch:23 step:18516 [D loss: 0.008147, acc.: 100.00%] [G loss: 0.133063]\n",
      "epoch:23 step:18517 [D loss: 0.025271, acc.: 100.00%] [G loss: 0.889594]\n",
      "epoch:23 step:18518 [D loss: 0.007115, acc.: 100.00%] [G loss: 0.201390]\n",
      "epoch:23 step:18519 [D loss: 0.044215, acc.: 98.44%] [G loss: 0.249124]\n",
      "epoch:23 step:18520 [D loss: 0.025118, acc.: 99.22%] [G loss: 0.078749]\n",
      "epoch:23 step:18521 [D loss: 0.002503, acc.: 100.00%] [G loss: 0.107631]\n",
      "epoch:23 step:18522 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.152805]\n",
      "epoch:23 step:18523 [D loss: 0.002660, acc.: 100.00%] [G loss: 2.801882]\n",
      "epoch:23 step:18524 [D loss: 0.004908, acc.: 100.00%] [G loss: 0.016254]\n",
      "epoch:23 step:18525 [D loss: 0.001413, acc.: 100.00%] [G loss: 0.099659]\n",
      "epoch:23 step:18526 [D loss: 0.001227, acc.: 100.00%] [G loss: 0.005263]\n",
      "epoch:23 step:18527 [D loss: 0.002647, acc.: 100.00%] [G loss: 0.014544]\n",
      "epoch:23 step:18528 [D loss: 0.003237, acc.: 100.00%] [G loss: 0.010005]\n",
      "epoch:23 step:18529 [D loss: 0.006751, acc.: 100.00%] [G loss: 0.008696]\n",
      "epoch:23 step:18530 [D loss: 0.003804, acc.: 100.00%] [G loss: 0.007102]\n",
      "epoch:23 step:18531 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.011849]\n",
      "epoch:23 step:18532 [D loss: 0.009023, acc.: 100.00%] [G loss: 0.018623]\n",
      "epoch:23 step:18533 [D loss: 0.016100, acc.: 100.00%] [G loss: 0.009344]\n",
      "epoch:23 step:18534 [D loss: 0.001552, acc.: 100.00%] [G loss: 1.135849]\n",
      "epoch:23 step:18535 [D loss: 0.001011, acc.: 100.00%] [G loss: 0.040855]\n",
      "epoch:23 step:18536 [D loss: 0.003051, acc.: 100.00%] [G loss: 0.016607]\n",
      "epoch:23 step:18537 [D loss: 0.008266, acc.: 100.00%] [G loss: 0.071525]\n",
      "epoch:23 step:18538 [D loss: 0.014728, acc.: 99.22%] [G loss: 0.058250]\n",
      "epoch:23 step:18539 [D loss: 0.002279, acc.: 100.00%] [G loss: 0.001782]\n",
      "epoch:23 step:18540 [D loss: 0.023180, acc.: 99.22%] [G loss: 0.004857]\n",
      "epoch:23 step:18541 [D loss: 0.001938, acc.: 100.00%] [G loss: 0.029886]\n",
      "epoch:23 step:18542 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.014607]\n",
      "epoch:23 step:18543 [D loss: 0.006670, acc.: 100.00%] [G loss: 0.004882]\n",
      "epoch:23 step:18544 [D loss: 0.001167, acc.: 100.00%] [G loss: 0.003577]\n",
      "epoch:23 step:18545 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.005184]\n",
      "epoch:23 step:18546 [D loss: 0.002347, acc.: 100.00%] [G loss: 0.003972]\n",
      "epoch:23 step:18547 [D loss: 0.000959, acc.: 100.00%] [G loss: 0.237560]\n",
      "epoch:23 step:18548 [D loss: 0.003747, acc.: 100.00%] [G loss: 1.153243]\n",
      "epoch:23 step:18549 [D loss: 0.016329, acc.: 100.00%] [G loss: 0.005430]\n",
      "epoch:23 step:18550 [D loss: 0.006552, acc.: 100.00%] [G loss: 0.040646]\n",
      "epoch:23 step:18551 [D loss: 0.015800, acc.: 100.00%] [G loss: 0.057868]\n",
      "epoch:23 step:18552 [D loss: 0.002670, acc.: 100.00%] [G loss: 0.192785]\n",
      "epoch:23 step:18553 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.041175]\n",
      "epoch:23 step:18554 [D loss: 0.002401, acc.: 100.00%] [G loss: 0.132629]\n",
      "epoch:23 step:18555 [D loss: 0.001529, acc.: 100.00%] [G loss: 0.045475]\n",
      "epoch:23 step:18556 [D loss: 0.018056, acc.: 99.22%] [G loss: 0.014095]\n",
      "epoch:23 step:18557 [D loss: 0.001338, acc.: 100.00%] [G loss: 0.014156]\n",
      "epoch:23 step:18558 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.005888]\n",
      "epoch:23 step:18559 [D loss: 0.002463, acc.: 100.00%] [G loss: 0.003822]\n",
      "epoch:23 step:18560 [D loss: 0.000855, acc.: 100.00%] [G loss: 0.002554]\n",
      "epoch:23 step:18561 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.014987]\n",
      "epoch:23 step:18562 [D loss: 0.001471, acc.: 100.00%] [G loss: 0.033475]\n",
      "epoch:23 step:18563 [D loss: 0.003039, acc.: 100.00%] [G loss: 0.011370]\n",
      "epoch:23 step:18564 [D loss: 0.001068, acc.: 100.00%] [G loss: 0.004499]\n",
      "epoch:23 step:18565 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.003966]\n",
      "epoch:23 step:18566 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:23 step:18567 [D loss: 0.005547, acc.: 100.00%] [G loss: 0.131254]\n",
      "epoch:23 step:18568 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.004286]\n",
      "epoch:23 step:18569 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.003495]\n",
      "epoch:23 step:18570 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.004817]\n",
      "epoch:23 step:18571 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.001855]\n",
      "epoch:23 step:18572 [D loss: 0.000751, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:23 step:18573 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.001711]\n",
      "epoch:23 step:18574 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.002296]\n",
      "epoch:23 step:18575 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.002475]\n",
      "epoch:23 step:18576 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.001678]\n",
      "epoch:23 step:18577 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:23 step:18578 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000580]\n",
      "epoch:23 step:18579 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:23 step:18580 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.000886]\n",
      "epoch:23 step:18581 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:23 step:18582 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.002687]\n",
      "epoch:23 step:18583 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.000823]\n",
      "epoch:23 step:18584 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.003437]\n",
      "epoch:23 step:18585 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:23 step:18586 [D loss: 0.002377, acc.: 100.00%] [G loss: 0.001296]\n",
      "epoch:23 step:18587 [D loss: 0.002548, acc.: 100.00%] [G loss: 0.036764]\n",
      "epoch:23 step:18588 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.007920]\n",
      "epoch:23 step:18589 [D loss: 0.011668, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:23 step:18590 [D loss: 0.000826, acc.: 100.00%] [G loss: 0.003699]\n",
      "epoch:23 step:18591 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.010278]\n",
      "epoch:23 step:18592 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.001527]\n",
      "epoch:23 step:18593 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000871]\n",
      "epoch:23 step:18594 [D loss: 0.002917, acc.: 100.00%] [G loss: 0.059265]\n",
      "epoch:23 step:18595 [D loss: 0.002614, acc.: 100.00%] [G loss: 0.002084]\n",
      "epoch:23 step:18596 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.004384]\n",
      "epoch:23 step:18597 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.002397]\n",
      "epoch:23 step:18598 [D loss: 0.001552, acc.: 100.00%] [G loss: 0.008419]\n",
      "epoch:23 step:18599 [D loss: 0.004608, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:23 step:18600 [D loss: 0.001087, acc.: 100.00%] [G loss: 0.003934]\n",
      "epoch:23 step:18601 [D loss: 0.000906, acc.: 100.00%] [G loss: 0.004175]\n",
      "epoch:23 step:18602 [D loss: 0.002212, acc.: 100.00%] [G loss: 0.005796]\n",
      "epoch:23 step:18603 [D loss: 0.001793, acc.: 100.00%] [G loss: 0.005015]\n",
      "epoch:23 step:18604 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.149973]\n",
      "epoch:23 step:18605 [D loss: 0.113424, acc.: 96.09%] [G loss: 1.203321]\n",
      "epoch:23 step:18606 [D loss: 0.030885, acc.: 99.22%] [G loss: 2.435619]\n",
      "epoch:23 step:18607 [D loss: 0.108170, acc.: 96.09%] [G loss: 0.016174]\n",
      "epoch:23 step:18608 [D loss: 0.004329, acc.: 100.00%] [G loss: 0.002790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18609 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.004686]\n",
      "epoch:23 step:18610 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.011589]\n",
      "epoch:23 step:18611 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.379526]\n",
      "epoch:23 step:18612 [D loss: 0.003121, acc.: 100.00%] [G loss: 0.000911]\n",
      "epoch:23 step:18613 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:23 step:18614 [D loss: 0.006261, acc.: 100.00%] [G loss: 0.006098]\n",
      "epoch:23 step:18615 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.007606]\n",
      "epoch:23 step:18616 [D loss: 0.002502, acc.: 100.00%] [G loss: 0.002192]\n",
      "epoch:23 step:18617 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000858]\n",
      "epoch:23 step:18618 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.001708]\n",
      "epoch:23 step:18619 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.003352]\n",
      "epoch:23 step:18620 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.017123]\n",
      "epoch:23 step:18621 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.006470]\n",
      "epoch:23 step:18622 [D loss: 0.001784, acc.: 100.00%] [G loss: 0.002766]\n",
      "epoch:23 step:18623 [D loss: 0.001371, acc.: 100.00%] [G loss: 0.003511]\n",
      "epoch:23 step:18624 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.006089]\n",
      "epoch:23 step:18625 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.005842]\n",
      "epoch:23 step:18626 [D loss: 0.002056, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:23 step:18627 [D loss: 0.004558, acc.: 100.00%] [G loss: 0.002406]\n",
      "epoch:23 step:18628 [D loss: 0.002402, acc.: 100.00%] [G loss: 0.014935]\n",
      "epoch:23 step:18629 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.002398]\n",
      "epoch:23 step:18630 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.031633]\n",
      "epoch:23 step:18631 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:23 step:18632 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.003072]\n",
      "epoch:23 step:18633 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.001098]\n",
      "epoch:23 step:18634 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.001338]\n",
      "epoch:23 step:18635 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.004130]\n",
      "epoch:23 step:18636 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.128725]\n",
      "epoch:23 step:18637 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.006667]\n",
      "epoch:23 step:18638 [D loss: 0.000657, acc.: 100.00%] [G loss: 0.001626]\n",
      "epoch:23 step:18639 [D loss: 0.002692, acc.: 100.00%] [G loss: 0.000574]\n",
      "epoch:23 step:18640 [D loss: 0.013983, acc.: 99.22%] [G loss: 0.012186]\n",
      "epoch:23 step:18641 [D loss: 0.010196, acc.: 100.00%] [G loss: 0.002766]\n",
      "epoch:23 step:18642 [D loss: 0.007069, acc.: 100.00%] [G loss: 0.005987]\n",
      "epoch:23 step:18643 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.004835]\n",
      "epoch:23 step:18644 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.216160]\n",
      "epoch:23 step:18645 [D loss: 0.006965, acc.: 100.00%] [G loss: 0.056096]\n",
      "epoch:23 step:18646 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.023373]\n",
      "epoch:23 step:18647 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.004232]\n",
      "epoch:23 step:18648 [D loss: 0.004682, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:23 step:18649 [D loss: 0.001155, acc.: 100.00%] [G loss: 0.004792]\n",
      "epoch:23 step:18650 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.006331]\n",
      "epoch:23 step:18651 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.003171]\n",
      "epoch:23 step:18652 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.003270]\n",
      "epoch:23 step:18653 [D loss: 0.002694, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:23 step:18654 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.001034]\n",
      "epoch:23 step:18655 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.006490]\n",
      "epoch:23 step:18656 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.002943]\n",
      "epoch:23 step:18657 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.017411]\n",
      "epoch:23 step:18658 [D loss: 0.002292, acc.: 100.00%] [G loss: 0.002101]\n",
      "epoch:23 step:18659 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.011349]\n",
      "epoch:23 step:18660 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.000519]\n",
      "epoch:23 step:18661 [D loss: 0.000835, acc.: 100.00%] [G loss: 0.003472]\n",
      "epoch:23 step:18662 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.076227]\n",
      "epoch:23 step:18663 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.055172]\n",
      "epoch:23 step:18664 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.001026]\n",
      "epoch:23 step:18665 [D loss: 0.003211, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:23 step:18666 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.083802]\n",
      "epoch:23 step:18667 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.001395]\n",
      "epoch:23 step:18668 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.003363]\n",
      "epoch:23 step:18669 [D loss: 0.000657, acc.: 100.00%] [G loss: 0.011218]\n",
      "epoch:23 step:18670 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.022225]\n",
      "epoch:23 step:18671 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.414493]\n",
      "epoch:23 step:18672 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.001442]\n",
      "epoch:23 step:18673 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.004698]\n",
      "epoch:23 step:18674 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.002232]\n",
      "epoch:23 step:18675 [D loss: 0.002310, acc.: 100.00%] [G loss: 0.017452]\n",
      "epoch:23 step:18676 [D loss: 0.023182, acc.: 99.22%] [G loss: 0.000979]\n",
      "epoch:23 step:18677 [D loss: 0.001437, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:23 step:18678 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.010408]\n",
      "epoch:23 step:18679 [D loss: 0.003970, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:23 step:18680 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:23 step:18681 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.002524]\n",
      "epoch:23 step:18682 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.003338]\n",
      "epoch:23 step:18683 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.003901]\n",
      "epoch:23 step:18684 [D loss: 0.002038, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:23 step:18685 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:23 step:18686 [D loss: 0.001400, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:23 step:18687 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.009878]\n",
      "epoch:23 step:18688 [D loss: 0.005323, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:23 step:18689 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.000385]\n",
      "epoch:23 step:18690 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:23 step:18691 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.005220]\n",
      "epoch:23 step:18692 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.010901]\n",
      "epoch:23 step:18693 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:23 step:18694 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:23 step:18695 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:23 step:18696 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000426]\n",
      "epoch:23 step:18697 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.001353]\n",
      "epoch:23 step:18698 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.004613]\n",
      "epoch:23 step:18699 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.028114]\n",
      "epoch:23 step:18700 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:23 step:18701 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000766]\n",
      "epoch:23 step:18702 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.017015]\n",
      "epoch:23 step:18703 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001252]\n",
      "epoch:23 step:18704 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:23 step:18705 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.035911]\n",
      "epoch:23 step:18706 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.009214]\n",
      "epoch:23 step:18707 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.001375]\n",
      "epoch:23 step:18708 [D loss: 0.003990, acc.: 100.00%] [G loss: 0.001392]\n",
      "epoch:23 step:18709 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:23 step:18710 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.007085]\n",
      "epoch:23 step:18711 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:23 step:18712 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001479]\n",
      "epoch:23 step:18713 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.009440]\n",
      "epoch:23 step:18714 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001202]\n",
      "epoch:23 step:18715 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:23 step:18716 [D loss: 0.005824, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:23 step:18717 [D loss: 0.001115, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:23 step:18718 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:23 step:18719 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.001357]\n",
      "epoch:23 step:18720 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000817]\n",
      "epoch:23 step:18721 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.004294]\n",
      "epoch:23 step:18722 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.004213]\n",
      "epoch:23 step:18723 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.000458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18724 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:23 step:18725 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:23 step:18726 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:23 step:18727 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000503]\n",
      "epoch:23 step:18728 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:23 step:18729 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.005054]\n",
      "epoch:23 step:18730 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:23 step:18731 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.001098]\n",
      "epoch:23 step:18732 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:23 step:18733 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.003257]\n",
      "epoch:23 step:18734 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:23 step:18735 [D loss: 0.004788, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:23 step:18736 [D loss: 0.001542, acc.: 100.00%] [G loss: 0.000599]\n",
      "epoch:23 step:18737 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:23 step:18738 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.022383]\n",
      "epoch:23 step:18739 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000924]\n",
      "epoch:23 step:18740 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:23 step:18741 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.002003]\n",
      "epoch:23 step:18742 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:23 step:18743 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002806]\n",
      "epoch:23 step:18744 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:24 step:18745 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.001746]\n",
      "epoch:24 step:18746 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.020587]\n",
      "epoch:24 step:18747 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:24 step:18748 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001033]\n",
      "epoch:24 step:18749 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.002188]\n",
      "epoch:24 step:18750 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:24 step:18751 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:24 step:18752 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000963]\n",
      "epoch:24 step:18753 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:24 step:18754 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:24 step:18755 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.004687]\n",
      "epoch:24 step:18756 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:24 step:18757 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:24 step:18758 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:24 step:18759 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.001658]\n",
      "epoch:24 step:18760 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.005780]\n",
      "epoch:24 step:18761 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:24 step:18762 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:24 step:18763 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.003051]\n",
      "epoch:24 step:18764 [D loss: 0.005804, acc.: 100.00%] [G loss: 0.002815]\n",
      "epoch:24 step:18765 [D loss: 0.003543, acc.: 100.00%] [G loss: 0.020782]\n",
      "epoch:24 step:18766 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.004221]\n",
      "epoch:24 step:18767 [D loss: 0.003583, acc.: 100.00%] [G loss: 0.000391]\n",
      "epoch:24 step:18768 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.001336]\n",
      "epoch:24 step:18769 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.014069]\n",
      "epoch:24 step:18770 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:24 step:18771 [D loss: 0.001414, acc.: 100.00%] [G loss: 0.042086]\n",
      "epoch:24 step:18772 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.013793]\n",
      "epoch:24 step:18773 [D loss: 0.003725, acc.: 100.00%] [G loss: 0.001126]\n",
      "epoch:24 step:18774 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.529020]\n",
      "epoch:24 step:18775 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.000634]\n",
      "epoch:24 step:18776 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:24 step:18777 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:24 step:18778 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.096395]\n",
      "epoch:24 step:18779 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:24 step:18780 [D loss: 0.001677, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:24 step:18781 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:24 step:18782 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.010027]\n",
      "epoch:24 step:18783 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.005906]\n",
      "epoch:24 step:18784 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:24 step:18785 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.025162]\n",
      "epoch:24 step:18786 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:24 step:18787 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.016588]\n",
      "epoch:24 step:18788 [D loss: 0.004846, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:24 step:18789 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.003644]\n",
      "epoch:24 step:18790 [D loss: 0.004609, acc.: 100.00%] [G loss: 0.001694]\n",
      "epoch:24 step:18791 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.005996]\n",
      "epoch:24 step:18792 [D loss: 0.002431, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:24 step:18793 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:24 step:18794 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.051866]\n",
      "epoch:24 step:18795 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.033814]\n",
      "epoch:24 step:18796 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:24 step:18797 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:24 step:18798 [D loss: 0.001255, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:24 step:18799 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000483]\n",
      "epoch:24 step:18800 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:24 step:18801 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:24 step:18802 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.013830]\n",
      "epoch:24 step:18803 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000894]\n",
      "epoch:24 step:18804 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.015009]\n",
      "epoch:24 step:18805 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.129901]\n",
      "epoch:24 step:18806 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:24 step:18807 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000790]\n",
      "epoch:24 step:18808 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.001566]\n",
      "epoch:24 step:18809 [D loss: 0.016129, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:24 step:18810 [D loss: 0.001607, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:24 step:18811 [D loss: 0.001520, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:24 step:18812 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:24 step:18813 [D loss: 0.008680, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:24 step:18814 [D loss: 0.000692, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:24 step:18815 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:24 step:18816 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:24 step:18817 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:24 step:18818 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:24 step:18819 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.004028]\n",
      "epoch:24 step:18820 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.002323]\n",
      "epoch:24 step:18821 [D loss: 0.000738, acc.: 100.00%] [G loss: 0.005380]\n",
      "epoch:24 step:18822 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:24 step:18823 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:24 step:18824 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:24 step:18825 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:24 step:18826 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:24 step:18827 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.003868]\n",
      "epoch:24 step:18828 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:24 step:18829 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000821]\n",
      "epoch:24 step:18830 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.002111]\n",
      "epoch:24 step:18831 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:24 step:18832 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:24 step:18833 [D loss: 0.000818, acc.: 100.00%] [G loss: 0.002096]\n",
      "epoch:24 step:18834 [D loss: 0.003043, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:24 step:18835 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:24 step:18836 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:24 step:18837 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18838 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:24 step:18839 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000817]\n",
      "epoch:24 step:18840 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:24 step:18841 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001802]\n",
      "epoch:24 step:18842 [D loss: 0.002332, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:24 step:18843 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:24 step:18844 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:24 step:18845 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:24 step:18846 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:24 step:18847 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:24 step:18848 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:24 step:18849 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:24 step:18850 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.002162]\n",
      "epoch:24 step:18851 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:24 step:18852 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.017108]\n",
      "epoch:24 step:18853 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18854 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:24 step:18855 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.004181]\n",
      "epoch:24 step:18856 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:24 step:18857 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:24 step:18858 [D loss: 0.006188, acc.: 100.00%] [G loss: 0.021506]\n",
      "epoch:24 step:18859 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.008775]\n",
      "epoch:24 step:18860 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:24 step:18861 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:24 step:18862 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:24 step:18863 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:24 step:18864 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.002034]\n",
      "epoch:24 step:18865 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:24 step:18866 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.055082]\n",
      "epoch:24 step:18867 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:24 step:18868 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:24 step:18869 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.006713]\n",
      "epoch:24 step:18870 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:24 step:18871 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:24 step:18872 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:24 step:18873 [D loss: 0.000959, acc.: 100.00%] [G loss: 0.004657]\n",
      "epoch:24 step:18874 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001132]\n",
      "epoch:24 step:18875 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:24 step:18876 [D loss: 0.001524, acc.: 100.00%] [G loss: 0.004885]\n",
      "epoch:24 step:18877 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:24 step:18878 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.043521]\n",
      "epoch:24 step:18879 [D loss: 0.012104, acc.: 100.00%] [G loss: 0.004723]\n",
      "epoch:24 step:18880 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.000543]\n",
      "epoch:24 step:18881 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.000802]\n",
      "epoch:24 step:18882 [D loss: 0.002528, acc.: 100.00%] [G loss: 0.009355]\n",
      "epoch:24 step:18883 [D loss: 0.001374, acc.: 100.00%] [G loss: 0.003950]\n",
      "epoch:24 step:18884 [D loss: 0.019086, acc.: 100.00%] [G loss: 0.882399]\n",
      "epoch:24 step:18885 [D loss: 0.002272, acc.: 100.00%] [G loss: 0.014749]\n",
      "epoch:24 step:18886 [D loss: 0.072050, acc.: 97.66%] [G loss: 0.000054]\n",
      "epoch:24 step:18887 [D loss: 0.081862, acc.: 97.66%] [G loss: 0.092809]\n",
      "epoch:24 step:18888 [D loss: 0.000624, acc.: 100.00%] [G loss: 2.452811]\n",
      "epoch:24 step:18889 [D loss: 0.073778, acc.: 96.88%] [G loss: 0.035800]\n",
      "epoch:24 step:18890 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.003596]\n",
      "epoch:24 step:18891 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:24 step:18892 [D loss: 0.005615, acc.: 100.00%] [G loss: 0.005206]\n",
      "epoch:24 step:18893 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.001710]\n",
      "epoch:24 step:18894 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.007225]\n",
      "epoch:24 step:18895 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.003632]\n",
      "epoch:24 step:18896 [D loss: 0.002880, acc.: 100.00%] [G loss: 0.006943]\n",
      "epoch:24 step:18897 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.011720]\n",
      "epoch:24 step:18898 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.012507]\n",
      "epoch:24 step:18899 [D loss: 0.008520, acc.: 100.00%] [G loss: 0.016318]\n",
      "epoch:24 step:18900 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.031287]\n",
      "epoch:24 step:18901 [D loss: 0.003692, acc.: 100.00%] [G loss: 0.027894]\n",
      "epoch:24 step:18902 [D loss: 0.003407, acc.: 100.00%] [G loss: 0.003223]\n",
      "epoch:24 step:18903 [D loss: 0.012783, acc.: 100.00%] [G loss: 0.026813]\n",
      "epoch:24 step:18904 [D loss: 0.000483, acc.: 100.00%] [G loss: 1.007905]\n",
      "epoch:24 step:18905 [D loss: 0.004562, acc.: 100.00%] [G loss: 0.003154]\n",
      "epoch:24 step:18906 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.071201]\n",
      "epoch:24 step:18907 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.041126]\n",
      "epoch:24 step:18908 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.020503]\n",
      "epoch:24 step:18909 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.011341]\n",
      "epoch:24 step:18910 [D loss: 0.002110, acc.: 100.00%] [G loss: 0.003792]\n",
      "epoch:24 step:18911 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.008292]\n",
      "epoch:24 step:18912 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.009298]\n",
      "epoch:24 step:18913 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.016524]\n",
      "epoch:24 step:18914 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.005350]\n",
      "epoch:24 step:18915 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.004829]\n",
      "epoch:24 step:18916 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.015846]\n",
      "epoch:24 step:18917 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.010760]\n",
      "epoch:24 step:18918 [D loss: 0.001762, acc.: 100.00%] [G loss: 0.003703]\n",
      "epoch:24 step:18919 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.002307]\n",
      "epoch:24 step:18920 [D loss: 0.001496, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:24 step:18921 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000557]\n",
      "epoch:24 step:18922 [D loss: 0.002478, acc.: 100.00%] [G loss: 0.013284]\n",
      "epoch:24 step:18923 [D loss: 0.009212, acc.: 100.00%] [G loss: 0.001212]\n",
      "epoch:24 step:18924 [D loss: 0.003048, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:24 step:18925 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:24 step:18926 [D loss: 0.006856, acc.: 100.00%] [G loss: 0.000674]\n",
      "epoch:24 step:18927 [D loss: 0.011605, acc.: 100.00%] [G loss: 0.003544]\n",
      "epoch:24 step:18928 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.010030]\n",
      "epoch:24 step:18929 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.107149]\n",
      "epoch:24 step:18930 [D loss: 0.254335, acc.: 88.28%] [G loss: 12.702053]\n",
      "epoch:24 step:18931 [D loss: 4.762528, acc.: 50.00%] [G loss: 0.127607]\n",
      "epoch:24 step:18932 [D loss: 0.446777, acc.: 78.91%] [G loss: 0.001101]\n",
      "epoch:24 step:18933 [D loss: 0.009337, acc.: 100.00%] [G loss: 4.731164]\n",
      "epoch:24 step:18934 [D loss: 0.013676, acc.: 100.00%] [G loss: 0.011142]\n",
      "epoch:24 step:18935 [D loss: 0.103041, acc.: 96.88%] [G loss: 4.608977]\n",
      "epoch:24 step:18936 [D loss: 0.077467, acc.: 96.09%] [G loss: 0.113797]\n",
      "epoch:24 step:18937 [D loss: 0.069873, acc.: 97.66%] [G loss: 0.004284]\n",
      "epoch:24 step:18938 [D loss: 0.672146, acc.: 70.31%] [G loss: 6.944874]\n",
      "epoch:24 step:18939 [D loss: 1.772497, acc.: 51.56%] [G loss: 4.033904]\n",
      "epoch:24 step:18940 [D loss: 0.439904, acc.: 78.12%] [G loss: 1.392129]\n",
      "epoch:24 step:18941 [D loss: 0.580272, acc.: 75.78%] [G loss: 1.413280]\n",
      "epoch:24 step:18942 [D loss: 0.130641, acc.: 92.97%] [G loss: 4.150740]\n",
      "epoch:24 step:18943 [D loss: 0.280904, acc.: 88.28%] [G loss: 0.670948]\n",
      "epoch:24 step:18944 [D loss: 0.065369, acc.: 99.22%] [G loss: 0.160433]\n",
      "epoch:24 step:18945 [D loss: 0.049386, acc.: 100.00%] [G loss: 2.169797]\n",
      "epoch:24 step:18946 [D loss: 0.049837, acc.: 100.00%] [G loss: 0.226504]\n",
      "epoch:24 step:18947 [D loss: 0.012563, acc.: 100.00%] [G loss: 0.228984]\n",
      "epoch:24 step:18948 [D loss: 0.009561, acc.: 100.00%] [G loss: 0.338170]\n",
      "epoch:24 step:18949 [D loss: 0.019883, acc.: 99.22%] [G loss: 0.824938]\n",
      "epoch:24 step:18950 [D loss: 0.063261, acc.: 100.00%] [G loss: 0.349375]\n",
      "epoch:24 step:18951 [D loss: 0.012828, acc.: 100.00%] [G loss: 1.375892]\n",
      "epoch:24 step:18952 [D loss: 0.028017, acc.: 100.00%] [G loss: 0.513039]\n",
      "epoch:24 step:18953 [D loss: 0.056630, acc.: 99.22%] [G loss: 0.391447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18954 [D loss: 0.084021, acc.: 97.66%] [G loss: 0.273176]\n",
      "epoch:24 step:18955 [D loss: 0.007588, acc.: 100.00%] [G loss: 0.454143]\n",
      "epoch:24 step:18956 [D loss: 0.021654, acc.: 98.44%] [G loss: 0.191006]\n",
      "epoch:24 step:18957 [D loss: 0.009398, acc.: 100.00%] [G loss: 0.095892]\n",
      "epoch:24 step:18958 [D loss: 0.106921, acc.: 96.09%] [G loss: 0.001710]\n",
      "epoch:24 step:18959 [D loss: 0.020130, acc.: 100.00%] [G loss: 0.001322]\n",
      "epoch:24 step:18960 [D loss: 0.035356, acc.: 98.44%] [G loss: 0.003524]\n",
      "epoch:24 step:18961 [D loss: 0.001493, acc.: 100.00%] [G loss: 0.590618]\n",
      "epoch:24 step:18962 [D loss: 0.002911, acc.: 100.00%] [G loss: 0.035343]\n",
      "epoch:24 step:18963 [D loss: 0.002216, acc.: 100.00%] [G loss: 0.171215]\n",
      "epoch:24 step:18964 [D loss: 0.007723, acc.: 100.00%] [G loss: 0.013847]\n",
      "epoch:24 step:18965 [D loss: 0.004227, acc.: 100.00%] [G loss: 0.052520]\n",
      "epoch:24 step:18966 [D loss: 0.008822, acc.: 100.00%] [G loss: 0.020410]\n",
      "epoch:24 step:18967 [D loss: 0.003039, acc.: 100.00%] [G loss: 0.015262]\n",
      "epoch:24 step:18968 [D loss: 0.002239, acc.: 100.00%] [G loss: 0.023220]\n",
      "epoch:24 step:18969 [D loss: 0.005174, acc.: 100.00%] [G loss: 0.006322]\n",
      "epoch:24 step:18970 [D loss: 0.003795, acc.: 100.00%] [G loss: 0.015352]\n",
      "epoch:24 step:18971 [D loss: 0.001070, acc.: 100.00%] [G loss: 0.004429]\n",
      "epoch:24 step:18972 [D loss: 0.012257, acc.: 100.00%] [G loss: 0.004076]\n",
      "epoch:24 step:18973 [D loss: 0.002010, acc.: 100.00%] [G loss: 0.011293]\n",
      "epoch:24 step:18974 [D loss: 0.036044, acc.: 100.00%] [G loss: 0.010473]\n",
      "epoch:24 step:18975 [D loss: 0.000844, acc.: 100.00%] [G loss: 0.003412]\n",
      "epoch:24 step:18976 [D loss: 0.001128, acc.: 100.00%] [G loss: 0.006504]\n",
      "epoch:24 step:18977 [D loss: 0.003817, acc.: 100.00%] [G loss: 0.005788]\n",
      "epoch:24 step:18978 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:24 step:18979 [D loss: 0.002801, acc.: 100.00%] [G loss: 0.027980]\n",
      "epoch:24 step:18980 [D loss: 0.023341, acc.: 98.44%] [G loss: 0.002244]\n",
      "epoch:24 step:18981 [D loss: 0.001261, acc.: 100.00%] [G loss: 0.001332]\n",
      "epoch:24 step:18982 [D loss: 0.002431, acc.: 100.00%] [G loss: 0.025891]\n",
      "epoch:24 step:18983 [D loss: 0.002537, acc.: 100.00%] [G loss: 0.002303]\n",
      "epoch:24 step:18984 [D loss: 0.003519, acc.: 100.00%] [G loss: 0.001057]\n",
      "epoch:24 step:18985 [D loss: 0.001434, acc.: 100.00%] [G loss: 0.092957]\n",
      "epoch:24 step:18986 [D loss: 0.001930, acc.: 100.00%] [G loss: 0.000733]\n",
      "epoch:24 step:18987 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.002004]\n",
      "epoch:24 step:18988 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.000478]\n",
      "epoch:24 step:18989 [D loss: 0.013085, acc.: 100.00%] [G loss: 0.008468]\n",
      "epoch:24 step:18990 [D loss: 0.004328, acc.: 100.00%] [G loss: 0.008269]\n",
      "epoch:24 step:18991 [D loss: 0.010336, acc.: 100.00%] [G loss: 0.001493]\n",
      "epoch:24 step:18992 [D loss: 0.001158, acc.: 100.00%] [G loss: 0.021863]\n",
      "epoch:24 step:18993 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.001240]\n",
      "epoch:24 step:18994 [D loss: 0.014087, acc.: 99.22%] [G loss: 0.001980]\n",
      "epoch:24 step:18995 [D loss: 0.006982, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:24 step:18996 [D loss: 0.003076, acc.: 100.00%] [G loss: 0.001412]\n",
      "epoch:24 step:18997 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.001440]\n",
      "epoch:24 step:18998 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.008605]\n",
      "epoch:24 step:18999 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.005865]\n",
      "epoch:24 step:19000 [D loss: 0.024012, acc.: 100.00%] [G loss: 0.005370]\n",
      "epoch:24 step:19001 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.002449]\n",
      "epoch:24 step:19002 [D loss: 0.002037, acc.: 100.00%] [G loss: 0.034636]\n",
      "epoch:24 step:19003 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.009336]\n",
      "epoch:24 step:19004 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.001821]\n",
      "epoch:24 step:19005 [D loss: 0.007434, acc.: 100.00%] [G loss: 0.002906]\n",
      "epoch:24 step:19006 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.079997]\n",
      "epoch:24 step:19007 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.003741]\n",
      "epoch:24 step:19008 [D loss: 0.004746, acc.: 100.00%] [G loss: 0.004021]\n",
      "epoch:24 step:19009 [D loss: 0.008887, acc.: 100.00%] [G loss: 0.012908]\n",
      "epoch:24 step:19010 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000599]\n",
      "epoch:24 step:19011 [D loss: 0.001799, acc.: 100.00%] [G loss: 0.002537]\n",
      "epoch:24 step:19012 [D loss: 0.001384, acc.: 100.00%] [G loss: 0.149649]\n",
      "epoch:24 step:19013 [D loss: 0.005543, acc.: 100.00%] [G loss: 0.001433]\n",
      "epoch:24 step:19014 [D loss: 0.015222, acc.: 100.00%] [G loss: 0.301460]\n",
      "epoch:24 step:19015 [D loss: 0.001610, acc.: 100.00%] [G loss: 0.015622]\n",
      "epoch:24 step:19016 [D loss: 0.001334, acc.: 100.00%] [G loss: 0.017311]\n",
      "epoch:24 step:19017 [D loss: 0.003717, acc.: 100.00%] [G loss: 0.012829]\n",
      "epoch:24 step:19018 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.013147]\n",
      "epoch:24 step:19019 [D loss: 0.015424, acc.: 99.22%] [G loss: 0.004604]\n",
      "epoch:24 step:19020 [D loss: 0.000818, acc.: 100.00%] [G loss: 0.001661]\n",
      "epoch:24 step:19021 [D loss: 0.032200, acc.: 98.44%] [G loss: 0.002887]\n",
      "epoch:24 step:19022 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:24 step:19023 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.001988]\n",
      "epoch:24 step:19024 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:24 step:19025 [D loss: 0.004865, acc.: 100.00%] [G loss: 0.020507]\n",
      "epoch:24 step:19026 [D loss: 0.004733, acc.: 100.00%] [G loss: 0.005680]\n",
      "epoch:24 step:19027 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:24 step:19028 [D loss: 0.003994, acc.: 100.00%] [G loss: 0.012958]\n",
      "epoch:24 step:19029 [D loss: 0.002315, acc.: 100.00%] [G loss: 0.020812]\n",
      "epoch:24 step:19030 [D loss: 0.004600, acc.: 100.00%] [G loss: 0.003937]\n",
      "epoch:24 step:19031 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.014484]\n",
      "epoch:24 step:19032 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.001302]\n",
      "epoch:24 step:19033 [D loss: 0.002084, acc.: 100.00%] [G loss: 0.077977]\n",
      "epoch:24 step:19034 [D loss: 0.009116, acc.: 100.00%] [G loss: 0.007005]\n",
      "epoch:24 step:19035 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.004594]\n",
      "epoch:24 step:19036 [D loss: 0.004527, acc.: 100.00%] [G loss: 0.006570]\n",
      "epoch:24 step:19037 [D loss: 0.002110, acc.: 100.00%] [G loss: 0.001630]\n",
      "epoch:24 step:19038 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.002244]\n",
      "epoch:24 step:19039 [D loss: 0.030081, acc.: 99.22%] [G loss: 0.191578]\n",
      "epoch:24 step:19040 [D loss: 0.035345, acc.: 100.00%] [G loss: 1.467582]\n",
      "epoch:24 step:19041 [D loss: 0.023852, acc.: 99.22%] [G loss: 0.162916]\n",
      "epoch:24 step:19042 [D loss: 0.004110, acc.: 100.00%] [G loss: 0.061277]\n",
      "epoch:24 step:19043 [D loss: 0.009243, acc.: 100.00%] [G loss: 0.623209]\n",
      "epoch:24 step:19044 [D loss: 0.362005, acc.: 85.94%] [G loss: 4.976893]\n",
      "epoch:24 step:19045 [D loss: 2.056772, acc.: 46.88%] [G loss: 0.338683]\n",
      "epoch:24 step:19046 [D loss: 0.041459, acc.: 99.22%] [G loss: 1.074278]\n",
      "epoch:24 step:19047 [D loss: 0.022922, acc.: 99.22%] [G loss: 1.209507]\n",
      "epoch:24 step:19048 [D loss: 0.025637, acc.: 99.22%] [G loss: 0.214911]\n",
      "epoch:24 step:19049 [D loss: 0.038617, acc.: 100.00%] [G loss: 0.656966]\n",
      "epoch:24 step:19050 [D loss: 0.035264, acc.: 99.22%] [G loss: 0.761232]\n",
      "epoch:24 step:19051 [D loss: 0.008512, acc.: 100.00%] [G loss: 1.392801]\n",
      "epoch:24 step:19052 [D loss: 0.078477, acc.: 99.22%] [G loss: 0.350691]\n",
      "epoch:24 step:19053 [D loss: 0.082974, acc.: 96.88%] [G loss: 0.716680]\n",
      "epoch:24 step:19054 [D loss: 0.014625, acc.: 100.00%] [G loss: 1.755443]\n",
      "epoch:24 step:19055 [D loss: 0.053441, acc.: 98.44%] [G loss: 0.613861]\n",
      "epoch:24 step:19056 [D loss: 0.045458, acc.: 97.66%] [G loss: 1.032393]\n",
      "epoch:24 step:19057 [D loss: 0.021264, acc.: 100.00%] [G loss: 0.538354]\n",
      "epoch:24 step:19058 [D loss: 0.058227, acc.: 98.44%] [G loss: 0.442295]\n",
      "epoch:24 step:19059 [D loss: 0.548454, acc.: 74.22%] [G loss: 4.050799]\n",
      "epoch:24 step:19060 [D loss: 0.069908, acc.: 97.66%] [G loss: 3.845987]\n",
      "epoch:24 step:19061 [D loss: 0.074658, acc.: 96.88%] [G loss: 0.544765]\n",
      "epoch:24 step:19062 [D loss: 0.020181, acc.: 100.00%] [G loss: 1.456667]\n",
      "epoch:24 step:19063 [D loss: 0.015778, acc.: 100.00%] [G loss: 0.942698]\n",
      "epoch:24 step:19064 [D loss: 0.073434, acc.: 97.66%] [G loss: 0.098495]\n",
      "epoch:24 step:19065 [D loss: 0.013992, acc.: 100.00%] [G loss: 0.043302]\n",
      "epoch:24 step:19066 [D loss: 0.002948, acc.: 100.00%] [G loss: 0.030386]\n",
      "epoch:24 step:19067 [D loss: 0.016746, acc.: 100.00%] [G loss: 0.073343]\n",
      "epoch:24 step:19068 [D loss: 0.008157, acc.: 100.00%] [G loss: 0.043231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19069 [D loss: 0.005180, acc.: 100.00%] [G loss: 2.446620]\n",
      "epoch:24 step:19070 [D loss: 0.061501, acc.: 98.44%] [G loss: 0.545120]\n",
      "epoch:24 step:19071 [D loss: 0.018422, acc.: 99.22%] [G loss: 0.516260]\n",
      "epoch:24 step:19072 [D loss: 0.010473, acc.: 100.00%] [G loss: 0.417648]\n",
      "epoch:24 step:19073 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.786469]\n",
      "epoch:24 step:19074 [D loss: 0.033867, acc.: 99.22%] [G loss: 0.083037]\n",
      "epoch:24 step:19075 [D loss: 0.014339, acc.: 99.22%] [G loss: 0.029685]\n",
      "epoch:24 step:19076 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.028706]\n",
      "epoch:24 step:19077 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.031550]\n",
      "epoch:24 step:19078 [D loss: 0.001244, acc.: 100.00%] [G loss: 0.025861]\n",
      "epoch:24 step:19079 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.001058]\n",
      "epoch:24 step:19080 [D loss: 0.002433, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:24 step:19081 [D loss: 0.002846, acc.: 100.00%] [G loss: 0.062977]\n",
      "epoch:24 step:19082 [D loss: 0.009781, acc.: 100.00%] [G loss: 0.001055]\n",
      "epoch:24 step:19083 [D loss: 0.001229, acc.: 100.00%] [G loss: 0.012503]\n",
      "epoch:24 step:19084 [D loss: 0.002281, acc.: 100.00%] [G loss: 0.020456]\n",
      "epoch:24 step:19085 [D loss: 0.002028, acc.: 100.00%] [G loss: 0.010262]\n",
      "epoch:24 step:19086 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.016474]\n",
      "epoch:24 step:19087 [D loss: 0.003460, acc.: 100.00%] [G loss: 0.011004]\n",
      "epoch:24 step:19088 [D loss: 0.000873, acc.: 100.00%] [G loss: 0.001275]\n",
      "epoch:24 step:19089 [D loss: 0.002642, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:24 step:19090 [D loss: 0.019447, acc.: 100.00%] [G loss: 0.003378]\n",
      "epoch:24 step:19091 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.035853]\n",
      "epoch:24 step:19092 [D loss: 0.001780, acc.: 100.00%] [G loss: 0.028988]\n",
      "epoch:24 step:19093 [D loss: 0.010444, acc.: 100.00%] [G loss: 0.065159]\n",
      "epoch:24 step:19094 [D loss: 0.178922, acc.: 91.41%] [G loss: 1.901509]\n",
      "epoch:24 step:19095 [D loss: 0.288734, acc.: 87.50%] [G loss: 0.345731]\n",
      "epoch:24 step:19096 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.017906]\n",
      "epoch:24 step:19097 [D loss: 0.005350, acc.: 100.00%] [G loss: 0.037250]\n",
      "epoch:24 step:19098 [D loss: 0.000378, acc.: 100.00%] [G loss: 1.952427]\n",
      "epoch:24 step:19099 [D loss: 0.004314, acc.: 100.00%] [G loss: 0.060278]\n",
      "epoch:24 step:19100 [D loss: 0.004976, acc.: 100.00%] [G loss: 0.000304]\n",
      "epoch:24 step:19101 [D loss: 0.013766, acc.: 100.00%] [G loss: 0.021027]\n",
      "epoch:24 step:19102 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.033140]\n",
      "epoch:24 step:19103 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.008059]\n",
      "epoch:24 step:19104 [D loss: 0.011160, acc.: 99.22%] [G loss: 1.409363]\n",
      "epoch:24 step:19105 [D loss: 0.003346, acc.: 100.00%] [G loss: 0.012473]\n",
      "epoch:24 step:19106 [D loss: 0.003623, acc.: 100.00%] [G loss: 0.088286]\n",
      "epoch:24 step:19107 [D loss: 0.002397, acc.: 100.00%] [G loss: 0.015432]\n",
      "epoch:24 step:19108 [D loss: 0.000692, acc.: 100.00%] [G loss: 0.027314]\n",
      "epoch:24 step:19109 [D loss: 0.001209, acc.: 100.00%] [G loss: 0.044335]\n",
      "epoch:24 step:19110 [D loss: 0.002714, acc.: 100.00%] [G loss: 0.126390]\n",
      "epoch:24 step:19111 [D loss: 0.001083, acc.: 100.00%] [G loss: 0.038588]\n",
      "epoch:24 step:19112 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.010622]\n",
      "epoch:24 step:19113 [D loss: 0.001300, acc.: 100.00%] [G loss: 0.027322]\n",
      "epoch:24 step:19114 [D loss: 0.005091, acc.: 100.00%] [G loss: 0.001846]\n",
      "epoch:24 step:19115 [D loss: 0.001975, acc.: 100.00%] [G loss: 0.010168]\n",
      "epoch:24 step:19116 [D loss: 0.002767, acc.: 100.00%] [G loss: 0.414496]\n",
      "epoch:24 step:19117 [D loss: 0.005013, acc.: 100.00%] [G loss: 0.004696]\n",
      "epoch:24 step:19118 [D loss: 0.002513, acc.: 100.00%] [G loss: 0.015669]\n",
      "epoch:24 step:19119 [D loss: 0.005440, acc.: 100.00%] [G loss: 0.588568]\n",
      "epoch:24 step:19120 [D loss: 0.023483, acc.: 100.00%] [G loss: 0.005113]\n",
      "epoch:24 step:19121 [D loss: 0.001476, acc.: 100.00%] [G loss: 1.209685]\n",
      "epoch:24 step:19122 [D loss: 0.012518, acc.: 100.00%] [G loss: 0.053185]\n",
      "epoch:24 step:19123 [D loss: 0.013551, acc.: 100.00%] [G loss: 0.112106]\n",
      "epoch:24 step:19124 [D loss: 0.005764, acc.: 100.00%] [G loss: 0.045082]\n",
      "epoch:24 step:19125 [D loss: 0.049604, acc.: 99.22%] [G loss: 0.526847]\n",
      "epoch:24 step:19126 [D loss: 0.033244, acc.: 99.22%] [G loss: 0.463671]\n",
      "epoch:24 step:19127 [D loss: 0.009717, acc.: 100.00%] [G loss: 0.162491]\n",
      "epoch:24 step:19128 [D loss: 0.040335, acc.: 98.44%] [G loss: 0.409822]\n",
      "epoch:24 step:19129 [D loss: 0.027961, acc.: 99.22%] [G loss: 2.048933]\n",
      "epoch:24 step:19130 [D loss: 0.540900, acc.: 79.69%] [G loss: 6.840360]\n",
      "epoch:24 step:19131 [D loss: 0.812357, acc.: 64.06%] [G loss: 3.449831]\n",
      "epoch:24 step:19132 [D loss: 0.441247, acc.: 76.56%] [G loss: 6.854828]\n",
      "epoch:24 step:19133 [D loss: 0.110970, acc.: 93.75%] [G loss: 3.825733]\n",
      "epoch:24 step:19134 [D loss: 0.352555, acc.: 84.38%] [G loss: 5.177391]\n",
      "epoch:24 step:19135 [D loss: 0.038268, acc.: 99.22%] [G loss: 0.119167]\n",
      "epoch:24 step:19136 [D loss: 0.051434, acc.: 99.22%] [G loss: 5.513940]\n",
      "epoch:24 step:19137 [D loss: 0.039470, acc.: 98.44%] [G loss: 5.853209]\n",
      "epoch:24 step:19138 [D loss: 0.004995, acc.: 100.00%] [G loss: 5.863851]\n",
      "epoch:24 step:19139 [D loss: 0.050164, acc.: 98.44%] [G loss: 4.839789]\n",
      "epoch:24 step:19140 [D loss: 0.008791, acc.: 100.00%] [G loss: 4.883305]\n",
      "epoch:24 step:19141 [D loss: 0.013132, acc.: 100.00%] [G loss: 4.078718]\n",
      "epoch:24 step:19142 [D loss: 0.013309, acc.: 100.00%] [G loss: 0.214614]\n",
      "epoch:24 step:19143 [D loss: 0.010855, acc.: 100.00%] [G loss: 4.224068]\n",
      "epoch:24 step:19144 [D loss: 0.007854, acc.: 100.00%] [G loss: 4.010391]\n",
      "epoch:24 step:19145 [D loss: 0.037720, acc.: 98.44%] [G loss: 3.209772]\n",
      "epoch:24 step:19146 [D loss: 0.856338, acc.: 62.50%] [G loss: 7.449006]\n",
      "epoch:24 step:19147 [D loss: 2.191691, acc.: 53.12%] [G loss: 3.904209]\n",
      "epoch:24 step:19148 [D loss: 0.665401, acc.: 75.78%] [G loss: 1.581174]\n",
      "epoch:24 step:19149 [D loss: 0.076780, acc.: 98.44%] [G loss: 0.960954]\n",
      "epoch:24 step:19150 [D loss: 0.066565, acc.: 99.22%] [G loss: 1.598303]\n",
      "epoch:24 step:19151 [D loss: 0.011953, acc.: 100.00%] [G loss: 0.946512]\n",
      "epoch:24 step:19152 [D loss: 0.008853, acc.: 100.00%] [G loss: 1.909431]\n",
      "epoch:24 step:19153 [D loss: 0.005321, acc.: 100.00%] [G loss: 0.494878]\n",
      "epoch:24 step:19154 [D loss: 0.027958, acc.: 99.22%] [G loss: 1.105532]\n",
      "epoch:24 step:19155 [D loss: 0.143442, acc.: 92.19%] [G loss: 0.010559]\n",
      "epoch:24 step:19156 [D loss: 0.194089, acc.: 92.97%] [G loss: 0.664368]\n",
      "epoch:24 step:19157 [D loss: 0.003160, acc.: 100.00%] [G loss: 1.304436]\n",
      "epoch:24 step:19158 [D loss: 0.017256, acc.: 100.00%] [G loss: 0.719897]\n",
      "epoch:24 step:19159 [D loss: 0.010488, acc.: 100.00%] [G loss: 2.436404]\n",
      "epoch:24 step:19160 [D loss: 0.002066, acc.: 100.00%] [G loss: 2.370207]\n",
      "epoch:24 step:19161 [D loss: 0.015455, acc.: 100.00%] [G loss: 0.112546]\n",
      "epoch:24 step:19162 [D loss: 0.009749, acc.: 100.00%] [G loss: 0.159303]\n",
      "epoch:24 step:19163 [D loss: 0.046918, acc.: 98.44%] [G loss: 0.484271]\n",
      "epoch:24 step:19164 [D loss: 0.061932, acc.: 99.22%] [G loss: 0.100834]\n",
      "epoch:24 step:19165 [D loss: 0.015412, acc.: 100.00%] [G loss: 0.067704]\n",
      "epoch:24 step:19166 [D loss: 0.022634, acc.: 100.00%] [G loss: 0.021025]\n",
      "epoch:24 step:19167 [D loss: 0.069088, acc.: 99.22%] [G loss: 0.066419]\n",
      "epoch:24 step:19168 [D loss: 0.021646, acc.: 100.00%] [G loss: 0.099448]\n",
      "epoch:24 step:19169 [D loss: 0.059267, acc.: 98.44%] [G loss: 0.254519]\n",
      "epoch:24 step:19170 [D loss: 0.009094, acc.: 100.00%] [G loss: 0.510436]\n",
      "epoch:24 step:19171 [D loss: 0.020182, acc.: 100.00%] [G loss: 0.197119]\n",
      "epoch:24 step:19172 [D loss: 2.933353, acc.: 29.69%] [G loss: 4.055819]\n",
      "epoch:24 step:19173 [D loss: 1.876155, acc.: 50.00%] [G loss: 3.337402]\n",
      "epoch:24 step:19174 [D loss: 1.176970, acc.: 57.03%] [G loss: 1.467865]\n",
      "epoch:24 step:19175 [D loss: 0.379202, acc.: 82.81%] [G loss: 0.692833]\n",
      "epoch:24 step:19176 [D loss: 0.327930, acc.: 88.28%] [G loss: 1.166368]\n",
      "epoch:24 step:19177 [D loss: 0.172838, acc.: 93.75%] [G loss: 1.234362]\n",
      "epoch:24 step:19178 [D loss: 0.093244, acc.: 99.22%] [G loss: 0.886135]\n",
      "epoch:24 step:19179 [D loss: 0.258739, acc.: 92.19%] [G loss: 0.653717]\n",
      "epoch:24 step:19180 [D loss: 0.158145, acc.: 96.88%] [G loss: 1.090546]\n",
      "epoch:24 step:19181 [D loss: 0.211675, acc.: 93.75%] [G loss: 2.455828]\n",
      "epoch:24 step:19182 [D loss: 0.221271, acc.: 89.06%] [G loss: 0.392274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19183 [D loss: 0.194002, acc.: 95.31%] [G loss: 0.536781]\n",
      "epoch:24 step:19184 [D loss: 0.081439, acc.: 97.66%] [G loss: 0.422726]\n",
      "epoch:24 step:19185 [D loss: 0.140534, acc.: 96.88%] [G loss: 0.280972]\n",
      "epoch:24 step:19186 [D loss: 0.037287, acc.: 99.22%] [G loss: 1.954886]\n",
      "epoch:24 step:19187 [D loss: 0.086450, acc.: 98.44%] [G loss: 0.284066]\n",
      "epoch:24 step:19188 [D loss: 0.070113, acc.: 99.22%] [G loss: 0.304003]\n",
      "epoch:24 step:19189 [D loss: 0.056140, acc.: 98.44%] [G loss: 0.171372]\n",
      "epoch:24 step:19190 [D loss: 0.032846, acc.: 100.00%] [G loss: 0.123393]\n",
      "epoch:24 step:19191 [D loss: 0.065056, acc.: 98.44%] [G loss: 1.234970]\n",
      "epoch:24 step:19192 [D loss: 0.166992, acc.: 93.75%] [G loss: 0.199703]\n",
      "epoch:24 step:19193 [D loss: 0.053182, acc.: 98.44%] [G loss: 1.808905]\n",
      "epoch:24 step:19194 [D loss: 0.174409, acc.: 94.53%] [G loss: 0.235135]\n",
      "epoch:24 step:19195 [D loss: 0.114636, acc.: 96.09%] [G loss: 0.769326]\n",
      "epoch:24 step:19196 [D loss: 0.048143, acc.: 99.22%] [G loss: 0.154713]\n",
      "epoch:24 step:19197 [D loss: 0.024103, acc.: 100.00%] [G loss: 0.035688]\n",
      "epoch:24 step:19198 [D loss: 0.054494, acc.: 99.22%] [G loss: 0.011133]\n",
      "epoch:24 step:19199 [D loss: 0.019720, acc.: 100.00%] [G loss: 0.018757]\n",
      "epoch:24 step:19200 [D loss: 0.019443, acc.: 100.00%] [G loss: 0.042837]\n",
      "epoch:24 step:19201 [D loss: 0.007710, acc.: 100.00%] [G loss: 0.499721]\n",
      "epoch:24 step:19202 [D loss: 0.011266, acc.: 100.00%] [G loss: 0.031645]\n",
      "epoch:24 step:19203 [D loss: 0.014883, acc.: 100.00%] [G loss: 0.384401]\n",
      "epoch:24 step:19204 [D loss: 0.117677, acc.: 96.09%] [G loss: 0.026020]\n",
      "epoch:24 step:19205 [D loss: 0.061571, acc.: 97.66%] [G loss: 0.061317]\n",
      "epoch:24 step:19206 [D loss: 0.036548, acc.: 99.22%] [G loss: 0.010258]\n",
      "epoch:24 step:19207 [D loss: 0.019261, acc.: 100.00%] [G loss: 0.897090]\n",
      "epoch:24 step:19208 [D loss: 0.046087, acc.: 100.00%] [G loss: 2.285920]\n",
      "epoch:24 step:19209 [D loss: 0.027417, acc.: 99.22%] [G loss: 0.023059]\n",
      "epoch:24 step:19210 [D loss: 0.029824, acc.: 99.22%] [G loss: 0.032634]\n",
      "epoch:24 step:19211 [D loss: 0.069787, acc.: 100.00%] [G loss: 0.027238]\n",
      "epoch:24 step:19212 [D loss: 0.050088, acc.: 98.44%] [G loss: 0.067758]\n",
      "epoch:24 step:19213 [D loss: 0.014197, acc.: 100.00%] [G loss: 0.020329]\n",
      "epoch:24 step:19214 [D loss: 0.010806, acc.: 100.00%] [G loss: 0.037060]\n",
      "epoch:24 step:19215 [D loss: 0.013374, acc.: 100.00%] [G loss: 0.714222]\n",
      "epoch:24 step:19216 [D loss: 0.014837, acc.: 100.00%] [G loss: 0.320453]\n",
      "epoch:24 step:19217 [D loss: 0.027365, acc.: 99.22%] [G loss: 0.008163]\n",
      "epoch:24 step:19218 [D loss: 0.032742, acc.: 100.00%] [G loss: 0.290806]\n",
      "epoch:24 step:19219 [D loss: 0.122744, acc.: 95.31%] [G loss: 0.045988]\n",
      "epoch:24 step:19220 [D loss: 0.309668, acc.: 84.38%] [G loss: 3.430877]\n",
      "epoch:24 step:19221 [D loss: 1.334395, acc.: 53.12%] [G loss: 1.387020]\n",
      "epoch:24 step:19222 [D loss: 0.076687, acc.: 97.66%] [G loss: 2.450374]\n",
      "epoch:24 step:19223 [D loss: 0.060313, acc.: 99.22%] [G loss: 0.041805]\n",
      "epoch:24 step:19224 [D loss: 0.012087, acc.: 100.00%] [G loss: 1.396099]\n",
      "epoch:24 step:19225 [D loss: 0.046888, acc.: 100.00%] [G loss: 0.049370]\n",
      "epoch:24 step:19226 [D loss: 0.282399, acc.: 85.94%] [G loss: 0.366064]\n",
      "epoch:24 step:19227 [D loss: 0.084831, acc.: 98.44%] [G loss: 0.498862]\n",
      "epoch:24 step:19228 [D loss: 0.137613, acc.: 95.31%] [G loss: 0.092189]\n",
      "epoch:24 step:19229 [D loss: 0.059587, acc.: 98.44%] [G loss: 0.036198]\n",
      "epoch:24 step:19230 [D loss: 0.088094, acc.: 98.44%] [G loss: 0.315478]\n",
      "epoch:24 step:19231 [D loss: 0.069562, acc.: 97.66%] [G loss: 0.168480]\n",
      "epoch:24 step:19232 [D loss: 0.059068, acc.: 99.22%] [G loss: 3.753393]\n",
      "epoch:24 step:19233 [D loss: 0.043643, acc.: 100.00%] [G loss: 0.450098]\n",
      "epoch:24 step:19234 [D loss: 0.059155, acc.: 98.44%] [G loss: 3.589195]\n",
      "epoch:24 step:19235 [D loss: 0.218439, acc.: 88.28%] [G loss: 1.146980]\n",
      "epoch:24 step:19236 [D loss: 0.035021, acc.: 99.22%] [G loss: 0.056086]\n",
      "epoch:24 step:19237 [D loss: 0.019669, acc.: 99.22%] [G loss: 0.581203]\n",
      "epoch:24 step:19238 [D loss: 0.078110, acc.: 98.44%] [G loss: 0.142326]\n",
      "epoch:24 step:19239 [D loss: 0.019327, acc.: 99.22%] [G loss: 0.363901]\n",
      "epoch:24 step:19240 [D loss: 0.023241, acc.: 100.00%] [G loss: 0.144188]\n",
      "epoch:24 step:19241 [D loss: 0.077421, acc.: 96.88%] [G loss: 0.259661]\n",
      "epoch:24 step:19242 [D loss: 0.030294, acc.: 99.22%] [G loss: 0.020295]\n",
      "epoch:24 step:19243 [D loss: 0.010168, acc.: 100.00%] [G loss: 0.015029]\n",
      "epoch:24 step:19244 [D loss: 0.019722, acc.: 100.00%] [G loss: 0.022756]\n",
      "epoch:24 step:19245 [D loss: 0.017573, acc.: 100.00%] [G loss: 0.020727]\n",
      "epoch:24 step:19246 [D loss: 0.016230, acc.: 100.00%] [G loss: 0.364780]\n",
      "epoch:24 step:19247 [D loss: 0.003034, acc.: 100.00%] [G loss: 0.009484]\n",
      "epoch:24 step:19248 [D loss: 0.008987, acc.: 100.00%] [G loss: 0.352942]\n",
      "epoch:24 step:19249 [D loss: 0.012215, acc.: 100.00%] [G loss: 0.040371]\n",
      "epoch:24 step:19250 [D loss: 0.011118, acc.: 100.00%] [G loss: 0.024888]\n",
      "epoch:24 step:19251 [D loss: 0.004625, acc.: 100.00%] [G loss: 0.031056]\n",
      "epoch:24 step:19252 [D loss: 0.006052, acc.: 100.00%] [G loss: 0.006501]\n",
      "epoch:24 step:19253 [D loss: 0.028093, acc.: 100.00%] [G loss: 0.004365]\n",
      "epoch:24 step:19254 [D loss: 0.049389, acc.: 98.44%] [G loss: 0.008070]\n",
      "epoch:24 step:19255 [D loss: 0.041074, acc.: 99.22%] [G loss: 0.030917]\n",
      "epoch:24 step:19256 [D loss: 0.016902, acc.: 100.00%] [G loss: 0.006731]\n",
      "epoch:24 step:19257 [D loss: 0.025589, acc.: 99.22%] [G loss: 0.023236]\n",
      "epoch:24 step:19258 [D loss: 0.023480, acc.: 100.00%] [G loss: 0.138210]\n",
      "epoch:24 step:19259 [D loss: 0.016581, acc.: 100.00%] [G loss: 0.075241]\n",
      "epoch:24 step:19260 [D loss: 0.048801, acc.: 100.00%] [G loss: 0.571675]\n",
      "epoch:24 step:19261 [D loss: 0.093980, acc.: 96.88%] [G loss: 0.477324]\n",
      "epoch:24 step:19262 [D loss: 0.030540, acc.: 99.22%] [G loss: 1.305854]\n",
      "epoch:24 step:19263 [D loss: 0.013521, acc.: 100.00%] [G loss: 2.232700]\n",
      "epoch:24 step:19264 [D loss: 0.043632, acc.: 98.44%] [G loss: 0.700173]\n",
      "epoch:24 step:19265 [D loss: 0.007441, acc.: 100.00%] [G loss: 0.740211]\n",
      "epoch:24 step:19266 [D loss: 0.035395, acc.: 99.22%] [G loss: 0.794125]\n",
      "epoch:24 step:19267 [D loss: 0.024714, acc.: 99.22%] [G loss: 0.981993]\n",
      "epoch:24 step:19268 [D loss: 0.108643, acc.: 98.44%] [G loss: 0.901206]\n",
      "epoch:24 step:19269 [D loss: 0.002244, acc.: 100.00%] [G loss: 1.056739]\n",
      "epoch:24 step:19270 [D loss: 0.197592, acc.: 92.19%] [G loss: 2.080609]\n",
      "epoch:24 step:19271 [D loss: 0.019488, acc.: 100.00%] [G loss: 3.522026]\n",
      "epoch:24 step:19272 [D loss: 0.021589, acc.: 100.00%] [G loss: 3.563406]\n",
      "epoch:24 step:19273 [D loss: 0.037942, acc.: 98.44%] [G loss: 4.942773]\n",
      "epoch:24 step:19274 [D loss: 0.016417, acc.: 100.00%] [G loss: 4.665555]\n",
      "epoch:24 step:19275 [D loss: 0.014673, acc.: 100.00%] [G loss: 4.841390]\n",
      "epoch:24 step:19276 [D loss: 0.878722, acc.: 65.62%] [G loss: 8.990974]\n",
      "epoch:24 step:19277 [D loss: 0.756145, acc.: 72.66%] [G loss: 6.811565]\n",
      "epoch:24 step:19278 [D loss: 0.017584, acc.: 100.00%] [G loss: 6.624719]\n",
      "epoch:24 step:19279 [D loss: 0.015994, acc.: 100.00%] [G loss: 6.035256]\n",
      "epoch:24 step:19280 [D loss: 0.027565, acc.: 98.44%] [G loss: 0.070838]\n",
      "epoch:24 step:19281 [D loss: 0.179745, acc.: 92.19%] [G loss: 6.905563]\n",
      "epoch:24 step:19282 [D loss: 0.050991, acc.: 98.44%] [G loss: 7.275507]\n",
      "epoch:24 step:19283 [D loss: 0.011314, acc.: 100.00%] [G loss: 0.307803]\n",
      "epoch:24 step:19284 [D loss: 0.020205, acc.: 100.00%] [G loss: 6.840983]\n",
      "epoch:24 step:19285 [D loss: 0.025623, acc.: 99.22%] [G loss: 6.678831]\n",
      "epoch:24 step:19286 [D loss: 0.013057, acc.: 100.00%] [G loss: 6.694681]\n",
      "epoch:24 step:19287 [D loss: 0.007296, acc.: 100.00%] [G loss: 0.130616]\n",
      "epoch:24 step:19288 [D loss: 0.017020, acc.: 100.00%] [G loss: 6.136496]\n",
      "epoch:24 step:19289 [D loss: 0.026282, acc.: 100.00%] [G loss: 6.655023]\n",
      "epoch:24 step:19290 [D loss: 0.008645, acc.: 100.00%] [G loss: 6.354148]\n",
      "epoch:24 step:19291 [D loss: 0.030026, acc.: 99.22%] [G loss: 6.097999]\n",
      "epoch:24 step:19292 [D loss: 0.013243, acc.: 100.00%] [G loss: 5.788521]\n",
      "epoch:24 step:19293 [D loss: 0.005841, acc.: 100.00%] [G loss: 5.958409]\n",
      "epoch:24 step:19294 [D loss: 0.041099, acc.: 100.00%] [G loss: 5.305257]\n",
      "epoch:24 step:19295 [D loss: 0.008635, acc.: 100.00%] [G loss: 5.281866]\n",
      "epoch:24 step:19296 [D loss: 0.003642, acc.: 100.00%] [G loss: 0.274329]\n",
      "epoch:24 step:19297 [D loss: 0.005460, acc.: 100.00%] [G loss: 5.153743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19298 [D loss: 0.011282, acc.: 100.00%] [G loss: 5.269234]\n",
      "epoch:24 step:19299 [D loss: 0.009259, acc.: 100.00%] [G loss: 5.063288]\n",
      "epoch:24 step:19300 [D loss: 0.005520, acc.: 100.00%] [G loss: 0.039729]\n",
      "epoch:24 step:19301 [D loss: 0.148111, acc.: 92.97%] [G loss: 6.224038]\n",
      "epoch:24 step:19302 [D loss: 0.146829, acc.: 90.62%] [G loss: 0.053530]\n",
      "epoch:24 step:19303 [D loss: 0.183153, acc.: 94.53%] [G loss: 6.834645]\n",
      "epoch:24 step:19304 [D loss: 0.040522, acc.: 98.44%] [G loss: 7.512527]\n",
      "epoch:24 step:19305 [D loss: 0.116150, acc.: 93.75%] [G loss: 6.317181]\n",
      "epoch:24 step:19306 [D loss: 0.020706, acc.: 100.00%] [G loss: 0.486121]\n",
      "epoch:24 step:19307 [D loss: 0.010375, acc.: 100.00%] [G loss: 5.315687]\n",
      "epoch:24 step:19308 [D loss: 0.011215, acc.: 100.00%] [G loss: 0.112134]\n",
      "epoch:24 step:19309 [D loss: 0.085421, acc.: 96.88%] [G loss: 6.064723]\n",
      "epoch:24 step:19310 [D loss: 0.029101, acc.: 98.44%] [G loss: 1.837212]\n",
      "epoch:24 step:19311 [D loss: 0.025939, acc.: 99.22%] [G loss: 6.600094]\n",
      "epoch:24 step:19312 [D loss: 0.040397, acc.: 99.22%] [G loss: 6.582541]\n",
      "epoch:24 step:19313 [D loss: 0.006331, acc.: 100.00%] [G loss: 6.243588]\n",
      "epoch:24 step:19314 [D loss: 0.004989, acc.: 100.00%] [G loss: 7.190728]\n",
      "epoch:24 step:19315 [D loss: 0.006030, acc.: 100.00%] [G loss: 5.954591]\n",
      "epoch:24 step:19316 [D loss: 0.014265, acc.: 100.00%] [G loss: 5.336665]\n",
      "epoch:24 step:19317 [D loss: 0.012548, acc.: 100.00%] [G loss: 4.801233]\n",
      "epoch:24 step:19318 [D loss: 0.014488, acc.: 100.00%] [G loss: 4.275271]\n",
      "epoch:24 step:19319 [D loss: 1.382391, acc.: 47.66%] [G loss: 8.745216]\n",
      "epoch:24 step:19320 [D loss: 0.142260, acc.: 93.75%] [G loss: 7.923709]\n",
      "epoch:24 step:19321 [D loss: 1.764549, acc.: 51.56%] [G loss: 4.308252]\n",
      "epoch:24 step:19322 [D loss: 0.003326, acc.: 100.00%] [G loss: 2.872769]\n",
      "epoch:24 step:19323 [D loss: 0.005168, acc.: 100.00%] [G loss: 2.245171]\n",
      "epoch:24 step:19324 [D loss: 0.006118, acc.: 100.00%] [G loss: 0.005815]\n",
      "epoch:24 step:19325 [D loss: 0.051345, acc.: 100.00%] [G loss: 2.277811]\n",
      "epoch:24 step:19326 [D loss: 0.013464, acc.: 100.00%] [G loss: 2.173914]\n",
      "epoch:24 step:19327 [D loss: 0.041303, acc.: 98.44%] [G loss: 1.907208]\n",
      "epoch:24 step:19328 [D loss: 0.007137, acc.: 100.00%] [G loss: 2.267662]\n",
      "epoch:24 step:19329 [D loss: 0.034110, acc.: 99.22%] [G loss: 1.502422]\n",
      "epoch:24 step:19330 [D loss: 0.018296, acc.: 100.00%] [G loss: 1.362158]\n",
      "epoch:24 step:19331 [D loss: 0.045435, acc.: 98.44%] [G loss: 0.357182]\n",
      "epoch:24 step:19332 [D loss: 0.007497, acc.: 100.00%] [G loss: 1.147996]\n",
      "epoch:24 step:19333 [D loss: 0.049208, acc.: 99.22%] [G loss: 1.094606]\n",
      "epoch:24 step:19334 [D loss: 0.048746, acc.: 98.44%] [G loss: 2.150918]\n",
      "epoch:24 step:19335 [D loss: 0.070999, acc.: 98.44%] [G loss: 2.133922]\n",
      "epoch:24 step:19336 [D loss: 0.176839, acc.: 93.75%] [G loss: 2.048329]\n",
      "epoch:24 step:19337 [D loss: 0.221608, acc.: 89.84%] [G loss: 1.154982]\n",
      "epoch:24 step:19338 [D loss: 0.025943, acc.: 100.00%] [G loss: 0.711992]\n",
      "epoch:24 step:19339 [D loss: 0.034393, acc.: 98.44%] [G loss: 0.351382]\n",
      "epoch:24 step:19340 [D loss: 0.181715, acc.: 93.75%] [G loss: 0.563509]\n",
      "epoch:24 step:19341 [D loss: 0.009112, acc.: 100.00%] [G loss: 2.684787]\n",
      "epoch:24 step:19342 [D loss: 0.008328, acc.: 100.00%] [G loss: 0.796605]\n",
      "epoch:24 step:19343 [D loss: 0.014704, acc.: 100.00%] [G loss: 2.192266]\n",
      "epoch:24 step:19344 [D loss: 0.063307, acc.: 96.88%] [G loss: 0.262321]\n",
      "epoch:24 step:19345 [D loss: 0.016848, acc.: 100.00%] [G loss: 0.116835]\n",
      "epoch:24 step:19346 [D loss: 0.045359, acc.: 99.22%] [G loss: 0.147396]\n",
      "epoch:24 step:19347 [D loss: 0.051246, acc.: 98.44%] [G loss: 0.807645]\n",
      "epoch:24 step:19348 [D loss: 0.103830, acc.: 96.88%] [G loss: 0.001806]\n",
      "epoch:24 step:19349 [D loss: 0.029949, acc.: 99.22%] [G loss: 0.000343]\n",
      "epoch:24 step:19350 [D loss: 0.018656, acc.: 100.00%] [G loss: 0.331428]\n",
      "epoch:24 step:19351 [D loss: 0.003003, acc.: 100.00%] [G loss: 0.173933]\n",
      "epoch:24 step:19352 [D loss: 0.001833, acc.: 100.00%] [G loss: 0.056182]\n",
      "epoch:24 step:19353 [D loss: 0.006645, acc.: 100.00%] [G loss: 0.044524]\n",
      "epoch:24 step:19354 [D loss: 0.001056, acc.: 100.00%] [G loss: 0.051399]\n",
      "epoch:24 step:19355 [D loss: 0.001828, acc.: 100.00%] [G loss: 0.676749]\n",
      "epoch:24 step:19356 [D loss: 0.008353, acc.: 100.00%] [G loss: 0.036834]\n",
      "epoch:24 step:19357 [D loss: 0.048142, acc.: 99.22%] [G loss: 0.015505]\n",
      "epoch:24 step:19358 [D loss: 0.033295, acc.: 99.22%] [G loss: 0.007160]\n",
      "epoch:24 step:19359 [D loss: 0.002963, acc.: 100.00%] [G loss: 0.004547]\n",
      "epoch:24 step:19360 [D loss: 0.008382, acc.: 100.00%] [G loss: 0.040795]\n",
      "epoch:24 step:19361 [D loss: 0.002436, acc.: 100.00%] [G loss: 0.016504]\n",
      "epoch:24 step:19362 [D loss: 0.004030, acc.: 100.00%] [G loss: 0.092019]\n",
      "epoch:24 step:19363 [D loss: 0.001907, acc.: 100.00%] [G loss: 0.003794]\n",
      "epoch:24 step:19364 [D loss: 0.005347, acc.: 100.00%] [G loss: 0.000519]\n",
      "epoch:24 step:19365 [D loss: 0.005334, acc.: 100.00%] [G loss: 0.083101]\n",
      "epoch:24 step:19366 [D loss: 0.003366, acc.: 100.00%] [G loss: 0.020919]\n",
      "epoch:24 step:19367 [D loss: 0.276610, acc.: 87.50%] [G loss: 2.252611]\n",
      "epoch:24 step:19368 [D loss: 0.173874, acc.: 92.97%] [G loss: 1.283756]\n",
      "epoch:24 step:19369 [D loss: 0.118035, acc.: 93.75%] [G loss: 0.067656]\n",
      "epoch:24 step:19370 [D loss: 0.000789, acc.: 100.00%] [G loss: 1.972135]\n",
      "epoch:24 step:19371 [D loss: 0.004545, acc.: 100.00%] [G loss: 0.028862]\n",
      "epoch:24 step:19372 [D loss: 0.001611, acc.: 100.00%] [G loss: 0.094261]\n",
      "epoch:24 step:19373 [D loss: 0.007329, acc.: 100.00%] [G loss: 0.036238]\n",
      "epoch:24 step:19374 [D loss: 0.001977, acc.: 100.00%] [G loss: 0.055449]\n",
      "epoch:24 step:19375 [D loss: 0.004883, acc.: 100.00%] [G loss: 0.010642]\n",
      "epoch:24 step:19376 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.005090]\n",
      "epoch:24 step:19377 [D loss: 0.002901, acc.: 100.00%] [G loss: 0.012981]\n",
      "epoch:24 step:19378 [D loss: 0.009125, acc.: 99.22%] [G loss: 0.006353]\n",
      "epoch:24 step:19379 [D loss: 0.001835, acc.: 100.00%] [G loss: 0.033914]\n",
      "epoch:24 step:19380 [D loss: 0.001774, acc.: 100.00%] [G loss: 1.099241]\n",
      "epoch:24 step:19381 [D loss: 0.047908, acc.: 98.44%] [G loss: 0.002105]\n",
      "epoch:24 step:19382 [D loss: 0.021340, acc.: 100.00%] [G loss: 0.372364]\n",
      "epoch:24 step:19383 [D loss: 0.008537, acc.: 100.00%] [G loss: 0.052767]\n",
      "epoch:24 step:19384 [D loss: 0.042998, acc.: 100.00%] [G loss: 0.126219]\n",
      "epoch:24 step:19385 [D loss: 0.001333, acc.: 100.00%] [G loss: 0.478152]\n",
      "epoch:24 step:19386 [D loss: 0.033040, acc.: 99.22%] [G loss: 0.258239]\n",
      "epoch:24 step:19387 [D loss: 0.006318, acc.: 100.00%] [G loss: 0.020542]\n",
      "epoch:24 step:19388 [D loss: 0.015683, acc.: 100.00%] [G loss: 0.026168]\n",
      "epoch:24 step:19389 [D loss: 0.030125, acc.: 100.00%] [G loss: 0.142731]\n",
      "epoch:24 step:19390 [D loss: 0.200182, acc.: 91.41%] [G loss: 0.005952]\n",
      "epoch:24 step:19391 [D loss: 0.003347, acc.: 100.00%] [G loss: 0.002673]\n",
      "epoch:24 step:19392 [D loss: 0.065370, acc.: 96.88%] [G loss: 0.099306]\n",
      "epoch:24 step:19393 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.507403]\n",
      "epoch:24 step:19394 [D loss: 0.010485, acc.: 99.22%] [G loss: 0.274118]\n",
      "epoch:24 step:19395 [D loss: 0.011497, acc.: 99.22%] [G loss: 0.249408]\n",
      "epoch:24 step:19396 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.127991]\n",
      "epoch:24 step:19397 [D loss: 0.001231, acc.: 100.00%] [G loss: 1.070590]\n",
      "epoch:24 step:19398 [D loss: 0.001881, acc.: 100.00%] [G loss: 0.043066]\n",
      "epoch:24 step:19399 [D loss: 0.010846, acc.: 100.00%] [G loss: 0.133821]\n",
      "epoch:24 step:19400 [D loss: 0.003589, acc.: 100.00%] [G loss: 0.018185]\n",
      "epoch:24 step:19401 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.032457]\n",
      "epoch:24 step:19402 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.021523]\n",
      "epoch:24 step:19403 [D loss: 0.014503, acc.: 100.00%] [G loss: 0.065770]\n",
      "epoch:24 step:19404 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.040463]\n",
      "epoch:24 step:19405 [D loss: 0.007255, acc.: 100.00%] [G loss: 0.134252]\n",
      "epoch:24 step:19406 [D loss: 0.003099, acc.: 100.00%] [G loss: 0.033786]\n",
      "epoch:24 step:19407 [D loss: 0.002244, acc.: 100.00%] [G loss: 0.026240]\n",
      "epoch:24 step:19408 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.011901]\n",
      "epoch:24 step:19409 [D loss: 0.022879, acc.: 100.00%] [G loss: 0.017815]\n",
      "epoch:24 step:19410 [D loss: 0.011423, acc.: 100.00%] [G loss: 0.402806]\n",
      "epoch:24 step:19411 [D loss: 0.001011, acc.: 100.00%] [G loss: 0.014740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19412 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.001650]\n",
      "epoch:24 step:19413 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.002580]\n",
      "epoch:24 step:19414 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.005319]\n",
      "epoch:24 step:19415 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.002969]\n",
      "epoch:24 step:19416 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.010102]\n",
      "epoch:24 step:19417 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.001476]\n",
      "epoch:24 step:19418 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.009878]\n",
      "epoch:24 step:19419 [D loss: 0.173712, acc.: 92.19%] [G loss: 2.615840]\n",
      "epoch:24 step:19420 [D loss: 0.084076, acc.: 96.88%] [G loss: 0.284033]\n",
      "epoch:24 step:19421 [D loss: 0.060779, acc.: 96.88%] [G loss: 1.680128]\n",
      "epoch:24 step:19422 [D loss: 0.005011, acc.: 100.00%] [G loss: 0.234913]\n",
      "epoch:24 step:19423 [D loss: 0.104853, acc.: 96.88%] [G loss: 0.816453]\n",
      "epoch:24 step:19424 [D loss: 0.042915, acc.: 98.44%] [G loss: 5.396194]\n",
      "epoch:24 step:19425 [D loss: 0.041289, acc.: 100.00%] [G loss: 0.738285]\n",
      "epoch:24 step:19426 [D loss: 0.574251, acc.: 74.22%] [G loss: 2.603679]\n",
      "epoch:24 step:19427 [D loss: 1.668304, acc.: 56.25%] [G loss: 1.303518]\n",
      "epoch:24 step:19428 [D loss: 0.321275, acc.: 92.19%] [G loss: 4.059571]\n",
      "epoch:24 step:19429 [D loss: 0.055990, acc.: 98.44%] [G loss: 0.713218]\n",
      "epoch:24 step:19430 [D loss: 0.104721, acc.: 96.09%] [G loss: 3.095915]\n",
      "epoch:24 step:19431 [D loss: 0.127325, acc.: 95.31%] [G loss: 0.190148]\n",
      "epoch:24 step:19432 [D loss: 0.006437, acc.: 100.00%] [G loss: 0.076964]\n",
      "epoch:24 step:19433 [D loss: 0.006509, acc.: 100.00%] [G loss: 0.029349]\n",
      "epoch:24 step:19434 [D loss: 0.014012, acc.: 99.22%] [G loss: 0.039142]\n",
      "epoch:24 step:19435 [D loss: 0.008070, acc.: 100.00%] [G loss: 0.019440]\n",
      "epoch:24 step:19436 [D loss: 0.005604, acc.: 100.00%] [G loss: 0.006365]\n",
      "epoch:24 step:19437 [D loss: 0.000630, acc.: 100.00%] [G loss: 1.834489]\n",
      "epoch:24 step:19438 [D loss: 0.002966, acc.: 100.00%] [G loss: 0.004353]\n",
      "epoch:24 step:19439 [D loss: 0.001692, acc.: 100.00%] [G loss: 0.426675]\n",
      "epoch:24 step:19440 [D loss: 0.021702, acc.: 100.00%] [G loss: 0.843918]\n",
      "epoch:24 step:19441 [D loss: 0.071623, acc.: 98.44%] [G loss: 0.018154]\n",
      "epoch:24 step:19442 [D loss: 0.002521, acc.: 100.00%] [G loss: 1.624044]\n",
      "epoch:24 step:19443 [D loss: 0.280432, acc.: 88.28%] [G loss: 0.254598]\n",
      "epoch:24 step:19444 [D loss: 0.487001, acc.: 79.69%] [G loss: 0.051985]\n",
      "epoch:24 step:19445 [D loss: 0.005906, acc.: 100.00%] [G loss: 0.001535]\n",
      "epoch:24 step:19446 [D loss: 0.001328, acc.: 100.00%] [G loss: 0.001096]\n",
      "epoch:24 step:19447 [D loss: 0.002903, acc.: 100.00%] [G loss: 1.289333]\n",
      "epoch:24 step:19448 [D loss: 0.040514, acc.: 98.44%] [G loss: 0.346385]\n",
      "epoch:24 step:19449 [D loss: 0.004494, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:24 step:19450 [D loss: 0.004407, acc.: 100.00%] [G loss: 0.459531]\n",
      "epoch:24 step:19451 [D loss: 0.002711, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:24 step:19452 [D loss: 0.006905, acc.: 100.00%] [G loss: 0.000550]\n",
      "epoch:24 step:19453 [D loss: 0.006396, acc.: 100.00%] [G loss: 0.085852]\n",
      "epoch:24 step:19454 [D loss: 0.005947, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:24 step:19455 [D loss: 0.003214, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:24 step:19456 [D loss: 0.001316, acc.: 100.00%] [G loss: 0.173635]\n",
      "epoch:24 step:19457 [D loss: 0.042396, acc.: 97.66%] [G loss: 0.000062]\n",
      "epoch:24 step:19458 [D loss: 0.037459, acc.: 99.22%] [G loss: 0.000799]\n",
      "epoch:24 step:19459 [D loss: 0.004793, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:24 step:19460 [D loss: 0.013170, acc.: 100.00%] [G loss: 0.000732]\n",
      "epoch:24 step:19461 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.002413]\n",
      "epoch:24 step:19462 [D loss: 0.003497, acc.: 100.00%] [G loss: 0.157588]\n",
      "epoch:24 step:19463 [D loss: 0.003647, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:24 step:19464 [D loss: 0.006804, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:24 step:19465 [D loss: 0.013861, acc.: 100.00%] [G loss: 0.087803]\n",
      "epoch:24 step:19466 [D loss: 0.010454, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:24 step:19467 [D loss: 0.003400, acc.: 100.00%] [G loss: 0.148322]\n",
      "epoch:24 step:19468 [D loss: 0.004964, acc.: 100.00%] [G loss: 0.002002]\n",
      "epoch:24 step:19469 [D loss: 0.035063, acc.: 98.44%] [G loss: 0.047146]\n",
      "epoch:24 step:19470 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.001655]\n",
      "epoch:24 step:19471 [D loss: 0.001270, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:24 step:19472 [D loss: 0.001285, acc.: 100.00%] [G loss: 0.010236]\n",
      "epoch:24 step:19473 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:24 step:19474 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.000971]\n",
      "epoch:24 step:19475 [D loss: 0.000936, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:24 step:19476 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.001404]\n",
      "epoch:24 step:19477 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:24 step:19478 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:24 step:19479 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.000680]\n",
      "epoch:24 step:19480 [D loss: 0.001657, acc.: 100.00%] [G loss: 0.067767]\n",
      "epoch:24 step:19481 [D loss: 0.000900, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:24 step:19482 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:24 step:19483 [D loss: 0.010919, acc.: 99.22%] [G loss: 0.000118]\n",
      "epoch:24 step:19484 [D loss: 0.004390, acc.: 100.00%] [G loss: 0.001136]\n",
      "epoch:24 step:19485 [D loss: 0.003546, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:24 step:19486 [D loss: 0.011861, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:24 step:19487 [D loss: 0.023110, acc.: 100.00%] [G loss: 0.010736]\n",
      "epoch:24 step:19488 [D loss: 0.013422, acc.: 100.00%] [G loss: 0.019983]\n",
      "epoch:24 step:19489 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.018652]\n",
      "epoch:24 step:19490 [D loss: 0.006632, acc.: 100.00%] [G loss: 0.015543]\n",
      "epoch:24 step:19491 [D loss: 0.014113, acc.: 99.22%] [G loss: 0.003936]\n",
      "epoch:24 step:19492 [D loss: 0.001359, acc.: 100.00%] [G loss: 0.009380]\n",
      "epoch:24 step:19493 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.001694]\n",
      "epoch:24 step:19494 [D loss: 0.001937, acc.: 100.00%] [G loss: 0.007423]\n",
      "epoch:24 step:19495 [D loss: 0.002656, acc.: 100.00%] [G loss: 1.089353]\n",
      "epoch:24 step:19496 [D loss: 0.007163, acc.: 100.00%] [G loss: 0.250351]\n",
      "epoch:24 step:19497 [D loss: 0.336380, acc.: 83.59%] [G loss: 7.080785]\n",
      "epoch:24 step:19498 [D loss: 1.183920, acc.: 60.94%] [G loss: 0.123390]\n",
      "epoch:24 step:19499 [D loss: 0.306580, acc.: 86.72%] [G loss: 1.617439]\n",
      "epoch:24 step:19500 [D loss: 0.023868, acc.: 99.22%] [G loss: 1.902663]\n",
      "epoch:24 step:19501 [D loss: 0.110907, acc.: 95.31%] [G loss: 0.114807]\n",
      "epoch:24 step:19502 [D loss: 0.008026, acc.: 100.00%] [G loss: 0.089669]\n",
      "epoch:24 step:19503 [D loss: 0.013264, acc.: 100.00%] [G loss: 1.701590]\n",
      "epoch:24 step:19504 [D loss: 0.144995, acc.: 91.41%] [G loss: 3.108710]\n",
      "epoch:24 step:19505 [D loss: 0.012404, acc.: 99.22%] [G loss: 1.522894]\n",
      "epoch:24 step:19506 [D loss: 0.150848, acc.: 93.75%] [G loss: 1.092098]\n",
      "epoch:24 step:19507 [D loss: 0.063816, acc.: 99.22%] [G loss: 2.088188]\n",
      "epoch:24 step:19508 [D loss: 0.011474, acc.: 100.00%] [G loss: 2.187162]\n",
      "epoch:24 step:19509 [D loss: 0.008561, acc.: 100.00%] [G loss: 3.699252]\n",
      "epoch:24 step:19510 [D loss: 0.010398, acc.: 100.00%] [G loss: 1.671088]\n",
      "epoch:24 step:19511 [D loss: 0.004700, acc.: 100.00%] [G loss: 1.832004]\n",
      "epoch:24 step:19512 [D loss: 0.015073, acc.: 100.00%] [G loss: 1.274375]\n",
      "epoch:24 step:19513 [D loss: 0.025585, acc.: 100.00%] [G loss: 1.386579]\n",
      "epoch:24 step:19514 [D loss: 0.008410, acc.: 100.00%] [G loss: 2.938143]\n",
      "epoch:24 step:19515 [D loss: 0.052073, acc.: 99.22%] [G loss: 4.193348]\n",
      "epoch:24 step:19516 [D loss: 0.030579, acc.: 100.00%] [G loss: 1.775309]\n",
      "epoch:24 step:19517 [D loss: 0.024787, acc.: 100.00%] [G loss: 2.385495]\n",
      "epoch:24 step:19518 [D loss: 0.014375, acc.: 100.00%] [G loss: 2.836012]\n",
      "epoch:24 step:19519 [D loss: 0.062178, acc.: 98.44%] [G loss: 4.473754]\n",
      "epoch:24 step:19520 [D loss: 0.012544, acc.: 100.00%] [G loss: 3.942224]\n",
      "epoch:24 step:19521 [D loss: 0.049855, acc.: 100.00%] [G loss: 3.219946]\n",
      "epoch:24 step:19522 [D loss: 0.018920, acc.: 100.00%] [G loss: 3.645014]\n",
      "epoch:24 step:19523 [D loss: 0.047946, acc.: 100.00%] [G loss: 3.354847]\n",
      "epoch:24 step:19524 [D loss: 0.009989, acc.: 100.00%] [G loss: 2.902603]\n",
      "epoch:24 step:19525 [D loss: 0.018863, acc.: 100.00%] [G loss: 3.453867]\n",
      "epoch:25 step:19526 [D loss: 0.018486, acc.: 100.00%] [G loss: 1.773582]\n",
      "epoch:25 step:19527 [D loss: 0.006232, acc.: 100.00%] [G loss: 4.416706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19528 [D loss: 0.009786, acc.: 100.00%] [G loss: 4.321964]\n",
      "epoch:25 step:19529 [D loss: 0.008762, acc.: 100.00%] [G loss: 3.934002]\n",
      "epoch:25 step:19530 [D loss: 0.005398, acc.: 100.00%] [G loss: 3.581709]\n",
      "epoch:25 step:19531 [D loss: 0.014758, acc.: 100.00%] [G loss: 3.772341]\n",
      "epoch:25 step:19532 [D loss: 0.017905, acc.: 100.00%] [G loss: 4.093097]\n",
      "epoch:25 step:19533 [D loss: 0.005249, acc.: 100.00%] [G loss: 4.347583]\n",
      "epoch:25 step:19534 [D loss: 0.035536, acc.: 100.00%] [G loss: 0.132244]\n",
      "epoch:25 step:19535 [D loss: 1.017861, acc.: 62.50%] [G loss: 9.531206]\n",
      "epoch:25 step:19536 [D loss: 3.025515, acc.: 50.00%] [G loss: 5.294130]\n",
      "epoch:25 step:19537 [D loss: 1.633410, acc.: 52.34%] [G loss: 1.410309]\n",
      "epoch:25 step:19538 [D loss: 0.070548, acc.: 98.44%] [G loss: 2.012909]\n",
      "epoch:25 step:19539 [D loss: 0.063872, acc.: 97.66%] [G loss: 1.831320]\n",
      "epoch:25 step:19540 [D loss: 0.017950, acc.: 100.00%] [G loss: 1.492453]\n",
      "epoch:25 step:19541 [D loss: 0.008363, acc.: 100.00%] [G loss: 0.969406]\n",
      "epoch:25 step:19542 [D loss: 0.004571, acc.: 100.00%] [G loss: 0.655140]\n",
      "epoch:25 step:19543 [D loss: 0.005117, acc.: 100.00%] [G loss: 0.569830]\n",
      "epoch:25 step:19544 [D loss: 0.005532, acc.: 100.00%] [G loss: 0.357180]\n",
      "epoch:25 step:19545 [D loss: 0.004678, acc.: 100.00%] [G loss: 0.363765]\n",
      "epoch:25 step:19546 [D loss: 0.007555, acc.: 100.00%] [G loss: 0.950496]\n",
      "epoch:25 step:19547 [D loss: 0.024349, acc.: 100.00%] [G loss: 0.304013]\n",
      "epoch:25 step:19548 [D loss: 0.005175, acc.: 100.00%] [G loss: 0.270603]\n",
      "epoch:25 step:19549 [D loss: 0.007049, acc.: 100.00%] [G loss: 1.171273]\n",
      "epoch:25 step:19550 [D loss: 0.095009, acc.: 98.44%] [G loss: 0.123796]\n",
      "epoch:25 step:19551 [D loss: 0.037282, acc.: 100.00%] [G loss: 0.673135]\n",
      "epoch:25 step:19552 [D loss: 0.034366, acc.: 100.00%] [G loss: 0.467739]\n",
      "epoch:25 step:19553 [D loss: 0.023787, acc.: 100.00%] [G loss: 0.226032]\n",
      "epoch:25 step:19554 [D loss: 0.014340, acc.: 100.00%] [G loss: 0.109224]\n",
      "epoch:25 step:19555 [D loss: 0.051192, acc.: 99.22%] [G loss: 0.906218]\n",
      "epoch:25 step:19556 [D loss: 0.151203, acc.: 94.53%] [G loss: 1.418356]\n",
      "epoch:25 step:19557 [D loss: 0.058774, acc.: 99.22%] [G loss: 1.530520]\n",
      "epoch:25 step:19558 [D loss: 0.074983, acc.: 98.44%] [G loss: 0.678975]\n",
      "epoch:25 step:19559 [D loss: 0.028789, acc.: 100.00%] [G loss: 0.086463]\n",
      "epoch:25 step:19560 [D loss: 0.039873, acc.: 99.22%] [G loss: 0.462015]\n",
      "epoch:25 step:19561 [D loss: 0.029679, acc.: 99.22%] [G loss: 0.192866]\n",
      "epoch:25 step:19562 [D loss: 0.014509, acc.: 100.00%] [G loss: 0.157044]\n",
      "epoch:25 step:19563 [D loss: 0.003895, acc.: 100.00%] [G loss: 0.311389]\n",
      "epoch:25 step:19564 [D loss: 0.010101, acc.: 100.00%] [G loss: 0.306674]\n",
      "epoch:25 step:19565 [D loss: 0.005847, acc.: 100.00%] [G loss: 0.099483]\n",
      "epoch:25 step:19566 [D loss: 0.009349, acc.: 100.00%] [G loss: 0.056329]\n",
      "epoch:25 step:19567 [D loss: 0.045601, acc.: 100.00%] [G loss: 0.448175]\n",
      "epoch:25 step:19568 [D loss: 0.031229, acc.: 100.00%] [G loss: 1.874069]\n",
      "epoch:25 step:19569 [D loss: 0.012856, acc.: 100.00%] [G loss: 1.095637]\n",
      "epoch:25 step:19570 [D loss: 0.076511, acc.: 98.44%] [G loss: 1.495974]\n",
      "epoch:25 step:19571 [D loss: 0.024290, acc.: 100.00%] [G loss: 2.453964]\n",
      "epoch:25 step:19572 [D loss: 0.022335, acc.: 100.00%] [G loss: 0.316456]\n",
      "epoch:25 step:19573 [D loss: 0.007074, acc.: 100.00%] [G loss: 3.093677]\n",
      "epoch:25 step:19574 [D loss: 0.021607, acc.: 100.00%] [G loss: 1.449376]\n",
      "epoch:25 step:19575 [D loss: 0.012980, acc.: 100.00%] [G loss: 1.436377]\n",
      "epoch:25 step:19576 [D loss: 0.013218, acc.: 100.00%] [G loss: 4.333591]\n",
      "epoch:25 step:19577 [D loss: 0.020260, acc.: 100.00%] [G loss: 1.852715]\n",
      "epoch:25 step:19578 [D loss: 0.008057, acc.: 100.00%] [G loss: 1.990359]\n",
      "epoch:25 step:19579 [D loss: 0.017733, acc.: 100.00%] [G loss: 1.654820]\n",
      "epoch:25 step:19580 [D loss: 0.010011, acc.: 100.00%] [G loss: 1.969501]\n",
      "epoch:25 step:19581 [D loss: 0.064710, acc.: 100.00%] [G loss: 3.653488]\n",
      "epoch:25 step:19582 [D loss: 0.013904, acc.: 100.00%] [G loss: 0.645078]\n",
      "epoch:25 step:19583 [D loss: 0.002567, acc.: 100.00%] [G loss: 3.460793]\n",
      "epoch:25 step:19584 [D loss: 0.006381, acc.: 100.00%] [G loss: 5.241171]\n",
      "epoch:25 step:19585 [D loss: 0.007408, acc.: 100.00%] [G loss: 3.443499]\n",
      "epoch:25 step:19586 [D loss: 0.006528, acc.: 100.00%] [G loss: 3.457175]\n",
      "epoch:25 step:19587 [D loss: 0.008478, acc.: 100.00%] [G loss: 3.318427]\n",
      "epoch:25 step:19588 [D loss: 0.016811, acc.: 100.00%] [G loss: 0.051848]\n",
      "epoch:25 step:19589 [D loss: 0.012879, acc.: 100.00%] [G loss: 3.030130]\n",
      "epoch:25 step:19590 [D loss: 0.039976, acc.: 99.22%] [G loss: 3.313780]\n",
      "epoch:25 step:19591 [D loss: 0.007006, acc.: 100.00%] [G loss: 3.495431]\n",
      "epoch:25 step:19592 [D loss: 0.006884, acc.: 100.00%] [G loss: 2.909922]\n",
      "epoch:25 step:19593 [D loss: 0.016946, acc.: 100.00%] [G loss: 3.597365]\n",
      "epoch:25 step:19594 [D loss: 0.003494, acc.: 100.00%] [G loss: 3.836139]\n",
      "epoch:25 step:19595 [D loss: 0.004248, acc.: 100.00%] [G loss: 3.783607]\n",
      "epoch:25 step:19596 [D loss: 0.005424, acc.: 100.00%] [G loss: 4.090450]\n",
      "epoch:25 step:19597 [D loss: 0.004610, acc.: 100.00%] [G loss: 0.007614]\n",
      "epoch:25 step:19598 [D loss: 0.133129, acc.: 96.09%] [G loss: 5.836050]\n",
      "epoch:25 step:19599 [D loss: 0.066112, acc.: 97.66%] [G loss: 5.457923]\n",
      "epoch:25 step:19600 [D loss: 0.103829, acc.: 95.31%] [G loss: 0.024099]\n",
      "epoch:25 step:19601 [D loss: 0.022889, acc.: 100.00%] [G loss: 3.150966]\n",
      "epoch:25 step:19602 [D loss: 0.007815, acc.: 100.00%] [G loss: 0.175619]\n",
      "epoch:25 step:19603 [D loss: 0.028801, acc.: 100.00%] [G loss: 2.195257]\n",
      "epoch:25 step:19604 [D loss: 0.003377, acc.: 100.00%] [G loss: 2.653454]\n",
      "epoch:25 step:19605 [D loss: 0.004859, acc.: 100.00%] [G loss: 0.085106]\n",
      "epoch:25 step:19606 [D loss: 0.005217, acc.: 100.00%] [G loss: 0.026722]\n",
      "epoch:25 step:19607 [D loss: 0.039708, acc.: 100.00%] [G loss: 2.862477]\n",
      "epoch:25 step:19608 [D loss: 0.034532, acc.: 98.44%] [G loss: 1.541971]\n",
      "epoch:25 step:19609 [D loss: 0.005066, acc.: 100.00%] [G loss: 1.291088]\n",
      "epoch:25 step:19610 [D loss: 0.009604, acc.: 100.00%] [G loss: 1.081365]\n",
      "epoch:25 step:19611 [D loss: 0.006156, acc.: 100.00%] [G loss: 0.519077]\n",
      "epoch:25 step:19612 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.032316]\n",
      "epoch:25 step:19613 [D loss: 0.006331, acc.: 100.00%] [G loss: 2.856895]\n",
      "epoch:25 step:19614 [D loss: 0.004612, acc.: 100.00%] [G loss: 0.710633]\n",
      "epoch:25 step:19615 [D loss: 0.019332, acc.: 98.44%] [G loss: 0.657086]\n",
      "epoch:25 step:19616 [D loss: 0.004293, acc.: 100.00%] [G loss: 0.534032]\n",
      "epoch:25 step:19617 [D loss: 0.009221, acc.: 100.00%] [G loss: 1.895543]\n",
      "epoch:25 step:19618 [D loss: 0.009413, acc.: 100.00%] [G loss: 0.626618]\n",
      "epoch:25 step:19619 [D loss: 0.003313, acc.: 100.00%] [G loss: 0.587714]\n",
      "epoch:25 step:19620 [D loss: 0.005142, acc.: 100.00%] [G loss: 0.606305]\n",
      "epoch:25 step:19621 [D loss: 0.005940, acc.: 100.00%] [G loss: 4.341158]\n",
      "epoch:25 step:19622 [D loss: 0.014495, acc.: 100.00%] [G loss: 1.906872]\n",
      "epoch:25 step:19623 [D loss: 0.002416, acc.: 100.00%] [G loss: 0.878824]\n",
      "epoch:25 step:19624 [D loss: 0.037202, acc.: 99.22%] [G loss: 0.397340]\n",
      "epoch:25 step:19625 [D loss: 0.011973, acc.: 100.00%] [G loss: 0.389911]\n",
      "epoch:25 step:19626 [D loss: 0.005430, acc.: 100.00%] [G loss: 0.552928]\n",
      "epoch:25 step:19627 [D loss: 0.007175, acc.: 100.00%] [G loss: 0.874178]\n",
      "epoch:25 step:19628 [D loss: 0.011628, acc.: 100.00%] [G loss: 1.018035]\n",
      "epoch:25 step:19629 [D loss: 0.008655, acc.: 100.00%] [G loss: 1.905825]\n",
      "epoch:25 step:19630 [D loss: 0.003514, acc.: 100.00%] [G loss: 1.363727]\n",
      "epoch:25 step:19631 [D loss: 0.004752, acc.: 100.00%] [G loss: 2.473427]\n",
      "epoch:25 step:19632 [D loss: 0.003539, acc.: 100.00%] [G loss: 1.294527]\n",
      "epoch:25 step:19633 [D loss: 0.020813, acc.: 100.00%] [G loss: 2.612248]\n",
      "epoch:25 step:19634 [D loss: 0.001472, acc.: 100.00%] [G loss: 3.033190]\n",
      "epoch:25 step:19635 [D loss: 0.006005, acc.: 100.00%] [G loss: 0.622165]\n",
      "epoch:25 step:19636 [D loss: 0.002024, acc.: 100.00%] [G loss: 4.071518]\n",
      "epoch:25 step:19637 [D loss: 0.004439, acc.: 100.00%] [G loss: 0.108466]\n",
      "epoch:25 step:19638 [D loss: 0.059826, acc.: 98.44%] [G loss: 1.111151]\n",
      "epoch:25 step:19639 [D loss: 0.091072, acc.: 96.88%] [G loss: 1.423643]\n",
      "epoch:25 step:19640 [D loss: 0.156905, acc.: 93.75%] [G loss: 7.521965]\n",
      "epoch:25 step:19641 [D loss: 0.074585, acc.: 98.44%] [G loss: 4.113130]\n",
      "epoch:25 step:19642 [D loss: 0.003983, acc.: 100.00%] [G loss: 3.045473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19643 [D loss: 0.003876, acc.: 100.00%] [G loss: 2.033093]\n",
      "epoch:25 step:19644 [D loss: 0.004487, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:25 step:19645 [D loss: 0.016526, acc.: 100.00%] [G loss: 3.081019]\n",
      "epoch:25 step:19646 [D loss: 0.000946, acc.: 100.00%] [G loss: 3.713007]\n",
      "epoch:25 step:19647 [D loss: 0.000796, acc.: 100.00%] [G loss: 3.256718]\n",
      "epoch:25 step:19648 [D loss: 0.000296, acc.: 100.00%] [G loss: 1.250109]\n",
      "epoch:25 step:19649 [D loss: 0.002262, acc.: 100.00%] [G loss: 2.478396]\n",
      "epoch:25 step:19650 [D loss: 0.002438, acc.: 100.00%] [G loss: 1.129914]\n",
      "epoch:25 step:19651 [D loss: 0.013405, acc.: 100.00%] [G loss: 0.115420]\n",
      "epoch:25 step:19652 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.049644]\n",
      "epoch:25 step:19653 [D loss: 0.007615, acc.: 100.00%] [G loss: 2.571846]\n",
      "epoch:25 step:19654 [D loss: 0.011143, acc.: 100.00%] [G loss: 0.937391]\n",
      "epoch:25 step:19655 [D loss: 0.000835, acc.: 100.00%] [G loss: 0.949197]\n",
      "epoch:25 step:19656 [D loss: 0.002847, acc.: 100.00%] [G loss: 0.407729]\n",
      "epoch:25 step:19657 [D loss: 0.002822, acc.: 100.00%] [G loss: 0.008469]\n",
      "epoch:25 step:19658 [D loss: 0.001141, acc.: 100.00%] [G loss: 0.448490]\n",
      "epoch:25 step:19659 [D loss: 0.015358, acc.: 100.00%] [G loss: 0.168941]\n",
      "epoch:25 step:19660 [D loss: 0.003248, acc.: 100.00%] [G loss: 0.219726]\n",
      "epoch:25 step:19661 [D loss: 0.003411, acc.: 100.00%] [G loss: 0.099351]\n",
      "epoch:25 step:19662 [D loss: 0.002481, acc.: 100.00%] [G loss: 0.037027]\n",
      "epoch:25 step:19663 [D loss: 0.002598, acc.: 100.00%] [G loss: 0.015701]\n",
      "epoch:25 step:19664 [D loss: 0.002302, acc.: 100.00%] [G loss: 0.077524]\n",
      "epoch:25 step:19665 [D loss: 0.277005, acc.: 83.59%] [G loss: 6.448697]\n",
      "epoch:25 step:19666 [D loss: 0.818134, acc.: 64.84%] [G loss: 0.315259]\n",
      "epoch:25 step:19667 [D loss: 0.162372, acc.: 93.75%] [G loss: 0.903914]\n",
      "epoch:25 step:19668 [D loss: 0.000905, acc.: 100.00%] [G loss: 6.236191]\n",
      "epoch:25 step:19669 [D loss: 0.047240, acc.: 99.22%] [G loss: 5.614357]\n",
      "epoch:25 step:19670 [D loss: 0.015890, acc.: 100.00%] [G loss: 0.252234]\n",
      "epoch:25 step:19671 [D loss: 0.021108, acc.: 100.00%] [G loss: 5.314376]\n",
      "epoch:25 step:19672 [D loss: 0.030030, acc.: 99.22%] [G loss: 3.493356]\n",
      "epoch:25 step:19673 [D loss: 0.025266, acc.: 100.00%] [G loss: 0.091391]\n",
      "epoch:25 step:19674 [D loss: 0.004457, acc.: 100.00%] [G loss: 5.015615]\n",
      "epoch:25 step:19675 [D loss: 0.158913, acc.: 95.31%] [G loss: 7.629921]\n",
      "epoch:25 step:19676 [D loss: 0.443658, acc.: 77.34%] [G loss: 3.068769]\n",
      "epoch:25 step:19677 [D loss: 0.084612, acc.: 96.09%] [G loss: 0.000300]\n",
      "epoch:25 step:19678 [D loss: 0.830129, acc.: 67.19%] [G loss: 9.169283]\n",
      "epoch:25 step:19679 [D loss: 4.008784, acc.: 50.00%] [G loss: 8.146578]\n",
      "epoch:25 step:19680 [D loss: 1.480847, acc.: 58.59%] [G loss: 2.737154]\n",
      "epoch:25 step:19681 [D loss: 0.116528, acc.: 95.31%] [G loss: 1.823780]\n",
      "epoch:25 step:19682 [D loss: 0.154963, acc.: 93.75%] [G loss: 1.824834]\n",
      "epoch:25 step:19683 [D loss: 0.031002, acc.: 97.66%] [G loss: 1.780132]\n",
      "epoch:25 step:19684 [D loss: 0.004363, acc.: 100.00%] [G loss: 1.037358]\n",
      "epoch:25 step:19685 [D loss: 0.028674, acc.: 99.22%] [G loss: 0.932145]\n",
      "epoch:25 step:19686 [D loss: 0.032742, acc.: 99.22%] [G loss: 1.542989]\n",
      "epoch:25 step:19687 [D loss: 0.007484, acc.: 100.00%] [G loss: 0.802142]\n",
      "epoch:25 step:19688 [D loss: 0.035867, acc.: 100.00%] [G loss: 0.070661]\n",
      "epoch:25 step:19689 [D loss: 0.064166, acc.: 99.22%] [G loss: 0.227675]\n",
      "epoch:25 step:19690 [D loss: 0.009555, acc.: 100.00%] [G loss: 0.512410]\n",
      "epoch:25 step:19691 [D loss: 0.009060, acc.: 100.00%] [G loss: 0.259320]\n",
      "epoch:25 step:19692 [D loss: 0.015291, acc.: 100.00%] [G loss: 0.047623]\n",
      "epoch:25 step:19693 [D loss: 0.007233, acc.: 100.00%] [G loss: 0.043941]\n",
      "epoch:25 step:19694 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.213079]\n",
      "epoch:25 step:19695 [D loss: 0.004837, acc.: 100.00%] [G loss: 0.022420]\n",
      "epoch:25 step:19696 [D loss: 0.006506, acc.: 100.00%] [G loss: 0.020612]\n",
      "epoch:25 step:19697 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.054076]\n",
      "epoch:25 step:19698 [D loss: 0.001588, acc.: 100.00%] [G loss: 0.004681]\n",
      "epoch:25 step:19699 [D loss: 0.010547, acc.: 100.00%] [G loss: 0.003759]\n",
      "epoch:25 step:19700 [D loss: 0.020629, acc.: 100.00%] [G loss: 0.030015]\n",
      "epoch:25 step:19701 [D loss: 0.030193, acc.: 98.44%] [G loss: 0.014353]\n",
      "epoch:25 step:19702 [D loss: 0.001677, acc.: 100.00%] [G loss: 0.009037]\n",
      "epoch:25 step:19703 [D loss: 0.006816, acc.: 100.00%] [G loss: 0.009420]\n",
      "epoch:25 step:19704 [D loss: 0.012150, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:25 step:19705 [D loss: 0.005322, acc.: 100.00%] [G loss: 0.001850]\n",
      "epoch:25 step:19706 [D loss: 0.064068, acc.: 98.44%] [G loss: 0.012431]\n",
      "epoch:25 step:19707 [D loss: 0.011583, acc.: 100.00%] [G loss: 0.037187]\n",
      "epoch:25 step:19708 [D loss: 0.011489, acc.: 100.00%] [G loss: 2.185928]\n",
      "epoch:25 step:19709 [D loss: 0.026835, acc.: 99.22%] [G loss: 0.275862]\n",
      "epoch:25 step:19710 [D loss: 0.029898, acc.: 100.00%] [G loss: 0.297513]\n",
      "epoch:25 step:19711 [D loss: 0.006714, acc.: 100.00%] [G loss: 1.820515]\n",
      "epoch:25 step:19712 [D loss: 0.378289, acc.: 80.47%] [G loss: 4.251391]\n",
      "epoch:25 step:19713 [D loss: 0.830881, acc.: 63.28%] [G loss: 2.647550]\n",
      "epoch:25 step:19714 [D loss: 0.019260, acc.: 100.00%] [G loss: 1.233377]\n",
      "epoch:25 step:19715 [D loss: 0.016802, acc.: 100.00%] [G loss: 0.102252]\n",
      "epoch:25 step:19716 [D loss: 0.027535, acc.: 100.00%] [G loss: 0.764327]\n",
      "epoch:25 step:19717 [D loss: 0.003267, acc.: 100.00%] [G loss: 0.130362]\n",
      "epoch:25 step:19718 [D loss: 0.013969, acc.: 100.00%] [G loss: 0.098151]\n",
      "epoch:25 step:19719 [D loss: 0.007946, acc.: 100.00%] [G loss: 0.034590]\n",
      "epoch:25 step:19720 [D loss: 0.010957, acc.: 100.00%] [G loss: 0.185967]\n",
      "epoch:25 step:19721 [D loss: 0.007827, acc.: 100.00%] [G loss: 0.259684]\n",
      "epoch:25 step:19722 [D loss: 0.017635, acc.: 100.00%] [G loss: 0.109507]\n",
      "epoch:25 step:19723 [D loss: 0.010665, acc.: 100.00%] [G loss: 0.050832]\n",
      "epoch:25 step:19724 [D loss: 0.011108, acc.: 100.00%] [G loss: 0.108015]\n",
      "epoch:25 step:19725 [D loss: 0.008891, acc.: 100.00%] [G loss: 0.019483]\n",
      "epoch:25 step:19726 [D loss: 0.001555, acc.: 100.00%] [G loss: 0.140922]\n",
      "epoch:25 step:19727 [D loss: 0.015081, acc.: 100.00%] [G loss: 0.034882]\n",
      "epoch:25 step:19728 [D loss: 0.005359, acc.: 100.00%] [G loss: 0.022297]\n",
      "epoch:25 step:19729 [D loss: 0.003345, acc.: 100.00%] [G loss: 0.098579]\n",
      "epoch:25 step:19730 [D loss: 0.004841, acc.: 100.00%] [G loss: 0.036085]\n",
      "epoch:25 step:19731 [D loss: 0.004599, acc.: 100.00%] [G loss: 0.032945]\n",
      "epoch:25 step:19732 [D loss: 0.002974, acc.: 100.00%] [G loss: 0.013992]\n",
      "epoch:25 step:19733 [D loss: 0.007784, acc.: 100.00%] [G loss: 0.005030]\n",
      "epoch:25 step:19734 [D loss: 0.023836, acc.: 100.00%] [G loss: 0.046225]\n",
      "epoch:25 step:19735 [D loss: 0.032797, acc.: 99.22%] [G loss: 0.111002]\n",
      "epoch:25 step:19736 [D loss: 0.001038, acc.: 100.00%] [G loss: 0.002273]\n",
      "epoch:25 step:19737 [D loss: 0.001149, acc.: 100.00%] [G loss: 0.008770]\n",
      "epoch:25 step:19738 [D loss: 0.003898, acc.: 100.00%] [G loss: 0.002798]\n",
      "epoch:25 step:19739 [D loss: 0.007737, acc.: 100.00%] [G loss: 0.003559]\n",
      "epoch:25 step:19740 [D loss: 0.001926, acc.: 100.00%] [G loss: 0.058666]\n",
      "epoch:25 step:19741 [D loss: 0.001964, acc.: 100.00%] [G loss: 0.106432]\n",
      "epoch:25 step:19742 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:25 step:19743 [D loss: 0.004453, acc.: 100.00%] [G loss: 0.017604]\n",
      "epoch:25 step:19744 [D loss: 0.001609, acc.: 100.00%] [G loss: 0.002487]\n",
      "epoch:25 step:19745 [D loss: 0.002182, acc.: 100.00%] [G loss: 0.002771]\n",
      "epoch:25 step:19746 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.051552]\n",
      "epoch:25 step:19747 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.003106]\n",
      "epoch:25 step:19748 [D loss: 0.000991, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:25 step:19749 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:25 step:19750 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.048054]\n",
      "epoch:25 step:19751 [D loss: 0.001646, acc.: 100.00%] [G loss: 0.000918]\n",
      "epoch:25 step:19752 [D loss: 0.001026, acc.: 100.00%] [G loss: 0.018641]\n",
      "epoch:25 step:19753 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.000789]\n",
      "epoch:25 step:19754 [D loss: 0.001617, acc.: 100.00%] [G loss: 0.002267]\n",
      "epoch:25 step:19755 [D loss: 0.001984, acc.: 100.00%] [G loss: 0.000763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19756 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.001347]\n",
      "epoch:25 step:19757 [D loss: 0.001538, acc.: 100.00%] [G loss: 0.001551]\n",
      "epoch:25 step:19758 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.016691]\n",
      "epoch:25 step:19759 [D loss: 0.001412, acc.: 100.00%] [G loss: 0.002074]\n",
      "epoch:25 step:19760 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.001367]\n",
      "epoch:25 step:19761 [D loss: 0.008620, acc.: 100.00%] [G loss: 0.001353]\n",
      "epoch:25 step:19762 [D loss: 0.011864, acc.: 100.00%] [G loss: 0.078134]\n",
      "epoch:25 step:19763 [D loss: 0.000930, acc.: 100.00%] [G loss: 0.062621]\n",
      "epoch:25 step:19764 [D loss: 0.000790, acc.: 100.00%] [G loss: 0.004637]\n",
      "epoch:25 step:19765 [D loss: 0.019058, acc.: 100.00%] [G loss: 0.990555]\n",
      "epoch:25 step:19766 [D loss: 0.022009, acc.: 100.00%] [G loss: 0.005150]\n",
      "epoch:25 step:19767 [D loss: 0.004382, acc.: 100.00%] [G loss: 0.005176]\n",
      "epoch:25 step:19768 [D loss: 0.011277, acc.: 100.00%] [G loss: 0.003039]\n",
      "epoch:25 step:19769 [D loss: 0.002488, acc.: 100.00%] [G loss: 0.233323]\n",
      "epoch:25 step:19770 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.929911]\n",
      "epoch:25 step:19771 [D loss: 0.021720, acc.: 100.00%] [G loss: 0.011103]\n",
      "epoch:25 step:19772 [D loss: 0.008843, acc.: 100.00%] [G loss: 0.558246]\n",
      "epoch:25 step:19773 [D loss: 0.037361, acc.: 99.22%] [G loss: 0.002472]\n",
      "epoch:25 step:19774 [D loss: 0.001443, acc.: 100.00%] [G loss: 0.022298]\n",
      "epoch:25 step:19775 [D loss: 0.056009, acc.: 97.66%] [G loss: 0.001056]\n",
      "epoch:25 step:19776 [D loss: 0.001911, acc.: 100.00%] [G loss: 0.001041]\n",
      "epoch:25 step:19777 [D loss: 0.063262, acc.: 96.88%] [G loss: 0.038839]\n",
      "epoch:25 step:19778 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.012009]\n",
      "epoch:25 step:19779 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.005637]\n",
      "epoch:25 step:19780 [D loss: 0.031040, acc.: 98.44%] [G loss: 0.032085]\n",
      "epoch:25 step:19781 [D loss: 0.000769, acc.: 100.00%] [G loss: 0.115876]\n",
      "epoch:25 step:19782 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.054012]\n",
      "epoch:25 step:19783 [D loss: 0.012079, acc.: 100.00%] [G loss: 0.008743]\n",
      "epoch:25 step:19784 [D loss: 0.001034, acc.: 100.00%] [G loss: 0.003334]\n",
      "epoch:25 step:19785 [D loss: 0.025057, acc.: 99.22%] [G loss: 0.945748]\n",
      "epoch:25 step:19786 [D loss: 0.016760, acc.: 100.00%] [G loss: 0.009949]\n",
      "epoch:25 step:19787 [D loss: 0.002228, acc.: 100.00%] [G loss: 0.044248]\n",
      "epoch:25 step:19788 [D loss: 0.018015, acc.: 100.00%] [G loss: 0.005819]\n",
      "epoch:25 step:19789 [D loss: 0.008507, acc.: 100.00%] [G loss: 0.443564]\n",
      "epoch:25 step:19790 [D loss: 0.060778, acc.: 98.44%] [G loss: 0.010755]\n",
      "epoch:25 step:19791 [D loss: 0.014078, acc.: 100.00%] [G loss: 0.002883]\n",
      "epoch:25 step:19792 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.008474]\n",
      "epoch:25 step:19793 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.342906]\n",
      "epoch:25 step:19794 [D loss: 0.001905, acc.: 100.00%] [G loss: 0.004053]\n",
      "epoch:25 step:19795 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.171161]\n",
      "epoch:25 step:19796 [D loss: 0.122729, acc.: 96.09%] [G loss: 0.043884]\n",
      "epoch:25 step:19797 [D loss: 0.053561, acc.: 96.88%] [G loss: 3.134770]\n",
      "epoch:25 step:19798 [D loss: 0.099348, acc.: 97.66%] [G loss: 0.311317]\n",
      "epoch:25 step:19799 [D loss: 0.004167, acc.: 100.00%] [G loss: 0.153988]\n",
      "epoch:25 step:19800 [D loss: 0.010068, acc.: 100.00%] [G loss: 0.002909]\n",
      "epoch:25 step:19801 [D loss: 0.006694, acc.: 100.00%] [G loss: 0.017497]\n",
      "epoch:25 step:19802 [D loss: 0.022747, acc.: 100.00%] [G loss: 0.086821]\n",
      "epoch:25 step:19803 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.163073]\n",
      "epoch:25 step:19804 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.008725]\n",
      "epoch:25 step:19805 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.009587]\n",
      "epoch:25 step:19806 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.130831]\n",
      "epoch:25 step:19807 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.065960]\n",
      "epoch:25 step:19808 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.216303]\n",
      "epoch:25 step:19809 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.001155]\n",
      "epoch:25 step:19810 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.003371]\n",
      "epoch:25 step:19811 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.001125]\n",
      "epoch:25 step:19812 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.004186]\n",
      "epoch:25 step:19813 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.002827]\n",
      "epoch:25 step:19814 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.054367]\n",
      "epoch:25 step:19815 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.002624]\n",
      "epoch:25 step:19816 [D loss: 0.001713, acc.: 100.00%] [G loss: 0.001593]\n",
      "epoch:25 step:19817 [D loss: 0.003086, acc.: 100.00%] [G loss: 0.006342]\n",
      "epoch:25 step:19818 [D loss: 0.006642, acc.: 100.00%] [G loss: 0.002830]\n",
      "epoch:25 step:19819 [D loss: 0.320373, acc.: 90.62%] [G loss: 4.775252]\n",
      "epoch:25 step:19820 [D loss: 1.334661, acc.: 57.03%] [G loss: 0.204942]\n",
      "epoch:25 step:19821 [D loss: 0.092713, acc.: 96.88%] [G loss: 0.000081]\n",
      "epoch:25 step:19822 [D loss: 0.011900, acc.: 100.00%] [G loss: 0.001124]\n",
      "epoch:25 step:19823 [D loss: 0.031514, acc.: 100.00%] [G loss: 4.455095]\n",
      "epoch:25 step:19824 [D loss: 0.002958, acc.: 100.00%] [G loss: 2.946643]\n",
      "epoch:25 step:19825 [D loss: 0.015920, acc.: 100.00%] [G loss: 0.039949]\n",
      "epoch:25 step:19826 [D loss: 0.046650, acc.: 99.22%] [G loss: 2.135115]\n",
      "epoch:25 step:19827 [D loss: 0.003281, acc.: 100.00%] [G loss: 0.373768]\n",
      "epoch:25 step:19828 [D loss: 0.001498, acc.: 100.00%] [G loss: 0.769500]\n",
      "epoch:25 step:19829 [D loss: 0.006323, acc.: 100.00%] [G loss: 0.126727]\n",
      "epoch:25 step:19830 [D loss: 0.005042, acc.: 100.00%] [G loss: 1.414220]\n",
      "epoch:25 step:19831 [D loss: 0.032177, acc.: 100.00%] [G loss: 0.126773]\n",
      "epoch:25 step:19832 [D loss: 0.029031, acc.: 100.00%] [G loss: 0.538035]\n",
      "epoch:25 step:19833 [D loss: 0.107104, acc.: 95.31%] [G loss: 1.759884]\n",
      "epoch:25 step:19834 [D loss: 0.167877, acc.: 93.75%] [G loss: 0.412772]\n",
      "epoch:25 step:19835 [D loss: 0.025196, acc.: 98.44%] [G loss: 0.082288]\n",
      "epoch:25 step:19836 [D loss: 0.002402, acc.: 100.00%] [G loss: 0.021495]\n",
      "epoch:25 step:19837 [D loss: 0.011218, acc.: 100.00%] [G loss: 0.007616]\n",
      "epoch:25 step:19838 [D loss: 0.000943, acc.: 100.00%] [G loss: 0.289715]\n",
      "epoch:25 step:19839 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.131506]\n",
      "epoch:25 step:19840 [D loss: 0.151145, acc.: 92.97%] [G loss: 0.033720]\n",
      "epoch:25 step:19841 [D loss: 0.001695, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:25 step:19842 [D loss: 0.000876, acc.: 100.00%] [G loss: 0.001132]\n",
      "epoch:25 step:19843 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:25 step:19844 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:25 step:19845 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.004293]\n",
      "epoch:25 step:19846 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:25 step:19847 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000958]\n",
      "epoch:25 step:19848 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:25 step:19849 [D loss: 0.001043, acc.: 100.00%] [G loss: 0.003919]\n",
      "epoch:25 step:19850 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.001499]\n",
      "epoch:25 step:19851 [D loss: 0.000980, acc.: 100.00%] [G loss: 0.006008]\n",
      "epoch:25 step:19852 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.200854]\n",
      "epoch:25 step:19853 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.000776]\n",
      "epoch:25 step:19854 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:25 step:19855 [D loss: 0.025059, acc.: 99.22%] [G loss: 0.003729]\n",
      "epoch:25 step:19856 [D loss: 0.000614, acc.: 100.00%] [G loss: 0.302308]\n",
      "epoch:25 step:19857 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.002947]\n",
      "epoch:25 step:19858 [D loss: 0.000788, acc.: 100.00%] [G loss: 0.002276]\n",
      "epoch:25 step:19859 [D loss: 0.003960, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:25 step:19860 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000855]\n",
      "epoch:25 step:19861 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:25 step:19862 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.015424]\n",
      "epoch:25 step:19863 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000600]\n",
      "epoch:25 step:19864 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:25 step:19865 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:25 step:19866 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000607]\n",
      "epoch:25 step:19867 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.023982]\n",
      "epoch:25 step:19868 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000804]\n",
      "epoch:25 step:19869 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.001878]\n",
      "epoch:25 step:19870 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19871 [D loss: 0.001091, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:25 step:19872 [D loss: 0.000780, acc.: 100.00%] [G loss: 0.002082]\n",
      "epoch:25 step:19873 [D loss: 0.019925, acc.: 100.00%] [G loss: 0.006210]\n",
      "epoch:25 step:19874 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:25 step:19875 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000712]\n",
      "epoch:25 step:19876 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.034133]\n",
      "epoch:25 step:19877 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.002175]\n",
      "epoch:25 step:19878 [D loss: 0.001595, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:25 step:19879 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:25 step:19880 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.017761]\n",
      "epoch:25 step:19881 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:25 step:19882 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001463]\n",
      "epoch:25 step:19883 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.001152]\n",
      "epoch:25 step:19884 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:25 step:19885 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:25 step:19886 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.022189]\n",
      "epoch:25 step:19887 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000638]\n",
      "epoch:25 step:19888 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.008467]\n",
      "epoch:25 step:19889 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.016704]\n",
      "epoch:25 step:19890 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000943]\n",
      "epoch:25 step:19891 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.003964]\n",
      "epoch:25 step:19892 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:25 step:19893 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.002556]\n",
      "epoch:25 step:19894 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.003605]\n",
      "epoch:25 step:19895 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000941]\n",
      "epoch:25 step:19896 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:25 step:19897 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:25 step:19898 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:25 step:19899 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001243]\n",
      "epoch:25 step:19900 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.000670]\n",
      "epoch:25 step:19901 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:25 step:19902 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.377764]\n",
      "epoch:25 step:19903 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:25 step:19904 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:25 step:19905 [D loss: 0.002898, acc.: 100.00%] [G loss: 0.001585]\n",
      "epoch:25 step:19906 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:25 step:19907 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:25 step:19908 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:25 step:19909 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.002749]\n",
      "epoch:25 step:19910 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:25 step:19911 [D loss: 0.006405, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:25 step:19912 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.001391]\n",
      "epoch:25 step:19913 [D loss: 0.001245, acc.: 100.00%] [G loss: 0.000604]\n",
      "epoch:25 step:19914 [D loss: 0.002814, acc.: 100.00%] [G loss: 0.003170]\n",
      "epoch:25 step:19915 [D loss: 0.023102, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:25 step:19916 [D loss: 0.009051, acc.: 100.00%] [G loss: 0.001965]\n",
      "epoch:25 step:19917 [D loss: 0.010002, acc.: 100.00%] [G loss: 0.002472]\n",
      "epoch:25 step:19918 [D loss: 0.042227, acc.: 100.00%] [G loss: 0.004345]\n",
      "epoch:25 step:19919 [D loss: 0.005135, acc.: 100.00%] [G loss: 0.092472]\n",
      "epoch:25 step:19920 [D loss: 0.924653, acc.: 59.38%] [G loss: 3.416072]\n",
      "epoch:25 step:19921 [D loss: 0.350317, acc.: 81.25%] [G loss: 0.413003]\n",
      "epoch:25 step:19922 [D loss: 0.003360, acc.: 100.00%] [G loss: 9.435186]\n",
      "epoch:25 step:19923 [D loss: 0.022412, acc.: 99.22%] [G loss: 3.770379]\n",
      "epoch:25 step:19924 [D loss: 0.010217, acc.: 100.00%] [G loss: 0.001837]\n",
      "epoch:25 step:19925 [D loss: 0.169487, acc.: 92.97%] [G loss: 0.695338]\n",
      "epoch:25 step:19926 [D loss: 0.275377, acc.: 86.72%] [G loss: 7.419135]\n",
      "epoch:25 step:19927 [D loss: 0.002318, acc.: 100.00%] [G loss: 6.667153]\n",
      "epoch:25 step:19928 [D loss: 0.003839, acc.: 100.00%] [G loss: 5.590545]\n",
      "epoch:25 step:19929 [D loss: 0.003375, acc.: 100.00%] [G loss: 0.091632]\n",
      "epoch:25 step:19930 [D loss: 0.008937, acc.: 100.00%] [G loss: 5.398954]\n",
      "epoch:25 step:19931 [D loss: 0.001177, acc.: 100.00%] [G loss: 5.380126]\n",
      "epoch:25 step:19932 [D loss: 0.000310, acc.: 100.00%] [G loss: 4.606956]\n",
      "epoch:25 step:19933 [D loss: 0.010783, acc.: 99.22%] [G loss: 2.982495]\n",
      "epoch:25 step:19934 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.027697]\n",
      "epoch:25 step:19935 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.084364]\n",
      "epoch:25 step:19936 [D loss: 0.016406, acc.: 100.00%] [G loss: 2.859655]\n",
      "epoch:25 step:19937 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:25 step:19938 [D loss: 0.006149, acc.: 100.00%] [G loss: 3.134783]\n",
      "epoch:25 step:19939 [D loss: 0.001220, acc.: 100.00%] [G loss: 0.000918]\n",
      "epoch:25 step:19940 [D loss: 0.010466, acc.: 100.00%] [G loss: 0.472498]\n",
      "epoch:25 step:19941 [D loss: 0.000715, acc.: 100.00%] [G loss: 1.745742]\n",
      "epoch:25 step:19942 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.502479]\n",
      "epoch:25 step:19943 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.312324]\n",
      "epoch:25 step:19944 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.146526]\n",
      "epoch:25 step:19945 [D loss: 0.005621, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:25 step:19946 [D loss: 0.003226, acc.: 100.00%] [G loss: 0.001223]\n",
      "epoch:25 step:19947 [D loss: 0.003458, acc.: 100.00%] [G loss: 0.024594]\n",
      "epoch:25 step:19948 [D loss: 0.002196, acc.: 100.00%] [G loss: 0.010477]\n",
      "epoch:25 step:19949 [D loss: 0.001949, acc.: 100.00%] [G loss: 0.043888]\n",
      "epoch:25 step:19950 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.011465]\n",
      "epoch:25 step:19951 [D loss: 0.001147, acc.: 100.00%] [G loss: 0.001315]\n",
      "epoch:25 step:19952 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:25 step:19953 [D loss: 0.071473, acc.: 100.00%] [G loss: 0.075855]\n",
      "epoch:25 step:19954 [D loss: 0.089106, acc.: 96.88%] [G loss: 0.013253]\n",
      "epoch:25 step:19955 [D loss: 0.039801, acc.: 100.00%] [G loss: 0.001700]\n",
      "epoch:25 step:19956 [D loss: 0.002703, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:25 step:19957 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:25 step:19958 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:25 step:19959 [D loss: 0.001389, acc.: 100.00%] [G loss: 0.005193]\n",
      "epoch:25 step:19960 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.004421]\n",
      "epoch:25 step:19961 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:25 step:19962 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.004111]\n",
      "epoch:25 step:19963 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:25 step:19964 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:25 step:19965 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:25 step:19966 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:25 step:19967 [D loss: 0.000966, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:25 step:19968 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:25 step:19969 [D loss: 0.015086, acc.: 100.00%] [G loss: 0.001486]\n",
      "epoch:25 step:19970 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:25 step:19971 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.005779]\n",
      "epoch:25 step:19972 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001149]\n",
      "epoch:25 step:19973 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.001493]\n",
      "epoch:25 step:19974 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:25 step:19975 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.005820]\n",
      "epoch:25 step:19976 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.002694]\n",
      "epoch:25 step:19977 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.019642]\n",
      "epoch:25 step:19978 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.029859]\n",
      "epoch:25 step:19979 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.003132]\n",
      "epoch:25 step:19980 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003879]\n",
      "epoch:25 step:19981 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001124]\n",
      "epoch:25 step:19982 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001723]\n",
      "epoch:25 step:19983 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004692]\n",
      "epoch:25 step:19984 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000691]\n",
      "epoch:25 step:19985 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.044729]\n",
      "epoch:25 step:19986 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19987 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.006741]\n",
      "epoch:25 step:19988 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001884]\n",
      "epoch:25 step:19989 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.029302]\n",
      "epoch:25 step:19990 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.017409]\n",
      "epoch:25 step:19991 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.001135]\n",
      "epoch:25 step:19992 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.002866]\n",
      "epoch:25 step:19993 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:25 step:19994 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001209]\n",
      "epoch:25 step:19995 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001482]\n",
      "epoch:25 step:19996 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:25 step:19997 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:25 step:19998 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000914]\n",
      "epoch:25 step:19999 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:25 step:20000 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001131]\n",
      "epoch:25 step:20001 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001352]\n",
      "epoch:25 step:20002 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000786]\n",
      "epoch:25 step:20003 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:25 step:20004 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.015662]\n",
      "epoch:25 step:20005 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:25 step:20006 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:25 step:20007 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:25 step:20008 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001426]\n",
      "epoch:25 step:20009 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:25 step:20010 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001596]\n",
      "epoch:25 step:20011 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:25 step:20012 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.017348]\n",
      "epoch:25 step:20013 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:25 step:20014 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.001277]\n",
      "epoch:25 step:20015 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.008173]\n",
      "epoch:25 step:20016 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:25 step:20017 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:25 step:20018 [D loss: 0.001718, acc.: 100.00%] [G loss: 0.002162]\n",
      "epoch:25 step:20019 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.004488]\n",
      "epoch:25 step:20020 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:25 step:20021 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:25 step:20022 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.001079]\n",
      "epoch:25 step:20023 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:25 step:20024 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:25 step:20025 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.001063]\n",
      "epoch:25 step:20026 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:25 step:20027 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:25 step:20028 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000485]\n",
      "epoch:25 step:20029 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.009889]\n",
      "epoch:25 step:20030 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.002849]\n",
      "epoch:25 step:20031 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000831]\n",
      "epoch:25 step:20032 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:25 step:20033 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.014515]\n",
      "epoch:25 step:20034 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.003366]\n",
      "epoch:25 step:20035 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001620]\n",
      "epoch:25 step:20036 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000481]\n",
      "epoch:25 step:20037 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000896]\n",
      "epoch:25 step:20038 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:25 step:20039 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:25 step:20040 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:25 step:20041 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:25 step:20042 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001714]\n",
      "epoch:25 step:20043 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:25 step:20044 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001233]\n",
      "epoch:25 step:20045 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001861]\n",
      "epoch:25 step:20046 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:25 step:20047 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001330]\n",
      "epoch:25 step:20048 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000416]\n",
      "epoch:25 step:20049 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001346]\n",
      "epoch:25 step:20050 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:25 step:20051 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.000838]\n",
      "epoch:25 step:20052 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:25 step:20053 [D loss: 0.001380, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:25 step:20054 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.002130]\n",
      "epoch:25 step:20055 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:25 step:20056 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:25 step:20057 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:25 step:20058 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:25 step:20059 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.003547]\n",
      "epoch:25 step:20060 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.001285]\n",
      "epoch:25 step:20061 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000769]\n",
      "epoch:25 step:20062 [D loss: 0.002214, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:25 step:20063 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:25 step:20064 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001141]\n",
      "epoch:25 step:20065 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:25 step:20066 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.005176]\n",
      "epoch:25 step:20067 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.001189]\n",
      "epoch:25 step:20068 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:25 step:20069 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:25 step:20070 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000650]\n",
      "epoch:25 step:20071 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:25 step:20072 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:25 step:20073 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:25 step:20074 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.003596]\n",
      "epoch:25 step:20075 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000770]\n",
      "epoch:25 step:20076 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:25 step:20077 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000894]\n",
      "epoch:25 step:20078 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:25 step:20079 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:25 step:20080 [D loss: 0.002201, acc.: 100.00%] [G loss: 0.000875]\n",
      "epoch:25 step:20081 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.006095]\n",
      "epoch:25 step:20082 [D loss: 0.005329, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:25 step:20083 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.005276]\n",
      "epoch:25 step:20084 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.003994]\n",
      "epoch:25 step:20085 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:25 step:20086 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000632]\n",
      "epoch:25 step:20087 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.001609]\n",
      "epoch:25 step:20088 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.001778]\n",
      "epoch:25 step:20089 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.002059]\n",
      "epoch:25 step:20090 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:25 step:20091 [D loss: 0.000708, acc.: 100.00%] [G loss: 0.001730]\n",
      "epoch:25 step:20092 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.001672]\n",
      "epoch:25 step:20093 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:25 step:20094 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000564]\n",
      "epoch:25 step:20095 [D loss: 0.001200, acc.: 100.00%] [G loss: 0.001125]\n",
      "epoch:25 step:20096 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:25 step:20097 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.006638]\n",
      "epoch:25 step:20098 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000785]\n",
      "epoch:25 step:20099 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.001070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20100 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.001646]\n",
      "epoch:25 step:20101 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.003211]\n",
      "epoch:25 step:20102 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:25 step:20103 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:25 step:20104 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.009962]\n",
      "epoch:25 step:20105 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.002069]\n",
      "epoch:25 step:20106 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:25 step:20107 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001248]\n",
      "epoch:25 step:20108 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.001118]\n",
      "epoch:25 step:20109 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.001254]\n",
      "epoch:25 step:20110 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000742]\n",
      "epoch:25 step:20111 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:25 step:20112 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:25 step:20113 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001004]\n",
      "epoch:25 step:20114 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:25 step:20115 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:25 step:20116 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:25 step:20117 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.002599]\n",
      "epoch:25 step:20118 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000850]\n",
      "epoch:25 step:20119 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.004603]\n",
      "epoch:25 step:20120 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:25 step:20121 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:25 step:20122 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:25 step:20123 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:25 step:20124 [D loss: 0.003985, acc.: 100.00%] [G loss: 0.001822]\n",
      "epoch:25 step:20125 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:25 step:20126 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:25 step:20127 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001273]\n",
      "epoch:25 step:20128 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001289]\n",
      "epoch:25 step:20129 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:25 step:20130 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:25 step:20131 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:25 step:20132 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:25 step:20133 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:25 step:20134 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.002338]\n",
      "epoch:25 step:20135 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.003192]\n",
      "epoch:25 step:20136 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.002157]\n",
      "epoch:25 step:20137 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000952]\n",
      "epoch:25 step:20138 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:25 step:20139 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001332]\n",
      "epoch:25 step:20140 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002386]\n",
      "epoch:25 step:20141 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:25 step:20142 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.001125]\n",
      "epoch:25 step:20143 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.001386]\n",
      "epoch:25 step:20144 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.003142]\n",
      "epoch:25 step:20145 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:25 step:20146 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000408]\n",
      "epoch:25 step:20147 [D loss: 0.003518, acc.: 100.00%] [G loss: 0.002883]\n",
      "epoch:25 step:20148 [D loss: 0.001694, acc.: 100.00%] [G loss: 0.008102]\n",
      "epoch:25 step:20149 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:25 step:20150 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000502]\n",
      "epoch:25 step:20151 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.006763]\n",
      "epoch:25 step:20152 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.001709]\n",
      "epoch:25 step:20153 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:25 step:20154 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.006370]\n",
      "epoch:25 step:20155 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:25 step:20156 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:25 step:20157 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:25 step:20158 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000922]\n",
      "epoch:25 step:20159 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.001679]\n",
      "epoch:25 step:20160 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000331]\n",
      "epoch:25 step:20161 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.005248]\n",
      "epoch:25 step:20162 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000607]\n",
      "epoch:25 step:20163 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:25 step:20164 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:25 step:20165 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:25 step:20166 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000649]\n",
      "epoch:25 step:20167 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:25 step:20168 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001373]\n",
      "epoch:25 step:20169 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.014744]\n",
      "epoch:25 step:20170 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:25 step:20171 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.006393]\n",
      "epoch:25 step:20172 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:25 step:20173 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001332]\n",
      "epoch:25 step:20174 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:25 step:20175 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:25 step:20176 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:25 step:20177 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.003477]\n",
      "epoch:25 step:20178 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.014644]\n",
      "epoch:25 step:20179 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:25 step:20180 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:25 step:20181 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:25 step:20182 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:25 step:20183 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:25 step:20184 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:25 step:20185 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:25 step:20186 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.002673]\n",
      "epoch:25 step:20187 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:25 step:20188 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:25 step:20189 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:25 step:20190 [D loss: 0.002987, acc.: 100.00%] [G loss: 0.001780]\n",
      "epoch:25 step:20191 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:25 step:20192 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.005760]\n",
      "epoch:25 step:20193 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:25 step:20194 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:25 step:20195 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:25 step:20196 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:25 step:20197 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000421]\n",
      "epoch:25 step:20198 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:25 step:20199 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:25 step:20200 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000451]\n",
      "epoch:25 step:20201 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:25 step:20202 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:25 step:20203 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001599]\n",
      "epoch:25 step:20204 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:25 step:20205 [D loss: 0.013824, acc.: 99.22%] [G loss: 0.000371]\n",
      "epoch:25 step:20206 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:25 step:20207 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:25 step:20208 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:25 step:20209 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:25 step:20210 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:25 step:20211 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:25 step:20212 [D loss: 0.001031, acc.: 100.00%] [G loss: 0.000030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20213 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:25 step:20214 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:25 step:20215 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:25 step:20216 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:25 step:20217 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:25 step:20218 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.002962]\n",
      "epoch:25 step:20219 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:25 step:20220 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:25 step:20221 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:25 step:20222 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:25 step:20223 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:25 step:20224 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:25 step:20225 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:25 step:20226 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:25 step:20227 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:25 step:20228 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:25 step:20229 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.001122]\n",
      "epoch:25 step:20230 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:25 step:20231 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:25 step:20232 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:25 step:20233 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:25 step:20234 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:25 step:20235 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:25 step:20236 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:25 step:20237 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:25 step:20238 [D loss: 0.001627, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:25 step:20239 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:25 step:20240 [D loss: 0.003568, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:25 step:20241 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:25 step:20242 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:25 step:20243 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:25 step:20244 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:25 step:20245 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:25 step:20246 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.002142]\n",
      "epoch:25 step:20247 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:25 step:20248 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001222]\n",
      "epoch:25 step:20249 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:25 step:20250 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:25 step:20251 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:25 step:20252 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:25 step:20253 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:25 step:20254 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000974]\n",
      "epoch:25 step:20255 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:25 step:20256 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:25 step:20257 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:25 step:20258 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:25 step:20259 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:25 step:20260 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:25 step:20261 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:25 step:20262 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001383]\n",
      "epoch:25 step:20263 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:25 step:20264 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:25 step:20265 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:25 step:20266 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:25 step:20267 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:25 step:20268 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:25 step:20269 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:25 step:20270 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:25 step:20271 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:25 step:20272 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:25 step:20273 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:25 step:20274 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:25 step:20275 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:25 step:20276 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:25 step:20277 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:25 step:20278 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:25 step:20279 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:25 step:20280 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:25 step:20281 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:25 step:20282 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:25 step:20283 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:25 step:20284 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:25 step:20285 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:25 step:20286 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:25 step:20287 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:25 step:20288 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:25 step:20289 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:25 step:20290 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:25 step:20291 [D loss: 0.009602, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:25 step:20292 [D loss: 0.004948, acc.: 100.00%] [G loss: 0.003255]\n",
      "epoch:25 step:20293 [D loss: 0.019353, acc.: 99.22%] [G loss: 0.000660]\n",
      "epoch:25 step:20294 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.006168]\n",
      "epoch:25 step:20295 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.010770]\n",
      "epoch:25 step:20296 [D loss: 0.183204, acc.: 92.97%] [G loss: 0.000010]\n",
      "epoch:25 step:20297 [D loss: 0.072794, acc.: 96.88%] [G loss: 0.000144]\n",
      "epoch:25 step:20298 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.031683]\n",
      "epoch:25 step:20299 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.951756]\n",
      "epoch:25 step:20300 [D loss: 0.001793, acc.: 100.00%] [G loss: 0.032550]\n",
      "epoch:25 step:20301 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.143716]\n",
      "epoch:25 step:20302 [D loss: 0.012126, acc.: 99.22%] [G loss: 0.000371]\n",
      "epoch:25 step:20303 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.092670]\n",
      "epoch:25 step:20304 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.062440]\n",
      "epoch:25 step:20305 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:25 step:20306 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:26 step:20307 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:26 step:20308 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000304]\n",
      "epoch:26 step:20309 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:26 step:20310 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004362]\n",
      "epoch:26 step:20311 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:26 step:20312 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20313 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.024507]\n",
      "epoch:26 step:20314 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:26 step:20315 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:26 step:20316 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:26 step:20317 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.010466]\n",
      "epoch:26 step:20318 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:26 step:20319 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:26 step:20320 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:26 step:20321 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.005018]\n",
      "epoch:26 step:20322 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.002235]\n",
      "epoch:26 step:20323 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:26 step:20324 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000656]\n",
      "epoch:26 step:20325 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:26 step:20326 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.000622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20327 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.017672]\n",
      "epoch:26 step:20328 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:26 step:20329 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:26 step:20330 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.004682]\n",
      "epoch:26 step:20331 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000602]\n",
      "epoch:26 step:20332 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:26 step:20333 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:26 step:20334 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.002203]\n",
      "epoch:26 step:20335 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:26 step:20336 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.004997]\n",
      "epoch:26 step:20337 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:26 step:20338 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:26 step:20339 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:26 step:20340 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:26 step:20341 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:26 step:20342 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:26 step:20343 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000607]\n",
      "epoch:26 step:20344 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000701]\n",
      "epoch:26 step:20345 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20346 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:26 step:20347 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.001879]\n",
      "epoch:26 step:20348 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000916]\n",
      "epoch:26 step:20349 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:26 step:20350 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:26 step:20351 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:26 step:20352 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000780]\n",
      "epoch:26 step:20353 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:26 step:20354 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:26 step:20355 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:26 step:20356 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001693]\n",
      "epoch:26 step:20357 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:26 step:20358 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.008638]\n",
      "epoch:26 step:20359 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:26 step:20360 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:26 step:20361 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:26 step:20362 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:26 step:20363 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:26 step:20364 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:26 step:20365 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:26 step:20366 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20367 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:26 step:20368 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:26 step:20369 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:26 step:20370 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:26 step:20371 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:26 step:20372 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000501]\n",
      "epoch:26 step:20373 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20374 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20375 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:26 step:20376 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003164]\n",
      "epoch:26 step:20377 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:26 step:20378 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:26 step:20379 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:26 step:20380 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:26 step:20381 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:26 step:20382 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:26 step:20383 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:26 step:20384 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:26 step:20385 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:26 step:20386 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:26 step:20387 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:26 step:20388 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:26 step:20389 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20390 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:26 step:20391 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:26 step:20392 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:26 step:20393 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001310]\n",
      "epoch:26 step:20394 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:26 step:20395 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:26 step:20396 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:26 step:20397 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:26 step:20398 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:26 step:20399 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:26 step:20400 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:26 step:20401 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:26 step:20402 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:26 step:20403 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:26 step:20404 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:26 step:20405 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20406 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:26 step:20407 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:26 step:20408 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:26 step:20409 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:26 step:20410 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:26 step:20411 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:26 step:20412 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001606]\n",
      "epoch:26 step:20413 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:26 step:20414 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:26 step:20415 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:26 step:20416 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:26 step:20417 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20418 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:26 step:20419 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:26 step:20420 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20421 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:26 step:20422 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:26 step:20423 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001567]\n",
      "epoch:26 step:20424 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:26 step:20425 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:26 step:20426 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:26 step:20427 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:26 step:20428 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:26 step:20429 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:26 step:20430 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20431 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:26 step:20432 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:26 step:20433 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:26 step:20434 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:26 step:20435 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:26 step:20436 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:26 step:20437 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:26 step:20438 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:26 step:20439 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:26 step:20440 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20441 [D loss: 0.001198, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:26 step:20442 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:26 step:20443 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:26 step:20444 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:26 step:20445 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:26 step:20446 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20447 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:26 step:20448 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:26 step:20449 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000478]\n",
      "epoch:26 step:20450 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:26 step:20451 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:26 step:20452 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:26 step:20453 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:26 step:20454 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:26 step:20455 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:26 step:20456 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:26 step:20457 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:26 step:20458 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:26 step:20459 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:26 step:20460 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:26 step:20461 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:26 step:20462 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:26 step:20463 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20464 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:26 step:20465 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:26 step:20466 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:26 step:20467 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:26 step:20468 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20469 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:26 step:20470 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:26 step:20471 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:26 step:20472 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20473 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:26 step:20474 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:26 step:20475 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:26 step:20476 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:26 step:20477 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:26 step:20478 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:26 step:20479 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:26 step:20480 [D loss: 0.001306, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:26 step:20481 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:26 step:20482 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:26 step:20483 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20484 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001319]\n",
      "epoch:26 step:20485 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20486 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:26 step:20487 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:26 step:20488 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20489 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:26 step:20490 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000662]\n",
      "epoch:26 step:20491 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:26 step:20492 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000538]\n",
      "epoch:26 step:20493 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:26 step:20494 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:26 step:20495 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:26 step:20496 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:26 step:20497 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:26 step:20498 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:26 step:20499 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:26 step:20500 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:26 step:20501 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:26 step:20502 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:26 step:20503 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20504 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:26 step:20505 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:26 step:20506 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000852]\n",
      "epoch:26 step:20507 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:26 step:20508 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:26 step:20509 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:26 step:20510 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:26 step:20511 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:26 step:20512 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:26 step:20513 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:26 step:20514 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:26 step:20515 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:26 step:20516 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:26 step:20517 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:26 step:20518 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:26 step:20519 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:26 step:20520 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20521 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:26 step:20522 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:20523 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:26 step:20524 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20525 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:26 step:20526 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:26 step:20527 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:26 step:20528 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:26 step:20529 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:26 step:20530 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:26 step:20531 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:26 step:20532 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:26 step:20533 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:26 step:20534 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:26 step:20535 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:26 step:20536 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:26 step:20537 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000660]\n",
      "epoch:26 step:20538 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20539 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001745]\n",
      "epoch:26 step:20540 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:26 step:20541 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:26 step:20542 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:26 step:20543 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:26 step:20544 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:26 step:20545 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:26 step:20546 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:26 step:20547 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:26 step:20548 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:26 step:20549 [D loss: 0.003548, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:26 step:20550 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:26 step:20551 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:26 step:20552 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20553 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20554 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:26 step:20555 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:26 step:20556 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20557 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:26 step:20558 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:26 step:20559 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:26 step:20560 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:26 step:20561 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20562 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:26 step:20563 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20564 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:26 step:20565 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:26 step:20566 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20567 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:26 step:20568 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:26 step:20569 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:26 step:20570 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20571 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20572 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000691]\n",
      "epoch:26 step:20573 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20574 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:26 step:20575 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:26 step:20576 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:26 step:20577 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:26 step:20578 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:26 step:20579 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:20580 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:26 step:20581 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001178]\n",
      "epoch:26 step:20582 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:26 step:20583 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:26 step:20584 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:26 step:20585 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:26 step:20586 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:26 step:20587 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:26 step:20588 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:26 step:20589 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000665]\n",
      "epoch:26 step:20590 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:26 step:20591 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:26 step:20592 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:26 step:20593 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:26 step:20594 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20595 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:26 step:20596 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001069]\n",
      "epoch:26 step:20597 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:26 step:20598 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:26 step:20599 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:26 step:20600 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:26 step:20601 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:26 step:20602 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001925]\n",
      "epoch:26 step:20603 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:26 step:20604 [D loss: 0.152751, acc.: 92.19%] [G loss: 1.868156]\n",
      "epoch:26 step:20605 [D loss: 0.909780, acc.: 67.97%] [G loss: 0.000000]\n",
      "epoch:26 step:20606 [D loss: 0.002119, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:26 step:20607 [D loss: 0.014424, acc.: 100.00%] [G loss: 0.116213]\n",
      "epoch:26 step:20608 [D loss: 0.001534, acc.: 100.00%] [G loss: 0.138091]\n",
      "epoch:26 step:20609 [D loss: 0.001085, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:26 step:20610 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:26 step:20611 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:26 step:20612 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:26 step:20613 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:26 step:20614 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20615 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:26 step:20616 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20617 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20618 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.010243]\n",
      "epoch:26 step:20619 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20620 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20621 [D loss: 0.004261, acc.: 100.00%] [G loss: 0.002796]\n",
      "epoch:26 step:20622 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20623 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20624 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:26 step:20625 [D loss: 0.022512, acc.: 99.22%] [G loss: 0.000006]\n",
      "epoch:26 step:20626 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20627 [D loss: 0.001995, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:26 step:20628 [D loss: 0.004424, acc.: 100.00%] [G loss: 0.036658]\n",
      "epoch:26 step:20629 [D loss: 0.009657, acc.: 99.22%] [G loss: 0.096826]\n",
      "epoch:26 step:20630 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000759]\n",
      "epoch:26 step:20631 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:26 step:20632 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:26 step:20633 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.008519]\n",
      "epoch:26 step:20634 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.021667]\n",
      "epoch:26 step:20635 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.046577]\n",
      "epoch:26 step:20636 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:26 step:20637 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:26 step:20638 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:26 step:20639 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.031797]\n",
      "epoch:26 step:20640 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:26 step:20641 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:26 step:20642 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:26 step:20643 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001210]\n",
      "epoch:26 step:20644 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.011412]\n",
      "epoch:26 step:20645 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.003906]\n",
      "epoch:26 step:20646 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.041880]\n",
      "epoch:26 step:20647 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:26 step:20648 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.021223]\n",
      "epoch:26 step:20649 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:26 step:20650 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.001713]\n",
      "epoch:26 step:20651 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20652 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:26 step:20653 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:26 step:20654 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:26 step:20655 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:26 step:20656 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:26 step:20657 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:26 step:20658 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:26 step:20659 [D loss: 0.029224, acc.: 99.22%] [G loss: 0.000001]\n",
      "epoch:26 step:20660 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20661 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20662 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:26 step:20663 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:26 step:20664 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:26 step:20665 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20666 [D loss: 0.008171, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:26 step:20667 [D loss: 0.005232, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20668 [D loss: 0.025904, acc.: 100.00%] [G loss: 0.052812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20669 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.084291]\n",
      "epoch:26 step:20670 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.004521]\n",
      "epoch:26 step:20671 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.022954]\n",
      "epoch:26 step:20672 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.012956]\n",
      "epoch:26 step:20673 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.005188]\n",
      "epoch:26 step:20674 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.010016]\n",
      "epoch:26 step:20675 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.003236]\n",
      "epoch:26 step:20676 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.050897]\n",
      "epoch:26 step:20677 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.017721]\n",
      "epoch:26 step:20678 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:26 step:20679 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.017901]\n",
      "epoch:26 step:20680 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.001556]\n",
      "epoch:26 step:20681 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.003486]\n",
      "epoch:26 step:20682 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.004664]\n",
      "epoch:26 step:20683 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.017115]\n",
      "epoch:26 step:20684 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:26 step:20685 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001758]\n",
      "epoch:26 step:20686 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.003385]\n",
      "epoch:26 step:20687 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001567]\n",
      "epoch:26 step:20688 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:26 step:20689 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.016921]\n",
      "epoch:26 step:20690 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:26 step:20691 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002802]\n",
      "epoch:26 step:20692 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:26 step:20693 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.004212]\n",
      "epoch:26 step:20694 [D loss: 0.001219, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:26 step:20695 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:26 step:20696 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003934]\n",
      "epoch:26 step:20697 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001274]\n",
      "epoch:26 step:20698 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:26 step:20699 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:26 step:20700 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:26 step:20701 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002688]\n",
      "epoch:26 step:20702 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:26 step:20703 [D loss: 0.007527, acc.: 99.22%] [G loss: 0.000382]\n",
      "epoch:26 step:20704 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:26 step:20705 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20706 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:26 step:20707 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:26 step:20708 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:26 step:20709 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:26 step:20710 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:26 step:20711 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:26 step:20712 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001157]\n",
      "epoch:26 step:20713 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001481]\n",
      "epoch:26 step:20714 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:26 step:20715 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.005594]\n",
      "epoch:26 step:20716 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:26 step:20717 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:26 step:20718 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:26 step:20719 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:26 step:20720 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:26 step:20721 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:26 step:20722 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:26 step:20723 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:26 step:20724 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:26 step:20725 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:26 step:20726 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:26 step:20727 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004710]\n",
      "epoch:26 step:20728 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:26 step:20729 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:26 step:20730 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20731 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:26 step:20732 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:26 step:20733 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:26 step:20734 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:26 step:20735 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:26 step:20736 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:26 step:20737 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:26 step:20738 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:26 step:20739 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:26 step:20740 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:26 step:20741 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:26 step:20742 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:26 step:20743 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:26 step:20744 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:26 step:20745 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:26 step:20746 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:26 step:20747 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:26 step:20748 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:26 step:20749 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:26 step:20750 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001109]\n",
      "epoch:26 step:20751 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:26 step:20752 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:26 step:20753 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20754 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:26 step:20755 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20756 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:26 step:20757 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:26 step:20758 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:26 step:20759 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:26 step:20760 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:26 step:20761 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20762 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:26 step:20763 [D loss: 0.018389, acc.: 99.22%] [G loss: 0.000876]\n",
      "epoch:26 step:20764 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:26 step:20765 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.005108]\n",
      "epoch:26 step:20766 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.002683]\n",
      "epoch:26 step:20767 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.111928]\n",
      "epoch:26 step:20768 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:26 step:20769 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.136856]\n",
      "epoch:26 step:20770 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.127350]\n",
      "epoch:26 step:20771 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:26 step:20772 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.006054]\n",
      "epoch:26 step:20773 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000997]\n",
      "epoch:26 step:20774 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001055]\n",
      "epoch:26 step:20775 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:26 step:20776 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002432]\n",
      "epoch:26 step:20777 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000671]\n",
      "epoch:26 step:20778 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:26 step:20779 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.000986]\n",
      "epoch:26 step:20780 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000477]\n",
      "epoch:26 step:20781 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:26 step:20782 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:26 step:20783 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:26 step:20784 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20785 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001109]\n",
      "epoch:26 step:20786 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:26 step:20787 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000683]\n",
      "epoch:26 step:20788 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:26 step:20789 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001366]\n",
      "epoch:26 step:20790 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:26 step:20791 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.002141]\n",
      "epoch:26 step:20792 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000503]\n",
      "epoch:26 step:20793 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:26 step:20794 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000604]\n",
      "epoch:26 step:20795 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:26 step:20796 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.000330]\n",
      "epoch:26 step:20797 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:26 step:20798 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:26 step:20799 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:26 step:20800 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002023]\n",
      "epoch:26 step:20801 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:26 step:20802 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:26 step:20803 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:26 step:20804 [D loss: 0.002607, acc.: 100.00%] [G loss: 0.000909]\n",
      "epoch:26 step:20805 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:26 step:20806 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:26 step:20807 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:26 step:20808 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.005521]\n",
      "epoch:26 step:20809 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:26 step:20810 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:26 step:20811 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:26 step:20812 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001717]\n",
      "epoch:26 step:20813 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:26 step:20814 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:26 step:20815 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.002008]\n",
      "epoch:26 step:20816 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20817 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001095]\n",
      "epoch:26 step:20818 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:26 step:20819 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:26 step:20820 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:26 step:20821 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001398]\n",
      "epoch:26 step:20822 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001215]\n",
      "epoch:26 step:20823 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:26 step:20824 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002438]\n",
      "epoch:26 step:20825 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:26 step:20826 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:26 step:20827 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:26 step:20828 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:26 step:20829 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:26 step:20830 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:26 step:20831 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002512]\n",
      "epoch:26 step:20832 [D loss: 0.011896, acc.: 99.22%] [G loss: 0.000031]\n",
      "epoch:26 step:20833 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:26 step:20834 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:26 step:20835 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:26 step:20836 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20837 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20838 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20839 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:26 step:20840 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20841 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20842 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:26 step:20843 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20844 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:26 step:20845 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20846 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:26 step:20847 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:26 step:20848 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:26 step:20849 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:26 step:20850 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:26 step:20851 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20852 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20853 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:26 step:20854 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:26 step:20855 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:26 step:20856 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20857 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20858 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:26 step:20859 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:26 step:20860 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20861 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:26 step:20862 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:26 step:20863 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:26 step:20864 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20865 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20866 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20867 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20868 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:26 step:20869 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.001498]\n",
      "epoch:26 step:20870 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:26 step:20871 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20872 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.002386]\n",
      "epoch:26 step:20873 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:26 step:20874 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:26 step:20875 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20876 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:26 step:20877 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:26 step:20878 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20879 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:26 step:20880 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20881 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20882 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20883 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20884 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:26 step:20885 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:26 step:20886 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20887 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20888 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:20889 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:26 step:20890 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:26 step:20891 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:26 step:20892 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20893 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:26 step:20894 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20895 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20896 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:26 step:20897 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:26 step:20898 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20899 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20900 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:26 step:20901 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20902 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:26 step:20903 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:26 step:20904 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:26 step:20905 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20906 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20907 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:26 step:20908 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20909 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:26 step:20910 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:26 step:20911 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:26 step:20912 [D loss: 0.003267, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:20913 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:26 step:20914 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:26 step:20915 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:26 step:20916 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:26 step:20917 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20918 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:26 step:20919 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:26 step:20920 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20921 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:26 step:20922 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20923 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:26 step:20924 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:26 step:20925 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:26 step:20926 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:26 step:20927 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:26 step:20928 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:20929 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:20930 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:26 step:20931 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:20932 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:26 step:20933 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:26 step:20934 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:26 step:20935 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:26 step:20936 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:26 step:20937 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:26 step:20938 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:26 step:20939 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:20940 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20941 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:26 step:20942 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20943 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:26 step:20944 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:26 step:20945 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:26 step:20946 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:26 step:20947 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:26 step:20948 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:26 step:20949 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:26 step:20950 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:20951 [D loss: 0.025268, acc.: 100.00%] [G loss: 0.013945]\n",
      "epoch:26 step:20952 [D loss: 0.004990, acc.: 100.00%] [G loss: 1.471693]\n",
      "epoch:26 step:20953 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.028557]\n",
      "epoch:26 step:20954 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.026373]\n",
      "epoch:26 step:20955 [D loss: 0.016120, acc.: 99.22%] [G loss: 0.000326]\n",
      "epoch:26 step:20956 [D loss: 0.013827, acc.: 100.00%] [G loss: 0.001983]\n",
      "epoch:26 step:20957 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.045947]\n",
      "epoch:26 step:20958 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.178470]\n",
      "epoch:26 step:20959 [D loss: 0.002559, acc.: 100.00%] [G loss: 0.070800]\n",
      "epoch:26 step:20960 [D loss: 0.001513, acc.: 100.00%] [G loss: 0.085779]\n",
      "epoch:26 step:20961 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.037015]\n",
      "epoch:26 step:20962 [D loss: 1.086645, acc.: 65.62%] [G loss: 10.585591]\n",
      "epoch:26 step:20963 [D loss: 6.457537, acc.: 50.00%] [G loss: 6.015727]\n",
      "epoch:26 step:20964 [D loss: 2.483253, acc.: 49.22%] [G loss: 2.287089]\n",
      "epoch:26 step:20965 [D loss: 1.156843, acc.: 42.97%] [G loss: 1.131536]\n",
      "epoch:26 step:20966 [D loss: 0.500090, acc.: 75.00%] [G loss: 1.506567]\n",
      "epoch:26 step:20967 [D loss: 0.437240, acc.: 83.59%] [G loss: 1.140599]\n",
      "epoch:26 step:20968 [D loss: 0.217175, acc.: 98.44%] [G loss: 1.156301]\n",
      "epoch:26 step:20969 [D loss: 0.263832, acc.: 94.53%] [G loss: 1.416818]\n",
      "epoch:26 step:20970 [D loss: 0.242993, acc.: 94.53%] [G loss: 1.461385]\n",
      "epoch:26 step:20971 [D loss: 0.493050, acc.: 76.56%] [G loss: 2.184476]\n",
      "epoch:26 step:20972 [D loss: 0.204448, acc.: 96.88%] [G loss: 1.104991]\n",
      "epoch:26 step:20973 [D loss: 0.143653, acc.: 96.88%] [G loss: 2.172901]\n",
      "epoch:26 step:20974 [D loss: 0.167553, acc.: 95.31%] [G loss: 0.938328]\n",
      "epoch:26 step:20975 [D loss: 0.199823, acc.: 96.09%] [G loss: 0.723029]\n",
      "epoch:26 step:20976 [D loss: 0.092272, acc.: 99.22%] [G loss: 2.684554]\n",
      "epoch:26 step:20977 [D loss: 0.129863, acc.: 98.44%] [G loss: 0.308951]\n",
      "epoch:26 step:20978 [D loss: 0.033839, acc.: 100.00%] [G loss: 2.917696]\n",
      "epoch:26 step:20979 [D loss: 0.115715, acc.: 99.22%] [G loss: 2.753597]\n",
      "epoch:26 step:20980 [D loss: 0.143629, acc.: 97.66%] [G loss: 0.783752]\n",
      "epoch:26 step:20981 [D loss: 0.062331, acc.: 98.44%] [G loss: 3.482415]\n",
      "epoch:26 step:20982 [D loss: 1.036301, acc.: 57.81%] [G loss: 3.800200]\n",
      "epoch:26 step:20983 [D loss: 1.369087, acc.: 53.91%] [G loss: 1.873967]\n",
      "epoch:26 step:20984 [D loss: 0.703725, acc.: 60.16%] [G loss: 0.353647]\n",
      "epoch:26 step:20985 [D loss: 0.206446, acc.: 92.97%] [G loss: 2.181235]\n",
      "epoch:26 step:20986 [D loss: 0.147668, acc.: 96.88%] [G loss: 2.401491]\n",
      "epoch:26 step:20987 [D loss: 0.102246, acc.: 98.44%] [G loss: 0.036290]\n",
      "epoch:26 step:20988 [D loss: 0.062410, acc.: 100.00%] [G loss: 2.032018]\n",
      "epoch:26 step:20989 [D loss: 0.196662, acc.: 92.97%] [G loss: 0.642631]\n",
      "epoch:26 step:20990 [D loss: 0.082924, acc.: 99.22%] [G loss: 2.696143]\n",
      "epoch:26 step:20991 [D loss: 0.090061, acc.: 99.22%] [G loss: 0.055619]\n",
      "epoch:26 step:20992 [D loss: 0.101340, acc.: 97.66%] [G loss: 0.013682]\n",
      "epoch:26 step:20993 [D loss: 0.128553, acc.: 97.66%] [G loss: 0.873069]\n",
      "epoch:26 step:20994 [D loss: 0.070181, acc.: 100.00%] [G loss: 1.483102]\n",
      "epoch:26 step:20995 [D loss: 0.229480, acc.: 92.19%] [G loss: 2.535936]\n",
      "epoch:26 step:20996 [D loss: 0.323016, acc.: 85.16%] [G loss: 0.008453]\n",
      "epoch:26 step:20997 [D loss: 0.138790, acc.: 93.75%] [G loss: 0.012244]\n",
      "epoch:26 step:20998 [D loss: 0.362580, acc.: 89.06%] [G loss: 0.127013]\n",
      "epoch:26 step:20999 [D loss: 0.461183, acc.: 82.81%] [G loss: 0.081261]\n",
      "epoch:26 step:21000 [D loss: 0.037267, acc.: 97.66%] [G loss: 4.663984]\n",
      "epoch:26 step:21001 [D loss: 0.253019, acc.: 92.19%] [G loss: 0.254870]\n",
      "epoch:26 step:21002 [D loss: 0.275410, acc.: 86.72%] [G loss: 0.083602]\n",
      "epoch:26 step:21003 [D loss: 0.130923, acc.: 94.53%] [G loss: 3.238571]\n",
      "epoch:26 step:21004 [D loss: 0.072623, acc.: 99.22%] [G loss: 0.026795]\n",
      "epoch:26 step:21005 [D loss: 0.057225, acc.: 100.00%] [G loss: 0.009705]\n",
      "epoch:26 step:21006 [D loss: 0.015716, acc.: 100.00%] [G loss: 0.003758]\n",
      "epoch:26 step:21007 [D loss: 0.011286, acc.: 100.00%] [G loss: 3.099039]\n",
      "epoch:26 step:21008 [D loss: 0.017988, acc.: 100.00%] [G loss: 0.002080]\n",
      "epoch:26 step:21009 [D loss: 0.122680, acc.: 97.66%] [G loss: 0.001204]\n",
      "epoch:26 step:21010 [D loss: 0.146246, acc.: 98.44%] [G loss: 0.001302]\n",
      "epoch:26 step:21011 [D loss: 0.006666, acc.: 100.00%] [G loss: 2.837080]\n",
      "epoch:26 step:21012 [D loss: 0.035805, acc.: 99.22%] [G loss: 3.573360]\n",
      "epoch:26 step:21013 [D loss: 0.057980, acc.: 99.22%] [G loss: 0.083238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:21014 [D loss: 0.138266, acc.: 95.31%] [G loss: 0.038745]\n",
      "epoch:26 step:21015 [D loss: 0.240579, acc.: 87.50%] [G loss: 0.001830]\n",
      "epoch:26 step:21016 [D loss: 0.013260, acc.: 100.00%] [G loss: 0.003789]\n",
      "epoch:26 step:21017 [D loss: 0.012265, acc.: 100.00%] [G loss: 0.867303]\n",
      "epoch:26 step:21018 [D loss: 0.026789, acc.: 100.00%] [G loss: 0.102466]\n",
      "epoch:26 step:21019 [D loss: 0.020368, acc.: 100.00%] [G loss: 0.830320]\n",
      "epoch:26 step:21020 [D loss: 0.014575, acc.: 100.00%] [G loss: 0.001699]\n",
      "epoch:26 step:21021 [D loss: 0.012938, acc.: 100.00%] [G loss: 0.076578]\n",
      "epoch:26 step:21022 [D loss: 0.081723, acc.: 97.66%] [G loss: 2.195802]\n",
      "epoch:26 step:21023 [D loss: 0.034308, acc.: 99.22%] [G loss: 0.022042]\n",
      "epoch:26 step:21024 [D loss: 0.098741, acc.: 97.66%] [G loss: 0.000437]\n",
      "epoch:26 step:21025 [D loss: 0.014926, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:26 step:21026 [D loss: 0.004827, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:26 step:21027 [D loss: 0.002455, acc.: 100.00%] [G loss: 0.002261]\n",
      "epoch:26 step:21028 [D loss: 0.003410, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:26 step:21029 [D loss: 0.003505, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:26 step:21030 [D loss: 0.004200, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:26 step:21031 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.015139]\n",
      "epoch:26 step:21032 [D loss: 0.002336, acc.: 100.00%] [G loss: 0.000673]\n",
      "epoch:26 step:21033 [D loss: 0.009904, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:26 step:21034 [D loss: 0.001855, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:26 step:21035 [D loss: 0.005885, acc.: 100.00%] [G loss: 0.001655]\n",
      "epoch:26 step:21036 [D loss: 0.039339, acc.: 97.66%] [G loss: 0.001198]\n",
      "epoch:26 step:21037 [D loss: 0.001073, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:26 step:21038 [D loss: 0.010255, acc.: 100.00%] [G loss: 0.002538]\n",
      "epoch:26 step:21039 [D loss: 0.003944, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:26 step:21040 [D loss: 0.007272, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:26 step:21041 [D loss: 0.005256, acc.: 100.00%] [G loss: 0.001020]\n",
      "epoch:26 step:21042 [D loss: 0.009952, acc.: 100.00%] [G loss: 0.001020]\n",
      "epoch:26 step:21043 [D loss: 0.029286, acc.: 99.22%] [G loss: 0.253860]\n",
      "epoch:26 step:21044 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.458176]\n",
      "epoch:26 step:21045 [D loss: 0.028353, acc.: 99.22%] [G loss: 0.000429]\n",
      "epoch:26 step:21046 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:26 step:21047 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:26 step:21048 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:26 step:21049 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:26 step:21050 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:26 step:21051 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:26 step:21052 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:26 step:21053 [D loss: 0.001491, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:26 step:21054 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.003134]\n",
      "epoch:26 step:21055 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:26 step:21056 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:26 step:21057 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:26 step:21058 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:26 step:21059 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:26 step:21060 [D loss: 0.003385, acc.: 100.00%] [G loss: 0.000910]\n",
      "epoch:26 step:21061 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:26 step:21062 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:26 step:21063 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:26 step:21064 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:26 step:21065 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:26 step:21066 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000678]\n",
      "epoch:26 step:21067 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:26 step:21068 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:26 step:21069 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:26 step:21070 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:26 step:21071 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:26 step:21072 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.000833]\n",
      "epoch:26 step:21073 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:26 step:21074 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:26 step:21075 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:26 step:21076 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:26 step:21077 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:26 step:21078 [D loss: 0.002667, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:26 step:21079 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:26 step:21080 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:26 step:21081 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:26 step:21082 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:26 step:21083 [D loss: 0.001415, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:26 step:21084 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:26 step:21085 [D loss: 0.001186, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:26 step:21086 [D loss: 0.017778, acc.: 100.00%] [G loss: 0.007354]\n",
      "epoch:26 step:21087 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:27 step:21088 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:27 step:21089 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.006939]\n",
      "epoch:27 step:21090 [D loss: 0.004866, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:27 step:21091 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.001388]\n",
      "epoch:27 step:21092 [D loss: 0.000904, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:27 step:21093 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.002602]\n",
      "epoch:27 step:21094 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.001037]\n",
      "epoch:27 step:21095 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:27 step:21096 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:27 step:21097 [D loss: 0.000882, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:27 step:21098 [D loss: 0.001423, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:27 step:21099 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.001212]\n",
      "epoch:27 step:21100 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:27 step:21101 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:27 step:21102 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.001324]\n",
      "epoch:27 step:21103 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000854]\n",
      "epoch:27 step:21104 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:27 step:21105 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000710]\n",
      "epoch:27 step:21106 [D loss: 0.002006, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:27 step:21107 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:27 step:21108 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:27 step:21109 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.002924]\n",
      "epoch:27 step:21110 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.001672]\n",
      "epoch:27 step:21111 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.001653]\n",
      "epoch:27 step:21112 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:27 step:21113 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:27 step:21114 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:27 step:21115 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.001266]\n",
      "epoch:27 step:21116 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:27 step:21117 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:27 step:21118 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:27 step:21119 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:27 step:21120 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:27 step:21121 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000725]\n",
      "epoch:27 step:21122 [D loss: 0.009557, acc.: 99.22%] [G loss: 0.001150]\n",
      "epoch:27 step:21123 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:27 step:21124 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:27 step:21125 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:27 step:21126 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21127 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.001311]\n",
      "epoch:27 step:21128 [D loss: 0.009055, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:27 step:21129 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:27 step:21130 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001594]\n",
      "epoch:27 step:21131 [D loss: 0.032780, acc.: 98.44%] [G loss: 0.000219]\n",
      "epoch:27 step:21132 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:27 step:21133 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:27 step:21134 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:27 step:21135 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:27 step:21136 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:27 step:21137 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:27 step:21138 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:27 step:21139 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:27 step:21140 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:27 step:21141 [D loss: 0.006500, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:27 step:21142 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:27 step:21143 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:27 step:21144 [D loss: 0.001388, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:27 step:21145 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:27 step:21146 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:27 step:21147 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:27 step:21148 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002363]\n",
      "epoch:27 step:21149 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000468]\n",
      "epoch:27 step:21150 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.001152]\n",
      "epoch:27 step:21151 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:27 step:21152 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:27 step:21153 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.001184]\n",
      "epoch:27 step:21154 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:27 step:21155 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:27 step:21156 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:27 step:21157 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:27 step:21158 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:27 step:21159 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:27 step:21160 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:27 step:21161 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:27 step:21162 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:27 step:21163 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:27 step:21164 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:27 step:21165 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:27 step:21166 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:27 step:21167 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:27 step:21168 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:27 step:21169 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:27 step:21170 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:27 step:21171 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:27 step:21172 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:27 step:21173 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:27 step:21174 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:27 step:21175 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:27 step:21176 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:27 step:21177 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:27 step:21178 [D loss: 0.039080, acc.: 99.22%] [G loss: 0.000850]\n",
      "epoch:27 step:21179 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.094741]\n",
      "epoch:27 step:21180 [D loss: 0.074323, acc.: 96.88%] [G loss: 0.269990]\n",
      "epoch:27 step:21181 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.150655]\n",
      "epoch:27 step:21182 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:27 step:21183 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:27 step:21184 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:27 step:21185 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.032619]\n",
      "epoch:27 step:21186 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.003886]\n",
      "epoch:27 step:21187 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:27 step:21188 [D loss: 0.002259, acc.: 100.00%] [G loss: 0.000546]\n",
      "epoch:27 step:21189 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.031511]\n",
      "epoch:27 step:21190 [D loss: 0.003389, acc.: 100.00%] [G loss: 0.023587]\n",
      "epoch:27 step:21191 [D loss: 0.021854, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:27 step:21192 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:27 step:21193 [D loss: 0.001112, acc.: 100.00%] [G loss: 0.670148]\n",
      "epoch:27 step:21194 [D loss: 0.147694, acc.: 93.75%] [G loss: 0.000473]\n",
      "epoch:27 step:21195 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.198654]\n",
      "epoch:27 step:21196 [D loss: 0.332113, acc.: 82.03%] [G loss: 0.000000]\n",
      "epoch:27 step:21197 [D loss: 3.698364, acc.: 50.78%] [G loss: 7.039219]\n",
      "epoch:27 step:21198 [D loss: 2.258862, acc.: 50.78%] [G loss: 2.279074]\n",
      "epoch:27 step:21199 [D loss: 1.293221, acc.: 52.34%] [G loss: 1.248008]\n",
      "epoch:27 step:21200 [D loss: 0.557396, acc.: 66.41%] [G loss: 0.680605]\n",
      "epoch:27 step:21201 [D loss: 0.378737, acc.: 83.59%] [G loss: 1.120261]\n",
      "epoch:27 step:21202 [D loss: 0.288139, acc.: 92.97%] [G loss: 0.256301]\n",
      "epoch:27 step:21203 [D loss: 0.267510, acc.: 94.53%] [G loss: 0.213308]\n",
      "epoch:27 step:21204 [D loss: 0.144721, acc.: 100.00%] [G loss: 1.764846]\n",
      "epoch:27 step:21205 [D loss: 0.196941, acc.: 96.09%] [G loss: 0.115834]\n",
      "epoch:27 step:21206 [D loss: 0.168913, acc.: 97.66%] [G loss: 1.756778]\n",
      "epoch:27 step:21207 [D loss: 0.107908, acc.: 100.00%] [G loss: 0.194247]\n",
      "epoch:27 step:21208 [D loss: 0.084743, acc.: 98.44%] [G loss: 0.213919]\n",
      "epoch:27 step:21209 [D loss: 0.111107, acc.: 98.44%] [G loss: 2.198695]\n",
      "epoch:27 step:21210 [D loss: 0.140303, acc.: 97.66%] [G loss: 0.102375]\n",
      "epoch:27 step:21211 [D loss: 0.122416, acc.: 98.44%] [G loss: 0.113404]\n",
      "epoch:27 step:21212 [D loss: 0.069344, acc.: 99.22%] [G loss: 0.036891]\n",
      "epoch:27 step:21213 [D loss: 0.044017, acc.: 100.00%] [G loss: 0.060769]\n",
      "epoch:27 step:21214 [D loss: 0.038283, acc.: 99.22%] [G loss: 0.029858]\n",
      "epoch:27 step:21215 [D loss: 0.054807, acc.: 99.22%] [G loss: 2.862323]\n",
      "epoch:27 step:21216 [D loss: 0.061716, acc.: 100.00%] [G loss: 3.515429]\n",
      "epoch:27 step:21217 [D loss: 0.094542, acc.: 98.44%] [G loss: 2.899935]\n",
      "epoch:27 step:21218 [D loss: 0.099837, acc.: 97.66%] [G loss: 0.345240]\n",
      "epoch:27 step:21219 [D loss: 0.042233, acc.: 100.00%] [G loss: 3.513947]\n",
      "epoch:27 step:21220 [D loss: 0.323728, acc.: 84.38%] [G loss: 0.840481]\n",
      "epoch:27 step:21221 [D loss: 0.366424, acc.: 79.69%] [G loss: 0.032126]\n",
      "epoch:27 step:21222 [D loss: 0.108119, acc.: 95.31%] [G loss: 0.009192]\n",
      "epoch:27 step:21223 [D loss: 0.014652, acc.: 100.00%] [G loss: 0.022603]\n",
      "epoch:27 step:21224 [D loss: 0.022290, acc.: 100.00%] [G loss: 0.009182]\n",
      "epoch:27 step:21225 [D loss: 0.033673, acc.: 100.00%] [G loss: 0.013664]\n",
      "epoch:27 step:21226 [D loss: 0.011442, acc.: 100.00%] [G loss: 4.949592]\n",
      "epoch:27 step:21227 [D loss: 0.021415, acc.: 100.00%] [G loss: 0.018563]\n",
      "epoch:27 step:21228 [D loss: 0.041142, acc.: 99.22%] [G loss: 0.169150]\n",
      "epoch:27 step:21229 [D loss: 0.031670, acc.: 98.44%] [G loss: 0.035058]\n",
      "epoch:27 step:21230 [D loss: 0.060620, acc.: 99.22%] [G loss: 0.066778]\n",
      "epoch:27 step:21231 [D loss: 0.026242, acc.: 100.00%] [G loss: 0.046347]\n",
      "epoch:27 step:21232 [D loss: 0.025698, acc.: 100.00%] [G loss: 0.137869]\n",
      "epoch:27 step:21233 [D loss: 0.129486, acc.: 96.88%] [G loss: 6.442061]\n",
      "epoch:27 step:21234 [D loss: 0.015256, acc.: 100.00%] [G loss: 6.514363]\n",
      "epoch:27 step:21235 [D loss: 0.132323, acc.: 92.19%] [G loss: 0.008617]\n",
      "epoch:27 step:21236 [D loss: 0.025840, acc.: 100.00%] [G loss: 3.405938]\n",
      "epoch:27 step:21237 [D loss: 0.069493, acc.: 97.66%] [G loss: 0.086270]\n",
      "epoch:27 step:21238 [D loss: 0.032062, acc.: 100.00%] [G loss: 0.286315]\n",
      "epoch:27 step:21239 [D loss: 0.052397, acc.: 98.44%] [G loss: 0.077406]\n",
      "epoch:27 step:21240 [D loss: 0.047483, acc.: 100.00%] [G loss: 0.037599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21241 [D loss: 0.018684, acc.: 99.22%] [G loss: 0.029130]\n",
      "epoch:27 step:21242 [D loss: 0.007368, acc.: 100.00%] [G loss: 0.013015]\n",
      "epoch:27 step:21243 [D loss: 0.044851, acc.: 100.00%] [G loss: 0.028387]\n",
      "epoch:27 step:21244 [D loss: 0.018656, acc.: 100.00%] [G loss: 0.014990]\n",
      "epoch:27 step:21245 [D loss: 0.041161, acc.: 99.22%] [G loss: 0.011183]\n",
      "epoch:27 step:21246 [D loss: 0.017566, acc.: 100.00%] [G loss: 0.006958]\n",
      "epoch:27 step:21247 [D loss: 0.024186, acc.: 100.00%] [G loss: 0.125326]\n",
      "epoch:27 step:21248 [D loss: 0.056577, acc.: 99.22%] [G loss: 0.015290]\n",
      "epoch:27 step:21249 [D loss: 0.005026, acc.: 100.00%] [G loss: 5.457674]\n",
      "epoch:27 step:21250 [D loss: 0.021297, acc.: 100.00%] [G loss: 1.756726]\n",
      "epoch:27 step:21251 [D loss: 0.525387, acc.: 73.44%] [G loss: 7.759449]\n",
      "epoch:27 step:21252 [D loss: 2.002223, acc.: 50.78%] [G loss: 5.685104]\n",
      "epoch:27 step:21253 [D loss: 0.230838, acc.: 90.62%] [G loss: 0.170084]\n",
      "epoch:27 step:21254 [D loss: 0.266262, acc.: 86.72%] [G loss: 4.552098]\n",
      "epoch:27 step:21255 [D loss: 0.069673, acc.: 98.44%] [G loss: 0.763288]\n",
      "epoch:27 step:21256 [D loss: 0.337441, acc.: 87.50%] [G loss: 4.437665]\n",
      "epoch:27 step:21257 [D loss: 0.234655, acc.: 89.84%] [G loss: 3.066380]\n",
      "epoch:27 step:21258 [D loss: 0.049665, acc.: 100.00%] [G loss: 0.062850]\n",
      "epoch:27 step:21259 [D loss: 0.020060, acc.: 100.00%] [G loss: 0.101951]\n",
      "epoch:27 step:21260 [D loss: 0.033080, acc.: 100.00%] [G loss: 0.892332]\n",
      "epoch:27 step:21261 [D loss: 0.044755, acc.: 99.22%] [G loss: 0.096927]\n",
      "epoch:27 step:21262 [D loss: 0.012167, acc.: 100.00%] [G loss: 0.341809]\n",
      "epoch:27 step:21263 [D loss: 0.022855, acc.: 100.00%] [G loss: 0.019403]\n",
      "epoch:27 step:21264 [D loss: 0.068396, acc.: 97.66%] [G loss: 0.068841]\n",
      "epoch:27 step:21265 [D loss: 0.005905, acc.: 100.00%] [G loss: 0.819347]\n",
      "epoch:27 step:21266 [D loss: 0.193391, acc.: 93.75%] [G loss: 0.003712]\n",
      "epoch:27 step:21267 [D loss: 0.034190, acc.: 98.44%] [G loss: 0.002885]\n",
      "epoch:27 step:21268 [D loss: 0.003014, acc.: 100.00%] [G loss: 0.015270]\n",
      "epoch:27 step:21269 [D loss: 0.003512, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:27 step:21270 [D loss: 0.007290, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:27 step:21271 [D loss: 0.005101, acc.: 100.00%] [G loss: 0.001095]\n",
      "epoch:27 step:21272 [D loss: 0.007286, acc.: 100.00%] [G loss: 0.055089]\n",
      "epoch:27 step:21273 [D loss: 0.075671, acc.: 99.22%] [G loss: 0.001583]\n",
      "epoch:27 step:21274 [D loss: 0.006979, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:27 step:21275 [D loss: 0.002423, acc.: 100.00%] [G loss: 0.133034]\n",
      "epoch:27 step:21276 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.212956]\n",
      "epoch:27 step:21277 [D loss: 0.003306, acc.: 100.00%] [G loss: 0.006152]\n",
      "epoch:27 step:21278 [D loss: 0.018081, acc.: 100.00%] [G loss: 0.085126]\n",
      "epoch:27 step:21279 [D loss: 0.014487, acc.: 100.00%] [G loss: 0.003907]\n",
      "epoch:27 step:21280 [D loss: 0.007037, acc.: 100.00%] [G loss: 0.012207]\n",
      "epoch:27 step:21281 [D loss: 0.013567, acc.: 100.00%] [G loss: 0.036659]\n",
      "epoch:27 step:21282 [D loss: 0.003380, acc.: 100.00%] [G loss: 0.001185]\n",
      "epoch:27 step:21283 [D loss: 0.004347, acc.: 100.00%] [G loss: 0.004989]\n",
      "epoch:27 step:21284 [D loss: 0.024189, acc.: 98.44%] [G loss: 0.001192]\n",
      "epoch:27 step:21285 [D loss: 0.003665, acc.: 100.00%] [G loss: 0.000769]\n",
      "epoch:27 step:21286 [D loss: 0.001736, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:27 step:21287 [D loss: 0.001701, acc.: 100.00%] [G loss: 0.001577]\n",
      "epoch:27 step:21288 [D loss: 0.001787, acc.: 100.00%] [G loss: 0.003440]\n",
      "epoch:27 step:21289 [D loss: 0.001487, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:27 step:21290 [D loss: 0.001460, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:27 step:21291 [D loss: 0.002125, acc.: 100.00%] [G loss: 0.001588]\n",
      "epoch:27 step:21292 [D loss: 0.005109, acc.: 100.00%] [G loss: 0.004546]\n",
      "epoch:27 step:21293 [D loss: 0.003845, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:27 step:21294 [D loss: 0.002749, acc.: 100.00%] [G loss: 0.002443]\n",
      "epoch:27 step:21295 [D loss: 0.002166, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:27 step:21296 [D loss: 0.025948, acc.: 98.44%] [G loss: 0.000773]\n",
      "epoch:27 step:21297 [D loss: 0.003893, acc.: 100.00%] [G loss: 0.000662]\n",
      "epoch:27 step:21298 [D loss: 0.002947, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:27 step:21299 [D loss: 0.005823, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:27 step:21300 [D loss: 0.001798, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:27 step:21301 [D loss: 0.002969, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:27 step:21302 [D loss: 0.003224, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:27 step:21303 [D loss: 0.006768, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:27 step:21304 [D loss: 0.002373, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:27 step:21305 [D loss: 0.022910, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:27 step:21306 [D loss: 0.017408, acc.: 100.00%] [G loss: 0.001471]\n",
      "epoch:27 step:21307 [D loss: 0.008329, acc.: 100.00%] [G loss: 0.001374]\n",
      "epoch:27 step:21308 [D loss: 0.030155, acc.: 99.22%] [G loss: 0.105453]\n",
      "epoch:27 step:21309 [D loss: 0.003392, acc.: 100.00%] [G loss: 0.014673]\n",
      "epoch:27 step:21310 [D loss: 0.003600, acc.: 100.00%] [G loss: 0.003376]\n",
      "epoch:27 step:21311 [D loss: 0.003247, acc.: 100.00%] [G loss: 0.006626]\n",
      "epoch:27 step:21312 [D loss: 0.006929, acc.: 100.00%] [G loss: 0.060184]\n",
      "epoch:27 step:21313 [D loss: 0.049221, acc.: 98.44%] [G loss: 0.000435]\n",
      "epoch:27 step:21314 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:27 step:21315 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.000543]\n",
      "epoch:27 step:21316 [D loss: 0.002760, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:27 step:21317 [D loss: 0.000715, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:27 step:21318 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.002306]\n",
      "epoch:27 step:21319 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.001847]\n",
      "epoch:27 step:21320 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.001451]\n",
      "epoch:27 step:21321 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:27 step:21322 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:27 step:21323 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.000318]\n",
      "epoch:27 step:21324 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:27 step:21325 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:27 step:21326 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:27 step:21327 [D loss: 0.001709, acc.: 100.00%] [G loss: 0.000802]\n",
      "epoch:27 step:21328 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:27 step:21329 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:27 step:21330 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:27 step:21331 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:27 step:21332 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:27 step:21333 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:27 step:21334 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.002939]\n",
      "epoch:27 step:21335 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:27 step:21336 [D loss: 0.001480, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:27 step:21337 [D loss: 0.002437, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:27 step:21338 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:27 step:21339 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:27 step:21340 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:27 step:21341 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:27 step:21342 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.006753]\n",
      "epoch:27 step:21343 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:27 step:21344 [D loss: 0.001223, acc.: 100.00%] [G loss: 0.014164]\n",
      "epoch:27 step:21345 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.000779]\n",
      "epoch:27 step:21346 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:27 step:21347 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.004721]\n",
      "epoch:27 step:21348 [D loss: 0.009897, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:27 step:21349 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:27 step:21350 [D loss: 0.001494, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:27 step:21351 [D loss: 0.001181, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:27 step:21352 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:27 step:21353 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:27 step:21354 [D loss: 0.002538, acc.: 100.00%] [G loss: 0.000027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21355 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:27 step:21356 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:27 step:21357 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:27 step:21358 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:27 step:21359 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:27 step:21360 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:27 step:21361 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:27 step:21362 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000935]\n",
      "epoch:27 step:21363 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000683]\n",
      "epoch:27 step:21364 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.000476]\n",
      "epoch:27 step:21365 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:27 step:21366 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:27 step:21367 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:27 step:21368 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:27 step:21369 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.001882]\n",
      "epoch:27 step:21370 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:27 step:21371 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:27 step:21372 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.022246]\n",
      "epoch:27 step:21373 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:27 step:21374 [D loss: 0.015604, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:27 step:21375 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:27 step:21376 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000691]\n",
      "epoch:27 step:21377 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:27 step:21378 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:27 step:21379 [D loss: 0.003527, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:27 step:21380 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:27 step:21381 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:27 step:21382 [D loss: 0.007376, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:27 step:21383 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:27 step:21384 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:27 step:21385 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:27 step:21386 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:27 step:21387 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:27 step:21388 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:27 step:21389 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000666]\n",
      "epoch:27 step:21390 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:27 step:21391 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:27 step:21392 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:27 step:21393 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:27 step:21394 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:27 step:21395 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000723]\n",
      "epoch:27 step:21396 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000580]\n",
      "epoch:27 step:21397 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.014990]\n",
      "epoch:27 step:21398 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:27 step:21399 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:27 step:21400 [D loss: 0.000747, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:27 step:21401 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:27 step:21402 [D loss: 0.001355, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:27 step:21403 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:27 step:21404 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:27 step:21405 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.001321]\n",
      "epoch:27 step:21406 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:27 step:21407 [D loss: 0.006767, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:27 step:21408 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:27 step:21409 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:27 step:21410 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:27 step:21411 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:27 step:21412 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:27 step:21413 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:27 step:21414 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:27 step:21415 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:27 step:21416 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:27 step:21417 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.005266]\n",
      "epoch:27 step:21418 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:27 step:21419 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:27 step:21420 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:27 step:21421 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:27 step:21422 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:27 step:21423 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:27 step:21424 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:27 step:21425 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:27 step:21426 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:27 step:21427 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:27 step:21428 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:27 step:21429 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:27 step:21430 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:27 step:21431 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:27 step:21432 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:27 step:21433 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:27 step:21434 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:27 step:21435 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:27 step:21436 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:27 step:21437 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:27 step:21438 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:27 step:21439 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:27 step:21440 [D loss: 0.004344, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:27 step:21441 [D loss: 0.000935, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:27 step:21442 [D loss: 0.001362, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:27 step:21443 [D loss: 0.088555, acc.: 97.66%] [G loss: 0.003076]\n",
      "epoch:27 step:21444 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.149195]\n",
      "epoch:27 step:21445 [D loss: 0.031412, acc.: 100.00%] [G loss: 0.078139]\n",
      "epoch:27 step:21446 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.000561]\n",
      "epoch:27 step:21447 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.005225]\n",
      "epoch:27 step:21448 [D loss: 0.033489, acc.: 99.22%] [G loss: 0.197886]\n",
      "epoch:27 step:21449 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:27 step:21450 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:27 step:21451 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:27 step:21452 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:27 step:21453 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:27 step:21454 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:27 step:21455 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:27 step:21456 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:27 step:21457 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:27 step:21458 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:27 step:21459 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:27 step:21460 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:27 step:21461 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.002878]\n",
      "epoch:27 step:21462 [D loss: 0.002256, acc.: 100.00%] [G loss: 0.000712]\n",
      "epoch:27 step:21463 [D loss: 0.001555, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:27 step:21464 [D loss: 0.053520, acc.: 99.22%] [G loss: 0.001000]\n",
      "epoch:27 step:21465 [D loss: 0.005421, acc.: 100.00%] [G loss: 0.019953]\n",
      "epoch:27 step:21466 [D loss: 0.001581, acc.: 100.00%] [G loss: 0.010112]\n",
      "epoch:27 step:21467 [D loss: 0.015354, acc.: 100.00%] [G loss: 0.004255]\n",
      "epoch:27 step:21468 [D loss: 0.223746, acc.: 88.28%] [G loss: 7.333098]\n",
      "epoch:27 step:21469 [D loss: 1.830708, acc.: 49.22%] [G loss: 2.678455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21470 [D loss: 0.387125, acc.: 81.25%] [G loss: 4.528031]\n",
      "epoch:27 step:21471 [D loss: 0.099963, acc.: 96.09%] [G loss: 3.721407]\n",
      "epoch:27 step:21472 [D loss: 0.162577, acc.: 92.97%] [G loss: 1.321937]\n",
      "epoch:27 step:21473 [D loss: 0.197142, acc.: 92.19%] [G loss: 1.078867]\n",
      "epoch:27 step:21474 [D loss: 0.017248, acc.: 99.22%] [G loss: 1.397902]\n",
      "epoch:27 step:21475 [D loss: 1.542027, acc.: 46.88%] [G loss: 4.718534]\n",
      "epoch:27 step:21476 [D loss: 0.208793, acc.: 91.41%] [G loss: 5.544353]\n",
      "epoch:27 step:21477 [D loss: 0.177557, acc.: 91.41%] [G loss: 4.326798]\n",
      "epoch:27 step:21478 [D loss: 0.114957, acc.: 96.88%] [G loss: 0.059429]\n",
      "epoch:27 step:21479 [D loss: 0.009931, acc.: 100.00%] [G loss: 2.489552]\n",
      "epoch:27 step:21480 [D loss: 0.008653, acc.: 100.00%] [G loss: 1.712176]\n",
      "epoch:27 step:21481 [D loss: 0.008629, acc.: 100.00%] [G loss: 2.582445]\n",
      "epoch:27 step:21482 [D loss: 0.364173, acc.: 85.16%] [G loss: 1.370077]\n",
      "epoch:27 step:21483 [D loss: 0.260883, acc.: 88.28%] [G loss: 2.246245]\n",
      "epoch:27 step:21484 [D loss: 0.065598, acc.: 99.22%] [G loss: 1.420868]\n",
      "epoch:27 step:21485 [D loss: 0.117972, acc.: 97.66%] [G loss: 0.006020]\n",
      "epoch:27 step:21486 [D loss: 0.041328, acc.: 99.22%] [G loss: 0.011371]\n",
      "epoch:27 step:21487 [D loss: 0.026668, acc.: 99.22%] [G loss: 0.008208]\n",
      "epoch:27 step:21488 [D loss: 0.086048, acc.: 96.88%] [G loss: 0.489268]\n",
      "epoch:27 step:21489 [D loss: 0.044658, acc.: 99.22%] [G loss: 0.259238]\n",
      "epoch:27 step:21490 [D loss: 0.012550, acc.: 100.00%] [G loss: 0.060579]\n",
      "epoch:27 step:21491 [D loss: 0.010093, acc.: 100.00%] [G loss: 0.128453]\n",
      "epoch:27 step:21492 [D loss: 0.104064, acc.: 99.22%] [G loss: 0.031133]\n",
      "epoch:27 step:21493 [D loss: 0.018938, acc.: 100.00%] [G loss: 0.087296]\n",
      "epoch:27 step:21494 [D loss: 0.005865, acc.: 100.00%] [G loss: 0.536515]\n",
      "epoch:27 step:21495 [D loss: 0.006464, acc.: 100.00%] [G loss: 0.142250]\n",
      "epoch:27 step:21496 [D loss: 0.021096, acc.: 100.00%] [G loss: 0.048161]\n",
      "epoch:27 step:21497 [D loss: 0.007216, acc.: 100.00%] [G loss: 0.052985]\n",
      "epoch:27 step:21498 [D loss: 0.135908, acc.: 95.31%] [G loss: 0.053091]\n",
      "epoch:27 step:21499 [D loss: 0.019571, acc.: 100.00%] [G loss: 0.034535]\n",
      "epoch:27 step:21500 [D loss: 0.004543, acc.: 100.00%] [G loss: 0.013601]\n",
      "epoch:27 step:21501 [D loss: 0.003059, acc.: 100.00%] [G loss: 0.005607]\n",
      "epoch:27 step:21502 [D loss: 0.005928, acc.: 100.00%] [G loss: 0.033790]\n",
      "epoch:27 step:21503 [D loss: 0.007341, acc.: 100.00%] [G loss: 0.028207]\n",
      "epoch:27 step:21504 [D loss: 0.006001, acc.: 100.00%] [G loss: 0.081084]\n",
      "epoch:27 step:21505 [D loss: 0.027289, acc.: 100.00%] [G loss: 0.043223]\n",
      "epoch:27 step:21506 [D loss: 0.001880, acc.: 100.00%] [G loss: 0.069197]\n",
      "epoch:27 step:21507 [D loss: 0.024638, acc.: 99.22%] [G loss: 0.253830]\n",
      "epoch:27 step:21508 [D loss: 0.001111, acc.: 100.00%] [G loss: 0.028474]\n",
      "epoch:27 step:21509 [D loss: 0.005080, acc.: 100.00%] [G loss: 0.006682]\n",
      "epoch:27 step:21510 [D loss: 0.000845, acc.: 100.00%] [G loss: 0.015091]\n",
      "epoch:27 step:21511 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.052453]\n",
      "epoch:27 step:21512 [D loss: 0.005025, acc.: 100.00%] [G loss: 0.010893]\n",
      "epoch:27 step:21513 [D loss: 0.008846, acc.: 100.00%] [G loss: 0.007281]\n",
      "epoch:27 step:21514 [D loss: 0.001120, acc.: 100.00%] [G loss: 0.009495]\n",
      "epoch:27 step:21515 [D loss: 0.009259, acc.: 100.00%] [G loss: 0.004233]\n",
      "epoch:27 step:21516 [D loss: 0.006473, acc.: 100.00%] [G loss: 0.009193]\n",
      "epoch:27 step:21517 [D loss: 0.143322, acc.: 91.41%] [G loss: 0.000607]\n",
      "epoch:27 step:21518 [D loss: 0.267594, acc.: 92.97%] [G loss: 0.156966]\n",
      "epoch:27 step:21519 [D loss: 0.031904, acc.: 98.44%] [G loss: 0.422496]\n",
      "epoch:27 step:21520 [D loss: 0.001510, acc.: 100.00%] [G loss: 3.194570]\n",
      "epoch:27 step:21521 [D loss: 0.003797, acc.: 100.00%] [G loss: 0.257758]\n",
      "epoch:27 step:21522 [D loss: 0.004278, acc.: 100.00%] [G loss: 0.279723]\n",
      "epoch:27 step:21523 [D loss: 0.042624, acc.: 99.22%] [G loss: 0.003734]\n",
      "epoch:27 step:21524 [D loss: 0.105430, acc.: 97.66%] [G loss: 0.212110]\n",
      "epoch:27 step:21525 [D loss: 0.021527, acc.: 99.22%] [G loss: 0.282155]\n",
      "epoch:27 step:21526 [D loss: 0.064443, acc.: 98.44%] [G loss: 0.059355]\n",
      "epoch:27 step:21527 [D loss: 0.000476, acc.: 100.00%] [G loss: 0.033462]\n",
      "epoch:27 step:21528 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.030644]\n",
      "epoch:27 step:21529 [D loss: 0.004066, acc.: 100.00%] [G loss: 0.085334]\n",
      "epoch:27 step:21530 [D loss: 0.002373, acc.: 100.00%] [G loss: 0.004491]\n",
      "epoch:27 step:21531 [D loss: 0.004225, acc.: 100.00%] [G loss: 0.008476]\n",
      "epoch:27 step:21532 [D loss: 0.105412, acc.: 95.31%] [G loss: 0.012101]\n",
      "epoch:27 step:21533 [D loss: 0.064378, acc.: 98.44%] [G loss: 0.074650]\n",
      "epoch:27 step:21534 [D loss: 0.014778, acc.: 100.00%] [G loss: 0.136080]\n",
      "epoch:27 step:21535 [D loss: 0.120131, acc.: 95.31%] [G loss: 0.107701]\n",
      "epoch:27 step:21536 [D loss: 0.037159, acc.: 98.44%] [G loss: 0.029047]\n",
      "epoch:27 step:21537 [D loss: 0.017373, acc.: 100.00%] [G loss: 0.178422]\n",
      "epoch:27 step:21538 [D loss: 0.008974, acc.: 100.00%] [G loss: 0.250094]\n",
      "epoch:27 step:21539 [D loss: 0.004840, acc.: 100.00%] [G loss: 0.018650]\n",
      "epoch:27 step:21540 [D loss: 0.027784, acc.: 100.00%] [G loss: 0.147970]\n",
      "epoch:27 step:21541 [D loss: 0.001554, acc.: 100.00%] [G loss: 0.082910]\n",
      "epoch:27 step:21542 [D loss: 0.002659, acc.: 100.00%] [G loss: 0.041299]\n",
      "epoch:27 step:21543 [D loss: 0.003067, acc.: 100.00%] [G loss: 0.540176]\n",
      "epoch:27 step:21544 [D loss: 0.010818, acc.: 99.22%] [G loss: 0.053628]\n",
      "epoch:27 step:21545 [D loss: 0.003399, acc.: 100.00%] [G loss: 0.027547]\n",
      "epoch:27 step:21546 [D loss: 0.006543, acc.: 100.00%] [G loss: 0.017071]\n",
      "epoch:27 step:21547 [D loss: 0.004543, acc.: 100.00%] [G loss: 0.015461]\n",
      "epoch:27 step:21548 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.008028]\n",
      "epoch:27 step:21549 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.020508]\n",
      "epoch:27 step:21550 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.015009]\n",
      "epoch:27 step:21551 [D loss: 0.020134, acc.: 98.44%] [G loss: 0.003730]\n",
      "epoch:27 step:21552 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.002166]\n",
      "epoch:27 step:21553 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.002465]\n",
      "epoch:27 step:21554 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.003703]\n",
      "epoch:27 step:21555 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.002721]\n",
      "epoch:27 step:21556 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001377]\n",
      "epoch:27 step:21557 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.001765]\n",
      "epoch:27 step:21558 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000916]\n",
      "epoch:27 step:21559 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.785846]\n",
      "epoch:27 step:21560 [D loss: 0.024863, acc.: 100.00%] [G loss: 0.003887]\n",
      "epoch:27 step:21561 [D loss: 0.022448, acc.: 99.22%] [G loss: 0.006903]\n",
      "epoch:27 step:21562 [D loss: 0.003517, acc.: 100.00%] [G loss: 0.005171]\n",
      "epoch:27 step:21563 [D loss: 0.154258, acc.: 94.53%] [G loss: 2.216477]\n",
      "epoch:27 step:21564 [D loss: 0.039129, acc.: 99.22%] [G loss: 2.302567]\n",
      "epoch:27 step:21565 [D loss: 0.475564, acc.: 80.47%] [G loss: 0.050277]\n",
      "epoch:27 step:21566 [D loss: 0.849846, acc.: 69.53%] [G loss: 1.522692]\n",
      "epoch:27 step:21567 [D loss: 0.277154, acc.: 86.72%] [G loss: 5.249559]\n",
      "epoch:27 step:21568 [D loss: 0.640745, acc.: 70.31%] [G loss: 0.197680]\n",
      "epoch:27 step:21569 [D loss: 0.660942, acc.: 71.88%] [G loss: 0.452927]\n",
      "epoch:27 step:21570 [D loss: 0.021591, acc.: 100.00%] [G loss: 2.081458]\n",
      "epoch:27 step:21571 [D loss: 0.651748, acc.: 75.78%] [G loss: 2.793764]\n",
      "epoch:27 step:21572 [D loss: 0.024783, acc.: 99.22%] [G loss: 1.658073]\n",
      "epoch:27 step:21573 [D loss: 0.051982, acc.: 99.22%] [G loss: 0.013313]\n",
      "epoch:27 step:21574 [D loss: 0.030547, acc.: 100.00%] [G loss: 0.314992]\n",
      "epoch:27 step:21575 [D loss: 0.041881, acc.: 100.00%] [G loss: 0.008566]\n",
      "epoch:27 step:21576 [D loss: 0.047687, acc.: 98.44%] [G loss: 0.668604]\n",
      "epoch:27 step:21577 [D loss: 0.081618, acc.: 98.44%] [G loss: 0.087325]\n",
      "epoch:27 step:21578 [D loss: 0.247518, acc.: 90.62%] [G loss: 0.015711]\n",
      "epoch:27 step:21579 [D loss: 0.007712, acc.: 100.00%] [G loss: 0.011229]\n",
      "epoch:27 step:21580 [D loss: 0.013913, acc.: 100.00%] [G loss: 0.004575]\n",
      "epoch:27 step:21581 [D loss: 0.006304, acc.: 100.00%] [G loss: 0.026855]\n",
      "epoch:27 step:21582 [D loss: 0.004398, acc.: 100.00%] [G loss: 0.005622]\n",
      "epoch:27 step:21583 [D loss: 0.022401, acc.: 100.00%] [G loss: 0.038377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21584 [D loss: 0.038097, acc.: 100.00%] [G loss: 0.027191]\n",
      "epoch:27 step:21585 [D loss: 0.069133, acc.: 99.22%] [G loss: 0.022592]\n",
      "epoch:27 step:21586 [D loss: 0.043190, acc.: 100.00%] [G loss: 0.054677]\n",
      "epoch:27 step:21587 [D loss: 0.006033, acc.: 100.00%] [G loss: 0.001538]\n",
      "epoch:27 step:21588 [D loss: 0.032024, acc.: 99.22%] [G loss: 0.006666]\n",
      "epoch:27 step:21589 [D loss: 0.048161, acc.: 100.00%] [G loss: 0.104036]\n",
      "epoch:27 step:21590 [D loss: 0.005873, acc.: 100.00%] [G loss: 0.162344]\n",
      "epoch:27 step:21591 [D loss: 0.071419, acc.: 98.44%] [G loss: 0.069190]\n",
      "epoch:27 step:21592 [D loss: 0.113191, acc.: 97.66%] [G loss: 3.278327]\n",
      "epoch:27 step:21593 [D loss: 0.061815, acc.: 98.44%] [G loss: 1.492159]\n",
      "epoch:27 step:21594 [D loss: 0.053212, acc.: 98.44%] [G loss: 0.097895]\n",
      "epoch:27 step:21595 [D loss: 0.005874, acc.: 100.00%] [G loss: 0.234355]\n",
      "epoch:27 step:21596 [D loss: 0.022816, acc.: 100.00%] [G loss: 0.075882]\n",
      "epoch:27 step:21597 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.024299]\n",
      "epoch:27 step:21598 [D loss: 0.003405, acc.: 100.00%] [G loss: 0.007982]\n",
      "epoch:27 step:21599 [D loss: 0.002965, acc.: 100.00%] [G loss: 0.038690]\n",
      "epoch:27 step:21600 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.003748]\n",
      "epoch:27 step:21601 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.044572]\n",
      "epoch:27 step:21602 [D loss: 0.001672, acc.: 100.00%] [G loss: 0.003189]\n",
      "epoch:27 step:21603 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.011796]\n",
      "epoch:27 step:21604 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.005367]\n",
      "epoch:27 step:21605 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.004507]\n",
      "epoch:27 step:21606 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.026154]\n",
      "epoch:27 step:21607 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000907]\n",
      "epoch:27 step:21608 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.010885]\n",
      "epoch:27 step:21609 [D loss: 0.001521, acc.: 100.00%] [G loss: 0.042095]\n",
      "epoch:27 step:21610 [D loss: 0.000476, acc.: 100.00%] [G loss: 0.006067]\n",
      "epoch:27 step:21611 [D loss: 0.005004, acc.: 100.00%] [G loss: 0.004801]\n",
      "epoch:27 step:21612 [D loss: 0.006779, acc.: 100.00%] [G loss: 0.001949]\n",
      "epoch:27 step:21613 [D loss: 0.044347, acc.: 99.22%] [G loss: 0.000300]\n",
      "epoch:27 step:21614 [D loss: 0.001204, acc.: 100.00%] [G loss: 0.004321]\n",
      "epoch:27 step:21615 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.002017]\n",
      "epoch:27 step:21616 [D loss: 0.001517, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:27 step:21617 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:27 step:21618 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.002127]\n",
      "epoch:27 step:21619 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.032534]\n",
      "epoch:27 step:21620 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.001726]\n",
      "epoch:27 step:21621 [D loss: 0.001247, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:27 step:21622 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001512]\n",
      "epoch:27 step:21623 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:27 step:21624 [D loss: 0.001947, acc.: 100.00%] [G loss: 0.001324]\n",
      "epoch:27 step:21625 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:27 step:21626 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.001395]\n",
      "epoch:27 step:21627 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.000809]\n",
      "epoch:27 step:21628 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:27 step:21629 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:27 step:21630 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.011599]\n",
      "epoch:27 step:21631 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:27 step:21632 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.001457]\n",
      "epoch:27 step:21633 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:27 step:21634 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:27 step:21635 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000875]\n",
      "epoch:27 step:21636 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.001579]\n",
      "epoch:27 step:21637 [D loss: 0.006840, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:27 step:21638 [D loss: 0.004284, acc.: 100.00%] [G loss: 0.001329]\n",
      "epoch:27 step:21639 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:27 step:21640 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.090071]\n",
      "epoch:27 step:21641 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000606]\n",
      "epoch:27 step:21642 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:27 step:21643 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.172323]\n",
      "epoch:27 step:21644 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:27 step:21645 [D loss: 0.078666, acc.: 97.66%] [G loss: 0.165187]\n",
      "epoch:27 step:21646 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.039081]\n",
      "epoch:27 step:21647 [D loss: 0.005853, acc.: 100.00%] [G loss: 0.023118]\n",
      "epoch:27 step:21648 [D loss: 0.003203, acc.: 100.00%] [G loss: 0.272537]\n",
      "epoch:27 step:21649 [D loss: 0.008783, acc.: 100.00%] [G loss: 0.077014]\n",
      "epoch:27 step:21650 [D loss: 0.017275, acc.: 100.00%] [G loss: 0.417679]\n",
      "epoch:27 step:21651 [D loss: 0.004211, acc.: 100.00%] [G loss: 0.016933]\n",
      "epoch:27 step:21652 [D loss: 0.017467, acc.: 100.00%] [G loss: 0.005880]\n",
      "epoch:27 step:21653 [D loss: 0.080283, acc.: 98.44%] [G loss: 0.142547]\n",
      "epoch:27 step:21654 [D loss: 0.073079, acc.: 98.44%] [G loss: 0.398452]\n",
      "epoch:27 step:21655 [D loss: 0.002798, acc.: 100.00%] [G loss: 1.055696]\n",
      "epoch:27 step:21656 [D loss: 0.023160, acc.: 100.00%] [G loss: 0.026083]\n",
      "epoch:27 step:21657 [D loss: 0.026839, acc.: 99.22%] [G loss: 0.020190]\n",
      "epoch:27 step:21658 [D loss: 0.044110, acc.: 99.22%] [G loss: 0.013504]\n",
      "epoch:27 step:21659 [D loss: 0.012417, acc.: 100.00%] [G loss: 0.012367]\n",
      "epoch:27 step:21660 [D loss: 0.009863, acc.: 100.00%] [G loss: 0.002207]\n",
      "epoch:27 step:21661 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.105117]\n",
      "epoch:27 step:21662 [D loss: 0.008225, acc.: 100.00%] [G loss: 0.000857]\n",
      "epoch:27 step:21663 [D loss: 0.005802, acc.: 100.00%] [G loss: 0.001026]\n",
      "epoch:27 step:21664 [D loss: 0.021315, acc.: 99.22%] [G loss: 0.007088]\n",
      "epoch:27 step:21665 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.043826]\n",
      "epoch:27 step:21666 [D loss: 0.001076, acc.: 100.00%] [G loss: 0.002208]\n",
      "epoch:27 step:21667 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.001130]\n",
      "epoch:27 step:21668 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.001107]\n",
      "epoch:27 step:21669 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.010250]\n",
      "epoch:27 step:21670 [D loss: 0.000929, acc.: 100.00%] [G loss: 0.004097]\n",
      "epoch:27 step:21671 [D loss: 0.002762, acc.: 100.00%] [G loss: 0.001290]\n",
      "epoch:27 step:21672 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.001019]\n",
      "epoch:27 step:21673 [D loss: 0.033673, acc.: 99.22%] [G loss: 0.016000]\n",
      "epoch:27 step:21674 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.179555]\n",
      "epoch:27 step:21675 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.093675]\n",
      "epoch:27 step:21676 [D loss: 0.002076, acc.: 100.00%] [G loss: 0.114370]\n",
      "epoch:27 step:21677 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.143287]\n",
      "epoch:27 step:21678 [D loss: 0.036389, acc.: 99.22%] [G loss: 0.000844]\n",
      "epoch:27 step:21679 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.001161]\n",
      "epoch:27 step:21680 [D loss: 0.005427, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:27 step:21681 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.000930]\n",
      "epoch:27 step:21682 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.001937]\n",
      "epoch:27 step:21683 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.001441]\n",
      "epoch:27 step:21684 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.013653]\n",
      "epoch:27 step:21685 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.001020]\n",
      "epoch:27 step:21686 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:27 step:21687 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.001018]\n",
      "epoch:27 step:21688 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:27 step:21689 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:27 step:21690 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:27 step:21691 [D loss: 0.002677, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:27 step:21692 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:27 step:21693 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.010448]\n",
      "epoch:27 step:21694 [D loss: 0.001272, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:27 step:21695 [D loss: 0.013073, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:27 step:21696 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.000479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21697 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.001966]\n",
      "epoch:27 step:21698 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.001355]\n",
      "epoch:27 step:21699 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.002409]\n",
      "epoch:27 step:21700 [D loss: 0.037161, acc.: 98.44%] [G loss: 0.005993]\n",
      "epoch:27 step:21701 [D loss: 0.005189, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:27 step:21702 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.001694]\n",
      "epoch:27 step:21703 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002440]\n",
      "epoch:27 step:21704 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.001181]\n",
      "epoch:27 step:21705 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.006864]\n",
      "epoch:27 step:21706 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.009410]\n",
      "epoch:27 step:21707 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000702]\n",
      "epoch:27 step:21708 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:27 step:21709 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:27 step:21710 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000501]\n",
      "epoch:27 step:21711 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:27 step:21712 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.005815]\n",
      "epoch:27 step:21713 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001288]\n",
      "epoch:27 step:21714 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000833]\n",
      "epoch:27 step:21715 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:27 step:21716 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:27 step:21717 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001074]\n",
      "epoch:27 step:21718 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.000782]\n",
      "epoch:27 step:21719 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000579]\n",
      "epoch:27 step:21720 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:27 step:21721 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000739]\n",
      "epoch:27 step:21722 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000832]\n",
      "epoch:27 step:21723 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:27 step:21724 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.013783]\n",
      "epoch:27 step:21725 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001462]\n",
      "epoch:27 step:21726 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000509]\n",
      "epoch:27 step:21727 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:27 step:21728 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:27 step:21729 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000460]\n",
      "epoch:27 step:21730 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:27 step:21731 [D loss: 0.003089, acc.: 100.00%] [G loss: 0.006887]\n",
      "epoch:27 step:21732 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.000452]\n",
      "epoch:27 step:21733 [D loss: 0.013809, acc.: 99.22%] [G loss: 0.001245]\n",
      "epoch:27 step:21734 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:27 step:21735 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.003672]\n",
      "epoch:27 step:21736 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.001542]\n",
      "epoch:27 step:21737 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:27 step:21738 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.000971]\n",
      "epoch:27 step:21739 [D loss: 0.017781, acc.: 100.00%] [G loss: 0.002064]\n",
      "epoch:27 step:21740 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.002734]\n",
      "epoch:27 step:21741 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.001326]\n",
      "epoch:27 step:21742 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.002427]\n",
      "epoch:27 step:21743 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.027961]\n",
      "epoch:27 step:21744 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.008908]\n",
      "epoch:27 step:21745 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.001180]\n",
      "epoch:27 step:21746 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.000874]\n",
      "epoch:27 step:21747 [D loss: 0.001086, acc.: 100.00%] [G loss: 0.003677]\n",
      "epoch:27 step:21748 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.017959]\n",
      "epoch:27 step:21749 [D loss: 0.011668, acc.: 100.00%] [G loss: 0.002234]\n",
      "epoch:27 step:21750 [D loss: 0.031800, acc.: 100.00%] [G loss: 0.020237]\n",
      "epoch:27 step:21751 [D loss: 0.001657, acc.: 100.00%] [G loss: 0.058499]\n",
      "epoch:27 step:21752 [D loss: 0.641247, acc.: 68.75%] [G loss: 4.652959]\n",
      "epoch:27 step:21753 [D loss: 0.003492, acc.: 100.00%] [G loss: 3.785919]\n",
      "epoch:27 step:21754 [D loss: 0.960115, acc.: 68.75%] [G loss: 5.647880]\n",
      "epoch:27 step:21755 [D loss: 1.666422, acc.: 51.56%] [G loss: 2.710780]\n",
      "epoch:27 step:21756 [D loss: 0.025147, acc.: 100.00%] [G loss: 1.729073]\n",
      "epoch:27 step:21757 [D loss: 0.595236, acc.: 73.44%] [G loss: 4.230616]\n",
      "epoch:27 step:21758 [D loss: 0.922855, acc.: 61.72%] [G loss: 3.850418]\n",
      "epoch:27 step:21759 [D loss: 0.078195, acc.: 97.66%] [G loss: 2.942726]\n",
      "epoch:27 step:21760 [D loss: 0.005393, acc.: 100.00%] [G loss: 0.973223]\n",
      "epoch:27 step:21761 [D loss: 0.025168, acc.: 99.22%] [G loss: 3.673782]\n",
      "epoch:27 step:21762 [D loss: 0.107985, acc.: 94.53%] [G loss: 2.125882]\n",
      "epoch:27 step:21763 [D loss: 0.017353, acc.: 99.22%] [G loss: 1.006171]\n",
      "epoch:27 step:21764 [D loss: 0.063360, acc.: 97.66%] [G loss: 0.061120]\n",
      "epoch:27 step:21765 [D loss: 0.020391, acc.: 99.22%] [G loss: 0.037566]\n",
      "epoch:27 step:21766 [D loss: 0.004192, acc.: 100.00%] [G loss: 0.014503]\n",
      "epoch:27 step:21767 [D loss: 0.001500, acc.: 100.00%] [G loss: 0.036415]\n",
      "epoch:27 step:21768 [D loss: 0.046044, acc.: 99.22%] [G loss: 0.061728]\n",
      "epoch:27 step:21769 [D loss: 0.005971, acc.: 100.00%] [G loss: 0.064000]\n",
      "epoch:27 step:21770 [D loss: 0.023870, acc.: 99.22%] [G loss: 0.005608]\n",
      "epoch:27 step:21771 [D loss: 0.002991, acc.: 100.00%] [G loss: 0.070218]\n",
      "epoch:27 step:21772 [D loss: 0.018106, acc.: 100.00%] [G loss: 0.004764]\n",
      "epoch:27 step:21773 [D loss: 0.002190, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:27 step:21774 [D loss: 0.011046, acc.: 100.00%] [G loss: 0.001878]\n",
      "epoch:27 step:21775 [D loss: 0.005069, acc.: 100.00%] [G loss: 0.001846]\n",
      "epoch:27 step:21776 [D loss: 0.001480, acc.: 100.00%] [G loss: 0.006602]\n",
      "epoch:27 step:21777 [D loss: 0.003757, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:27 step:21778 [D loss: 0.001307, acc.: 100.00%] [G loss: 0.002413]\n",
      "epoch:27 step:21779 [D loss: 0.001767, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:27 step:21780 [D loss: 0.001671, acc.: 100.00%] [G loss: 0.000606]\n",
      "epoch:27 step:21781 [D loss: 0.001254, acc.: 100.00%] [G loss: 0.026782]\n",
      "epoch:27 step:21782 [D loss: 0.009007, acc.: 100.00%] [G loss: 0.004222]\n",
      "epoch:27 step:21783 [D loss: 0.006706, acc.: 100.00%] [G loss: 0.007659]\n",
      "epoch:27 step:21784 [D loss: 0.004779, acc.: 100.00%] [G loss: 0.007040]\n",
      "epoch:27 step:21785 [D loss: 0.003595, acc.: 100.00%] [G loss: 0.008380]\n",
      "epoch:27 step:21786 [D loss: 0.090174, acc.: 97.66%] [G loss: 0.372454]\n",
      "epoch:27 step:21787 [D loss: 0.006531, acc.: 100.00%] [G loss: 0.701324]\n",
      "epoch:27 step:21788 [D loss: 0.033827, acc.: 99.22%] [G loss: 3.559777]\n",
      "epoch:27 step:21789 [D loss: 0.019790, acc.: 100.00%] [G loss: 0.678372]\n",
      "epoch:27 step:21790 [D loss: 0.003344, acc.: 100.00%] [G loss: 0.001844]\n",
      "epoch:27 step:21791 [D loss: 0.222528, acc.: 89.84%] [G loss: 0.627768]\n",
      "epoch:27 step:21792 [D loss: 0.013052, acc.: 99.22%] [G loss: 6.539409]\n",
      "epoch:27 step:21793 [D loss: 0.200720, acc.: 87.50%] [G loss: 0.836994]\n",
      "epoch:27 step:21794 [D loss: 0.020833, acc.: 99.22%] [G loss: 0.014481]\n",
      "epoch:27 step:21795 [D loss: 0.002144, acc.: 100.00%] [G loss: 0.034902]\n",
      "epoch:27 step:21796 [D loss: 0.015417, acc.: 100.00%] [G loss: 0.008986]\n",
      "epoch:27 step:21797 [D loss: 0.003159, acc.: 100.00%] [G loss: 0.145735]\n",
      "epoch:27 step:21798 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.003534]\n",
      "epoch:27 step:21799 [D loss: 0.001743, acc.: 100.00%] [G loss: 0.027839]\n",
      "epoch:27 step:21800 [D loss: 0.692324, acc.: 71.88%] [G loss: 0.235957]\n",
      "epoch:27 step:21801 [D loss: 0.016860, acc.: 99.22%] [G loss: 1.431920]\n",
      "epoch:27 step:21802 [D loss: 2.039393, acc.: 52.34%] [G loss: 0.000359]\n",
      "epoch:27 step:21803 [D loss: 0.351835, acc.: 87.50%] [G loss: 0.002307]\n",
      "epoch:27 step:21804 [D loss: 0.001210, acc.: 100.00%] [G loss: 2.482675]\n",
      "epoch:27 step:21805 [D loss: 0.060339, acc.: 97.66%] [G loss: 0.041814]\n",
      "epoch:27 step:21806 [D loss: 0.001615, acc.: 100.00%] [G loss: 0.088287]\n",
      "epoch:27 step:21807 [D loss: 0.007246, acc.: 100.00%] [G loss: 1.188673]\n",
      "epoch:27 step:21808 [D loss: 0.039584, acc.: 100.00%] [G loss: 0.006568]\n",
      "epoch:27 step:21809 [D loss: 0.002204, acc.: 100.00%] [G loss: 0.017411]\n",
      "epoch:27 step:21810 [D loss: 0.020568, acc.: 99.22%] [G loss: 0.019702]\n",
      "epoch:27 step:21811 [D loss: 0.003488, acc.: 100.00%] [G loss: 0.002530]\n",
      "epoch:27 step:21812 [D loss: 0.039231, acc.: 100.00%] [G loss: 0.004794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21813 [D loss: 0.019739, acc.: 100.00%] [G loss: 0.054654]\n",
      "epoch:27 step:21814 [D loss: 0.092422, acc.: 97.66%] [G loss: 2.027371]\n",
      "epoch:27 step:21815 [D loss: 0.059399, acc.: 99.22%] [G loss: 0.093018]\n",
      "epoch:27 step:21816 [D loss: 0.068504, acc.: 98.44%] [G loss: 0.248363]\n",
      "epoch:27 step:21817 [D loss: 0.213185, acc.: 93.75%] [G loss: 2.222495]\n",
      "epoch:27 step:21818 [D loss: 0.002470, acc.: 100.00%] [G loss: 1.099886]\n",
      "epoch:27 step:21819 [D loss: 0.169382, acc.: 92.19%] [G loss: 0.049439]\n",
      "epoch:27 step:21820 [D loss: 0.035760, acc.: 99.22%] [G loss: 0.267296]\n",
      "epoch:27 step:21821 [D loss: 0.004385, acc.: 100.00%] [G loss: 0.076104]\n",
      "epoch:27 step:21822 [D loss: 0.003926, acc.: 100.00%] [G loss: 0.017948]\n",
      "epoch:27 step:21823 [D loss: 0.008245, acc.: 100.00%] [G loss: 0.173820]\n",
      "epoch:27 step:21824 [D loss: 0.004997, acc.: 100.00%] [G loss: 0.058238]\n",
      "epoch:27 step:21825 [D loss: 0.004159, acc.: 100.00%] [G loss: 0.054831]\n",
      "epoch:27 step:21826 [D loss: 0.002676, acc.: 100.00%] [G loss: 0.054635]\n",
      "epoch:27 step:21827 [D loss: 0.021650, acc.: 100.00%] [G loss: 0.101525]\n",
      "epoch:27 step:21828 [D loss: 0.010782, acc.: 100.00%] [G loss: 0.024007]\n",
      "epoch:27 step:21829 [D loss: 0.011577, acc.: 100.00%] [G loss: 0.089215]\n",
      "epoch:27 step:21830 [D loss: 0.008152, acc.: 99.22%] [G loss: 0.012394]\n",
      "epoch:27 step:21831 [D loss: 0.003360, acc.: 100.00%] [G loss: 0.004732]\n",
      "epoch:27 step:21832 [D loss: 0.003118, acc.: 100.00%] [G loss: 0.016356]\n",
      "epoch:27 step:21833 [D loss: 0.003311, acc.: 100.00%] [G loss: 0.004600]\n",
      "epoch:27 step:21834 [D loss: 0.002755, acc.: 100.00%] [G loss: 0.024805]\n",
      "epoch:27 step:21835 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.013429]\n",
      "epoch:27 step:21836 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.011275]\n",
      "epoch:27 step:21837 [D loss: 0.002810, acc.: 100.00%] [G loss: 0.013906]\n",
      "epoch:27 step:21838 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.005755]\n",
      "epoch:27 step:21839 [D loss: 0.000987, acc.: 100.00%] [G loss: 0.007563]\n",
      "epoch:27 step:21840 [D loss: 0.004020, acc.: 100.00%] [G loss: 0.002405]\n",
      "epoch:27 step:21841 [D loss: 0.004872, acc.: 100.00%] [G loss: 0.001787]\n",
      "epoch:27 step:21842 [D loss: 0.003157, acc.: 100.00%] [G loss: 0.005520]\n",
      "epoch:27 step:21843 [D loss: 0.001295, acc.: 100.00%] [G loss: 0.001323]\n",
      "epoch:27 step:21844 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.157112]\n",
      "epoch:27 step:21845 [D loss: 0.002029, acc.: 100.00%] [G loss: 0.008068]\n",
      "epoch:27 step:21846 [D loss: 0.002037, acc.: 100.00%] [G loss: 0.000651]\n",
      "epoch:27 step:21847 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.005473]\n",
      "epoch:27 step:21848 [D loss: 0.001301, acc.: 100.00%] [G loss: 0.003300]\n",
      "epoch:27 step:21849 [D loss: 0.004945, acc.: 100.00%] [G loss: 0.872968]\n",
      "epoch:27 step:21850 [D loss: 0.004182, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:27 step:21851 [D loss: 0.004496, acc.: 100.00%] [G loss: 0.079073]\n",
      "epoch:27 step:21852 [D loss: 0.003564, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:27 step:21853 [D loss: 0.021541, acc.: 99.22%] [G loss: 0.041632]\n",
      "epoch:27 step:21854 [D loss: 0.043119, acc.: 99.22%] [G loss: 0.003724]\n",
      "epoch:27 step:21855 [D loss: 0.009434, acc.: 100.00%] [G loss: 0.070868]\n",
      "epoch:27 step:21856 [D loss: 0.001045, acc.: 100.00%] [G loss: 0.016684]\n",
      "epoch:27 step:21857 [D loss: 0.035363, acc.: 98.44%] [G loss: 0.039367]\n",
      "epoch:27 step:21858 [D loss: 0.031886, acc.: 98.44%] [G loss: 0.000285]\n",
      "epoch:27 step:21859 [D loss: 0.004782, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:27 step:21860 [D loss: 0.008396, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:27 step:21861 [D loss: 0.002108, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:27 step:21862 [D loss: 0.021401, acc.: 100.00%] [G loss: 0.000579]\n",
      "epoch:27 step:21863 [D loss: 0.006452, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:27 step:21864 [D loss: 0.005226, acc.: 100.00%] [G loss: 0.135824]\n",
      "epoch:27 step:21865 [D loss: 0.008417, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:27 step:21866 [D loss: 0.001060, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:27 step:21867 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.001923]\n",
      "epoch:27 step:21868 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.007638]\n",
      "epoch:28 step:21869 [D loss: 0.001172, acc.: 100.00%] [G loss: 0.002284]\n",
      "epoch:28 step:21870 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.106013]\n",
      "epoch:28 step:21871 [D loss: 0.007119, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:28 step:21872 [D loss: 0.003762, acc.: 100.00%] [G loss: 0.000943]\n",
      "epoch:28 step:21873 [D loss: 0.283912, acc.: 87.50%] [G loss: 3.172677]\n",
      "epoch:28 step:21874 [D loss: 0.258727, acc.: 88.28%] [G loss: 1.429516]\n",
      "epoch:28 step:21875 [D loss: 0.032068, acc.: 98.44%] [G loss: 0.319945]\n",
      "epoch:28 step:21876 [D loss: 0.053866, acc.: 98.44%] [G loss: 0.556524]\n",
      "epoch:28 step:21877 [D loss: 0.012291, acc.: 100.00%] [G loss: 0.250969]\n",
      "epoch:28 step:21878 [D loss: 0.011410, acc.: 100.00%] [G loss: 0.886267]\n",
      "epoch:28 step:21879 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.616268]\n",
      "epoch:28 step:21880 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.134752]\n",
      "epoch:28 step:21881 [D loss: 0.024368, acc.: 98.44%] [G loss: 0.013126]\n",
      "epoch:28 step:21882 [D loss: 0.003611, acc.: 100.00%] [G loss: 1.646727]\n",
      "epoch:28 step:21883 [D loss: 0.027356, acc.: 100.00%] [G loss: 0.016671]\n",
      "epoch:28 step:21884 [D loss: 0.005524, acc.: 100.00%] [G loss: 0.030202]\n",
      "epoch:28 step:21885 [D loss: 0.042184, acc.: 99.22%] [G loss: 0.143383]\n",
      "epoch:28 step:21886 [D loss: 0.008573, acc.: 100.00%] [G loss: 0.356009]\n",
      "epoch:28 step:21887 [D loss: 0.004373, acc.: 100.00%] [G loss: 0.015892]\n",
      "epoch:28 step:21888 [D loss: 0.008871, acc.: 100.00%] [G loss: 0.653887]\n",
      "epoch:28 step:21889 [D loss: 0.021335, acc.: 100.00%] [G loss: 0.017316]\n",
      "epoch:28 step:21890 [D loss: 0.014377, acc.: 100.00%] [G loss: 0.035115]\n",
      "epoch:28 step:21891 [D loss: 0.046611, acc.: 99.22%] [G loss: 0.004448]\n",
      "epoch:28 step:21892 [D loss: 0.110344, acc.: 97.66%] [G loss: 0.291044]\n",
      "epoch:28 step:21893 [D loss: 0.053769, acc.: 96.88%] [G loss: 0.336918]\n",
      "epoch:28 step:21894 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.052363]\n",
      "epoch:28 step:21895 [D loss: 0.004064, acc.: 100.00%] [G loss: 1.542806]\n",
      "epoch:28 step:21896 [D loss: 0.009270, acc.: 100.00%] [G loss: 0.001920]\n",
      "epoch:28 step:21897 [D loss: 0.009345, acc.: 100.00%] [G loss: 0.007497]\n",
      "epoch:28 step:21898 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.020232]\n",
      "epoch:28 step:21899 [D loss: 0.004224, acc.: 100.00%] [G loss: 0.040896]\n",
      "epoch:28 step:21900 [D loss: 0.015628, acc.: 100.00%] [G loss: 0.030282]\n",
      "epoch:28 step:21901 [D loss: 0.004273, acc.: 100.00%] [G loss: 0.005477]\n",
      "epoch:28 step:21902 [D loss: 0.001711, acc.: 100.00%] [G loss: 0.009410]\n",
      "epoch:28 step:21903 [D loss: 0.002672, acc.: 100.00%] [G loss: 0.009258]\n",
      "epoch:28 step:21904 [D loss: 0.054191, acc.: 97.66%] [G loss: 0.001055]\n",
      "epoch:28 step:21905 [D loss: 0.025542, acc.: 99.22%] [G loss: 0.009504]\n",
      "epoch:28 step:21906 [D loss: 0.003836, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:28 step:21907 [D loss: 0.034651, acc.: 100.00%] [G loss: 1.073480]\n",
      "epoch:28 step:21908 [D loss: 0.008758, acc.: 100.00%] [G loss: 0.278108]\n",
      "epoch:28 step:21909 [D loss: 0.055366, acc.: 98.44%] [G loss: 1.495773]\n",
      "epoch:28 step:21910 [D loss: 0.339533, acc.: 84.38%] [G loss: 8.107948]\n",
      "epoch:28 step:21911 [D loss: 2.299316, acc.: 51.56%] [G loss: 0.662186]\n",
      "epoch:28 step:21912 [D loss: 0.221543, acc.: 94.53%] [G loss: 5.222545]\n",
      "epoch:28 step:21913 [D loss: 0.013471, acc.: 100.00%] [G loss: 3.046998]\n",
      "epoch:28 step:21914 [D loss: 0.094981, acc.: 96.88%] [G loss: 0.686696]\n",
      "epoch:28 step:21915 [D loss: 0.400959, acc.: 82.03%] [G loss: 0.058608]\n",
      "epoch:28 step:21916 [D loss: 0.043117, acc.: 98.44%] [G loss: 0.135205]\n",
      "epoch:28 step:21917 [D loss: 0.014329, acc.: 100.00%] [G loss: 0.046929]\n",
      "epoch:28 step:21918 [D loss: 0.015315, acc.: 100.00%] [G loss: 0.006053]\n",
      "epoch:28 step:21919 [D loss: 0.010432, acc.: 100.00%] [G loss: 0.001844]\n",
      "epoch:28 step:21920 [D loss: 0.181499, acc.: 90.62%] [G loss: 0.089855]\n",
      "epoch:28 step:21921 [D loss: 0.007897, acc.: 100.00%] [G loss: 0.725439]\n",
      "epoch:28 step:21922 [D loss: 0.166295, acc.: 93.75%] [G loss: 5.142955]\n",
      "epoch:28 step:21923 [D loss: 0.047060, acc.: 99.22%] [G loss: 2.759628]\n",
      "epoch:28 step:21924 [D loss: 0.005650, acc.: 100.00%] [G loss: 0.015814]\n",
      "epoch:28 step:21925 [D loss: 0.094311, acc.: 96.09%] [G loss: 2.665312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:21926 [D loss: 0.035213, acc.: 99.22%] [G loss: 0.241665]\n",
      "epoch:28 step:21927 [D loss: 0.024149, acc.: 99.22%] [G loss: 0.060698]\n",
      "epoch:28 step:21928 [D loss: 0.001007, acc.: 100.00%] [G loss: 0.022465]\n",
      "epoch:28 step:21929 [D loss: 0.003189, acc.: 100.00%] [G loss: 0.069643]\n",
      "epoch:28 step:21930 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.001330]\n",
      "epoch:28 step:21931 [D loss: 0.001642, acc.: 100.00%] [G loss: 0.341453]\n",
      "epoch:28 step:21932 [D loss: 0.003346, acc.: 100.00%] [G loss: 0.001074]\n",
      "epoch:28 step:21933 [D loss: 0.017013, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:28 step:21934 [D loss: 0.004796, acc.: 100.00%] [G loss: 0.001713]\n",
      "epoch:28 step:21935 [D loss: 0.004359, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:28 step:21936 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:28 step:21937 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:28 step:21938 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:28 step:21939 [D loss: 0.004344, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:28 step:21940 [D loss: 0.000658, acc.: 100.00%] [G loss: 0.003686]\n",
      "epoch:28 step:21941 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:28 step:21942 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:28 step:21943 [D loss: 0.000702, acc.: 100.00%] [G loss: 0.001119]\n",
      "epoch:28 step:21944 [D loss: 0.003286, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:28 step:21945 [D loss: 0.001464, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:28 step:21946 [D loss: 0.012756, acc.: 100.00%] [G loss: 0.004784]\n",
      "epoch:28 step:21947 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:28 step:21948 [D loss: 0.002440, acc.: 100.00%] [G loss: 0.006634]\n",
      "epoch:28 step:21949 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:28 step:21950 [D loss: 0.001848, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:28 step:21951 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.001904]\n",
      "epoch:28 step:21952 [D loss: 0.001363, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:28 step:21953 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.011968]\n",
      "epoch:28 step:21954 [D loss: 0.000614, acc.: 100.00%] [G loss: 0.001670]\n",
      "epoch:28 step:21955 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.101070]\n",
      "epoch:28 step:21956 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:28 step:21957 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.005423]\n",
      "epoch:28 step:21958 [D loss: 0.159543, acc.: 96.88%] [G loss: 0.001241]\n",
      "epoch:28 step:21959 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.004344]\n",
      "epoch:28 step:21960 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.015711]\n",
      "epoch:28 step:21961 [D loss: 0.010248, acc.: 100.00%] [G loss: 0.191921]\n",
      "epoch:28 step:21962 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.043272]\n",
      "epoch:28 step:21963 [D loss: 0.006315, acc.: 99.22%] [G loss: 0.054731]\n",
      "epoch:28 step:21964 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.002186]\n",
      "epoch:28 step:21965 [D loss: 0.004534, acc.: 100.00%] [G loss: 0.001412]\n",
      "epoch:28 step:21966 [D loss: 0.002425, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:28 step:21967 [D loss: 0.001966, acc.: 100.00%] [G loss: 0.003460]\n",
      "epoch:28 step:21968 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:28 step:21969 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:28 step:21970 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:28 step:21971 [D loss: 0.002052, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:28 step:21972 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:28 step:21973 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:28 step:21974 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:28 step:21975 [D loss: 0.005056, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:28 step:21976 [D loss: 0.001174, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:28 step:21977 [D loss: 0.001263, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:28 step:21978 [D loss: 0.033789, acc.: 99.22%] [G loss: 0.010638]\n",
      "epoch:28 step:21979 [D loss: 0.037066, acc.: 99.22%] [G loss: 0.001623]\n",
      "epoch:28 step:21980 [D loss: 0.006630, acc.: 100.00%] [G loss: 0.005223]\n",
      "epoch:28 step:21981 [D loss: 0.074442, acc.: 96.88%] [G loss: 2.066190]\n",
      "epoch:28 step:21982 [D loss: 0.039741, acc.: 98.44%] [G loss: 2.388604]\n",
      "epoch:28 step:21983 [D loss: 0.061533, acc.: 99.22%] [G loss: 0.962312]\n",
      "epoch:28 step:21984 [D loss: 0.041897, acc.: 98.44%] [G loss: 0.184598]\n",
      "epoch:28 step:21985 [D loss: 0.018201, acc.: 100.00%] [G loss: 0.171748]\n",
      "epoch:28 step:21986 [D loss: 0.044040, acc.: 100.00%] [G loss: 1.050164]\n",
      "epoch:28 step:21987 [D loss: 0.020339, acc.: 100.00%] [G loss: 3.367257]\n",
      "epoch:28 step:21988 [D loss: 0.018241, acc.: 100.00%] [G loss: 4.568802]\n",
      "epoch:28 step:21989 [D loss: 0.231182, acc.: 91.41%] [G loss: 5.946376]\n",
      "epoch:28 step:21990 [D loss: 0.112463, acc.: 95.31%] [G loss: 2.606043]\n",
      "epoch:28 step:21991 [D loss: 0.036105, acc.: 98.44%] [G loss: 0.037729]\n",
      "epoch:28 step:21992 [D loss: 0.011243, acc.: 100.00%] [G loss: 2.914414]\n",
      "epoch:28 step:21993 [D loss: 0.002838, acc.: 100.00%] [G loss: 1.631527]\n",
      "epoch:28 step:21994 [D loss: 0.018356, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:28 step:21995 [D loss: 0.001978, acc.: 100.00%] [G loss: 0.548121]\n",
      "epoch:28 step:21996 [D loss: 0.009360, acc.: 100.00%] [G loss: 0.571317]\n",
      "epoch:28 step:21997 [D loss: 0.002553, acc.: 100.00%] [G loss: 0.201011]\n",
      "epoch:28 step:21998 [D loss: 0.009677, acc.: 100.00%] [G loss: 0.031187]\n",
      "epoch:28 step:21999 [D loss: 0.008680, acc.: 100.00%] [G loss: 0.026091]\n",
      "epoch:28 step:22000 [D loss: 0.002061, acc.: 100.00%] [G loss: 0.275785]\n",
      "epoch:28 step:22001 [D loss: 0.002910, acc.: 100.00%] [G loss: 0.053007]\n",
      "epoch:28 step:22002 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.026330]\n",
      "epoch:28 step:22003 [D loss: 0.005098, acc.: 100.00%] [G loss: 0.011457]\n",
      "epoch:28 step:22004 [D loss: 0.007411, acc.: 100.00%] [G loss: 0.005604]\n",
      "epoch:28 step:22005 [D loss: 0.005379, acc.: 100.00%] [G loss: 0.002650]\n",
      "epoch:28 step:22006 [D loss: 0.002038, acc.: 100.00%] [G loss: 0.017943]\n",
      "epoch:28 step:22007 [D loss: 0.003705, acc.: 100.00%] [G loss: 0.000927]\n",
      "epoch:28 step:22008 [D loss: 0.010665, acc.: 99.22%] [G loss: 0.011841]\n",
      "epoch:28 step:22009 [D loss: 0.059932, acc.: 99.22%] [G loss: 0.017017]\n",
      "epoch:28 step:22010 [D loss: 0.004111, acc.: 100.00%] [G loss: 0.735186]\n",
      "epoch:28 step:22011 [D loss: 0.063128, acc.: 96.88%] [G loss: 0.019144]\n",
      "epoch:28 step:22012 [D loss: 0.001871, acc.: 100.00%] [G loss: 0.001650]\n",
      "epoch:28 step:22013 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:28 step:22014 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000469]\n",
      "epoch:28 step:22015 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.003707]\n",
      "epoch:28 step:22016 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:28 step:22017 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.001038]\n",
      "epoch:28 step:22018 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.010701]\n",
      "epoch:28 step:22019 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:28 step:22020 [D loss: 0.002718, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:28 step:22021 [D loss: 0.004114, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:28 step:22022 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.014089]\n",
      "epoch:28 step:22023 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:28 step:22024 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:28 step:22025 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.002565]\n",
      "epoch:28 step:22026 [D loss: 0.002865, acc.: 100.00%] [G loss: 0.001423]\n",
      "epoch:28 step:22027 [D loss: 0.002711, acc.: 100.00%] [G loss: 0.002945]\n",
      "epoch:28 step:22028 [D loss: 0.002758, acc.: 100.00%] [G loss: 0.001280]\n",
      "epoch:28 step:22029 [D loss: 0.016380, acc.: 100.00%] [G loss: 0.026330]\n",
      "epoch:28 step:22030 [D loss: 0.001274, acc.: 100.00%] [G loss: 0.003076]\n",
      "epoch:28 step:22031 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.010377]\n",
      "epoch:28 step:22032 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.007156]\n",
      "epoch:28 step:22033 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.013182]\n",
      "epoch:28 step:22034 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:28 step:22035 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:28 step:22036 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.171284]\n",
      "epoch:28 step:22037 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:28 step:22038 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:28 step:22039 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.002982]\n",
      "epoch:28 step:22040 [D loss: 0.003862, acc.: 100.00%] [G loss: 0.000268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22041 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:28 step:22042 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000678]\n",
      "epoch:28 step:22043 [D loss: 0.018241, acc.: 100.00%] [G loss: 0.300719]\n",
      "epoch:28 step:22044 [D loss: 0.050648, acc.: 99.22%] [G loss: 0.001183]\n",
      "epoch:28 step:22045 [D loss: 0.023546, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:28 step:22046 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:28 step:22047 [D loss: 0.024050, acc.: 99.22%] [G loss: 0.000050]\n",
      "epoch:28 step:22048 [D loss: 0.001928, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:28 step:22049 [D loss: 0.001984, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:28 step:22050 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:28 step:22051 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:28 step:22052 [D loss: 0.016014, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:28 step:22053 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.004808]\n",
      "epoch:28 step:22054 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:28 step:22055 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.001677]\n",
      "epoch:28 step:22056 [D loss: 0.000798, acc.: 100.00%] [G loss: 0.001524]\n",
      "epoch:28 step:22057 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.005551]\n",
      "epoch:28 step:22058 [D loss: 0.001582, acc.: 100.00%] [G loss: 0.005047]\n",
      "epoch:28 step:22059 [D loss: 0.001612, acc.: 100.00%] [G loss: 0.000523]\n",
      "epoch:28 step:22060 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000839]\n",
      "epoch:28 step:22061 [D loss: 0.001097, acc.: 100.00%] [G loss: 3.544830]\n",
      "epoch:28 step:22062 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.002854]\n",
      "epoch:28 step:22063 [D loss: 0.590855, acc.: 76.56%] [G loss: 9.687132]\n",
      "epoch:28 step:22064 [D loss: 3.122955, acc.: 50.78%] [G loss: 1.888382]\n",
      "epoch:28 step:22065 [D loss: 0.648428, acc.: 76.56%] [G loss: 0.761107]\n",
      "epoch:28 step:22066 [D loss: 0.071201, acc.: 96.88%] [G loss: 0.766620]\n",
      "epoch:28 step:22067 [D loss: 0.023182, acc.: 100.00%] [G loss: 0.177126]\n",
      "epoch:28 step:22068 [D loss: 0.018985, acc.: 99.22%] [G loss: 0.155426]\n",
      "epoch:28 step:22069 [D loss: 0.051520, acc.: 98.44%] [G loss: 0.270849]\n",
      "epoch:28 step:22070 [D loss: 0.057588, acc.: 98.44%] [G loss: 0.424459]\n",
      "epoch:28 step:22071 [D loss: 0.014229, acc.: 99.22%] [G loss: 0.416032]\n",
      "epoch:28 step:22072 [D loss: 0.040552, acc.: 100.00%] [G loss: 0.759161]\n",
      "epoch:28 step:22073 [D loss: 0.074256, acc.: 99.22%] [G loss: 3.941167]\n",
      "epoch:28 step:22074 [D loss: 0.091510, acc.: 96.88%] [G loss: 0.394685]\n",
      "epoch:28 step:22075 [D loss: 0.011538, acc.: 100.00%] [G loss: 1.586834]\n",
      "epoch:28 step:22076 [D loss: 0.039873, acc.: 99.22%] [G loss: 0.170239]\n",
      "epoch:28 step:22077 [D loss: 0.113473, acc.: 96.88%] [G loss: 0.002516]\n",
      "epoch:28 step:22078 [D loss: 0.178745, acc.: 94.53%] [G loss: 0.371478]\n",
      "epoch:28 step:22079 [D loss: 0.002285, acc.: 100.00%] [G loss: 0.987520]\n",
      "epoch:28 step:22080 [D loss: 0.028106, acc.: 98.44%] [G loss: 0.112142]\n",
      "epoch:28 step:22081 [D loss: 0.019298, acc.: 100.00%] [G loss: 0.117794]\n",
      "epoch:28 step:22082 [D loss: 0.267955, acc.: 85.94%] [G loss: 0.000155]\n",
      "epoch:28 step:22083 [D loss: 0.034777, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:28 step:22084 [D loss: 0.087604, acc.: 98.44%] [G loss: 0.057887]\n",
      "epoch:28 step:22085 [D loss: 0.011016, acc.: 100.00%] [G loss: 0.492401]\n",
      "epoch:28 step:22086 [D loss: 0.054910, acc.: 98.44%] [G loss: 0.154918]\n",
      "epoch:28 step:22087 [D loss: 0.031791, acc.: 100.00%] [G loss: 0.051506]\n",
      "epoch:28 step:22088 [D loss: 0.196691, acc.: 92.19%] [G loss: 0.000896]\n",
      "epoch:28 step:22089 [D loss: 0.528073, acc.: 68.75%] [G loss: 0.934690]\n",
      "epoch:28 step:22090 [D loss: 0.464248, acc.: 75.78%] [G loss: 0.036312]\n",
      "epoch:28 step:22091 [D loss: 0.015609, acc.: 99.22%] [G loss: 5.925899]\n",
      "epoch:28 step:22092 [D loss: 0.000409, acc.: 100.00%] [G loss: 1.923996]\n",
      "epoch:28 step:22093 [D loss: 0.088436, acc.: 96.88%] [G loss: 1.505438]\n",
      "epoch:28 step:22094 [D loss: 0.121272, acc.: 96.88%] [G loss: 5.209327]\n",
      "epoch:28 step:22095 [D loss: 0.016642, acc.: 100.00%] [G loss: 2.664972]\n",
      "epoch:28 step:22096 [D loss: 0.118807, acc.: 95.31%] [G loss: 0.006315]\n",
      "epoch:28 step:22097 [D loss: 1.235400, acc.: 60.94%] [G loss: 2.676186]\n",
      "epoch:28 step:22098 [D loss: 2.641004, acc.: 50.78%] [G loss: 1.374725]\n",
      "epoch:28 step:22099 [D loss: 0.676355, acc.: 73.44%] [G loss: 0.084001]\n",
      "epoch:28 step:22100 [D loss: 0.047482, acc.: 99.22%] [G loss: 1.225481]\n",
      "epoch:28 step:22101 [D loss: 0.323643, acc.: 82.81%] [G loss: 0.027892]\n",
      "epoch:28 step:22102 [D loss: 0.077836, acc.: 96.88%] [G loss: 3.730227]\n",
      "epoch:28 step:22103 [D loss: 0.143276, acc.: 95.31%] [G loss: 0.036084]\n",
      "epoch:28 step:22104 [D loss: 0.129958, acc.: 94.53%] [G loss: 0.002439]\n",
      "epoch:28 step:22105 [D loss: 0.009768, acc.: 100.00%] [G loss: 0.730277]\n",
      "epoch:28 step:22106 [D loss: 0.091973, acc.: 96.88%] [G loss: 0.001335]\n",
      "epoch:28 step:22107 [D loss: 0.006869, acc.: 100.00%] [G loss: 0.003238]\n",
      "epoch:28 step:22108 [D loss: 0.062598, acc.: 97.66%] [G loss: 0.740068]\n",
      "epoch:28 step:22109 [D loss: 0.015673, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:28 step:22110 [D loss: 0.005564, acc.: 100.00%] [G loss: 0.159510]\n",
      "epoch:28 step:22111 [D loss: 0.014839, acc.: 100.00%] [G loss: 0.385126]\n",
      "epoch:28 step:22112 [D loss: 0.214432, acc.: 91.41%] [G loss: 2.356718]\n",
      "epoch:28 step:22113 [D loss: 0.029137, acc.: 99.22%] [G loss: 0.089718]\n",
      "epoch:28 step:22114 [D loss: 0.184363, acc.: 91.41%] [G loss: 0.005115]\n",
      "epoch:28 step:22115 [D loss: 0.056964, acc.: 99.22%] [G loss: 0.003574]\n",
      "epoch:28 step:22116 [D loss: 0.008886, acc.: 100.00%] [G loss: 0.005445]\n",
      "epoch:28 step:22117 [D loss: 0.032079, acc.: 99.22%] [G loss: 0.202211]\n",
      "epoch:28 step:22118 [D loss: 0.097119, acc.: 96.88%] [G loss: 0.018410]\n",
      "epoch:28 step:22119 [D loss: 0.006875, acc.: 100.00%] [G loss: 0.024901]\n",
      "epoch:28 step:22120 [D loss: 0.033922, acc.: 99.22%] [G loss: 0.896209]\n",
      "epoch:28 step:22121 [D loss: 0.019120, acc.: 99.22%] [G loss: 0.244836]\n",
      "epoch:28 step:22122 [D loss: 0.124574, acc.: 95.31%] [G loss: 0.013418]\n",
      "epoch:28 step:22123 [D loss: 0.041909, acc.: 99.22%] [G loss: 0.144471]\n",
      "epoch:28 step:22124 [D loss: 0.026414, acc.: 99.22%] [G loss: 0.819792]\n",
      "epoch:28 step:22125 [D loss: 0.008618, acc.: 100.00%] [G loss: 0.007032]\n",
      "epoch:28 step:22126 [D loss: 0.345863, acc.: 83.59%] [G loss: 0.909284]\n",
      "epoch:28 step:22127 [D loss: 0.398600, acc.: 79.69%] [G loss: 0.048618]\n",
      "epoch:28 step:22128 [D loss: 0.053249, acc.: 98.44%] [G loss: 1.992483]\n",
      "epoch:28 step:22129 [D loss: 0.044220, acc.: 99.22%] [G loss: 0.550994]\n",
      "epoch:28 step:22130 [D loss: 0.002925, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:28 step:22131 [D loss: 0.031548, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:28 step:22132 [D loss: 0.006522, acc.: 100.00%] [G loss: 0.000870]\n",
      "epoch:28 step:22133 [D loss: 0.007205, acc.: 100.00%] [G loss: 0.563069]\n",
      "epoch:28 step:22134 [D loss: 0.004692, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:28 step:22135 [D loss: 0.011962, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:28 step:22136 [D loss: 0.006034, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:28 step:22137 [D loss: 0.008556, acc.: 100.00%] [G loss: 0.082217]\n",
      "epoch:28 step:22138 [D loss: 0.003400, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:28 step:22139 [D loss: 0.018635, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:28 step:22140 [D loss: 0.008718, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:28 step:22141 [D loss: 0.027483, acc.: 100.00%] [G loss: 0.000969]\n",
      "epoch:28 step:22142 [D loss: 0.012207, acc.: 100.00%] [G loss: 0.426705]\n",
      "epoch:28 step:22143 [D loss: 0.114475, acc.: 95.31%] [G loss: 0.000063]\n",
      "epoch:28 step:22144 [D loss: 0.006839, acc.: 100.00%] [G loss: 0.071095]\n",
      "epoch:28 step:22145 [D loss: 0.126663, acc.: 96.88%] [G loss: 0.080579]\n",
      "epoch:28 step:22146 [D loss: 0.008638, acc.: 100.00%] [G loss: 0.120379]\n",
      "epoch:28 step:22147 [D loss: 0.047651, acc.: 98.44%] [G loss: 0.535194]\n",
      "epoch:28 step:22148 [D loss: 0.060096, acc.: 98.44%] [G loss: 0.003234]\n",
      "epoch:28 step:22149 [D loss: 0.003782, acc.: 100.00%] [G loss: 2.017375]\n",
      "epoch:28 step:22150 [D loss: 0.458557, acc.: 78.91%] [G loss: 0.072170]\n",
      "epoch:28 step:22151 [D loss: 0.153701, acc.: 95.31%] [G loss: 2.243371]\n",
      "epoch:28 step:22152 [D loss: 0.008299, acc.: 100.00%] [G loss: 2.209991]\n",
      "epoch:28 step:22153 [D loss: 0.065174, acc.: 99.22%] [G loss: 0.592782]\n",
      "epoch:28 step:22154 [D loss: 0.239589, acc.: 90.62%] [G loss: 0.303838]\n",
      "epoch:28 step:22155 [D loss: 0.235741, acc.: 87.50%] [G loss: 3.868748]\n",
      "epoch:28 step:22156 [D loss: 0.128514, acc.: 92.19%] [G loss: 4.206346]\n",
      "epoch:28 step:22157 [D loss: 0.067264, acc.: 98.44%] [G loss: 0.219306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22158 [D loss: 0.016812, acc.: 100.00%] [G loss: 0.018277]\n",
      "epoch:28 step:22159 [D loss: 0.015897, acc.: 100.00%] [G loss: 1.444895]\n",
      "epoch:28 step:22160 [D loss: 0.114008, acc.: 96.09%] [G loss: 0.012750]\n",
      "epoch:28 step:22161 [D loss: 0.011210, acc.: 100.00%] [G loss: 2.121124]\n",
      "epoch:28 step:22162 [D loss: 0.026105, acc.: 100.00%] [G loss: 2.911826]\n",
      "epoch:28 step:22163 [D loss: 0.113626, acc.: 96.09%] [G loss: 0.007490]\n",
      "epoch:28 step:22164 [D loss: 0.071177, acc.: 98.44%] [G loss: 2.368537]\n",
      "epoch:28 step:22165 [D loss: 0.074846, acc.: 100.00%] [G loss: 0.051465]\n",
      "epoch:28 step:22166 [D loss: 0.010731, acc.: 100.00%] [G loss: 2.018709]\n",
      "epoch:28 step:22167 [D loss: 0.039123, acc.: 98.44%] [G loss: 0.013658]\n",
      "epoch:28 step:22168 [D loss: 0.321321, acc.: 89.06%] [G loss: 0.701792]\n",
      "epoch:28 step:22169 [D loss: 0.009289, acc.: 99.22%] [G loss: 3.334086]\n",
      "epoch:28 step:22170 [D loss: 0.004440, acc.: 100.00%] [G loss: 3.004060]\n",
      "epoch:28 step:22171 [D loss: 0.092679, acc.: 96.88%] [G loss: 0.006588]\n",
      "epoch:28 step:22172 [D loss: 0.001972, acc.: 100.00%] [G loss: 0.154341]\n",
      "epoch:28 step:22173 [D loss: 0.016098, acc.: 100.00%] [G loss: 0.009467]\n",
      "epoch:28 step:22174 [D loss: 0.001342, acc.: 100.00%] [G loss: 0.027960]\n",
      "epoch:28 step:22175 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.004600]\n",
      "epoch:28 step:22176 [D loss: 0.001669, acc.: 100.00%] [G loss: 0.063461]\n",
      "epoch:28 step:22177 [D loss: 0.001536, acc.: 100.00%] [G loss: 0.004905]\n",
      "epoch:28 step:22178 [D loss: 0.004530, acc.: 100.00%] [G loss: 0.009516]\n",
      "epoch:28 step:22179 [D loss: 0.000796, acc.: 100.00%] [G loss: 0.003860]\n",
      "epoch:28 step:22180 [D loss: 0.012028, acc.: 100.00%] [G loss: 0.002260]\n",
      "epoch:28 step:22181 [D loss: 0.192024, acc.: 90.62%] [G loss: 1.544400]\n",
      "epoch:28 step:22182 [D loss: 0.115272, acc.: 96.88%] [G loss: 0.290790]\n",
      "epoch:28 step:22183 [D loss: 2.393568, acc.: 50.78%] [G loss: 0.007311]\n",
      "epoch:28 step:22184 [D loss: 0.466223, acc.: 77.34%] [G loss: 0.022131]\n",
      "epoch:28 step:22185 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.485470]\n",
      "epoch:28 step:22186 [D loss: 0.041298, acc.: 98.44%] [G loss: 0.189506]\n",
      "epoch:28 step:22187 [D loss: 0.058261, acc.: 100.00%] [G loss: 0.003052]\n",
      "epoch:28 step:22188 [D loss: 0.012746, acc.: 100.00%] [G loss: 3.606154]\n",
      "epoch:28 step:22189 [D loss: 0.001298, acc.: 100.00%] [G loss: 1.239339]\n",
      "epoch:28 step:22190 [D loss: 0.000780, acc.: 100.00%] [G loss: 0.866918]\n",
      "epoch:28 step:22191 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.229538]\n",
      "epoch:28 step:22192 [D loss: 0.005984, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:28 step:22193 [D loss: 0.006247, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:28 step:22194 [D loss: 0.014447, acc.: 100.00%] [G loss: 0.004397]\n",
      "epoch:28 step:22195 [D loss: 0.021310, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:28 step:22196 [D loss: 0.016290, acc.: 100.00%] [G loss: 0.000986]\n",
      "epoch:28 step:22197 [D loss: 0.007128, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:28 step:22198 [D loss: 0.003205, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:28 step:22199 [D loss: 0.002538, acc.: 100.00%] [G loss: 0.000969]\n",
      "epoch:28 step:22200 [D loss: 0.003077, acc.: 100.00%] [G loss: 0.026873]\n",
      "epoch:28 step:22201 [D loss: 0.003096, acc.: 100.00%] [G loss: 0.000550]\n",
      "epoch:28 step:22202 [D loss: 0.002737, acc.: 100.00%] [G loss: 0.026073]\n",
      "epoch:28 step:22203 [D loss: 0.003765, acc.: 100.00%] [G loss: 0.031293]\n",
      "epoch:28 step:22204 [D loss: 0.007266, acc.: 100.00%] [G loss: 0.022059]\n",
      "epoch:28 step:22205 [D loss: 0.004739, acc.: 100.00%] [G loss: 0.111668]\n",
      "epoch:28 step:22206 [D loss: 0.022251, acc.: 100.00%] [G loss: 0.120809]\n",
      "epoch:28 step:22207 [D loss: 0.021705, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:28 step:22208 [D loss: 0.010728, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:28 step:22209 [D loss: 0.002237, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:28 step:22210 [D loss: 0.002085, acc.: 100.00%] [G loss: 0.000726]\n",
      "epoch:28 step:22211 [D loss: 0.003668, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:28 step:22212 [D loss: 0.001758, acc.: 100.00%] [G loss: 0.109104]\n",
      "epoch:28 step:22213 [D loss: 0.003461, acc.: 100.00%] [G loss: 0.032899]\n",
      "epoch:28 step:22214 [D loss: 0.005784, acc.: 100.00%] [G loss: 0.003642]\n",
      "epoch:28 step:22215 [D loss: 0.011254, acc.: 99.22%] [G loss: 0.009596]\n",
      "epoch:28 step:22216 [D loss: 0.045597, acc.: 100.00%] [G loss: 0.045616]\n",
      "epoch:28 step:22217 [D loss: 0.008071, acc.: 100.00%] [G loss: 0.237025]\n",
      "epoch:28 step:22218 [D loss: 0.026828, acc.: 99.22%] [G loss: 0.000711]\n",
      "epoch:28 step:22219 [D loss: 0.004762, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:28 step:22220 [D loss: 0.003862, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:28 step:22221 [D loss: 0.033278, acc.: 98.44%] [G loss: 0.000020]\n",
      "epoch:28 step:22222 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.000473]\n",
      "epoch:28 step:22223 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:28 step:22224 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:28 step:22225 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:28 step:22226 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.041271]\n",
      "epoch:28 step:22227 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:28 step:22228 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.047936]\n",
      "epoch:28 step:22229 [D loss: 0.007784, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:28 step:22230 [D loss: 0.005127, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:28 step:22231 [D loss: 0.004979, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:28 step:22232 [D loss: 0.005159, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:28 step:22233 [D loss: 0.003723, acc.: 100.00%] [G loss: 0.043365]\n",
      "epoch:28 step:22234 [D loss: 0.054203, acc.: 100.00%] [G loss: 0.407393]\n",
      "epoch:28 step:22235 [D loss: 0.002110, acc.: 100.00%] [G loss: 0.280937]\n",
      "epoch:28 step:22236 [D loss: 0.005042, acc.: 100.00%] [G loss: 0.075629]\n",
      "epoch:28 step:22237 [D loss: 0.004278, acc.: 100.00%] [G loss: 0.028076]\n",
      "epoch:28 step:22238 [D loss: 0.030118, acc.: 100.00%] [G loss: 0.003246]\n",
      "epoch:28 step:22239 [D loss: 0.001559, acc.: 100.00%] [G loss: 0.005363]\n",
      "epoch:28 step:22240 [D loss: 0.001352, acc.: 100.00%] [G loss: 0.003047]\n",
      "epoch:28 step:22241 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.142024]\n",
      "epoch:28 step:22242 [D loss: 0.004538, acc.: 100.00%] [G loss: 0.002767]\n",
      "epoch:28 step:22243 [D loss: 0.036455, acc.: 98.44%] [G loss: 0.002650]\n",
      "epoch:28 step:22244 [D loss: 0.003515, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:28 step:22245 [D loss: 0.001982, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:28 step:22246 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.007684]\n",
      "epoch:28 step:22247 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:28 step:22248 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.001717]\n",
      "epoch:28 step:22249 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000482]\n",
      "epoch:28 step:22250 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:28 step:22251 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:28 step:22252 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:28 step:22253 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:28 step:22254 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.001304]\n",
      "epoch:28 step:22255 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:28 step:22256 [D loss: 0.004740, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:28 step:22257 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:28 step:22258 [D loss: 0.000987, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:28 step:22259 [D loss: 0.003682, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:28 step:22260 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:28 step:22261 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:28 step:22262 [D loss: 0.003147, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22263 [D loss: 0.003322, acc.: 100.00%] [G loss: 0.001404]\n",
      "epoch:28 step:22264 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:28 step:22265 [D loss: 0.006393, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:28 step:22266 [D loss: 0.032196, acc.: 100.00%] [G loss: 0.481223]\n",
      "epoch:28 step:22267 [D loss: 0.005679, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:28 step:22268 [D loss: 0.002418, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:28 step:22269 [D loss: 0.007355, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:28 step:22270 [D loss: 0.002268, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:28 step:22271 [D loss: 0.001434, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:28 step:22272 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.042371]\n",
      "epoch:28 step:22273 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.000132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22274 [D loss: 0.001291, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:28 step:22275 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:28 step:22276 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:28 step:22277 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:28 step:22278 [D loss: 0.001482, acc.: 100.00%] [G loss: 0.095207]\n",
      "epoch:28 step:22279 [D loss: 0.036113, acc.: 99.22%] [G loss: 0.000128]\n",
      "epoch:28 step:22280 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:28 step:22281 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22282 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22283 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:28 step:22284 [D loss: 0.001110, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22285 [D loss: 0.001990, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:28 step:22286 [D loss: 0.001233, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:28 step:22287 [D loss: 0.002265, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22288 [D loss: 0.002039, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:28 step:22289 [D loss: 0.006002, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:28 step:22290 [D loss: 0.003430, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:28 step:22291 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:28 step:22292 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:28 step:22293 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:28 step:22294 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.000509]\n",
      "epoch:28 step:22295 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:28 step:22296 [D loss: 0.002486, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22297 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:28 step:22298 [D loss: 0.013843, acc.: 99.22%] [G loss: 0.000060]\n",
      "epoch:28 step:22299 [D loss: 0.001796, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22300 [D loss: 0.000971, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:28 step:22301 [D loss: 0.002793, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22302 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:28 step:22303 [D loss: 0.000487, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22304 [D loss: 0.002556, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22305 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22306 [D loss: 0.002287, acc.: 100.00%] [G loss: 0.001922]\n",
      "epoch:28 step:22307 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:28 step:22308 [D loss: 0.008921, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22309 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:28 step:22310 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:28 step:22311 [D loss: 0.000685, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22312 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:28 step:22313 [D loss: 0.001973, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22314 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:28 step:22315 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:28 step:22316 [D loss: 0.000769, acc.: 100.00%] [G loss: 0.002603]\n",
      "epoch:28 step:22317 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22318 [D loss: 0.001203, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:28 step:22319 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:28 step:22320 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22321 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:28 step:22322 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22323 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.005603]\n",
      "epoch:28 step:22324 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:28 step:22325 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22326 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:28 step:22327 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22328 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:28 step:22329 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22330 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:28 step:22331 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:28 step:22332 [D loss: 0.001604, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:28 step:22333 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:28 step:22334 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:28 step:22335 [D loss: 0.001071, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:28 step:22336 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:28 step:22337 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:28 step:22338 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:28 step:22339 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22340 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22341 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22342 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22343 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22344 [D loss: 0.001031, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:28 step:22345 [D loss: 0.002015, acc.: 100.00%] [G loss: 0.007584]\n",
      "epoch:28 step:22346 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:28 step:22347 [D loss: 0.022489, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:28 step:22348 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:28 step:22349 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.055752]\n",
      "epoch:28 step:22350 [D loss: 0.020322, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:28 step:22351 [D loss: 0.019788, acc.: 100.00%] [G loss: 0.947034]\n",
      "epoch:28 step:22352 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.040632]\n",
      "epoch:28 step:22353 [D loss: 0.012285, acc.: 100.00%] [G loss: 0.070867]\n",
      "epoch:28 step:22354 [D loss: 0.032558, acc.: 99.22%] [G loss: 0.000588]\n",
      "epoch:28 step:22355 [D loss: 0.001166, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:28 step:22356 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:28 step:22357 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:28 step:22358 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:28 step:22359 [D loss: 0.001265, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22360 [D loss: 0.003265, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:28 step:22361 [D loss: 0.001200, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:28 step:22362 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:28 step:22363 [D loss: 0.001619, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:28 step:22364 [D loss: 0.001606, acc.: 100.00%] [G loss: 0.002824]\n",
      "epoch:28 step:22365 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.007145]\n",
      "epoch:28 step:22366 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:28 step:22367 [D loss: 0.001814, acc.: 100.00%] [G loss: 0.000384]\n",
      "epoch:28 step:22368 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:28 step:22369 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:28 step:22370 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:28 step:22371 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:28 step:22372 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:28 step:22373 [D loss: 0.003593, acc.: 100.00%] [G loss: 0.002473]\n",
      "epoch:28 step:22374 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001891]\n",
      "epoch:28 step:22375 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:28 step:22376 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001311]\n",
      "epoch:28 step:22377 [D loss: 0.001293, acc.: 100.00%] [G loss: 0.001038]\n",
      "epoch:28 step:22378 [D loss: 0.001508, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:28 step:22379 [D loss: 0.001004, acc.: 100.00%] [G loss: 0.003605]\n",
      "epoch:28 step:22380 [D loss: 0.006700, acc.: 100.00%] [G loss: 0.011995]\n",
      "epoch:28 step:22381 [D loss: 0.019190, acc.: 100.00%] [G loss: 0.000667]\n",
      "epoch:28 step:22382 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000930]\n",
      "epoch:28 step:22383 [D loss: 0.039895, acc.: 98.44%] [G loss: 0.000207]\n",
      "epoch:28 step:22384 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.001970]\n",
      "epoch:28 step:22385 [D loss: 0.002081, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:28 step:22386 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22387 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.246928]\n",
      "epoch:28 step:22388 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:28 step:22389 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22390 [D loss: 0.001607, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:28 step:22391 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:28 step:22392 [D loss: 0.003452, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22393 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:28 step:22394 [D loss: 0.008824, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22395 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:28 step:22396 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:28 step:22397 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:28 step:22398 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22399 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22400 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22401 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22402 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:28 step:22403 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:28 step:22404 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:28 step:22405 [D loss: 0.001469, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:28 step:22406 [D loss: 0.002537, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22407 [D loss: 0.042412, acc.: 98.44%] [G loss: 0.047095]\n",
      "epoch:28 step:22408 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.005712]\n",
      "epoch:28 step:22409 [D loss: 0.003694, acc.: 100.00%] [G loss: 0.140996]\n",
      "epoch:28 step:22410 [D loss: 0.059790, acc.: 97.66%] [G loss: 0.000025]\n",
      "epoch:28 step:22411 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:28 step:22412 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22413 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:28 step:22414 [D loss: 0.001789, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:28 step:22415 [D loss: 0.002939, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:28 step:22416 [D loss: 0.000958, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22417 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22418 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:28 step:22419 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:28 step:22420 [D loss: 0.005473, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:28 step:22421 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:28 step:22422 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:28 step:22423 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:28 step:22424 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:28 step:22425 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:28 step:22426 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:28 step:22427 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22428 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:28 step:22429 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:28 step:22430 [D loss: 0.021092, acc.: 99.22%] [G loss: 0.000305]\n",
      "epoch:28 step:22431 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:28 step:22432 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001025]\n",
      "epoch:28 step:22433 [D loss: 0.004904, acc.: 100.00%] [G loss: 0.002078]\n",
      "epoch:28 step:22434 [D loss: 0.002551, acc.: 100.00%] [G loss: 0.069996]\n",
      "epoch:28 step:22435 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:28 step:22436 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:28 step:22437 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:28 step:22438 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.123928]\n",
      "epoch:28 step:22439 [D loss: 0.001605, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:28 step:22440 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:28 step:22441 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:28 step:22442 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:28 step:22443 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:28 step:22444 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:28 step:22445 [D loss: 0.002899, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:28 step:22446 [D loss: 0.016498, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:28 step:22447 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.002065]\n",
      "epoch:28 step:22448 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.014419]\n",
      "epoch:28 step:22449 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.000873]\n",
      "epoch:28 step:22450 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.158451]\n",
      "epoch:28 step:22451 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.001077]\n",
      "epoch:28 step:22452 [D loss: 0.001375, acc.: 100.00%] [G loss: 0.004657]\n",
      "epoch:28 step:22453 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.002823]\n",
      "epoch:28 step:22454 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000318]\n",
      "epoch:28 step:22455 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000989]\n",
      "epoch:28 step:22456 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001582]\n",
      "epoch:28 step:22457 [D loss: 0.004368, acc.: 100.00%] [G loss: 0.001291]\n",
      "epoch:28 step:22458 [D loss: 0.011624, acc.: 100.00%] [G loss: 0.013596]\n",
      "epoch:28 step:22459 [D loss: 0.019819, acc.: 100.00%] [G loss: 0.011775]\n",
      "epoch:28 step:22460 [D loss: 0.013691, acc.: 100.00%] [G loss: 2.577573]\n",
      "epoch:28 step:22461 [D loss: 0.420841, acc.: 78.12%] [G loss: 9.002022]\n",
      "epoch:28 step:22462 [D loss: 1.310836, acc.: 54.69%] [G loss: 4.399389]\n",
      "epoch:28 step:22463 [D loss: 0.129663, acc.: 96.88%] [G loss: 5.452814]\n",
      "epoch:28 step:22464 [D loss: 0.017502, acc.: 100.00%] [G loss: 5.465862]\n",
      "epoch:28 step:22465 [D loss: 0.007238, acc.: 100.00%] [G loss: 5.501184]\n",
      "epoch:28 step:22466 [D loss: 0.036330, acc.: 98.44%] [G loss: 6.464252]\n",
      "epoch:28 step:22467 [D loss: 0.004149, acc.: 100.00%] [G loss: 6.481291]\n",
      "epoch:28 step:22468 [D loss: 0.023283, acc.: 100.00%] [G loss: 6.453611]\n",
      "epoch:28 step:22469 [D loss: 0.005749, acc.: 100.00%] [G loss: 6.228834]\n",
      "epoch:28 step:22470 [D loss: 0.015146, acc.: 100.00%] [G loss: 1.479532]\n",
      "epoch:28 step:22471 [D loss: 0.711457, acc.: 81.25%] [G loss: 9.422569]\n",
      "epoch:28 step:22472 [D loss: 3.017144, acc.: 50.00%] [G loss: 1.770035]\n",
      "epoch:28 step:22473 [D loss: 0.138894, acc.: 96.09%] [G loss: 1.548216]\n",
      "epoch:28 step:22474 [D loss: 0.014529, acc.: 100.00%] [G loss: 0.951349]\n",
      "epoch:28 step:22475 [D loss: 0.038236, acc.: 99.22%] [G loss: 0.529845]\n",
      "epoch:28 step:22476 [D loss: 0.085684, acc.: 97.66%] [G loss: 0.697196]\n",
      "epoch:28 step:22477 [D loss: 0.011303, acc.: 100.00%] [G loss: 0.509944]\n",
      "epoch:28 step:22478 [D loss: 0.019844, acc.: 100.00%] [G loss: 0.391164]\n",
      "epoch:28 step:22479 [D loss: 0.015276, acc.: 100.00%] [G loss: 0.245780]\n",
      "epoch:28 step:22480 [D loss: 0.040756, acc.: 100.00%] [G loss: 0.015429]\n",
      "epoch:28 step:22481 [D loss: 0.070879, acc.: 96.88%] [G loss: 0.057800]\n",
      "epoch:28 step:22482 [D loss: 0.001665, acc.: 100.00%] [G loss: 0.139067]\n",
      "epoch:28 step:22483 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.046209]\n",
      "epoch:28 step:22484 [D loss: 0.003904, acc.: 100.00%] [G loss: 0.022006]\n",
      "epoch:28 step:22485 [D loss: 0.007420, acc.: 100.00%] [G loss: 0.028453]\n",
      "epoch:28 step:22486 [D loss: 0.018007, acc.: 100.00%] [G loss: 0.033190]\n",
      "epoch:28 step:22487 [D loss: 0.033357, acc.: 100.00%] [G loss: 0.000875]\n",
      "epoch:28 step:22488 [D loss: 0.016592, acc.: 99.22%] [G loss: 0.050787]\n",
      "epoch:28 step:22489 [D loss: 0.002006, acc.: 100.00%] [G loss: 0.051226]\n",
      "epoch:28 step:22490 [D loss: 0.002462, acc.: 100.00%] [G loss: 0.044686]\n",
      "epoch:28 step:22491 [D loss: 0.017216, acc.: 100.00%] [G loss: 0.009099]\n",
      "epoch:28 step:22492 [D loss: 0.006120, acc.: 100.00%] [G loss: 0.006086]\n",
      "epoch:28 step:22493 [D loss: 0.006931, acc.: 100.00%] [G loss: 0.008840]\n",
      "epoch:28 step:22494 [D loss: 0.013634, acc.: 100.00%] [G loss: 0.008501]\n",
      "epoch:28 step:22495 [D loss: 0.031231, acc.: 100.00%] [G loss: 0.040633]\n",
      "epoch:28 step:22496 [D loss: 0.007975, acc.: 100.00%] [G loss: 0.292955]\n",
      "epoch:28 step:22497 [D loss: 0.009133, acc.: 100.00%] [G loss: 0.043728]\n",
      "epoch:28 step:22498 [D loss: 0.009313, acc.: 100.00%] [G loss: 0.118466]\n",
      "epoch:28 step:22499 [D loss: 0.046960, acc.: 99.22%] [G loss: 0.089274]\n",
      "epoch:28 step:22500 [D loss: 0.015754, acc.: 100.00%] [G loss: 0.592885]\n",
      "epoch:28 step:22501 [D loss: 0.010006, acc.: 100.00%] [G loss: 0.491403]\n",
      "epoch:28 step:22502 [D loss: 0.080906, acc.: 99.22%] [G loss: 2.215398]\n",
      "epoch:28 step:22503 [D loss: 0.100386, acc.: 96.09%] [G loss: 2.325200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22504 [D loss: 0.065615, acc.: 96.88%] [G loss: 5.750277]\n",
      "epoch:28 step:22505 [D loss: 0.022838, acc.: 99.22%] [G loss: 4.791560]\n",
      "epoch:28 step:22506 [D loss: 0.029454, acc.: 100.00%] [G loss: 0.004114]\n",
      "epoch:28 step:22507 [D loss: 0.022237, acc.: 99.22%] [G loss: 4.485948]\n",
      "epoch:28 step:22508 [D loss: 0.079817, acc.: 98.44%] [G loss: 0.003575]\n",
      "epoch:28 step:22509 [D loss: 0.020331, acc.: 99.22%] [G loss: 4.049294]\n",
      "epoch:28 step:22510 [D loss: 0.109988, acc.: 96.88%] [G loss: 2.461345]\n",
      "epoch:28 step:22511 [D loss: 0.011717, acc.: 100.00%] [G loss: 1.857634]\n",
      "epoch:28 step:22512 [D loss: 0.041732, acc.: 100.00%] [G loss: 1.021550]\n",
      "epoch:28 step:22513 [D loss: 0.198466, acc.: 91.41%] [G loss: 3.575124]\n",
      "epoch:28 step:22514 [D loss: 0.891652, acc.: 60.94%] [G loss: 1.369407]\n",
      "epoch:28 step:22515 [D loss: 0.034392, acc.: 100.00%] [G loss: 0.365352]\n",
      "epoch:28 step:22516 [D loss: 0.032653, acc.: 98.44%] [G loss: 1.198346]\n",
      "epoch:28 step:22517 [D loss: 0.003082, acc.: 100.00%] [G loss: 1.743407]\n",
      "epoch:28 step:22518 [D loss: 0.004266, acc.: 100.00%] [G loss: 0.028652]\n",
      "epoch:28 step:22519 [D loss: 0.005105, acc.: 100.00%] [G loss: 1.267333]\n",
      "epoch:28 step:22520 [D loss: 0.027223, acc.: 99.22%] [G loss: 0.705835]\n",
      "epoch:28 step:22521 [D loss: 0.003389, acc.: 100.00%] [G loss: 1.075238]\n",
      "epoch:28 step:22522 [D loss: 0.003895, acc.: 100.00%] [G loss: 0.534773]\n",
      "epoch:28 step:22523 [D loss: 0.002339, acc.: 100.00%] [G loss: 0.269086]\n",
      "epoch:28 step:22524 [D loss: 0.006001, acc.: 100.00%] [G loss: 0.029427]\n",
      "epoch:28 step:22525 [D loss: 0.003414, acc.: 100.00%] [G loss: 0.468555]\n",
      "epoch:28 step:22526 [D loss: 0.002541, acc.: 100.00%] [G loss: 0.034003]\n",
      "epoch:28 step:22527 [D loss: 0.006269, acc.: 100.00%] [G loss: 0.005642]\n",
      "epoch:28 step:22528 [D loss: 0.013773, acc.: 99.22%] [G loss: 0.148532]\n",
      "epoch:28 step:22529 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.030684]\n",
      "epoch:28 step:22530 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.102603]\n",
      "epoch:28 step:22531 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.077993]\n",
      "epoch:28 step:22532 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.002245]\n",
      "epoch:28 step:22533 [D loss: 0.010344, acc.: 100.00%] [G loss: 0.495746]\n",
      "epoch:28 step:22534 [D loss: 0.000971, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:28 step:22535 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.009495]\n",
      "epoch:28 step:22536 [D loss: 0.001251, acc.: 100.00%] [G loss: 0.083119]\n",
      "epoch:28 step:22537 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.045725]\n",
      "epoch:28 step:22538 [D loss: 0.000982, acc.: 100.00%] [G loss: 0.054154]\n",
      "epoch:28 step:22539 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000703]\n",
      "epoch:28 step:22540 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.015515]\n",
      "epoch:28 step:22541 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.015832]\n",
      "epoch:28 step:22542 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.001419]\n",
      "epoch:28 step:22543 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.023208]\n",
      "epoch:28 step:22544 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.004243]\n",
      "epoch:28 step:22545 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.004298]\n",
      "epoch:28 step:22546 [D loss: 0.000280, acc.: 100.00%] [G loss: 3.597828]\n",
      "epoch:28 step:22547 [D loss: 0.001783, acc.: 100.00%] [G loss: 0.009443]\n",
      "epoch:28 step:22548 [D loss: 0.008823, acc.: 100.00%] [G loss: 0.020089]\n",
      "epoch:28 step:22549 [D loss: 0.083628, acc.: 100.00%] [G loss: 0.258069]\n",
      "epoch:28 step:22550 [D loss: 0.005115, acc.: 100.00%] [G loss: 0.665411]\n",
      "epoch:28 step:22551 [D loss: 0.016671, acc.: 99.22%] [G loss: 0.590427]\n",
      "epoch:28 step:22552 [D loss: 0.019066, acc.: 100.00%] [G loss: 0.192744]\n",
      "epoch:28 step:22553 [D loss: 0.009863, acc.: 100.00%] [G loss: 0.076686]\n",
      "epoch:28 step:22554 [D loss: 0.037693, acc.: 99.22%] [G loss: 0.127364]\n",
      "epoch:28 step:22555 [D loss: 0.015166, acc.: 100.00%] [G loss: 0.580325]\n",
      "epoch:28 step:22556 [D loss: 0.007154, acc.: 100.00%] [G loss: 0.099411]\n",
      "epoch:28 step:22557 [D loss: 0.003216, acc.: 100.00%] [G loss: 0.633145]\n",
      "epoch:28 step:22558 [D loss: 0.008634, acc.: 100.00%] [G loss: 0.117497]\n",
      "epoch:28 step:22559 [D loss: 0.004896, acc.: 100.00%] [G loss: 0.042436]\n",
      "epoch:28 step:22560 [D loss: 0.001406, acc.: 100.00%] [G loss: 0.953865]\n",
      "epoch:28 step:22561 [D loss: 0.001591, acc.: 100.00%] [G loss: 0.326256]\n",
      "epoch:28 step:22562 [D loss: 0.003281, acc.: 100.00%] [G loss: 0.302655]\n",
      "epoch:28 step:22563 [D loss: 0.001746, acc.: 100.00%] [G loss: 0.182070]\n",
      "epoch:28 step:22564 [D loss: 0.011916, acc.: 100.00%] [G loss: 0.097195]\n",
      "epoch:28 step:22565 [D loss: 0.004563, acc.: 100.00%] [G loss: 0.012296]\n",
      "epoch:28 step:22566 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.120735]\n",
      "epoch:28 step:22567 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.004474]\n",
      "epoch:28 step:22568 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.225180]\n",
      "epoch:28 step:22569 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.032925]\n",
      "epoch:28 step:22570 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.001900]\n",
      "epoch:28 step:22571 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.047002]\n",
      "epoch:28 step:22572 [D loss: 0.001464, acc.: 100.00%] [G loss: 0.028364]\n",
      "epoch:28 step:22573 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.117192]\n",
      "epoch:28 step:22574 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.034706]\n",
      "epoch:28 step:22575 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.024268]\n",
      "epoch:28 step:22576 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.070445]\n",
      "epoch:28 step:22577 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.013253]\n",
      "epoch:28 step:22578 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.036933]\n",
      "epoch:28 step:22579 [D loss: 0.004022, acc.: 100.00%] [G loss: 0.016065]\n",
      "epoch:28 step:22580 [D loss: 0.001082, acc.: 100.00%] [G loss: 0.044321]\n",
      "epoch:28 step:22581 [D loss: 0.033247, acc.: 99.22%] [G loss: 0.001761]\n",
      "epoch:28 step:22582 [D loss: 0.025870, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:28 step:22583 [D loss: 0.007960, acc.: 100.00%] [G loss: 0.003570]\n",
      "epoch:28 step:22584 [D loss: 0.001432, acc.: 100.00%] [G loss: 0.000795]\n",
      "epoch:28 step:22585 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000500]\n",
      "epoch:28 step:22586 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:28 step:22587 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.014459]\n",
      "epoch:28 step:22588 [D loss: 0.001736, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:28 step:22589 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.097327]\n",
      "epoch:28 step:22590 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.080338]\n",
      "epoch:28 step:22591 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.015273]\n",
      "epoch:28 step:22592 [D loss: 0.001004, acc.: 100.00%] [G loss: 0.005784]\n",
      "epoch:28 step:22593 [D loss: 0.009038, acc.: 100.00%] [G loss: 0.001308]\n",
      "epoch:28 step:22594 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:28 step:22595 [D loss: 0.001984, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:28 step:22596 [D loss: 0.013182, acc.: 100.00%] [G loss: 3.047828]\n",
      "epoch:28 step:22597 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.029079]\n",
      "epoch:28 step:22598 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.199855]\n",
      "epoch:28 step:22599 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.034740]\n",
      "epoch:28 step:22600 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.025096]\n",
      "epoch:28 step:22601 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:28 step:22602 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.004595]\n",
      "epoch:28 step:22603 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.001127]\n",
      "epoch:28 step:22604 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.087578]\n",
      "epoch:28 step:22605 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.009203]\n",
      "epoch:28 step:22606 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.154683]\n",
      "epoch:28 step:22607 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.000969]\n",
      "epoch:28 step:22608 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.038780]\n",
      "epoch:28 step:22609 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.008001]\n",
      "epoch:28 step:22610 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.001629]\n",
      "epoch:28 step:22611 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.003775]\n",
      "epoch:28 step:22612 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.011975]\n",
      "epoch:28 step:22613 [D loss: 0.001585, acc.: 100.00%] [G loss: 0.005272]\n",
      "epoch:28 step:22614 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.004340]\n",
      "epoch:28 step:22615 [D loss: 0.002145, acc.: 100.00%] [G loss: 0.014756]\n",
      "epoch:28 step:22616 [D loss: 0.090958, acc.: 97.66%] [G loss: 0.183219]\n",
      "epoch:28 step:22617 [D loss: 0.021761, acc.: 99.22%] [G loss: 0.706540]\n",
      "epoch:28 step:22618 [D loss: 0.103250, acc.: 95.31%] [G loss: 0.121938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22619 [D loss: 0.076242, acc.: 98.44%] [G loss: 0.010585]\n",
      "epoch:28 step:22620 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.186147]\n",
      "epoch:28 step:22621 [D loss: 0.008121, acc.: 100.00%] [G loss: 0.125915]\n",
      "epoch:28 step:22622 [D loss: 0.051640, acc.: 97.66%] [G loss: 0.386790]\n",
      "epoch:28 step:22623 [D loss: 0.002113, acc.: 100.00%] [G loss: 0.110443]\n",
      "epoch:28 step:22624 [D loss: 0.002556, acc.: 100.00%] [G loss: 0.096921]\n",
      "epoch:28 step:22625 [D loss: 0.000691, acc.: 100.00%] [G loss: 0.111712]\n",
      "epoch:28 step:22626 [D loss: 0.003582, acc.: 100.00%] [G loss: 0.020345]\n",
      "epoch:28 step:22627 [D loss: 0.009576, acc.: 100.00%] [G loss: 0.022488]\n",
      "epoch:28 step:22628 [D loss: 0.015564, acc.: 100.00%] [G loss: 0.071028]\n",
      "epoch:28 step:22629 [D loss: 0.004517, acc.: 100.00%] [G loss: 1.173182]\n",
      "epoch:28 step:22630 [D loss: 0.039639, acc.: 99.22%] [G loss: 0.000115]\n",
      "epoch:28 step:22631 [D loss: 0.002135, acc.: 100.00%] [G loss: 0.324169]\n",
      "epoch:28 step:22632 [D loss: 0.006323, acc.: 100.00%] [G loss: 0.197684]\n",
      "epoch:28 step:22633 [D loss: 0.009072, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:28 step:22634 [D loss: 0.001972, acc.: 100.00%] [G loss: 0.227502]\n",
      "epoch:28 step:22635 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.002166]\n",
      "epoch:28 step:22636 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.003190]\n",
      "epoch:28 step:22637 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.225668]\n",
      "epoch:28 step:22638 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.327745]\n",
      "epoch:28 step:22639 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.102006]\n",
      "epoch:28 step:22640 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.123783]\n",
      "epoch:28 step:22641 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.047259]\n",
      "epoch:28 step:22642 [D loss: 0.004204, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:28 step:22643 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.293587]\n",
      "epoch:28 step:22644 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.024174]\n",
      "epoch:28 step:22645 [D loss: 0.001074, acc.: 100.00%] [G loss: 0.026674]\n",
      "epoch:28 step:22646 [D loss: 0.004252, acc.: 100.00%] [G loss: 0.002156]\n",
      "epoch:28 step:22647 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.157646]\n",
      "epoch:28 step:22648 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.037948]\n",
      "epoch:28 step:22649 [D loss: 0.004210, acc.: 100.00%] [G loss: 0.010483]\n",
      "epoch:29 step:22650 [D loss: 0.005447, acc.: 100.00%] [G loss: 0.003463]\n",
      "epoch:29 step:22651 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:29 step:22652 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.002311]\n",
      "epoch:29 step:22653 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:29 step:22654 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.030264]\n",
      "epoch:29 step:22655 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.003823]\n",
      "epoch:29 step:22656 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.002906]\n",
      "epoch:29 step:22657 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:29 step:22658 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.006720]\n",
      "epoch:29 step:22659 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.001946]\n",
      "epoch:29 step:22660 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:29 step:22661 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.013024]\n",
      "epoch:29 step:22662 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.007488]\n",
      "epoch:29 step:22663 [D loss: 0.000773, acc.: 100.00%] [G loss: 0.008123]\n",
      "epoch:29 step:22664 [D loss: 0.003227, acc.: 100.00%] [G loss: 0.001436]\n",
      "epoch:29 step:22665 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.056362]\n",
      "epoch:29 step:22666 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.013984]\n",
      "epoch:29 step:22667 [D loss: 0.000910, acc.: 100.00%] [G loss: 0.005686]\n",
      "epoch:29 step:22668 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.001633]\n",
      "epoch:29 step:22669 [D loss: 0.004386, acc.: 100.00%] [G loss: 0.002204]\n",
      "epoch:29 step:22670 [D loss: 0.003039, acc.: 100.00%] [G loss: 0.004367]\n",
      "epoch:29 step:22671 [D loss: 0.002521, acc.: 100.00%] [G loss: 3.591427]\n",
      "epoch:29 step:22672 [D loss: 0.094146, acc.: 96.09%] [G loss: 3.112438]\n",
      "epoch:29 step:22673 [D loss: 0.002179, acc.: 100.00%] [G loss: 4.901186]\n",
      "epoch:29 step:22674 [D loss: 0.229961, acc.: 91.41%] [G loss: 0.000672]\n",
      "epoch:29 step:22675 [D loss: 0.245417, acc.: 89.06%] [G loss: 3.335040]\n",
      "epoch:29 step:22676 [D loss: 0.023492, acc.: 99.22%] [G loss: 6.235958]\n",
      "epoch:29 step:22677 [D loss: 0.945657, acc.: 67.19%] [G loss: 0.365900]\n",
      "epoch:29 step:22678 [D loss: 0.437826, acc.: 82.03%] [G loss: 2.684505]\n",
      "epoch:29 step:22679 [D loss: 0.000146, acc.: 100.00%] [G loss: 5.015744]\n",
      "epoch:29 step:22680 [D loss: 0.257515, acc.: 93.75%] [G loss: 1.259692]\n",
      "epoch:29 step:22681 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.055207]\n",
      "epoch:29 step:22682 [D loss: 0.007117, acc.: 100.00%] [G loss: 1.145669]\n",
      "epoch:29 step:22683 [D loss: 0.000241, acc.: 100.00%] [G loss: 1.102253]\n",
      "epoch:29 step:22684 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.553353]\n",
      "epoch:29 step:22685 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.143347]\n",
      "epoch:29 step:22686 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.163144]\n",
      "epoch:29 step:22687 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.128555]\n",
      "epoch:29 step:22688 [D loss: 0.002309, acc.: 100.00%] [G loss: 0.032437]\n",
      "epoch:29 step:22689 [D loss: 0.001586, acc.: 100.00%] [G loss: 0.051440]\n",
      "epoch:29 step:22690 [D loss: 0.002737, acc.: 100.00%] [G loss: 0.066932]\n",
      "epoch:29 step:22691 [D loss: 0.029978, acc.: 99.22%] [G loss: 0.012504]\n",
      "epoch:29 step:22692 [D loss: 0.002612, acc.: 100.00%] [G loss: 0.055901]\n",
      "epoch:29 step:22693 [D loss: 0.001986, acc.: 100.00%] [G loss: 0.079791]\n",
      "epoch:29 step:22694 [D loss: 0.002576, acc.: 100.00%] [G loss: 0.040470]\n",
      "epoch:29 step:22695 [D loss: 0.001365, acc.: 100.00%] [G loss: 0.063378]\n",
      "epoch:29 step:22696 [D loss: 0.004553, acc.: 100.00%] [G loss: 0.027510]\n",
      "epoch:29 step:22697 [D loss: 0.006096, acc.: 100.00%] [G loss: 0.031121]\n",
      "epoch:29 step:22698 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.085840]\n",
      "epoch:29 step:22699 [D loss: 0.002984, acc.: 100.00%] [G loss: 0.038869]\n",
      "epoch:29 step:22700 [D loss: 0.002306, acc.: 100.00%] [G loss: 5.648982]\n",
      "epoch:29 step:22701 [D loss: 0.125672, acc.: 93.75%] [G loss: 1.539792]\n",
      "epoch:29 step:22702 [D loss: 0.000625, acc.: 100.00%] [G loss: 2.000564]\n",
      "epoch:29 step:22703 [D loss: 0.240364, acc.: 86.72%] [G loss: 0.119459]\n",
      "epoch:29 step:22704 [D loss: 0.005524, acc.: 100.00%] [G loss: 0.170410]\n",
      "epoch:29 step:22705 [D loss: 0.007803, acc.: 100.00%] [G loss: 0.173212]\n",
      "epoch:29 step:22706 [D loss: 0.078320, acc.: 97.66%] [G loss: 3.509611]\n",
      "epoch:29 step:22707 [D loss: 0.000585, acc.: 100.00%] [G loss: 5.868745]\n",
      "epoch:29 step:22708 [D loss: 0.000426, acc.: 100.00%] [G loss: 6.258724]\n",
      "epoch:29 step:22709 [D loss: 0.000827, acc.: 100.00%] [G loss: 5.752061]\n",
      "epoch:29 step:22710 [D loss: 0.004594, acc.: 100.00%] [G loss: 5.686077]\n",
      "epoch:29 step:22711 [D loss: 0.002104, acc.: 100.00%] [G loss: 4.457162]\n",
      "epoch:29 step:22712 [D loss: 0.002969, acc.: 100.00%] [G loss: 0.423037]\n",
      "epoch:29 step:22713 [D loss: 0.002331, acc.: 100.00%] [G loss: 4.164538]\n",
      "epoch:29 step:22714 [D loss: 0.006262, acc.: 100.00%] [G loss: 0.000972]\n",
      "epoch:29 step:22715 [D loss: 0.020240, acc.: 100.00%] [G loss: 3.171647]\n",
      "epoch:29 step:22716 [D loss: 0.004650, acc.: 100.00%] [G loss: 0.022188]\n",
      "epoch:29 step:22717 [D loss: 0.014946, acc.: 100.00%] [G loss: 7.428576]\n",
      "epoch:29 step:22718 [D loss: 0.001360, acc.: 100.00%] [G loss: 3.306426]\n",
      "epoch:29 step:22719 [D loss: 0.044166, acc.: 100.00%] [G loss: 3.750370]\n",
      "epoch:29 step:22720 [D loss: 0.006300, acc.: 100.00%] [G loss: 4.027562]\n",
      "epoch:29 step:22721 [D loss: 0.003381, acc.: 100.00%] [G loss: 2.687103]\n",
      "epoch:29 step:22722 [D loss: 0.001667, acc.: 100.00%] [G loss: 2.637751]\n",
      "epoch:29 step:22723 [D loss: 0.000559, acc.: 100.00%] [G loss: 1.599533]\n",
      "epoch:29 step:22724 [D loss: 0.000690, acc.: 100.00%] [G loss: 2.215645]\n",
      "epoch:29 step:22725 [D loss: 0.004072, acc.: 100.00%] [G loss: 1.210106]\n",
      "epoch:29 step:22726 [D loss: 0.004904, acc.: 100.00%] [G loss: 0.756543]\n",
      "epoch:29 step:22727 [D loss: 0.002111, acc.: 100.00%] [G loss: 0.330577]\n",
      "epoch:29 step:22728 [D loss: 0.002016, acc.: 100.00%] [G loss: 0.670609]\n",
      "epoch:29 step:22729 [D loss: 0.000980, acc.: 100.00%] [G loss: 7.141791]\n",
      "epoch:29 step:22730 [D loss: 0.002106, acc.: 100.00%] [G loss: 0.388576]\n",
      "epoch:29 step:22731 [D loss: 0.003706, acc.: 100.00%] [G loss: 0.328561]\n",
      "epoch:29 step:22732 [D loss: 0.005533, acc.: 100.00%] [G loss: 0.034624]\n",
      "epoch:29 step:22733 [D loss: 0.003329, acc.: 100.00%] [G loss: 0.352421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22734 [D loss: 0.003697, acc.: 100.00%] [G loss: 0.486442]\n",
      "epoch:29 step:22735 [D loss: 0.004169, acc.: 100.00%] [G loss: 0.397780]\n",
      "epoch:29 step:22736 [D loss: 0.002902, acc.: 100.00%] [G loss: 0.329467]\n",
      "epoch:29 step:22737 [D loss: 0.005779, acc.: 100.00%] [G loss: 0.001293]\n",
      "epoch:29 step:22738 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.161620]\n",
      "epoch:29 step:22739 [D loss: 0.011172, acc.: 100.00%] [G loss: 3.617236]\n",
      "epoch:29 step:22740 [D loss: 0.079423, acc.: 98.44%] [G loss: 0.630354]\n",
      "epoch:29 step:22741 [D loss: 0.000926, acc.: 100.00%] [G loss: 1.082542]\n",
      "epoch:29 step:22742 [D loss: 0.007815, acc.: 100.00%] [G loss: 0.911670]\n",
      "epoch:29 step:22743 [D loss: 0.004272, acc.: 100.00%] [G loss: 0.871397]\n",
      "epoch:29 step:22744 [D loss: 0.005537, acc.: 100.00%] [G loss: 0.603207]\n",
      "epoch:29 step:22745 [D loss: 0.002112, acc.: 100.00%] [G loss: 0.022705]\n",
      "epoch:29 step:22746 [D loss: 0.002053, acc.: 100.00%] [G loss: 0.355251]\n",
      "epoch:29 step:22747 [D loss: 0.008260, acc.: 100.00%] [G loss: 1.818620]\n",
      "epoch:29 step:22748 [D loss: 0.171519, acc.: 95.31%] [G loss: 1.772476]\n",
      "epoch:29 step:22749 [D loss: 0.070466, acc.: 96.88%] [G loss: 1.347384]\n",
      "epoch:29 step:22750 [D loss: 0.006642, acc.: 100.00%] [G loss: 2.133314]\n",
      "epoch:29 step:22751 [D loss: 0.002858, acc.: 100.00%] [G loss: 1.430657]\n",
      "epoch:29 step:22752 [D loss: 0.013533, acc.: 100.00%] [G loss: 0.034455]\n",
      "epoch:29 step:22753 [D loss: 0.024436, acc.: 99.22%] [G loss: 0.000443]\n",
      "epoch:29 step:22754 [D loss: 0.006657, acc.: 100.00%] [G loss: 0.512739]\n",
      "epoch:29 step:22755 [D loss: 0.006567, acc.: 100.00%] [G loss: 0.294158]\n",
      "epoch:29 step:22756 [D loss: 0.001245, acc.: 100.00%] [G loss: 0.326272]\n",
      "epoch:29 step:22757 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.144862]\n",
      "epoch:29 step:22758 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.091442]\n",
      "epoch:29 step:22759 [D loss: 0.001953, acc.: 100.00%] [G loss: 0.001323]\n",
      "epoch:29 step:22760 [D loss: 0.006785, acc.: 100.00%] [G loss: 0.133014]\n",
      "epoch:29 step:22761 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.008575]\n",
      "epoch:29 step:22762 [D loss: 0.002940, acc.: 100.00%] [G loss: 0.001469]\n",
      "epoch:29 step:22763 [D loss: 0.010292, acc.: 100.00%] [G loss: 0.166587]\n",
      "epoch:29 step:22764 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.148215]\n",
      "epoch:29 step:22765 [D loss: 0.005570, acc.: 100.00%] [G loss: 0.024611]\n",
      "epoch:29 step:22766 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.006653]\n",
      "epoch:29 step:22767 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.015361]\n",
      "epoch:29 step:22768 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.013811]\n",
      "epoch:29 step:22769 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.034168]\n",
      "epoch:29 step:22770 [D loss: 0.001559, acc.: 100.00%] [G loss: 0.038454]\n",
      "epoch:29 step:22771 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.027344]\n",
      "epoch:29 step:22772 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.037095]\n",
      "epoch:29 step:22773 [D loss: 0.001002, acc.: 100.00%] [G loss: 0.001310]\n",
      "epoch:29 step:22774 [D loss: 0.002947, acc.: 100.00%] [G loss: 0.024437]\n",
      "epoch:29 step:22775 [D loss: 0.033426, acc.: 100.00%] [G loss: 0.102934]\n",
      "epoch:29 step:22776 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.216538]\n",
      "epoch:29 step:22777 [D loss: 0.033890, acc.: 98.44%] [G loss: 0.030439]\n",
      "epoch:29 step:22778 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.010006]\n",
      "epoch:29 step:22779 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.034688]\n",
      "epoch:29 step:22780 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.011419]\n",
      "epoch:29 step:22781 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.006713]\n",
      "epoch:29 step:22782 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.010739]\n",
      "epoch:29 step:22783 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.085077]\n",
      "epoch:29 step:22784 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.002905]\n",
      "epoch:29 step:22785 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.014707]\n",
      "epoch:29 step:22786 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.008367]\n",
      "epoch:29 step:22787 [D loss: 0.002187, acc.: 100.00%] [G loss: 0.011271]\n",
      "epoch:29 step:22788 [D loss: 0.002914, acc.: 100.00%] [G loss: 0.018772]\n",
      "epoch:29 step:22789 [D loss: 0.001998, acc.: 100.00%] [G loss: 0.016016]\n",
      "epoch:29 step:22790 [D loss: 0.000555, acc.: 100.00%] [G loss: 6.825171]\n",
      "epoch:29 step:22791 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.003245]\n",
      "epoch:29 step:22792 [D loss: 0.000833, acc.: 100.00%] [G loss: 0.039525]\n",
      "epoch:29 step:22793 [D loss: 0.011522, acc.: 100.00%] [G loss: 0.242624]\n",
      "epoch:29 step:22794 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.039799]\n",
      "epoch:29 step:22795 [D loss: 0.002177, acc.: 100.00%] [G loss: 0.021119]\n",
      "epoch:29 step:22796 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.196187]\n",
      "epoch:29 step:22797 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.004830]\n",
      "epoch:29 step:22798 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.015107]\n",
      "epoch:29 step:22799 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.030686]\n",
      "epoch:29 step:22800 [D loss: 0.001772, acc.: 100.00%] [G loss: 5.215306]\n",
      "epoch:29 step:22801 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.053849]\n",
      "epoch:29 step:22802 [D loss: 0.003738, acc.: 100.00%] [G loss: 0.026857]\n",
      "epoch:29 step:22803 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.017584]\n",
      "epoch:29 step:22804 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.002732]\n",
      "epoch:29 step:22805 [D loss: 0.004468, acc.: 100.00%] [G loss: 0.825121]\n",
      "epoch:29 step:22806 [D loss: 0.001221, acc.: 100.00%] [G loss: 0.004562]\n",
      "epoch:29 step:22807 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:29 step:22808 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.002768]\n",
      "epoch:29 step:22809 [D loss: 0.002474, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:29 step:22810 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.017144]\n",
      "epoch:29 step:22811 [D loss: 0.002007, acc.: 100.00%] [G loss: 0.011838]\n",
      "epoch:29 step:22812 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.008307]\n",
      "epoch:29 step:22813 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.025972]\n",
      "epoch:29 step:22814 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.002730]\n",
      "epoch:29 step:22815 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.002765]\n",
      "epoch:29 step:22816 [D loss: 0.001346, acc.: 100.00%] [G loss: 0.003082]\n",
      "epoch:29 step:22817 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.003941]\n",
      "epoch:29 step:22818 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.003582]\n",
      "epoch:29 step:22819 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.002094]\n",
      "epoch:29 step:22820 [D loss: 0.004797, acc.: 100.00%] [G loss: 0.000550]\n",
      "epoch:29 step:22821 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:29 step:22822 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.004179]\n",
      "epoch:29 step:22823 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.006247]\n",
      "epoch:29 step:22824 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.603166]\n",
      "epoch:29 step:22825 [D loss: 0.028859, acc.: 100.00%] [G loss: 0.032869]\n",
      "epoch:29 step:22826 [D loss: 0.003958, acc.: 100.00%] [G loss: 0.618162]\n",
      "epoch:29 step:22827 [D loss: 0.025798, acc.: 100.00%] [G loss: 0.734379]\n",
      "epoch:29 step:22828 [D loss: 0.588063, acc.: 70.31%] [G loss: 8.703566]\n",
      "epoch:29 step:22829 [D loss: 4.805283, acc.: 50.00%] [G loss: 5.607189]\n",
      "epoch:29 step:22830 [D loss: 0.964312, acc.: 58.59%] [G loss: 0.128602]\n",
      "epoch:29 step:22831 [D loss: 0.022076, acc.: 100.00%] [G loss: 0.008962]\n",
      "epoch:29 step:22832 [D loss: 0.034070, acc.: 100.00%] [G loss: 0.881505]\n",
      "epoch:29 step:22833 [D loss: 0.062225, acc.: 99.22%] [G loss: 0.533981]\n",
      "epoch:29 step:22834 [D loss: 0.025392, acc.: 100.00%] [G loss: 0.024754]\n",
      "epoch:29 step:22835 [D loss: 0.055286, acc.: 100.00%] [G loss: 0.111998]\n",
      "epoch:29 step:22836 [D loss: 0.010151, acc.: 100.00%] [G loss: 0.151552]\n",
      "epoch:29 step:22837 [D loss: 0.017820, acc.: 100.00%] [G loss: 1.091595]\n",
      "epoch:29 step:22838 [D loss: 0.004878, acc.: 100.00%] [G loss: 0.039783]\n",
      "epoch:29 step:22839 [D loss: 0.149795, acc.: 96.09%] [G loss: 1.145242]\n",
      "epoch:29 step:22840 [D loss: 0.017141, acc.: 99.22%] [G loss: 2.008577]\n",
      "epoch:29 step:22841 [D loss: 0.037909, acc.: 97.66%] [G loss: 6.046594]\n",
      "epoch:29 step:22842 [D loss: 0.037415, acc.: 97.66%] [G loss: 0.089453]\n",
      "epoch:29 step:22843 [D loss: 0.008721, acc.: 100.00%] [G loss: 0.024492]\n",
      "epoch:29 step:22844 [D loss: 0.007442, acc.: 100.00%] [G loss: 0.093094]\n",
      "epoch:29 step:22845 [D loss: 0.022342, acc.: 100.00%] [G loss: 0.135785]\n",
      "epoch:29 step:22846 [D loss: 0.013266, acc.: 100.00%] [G loss: 2.817780]\n",
      "epoch:29 step:22847 [D loss: 0.302179, acc.: 88.28%] [G loss: 6.938163]\n",
      "epoch:29 step:22848 [D loss: 0.645705, acc.: 67.19%] [G loss: 1.810932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22849 [D loss: 0.016779, acc.: 100.00%] [G loss: 0.132506]\n",
      "epoch:29 step:22850 [D loss: 0.060606, acc.: 98.44%] [G loss: 4.264971]\n",
      "epoch:29 step:22851 [D loss: 0.013035, acc.: 100.00%] [G loss: 0.211793]\n",
      "epoch:29 step:22852 [D loss: 0.016705, acc.: 100.00%] [G loss: 0.093758]\n",
      "epoch:29 step:22853 [D loss: 0.060758, acc.: 100.00%] [G loss: 3.437951]\n",
      "epoch:29 step:22854 [D loss: 0.048402, acc.: 98.44%] [G loss: 2.521136]\n",
      "epoch:29 step:22855 [D loss: 0.016704, acc.: 100.00%] [G loss: 1.550845]\n",
      "epoch:29 step:22856 [D loss: 0.008512, acc.: 100.00%] [G loss: 0.137991]\n",
      "epoch:29 step:22857 [D loss: 0.025062, acc.: 100.00%] [G loss: 0.134500]\n",
      "epoch:29 step:22858 [D loss: 0.031370, acc.: 99.22%] [G loss: 0.948624]\n",
      "epoch:29 step:22859 [D loss: 0.042935, acc.: 98.44%] [G loss: 0.326069]\n",
      "epoch:29 step:22860 [D loss: 0.004023, acc.: 100.00%] [G loss: 0.374993]\n",
      "epoch:29 step:22861 [D loss: 0.003428, acc.: 100.00%] [G loss: 0.145197]\n",
      "epoch:29 step:22862 [D loss: 0.002703, acc.: 100.00%] [G loss: 0.146361]\n",
      "epoch:29 step:22863 [D loss: 0.009111, acc.: 100.00%] [G loss: 0.054323]\n",
      "epoch:29 step:22864 [D loss: 0.004656, acc.: 100.00%] [G loss: 0.051047]\n",
      "epoch:29 step:22865 [D loss: 0.008738, acc.: 100.00%] [G loss: 0.009366]\n",
      "epoch:29 step:22866 [D loss: 0.020862, acc.: 100.00%] [G loss: 0.186605]\n",
      "epoch:29 step:22867 [D loss: 0.003730, acc.: 100.00%] [G loss: 0.100637]\n",
      "epoch:29 step:22868 [D loss: 0.001847, acc.: 100.00%] [G loss: 0.056144]\n",
      "epoch:29 step:22869 [D loss: 0.006405, acc.: 100.00%] [G loss: 0.079400]\n",
      "epoch:29 step:22870 [D loss: 0.001455, acc.: 100.00%] [G loss: 0.070910]\n",
      "epoch:29 step:22871 [D loss: 0.001886, acc.: 100.00%] [G loss: 0.037547]\n",
      "epoch:29 step:22872 [D loss: 0.002608, acc.: 100.00%] [G loss: 0.078510]\n",
      "epoch:29 step:22873 [D loss: 0.017547, acc.: 100.00%] [G loss: 0.157718]\n",
      "epoch:29 step:22874 [D loss: 0.008593, acc.: 100.00%] [G loss: 0.138482]\n",
      "epoch:29 step:22875 [D loss: 0.027369, acc.: 99.22%] [G loss: 0.088848]\n",
      "epoch:29 step:22876 [D loss: 0.027755, acc.: 100.00%] [G loss: 0.357425]\n",
      "epoch:29 step:22877 [D loss: 0.023356, acc.: 100.00%] [G loss: 0.432483]\n",
      "epoch:29 step:22878 [D loss: 0.037895, acc.: 100.00%] [G loss: 0.212348]\n",
      "epoch:29 step:22879 [D loss: 0.045767, acc.: 98.44%] [G loss: 0.090702]\n",
      "epoch:29 step:22880 [D loss: 0.004966, acc.: 100.00%] [G loss: 0.132806]\n",
      "epoch:29 step:22881 [D loss: 0.003800, acc.: 100.00%] [G loss: 0.018147]\n",
      "epoch:29 step:22882 [D loss: 0.007538, acc.: 100.00%] [G loss: 0.092151]\n",
      "epoch:29 step:22883 [D loss: 0.003908, acc.: 100.00%] [G loss: 0.025600]\n",
      "epoch:29 step:22884 [D loss: 0.002040, acc.: 100.00%] [G loss: 3.568897]\n",
      "epoch:29 step:22885 [D loss: 0.009122, acc.: 100.00%] [G loss: 0.019394]\n",
      "epoch:29 step:22886 [D loss: 0.003332, acc.: 100.00%] [G loss: 0.028605]\n",
      "epoch:29 step:22887 [D loss: 0.030579, acc.: 100.00%] [G loss: 0.100674]\n",
      "epoch:29 step:22888 [D loss: 0.001543, acc.: 100.00%] [G loss: 0.280575]\n",
      "epoch:29 step:22889 [D loss: 0.058801, acc.: 98.44%] [G loss: 0.091365]\n",
      "epoch:29 step:22890 [D loss: 0.001542, acc.: 100.00%] [G loss: 0.008393]\n",
      "epoch:29 step:22891 [D loss: 0.002608, acc.: 100.00%] [G loss: 0.037011]\n",
      "epoch:29 step:22892 [D loss: 0.002584, acc.: 100.00%] [G loss: 0.005429]\n",
      "epoch:29 step:22893 [D loss: 0.005005, acc.: 100.00%] [G loss: 0.001849]\n",
      "epoch:29 step:22894 [D loss: 0.060874, acc.: 98.44%] [G loss: 0.184599]\n",
      "epoch:29 step:22895 [D loss: 0.010421, acc.: 100.00%] [G loss: 0.315377]\n",
      "epoch:29 step:22896 [D loss: 0.127103, acc.: 95.31%] [G loss: 0.045340]\n",
      "epoch:29 step:22897 [D loss: 0.001882, acc.: 100.00%] [G loss: 0.009644]\n",
      "epoch:29 step:22898 [D loss: 0.002255, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:29 step:22899 [D loss: 0.005465, acc.: 100.00%] [G loss: 0.002037]\n",
      "epoch:29 step:22900 [D loss: 0.124992, acc.: 95.31%] [G loss: 0.445984]\n",
      "epoch:29 step:22901 [D loss: 0.084197, acc.: 97.66%] [G loss: 5.521121]\n",
      "epoch:29 step:22902 [D loss: 0.046388, acc.: 98.44%] [G loss: 0.131813]\n",
      "epoch:29 step:22903 [D loss: 0.000895, acc.: 100.00%] [G loss: 1.117279]\n",
      "epoch:29 step:22904 [D loss: 0.007569, acc.: 100.00%] [G loss: 0.072688]\n",
      "epoch:29 step:22905 [D loss: 0.000917, acc.: 100.00%] [G loss: 0.025489]\n",
      "epoch:29 step:22906 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.013403]\n",
      "epoch:29 step:22907 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.045924]\n",
      "epoch:29 step:22908 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.049956]\n",
      "epoch:29 step:22909 [D loss: 0.001905, acc.: 100.00%] [G loss: 3.122581]\n",
      "epoch:29 step:22910 [D loss: 0.007682, acc.: 100.00%] [G loss: 0.333827]\n",
      "epoch:29 step:22911 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.065715]\n",
      "epoch:29 step:22912 [D loss: 0.000648, acc.: 100.00%] [G loss: 0.068681]\n",
      "epoch:29 step:22913 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.191688]\n",
      "epoch:29 step:22914 [D loss: 0.006758, acc.: 100.00%] [G loss: 0.840868]\n",
      "epoch:29 step:22915 [D loss: 0.001109, acc.: 100.00%] [G loss: 0.028045]\n",
      "epoch:29 step:22916 [D loss: 0.008233, acc.: 100.00%] [G loss: 0.007393]\n",
      "epoch:29 step:22917 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.004791]\n",
      "epoch:29 step:22918 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.005650]\n",
      "epoch:29 step:22919 [D loss: 0.002186, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:29 step:22920 [D loss: 0.004539, acc.: 100.00%] [G loss: 0.014601]\n",
      "epoch:29 step:22921 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.009733]\n",
      "epoch:29 step:22922 [D loss: 0.002170, acc.: 100.00%] [G loss: 0.008832]\n",
      "epoch:29 step:22923 [D loss: 0.001674, acc.: 100.00%] [G loss: 0.004627]\n",
      "epoch:29 step:22924 [D loss: 0.001087, acc.: 100.00%] [G loss: 0.001409]\n",
      "epoch:29 step:22925 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:29 step:22926 [D loss: 0.003183, acc.: 100.00%] [G loss: 0.006905]\n",
      "epoch:29 step:22927 [D loss: 0.001972, acc.: 100.00%] [G loss: 0.001405]\n",
      "epoch:29 step:22928 [D loss: 0.007301, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:29 step:22929 [D loss: 0.000925, acc.: 100.00%] [G loss: 0.001698]\n",
      "epoch:29 step:22930 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.000877]\n",
      "epoch:29 step:22931 [D loss: 0.001164, acc.: 100.00%] [G loss: 0.003108]\n",
      "epoch:29 step:22932 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:29 step:22933 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.003578]\n",
      "epoch:29 step:22934 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.002011]\n",
      "epoch:29 step:22935 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.002571]\n",
      "epoch:29 step:22936 [D loss: 0.000708, acc.: 100.00%] [G loss: 0.001927]\n",
      "epoch:29 step:22937 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.001881]\n",
      "epoch:29 step:22938 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.002086]\n",
      "epoch:29 step:22939 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.033179]\n",
      "epoch:29 step:22940 [D loss: 0.002843, acc.: 100.00%] [G loss: 0.009550]\n",
      "epoch:29 step:22941 [D loss: 0.001098, acc.: 100.00%] [G loss: 0.008895]\n",
      "epoch:29 step:22942 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.007567]\n",
      "epoch:29 step:22943 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.001236]\n",
      "epoch:29 step:22944 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.004026]\n",
      "epoch:29 step:22945 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.002506]\n",
      "epoch:29 step:22946 [D loss: 0.003578, acc.: 100.00%] [G loss: 0.001097]\n",
      "epoch:29 step:22947 [D loss: 0.009175, acc.: 100.00%] [G loss: 0.008970]\n",
      "epoch:29 step:22948 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.003996]\n",
      "epoch:29 step:22949 [D loss: 0.000633, acc.: 100.00%] [G loss: 0.009904]\n",
      "epoch:29 step:22950 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.007902]\n",
      "epoch:29 step:22951 [D loss: 0.000098, acc.: 100.00%] [G loss: 1.272020]\n",
      "epoch:29 step:22952 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.006394]\n",
      "epoch:29 step:22953 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.006233]\n",
      "epoch:29 step:22954 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.014320]\n",
      "epoch:29 step:22955 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.005312]\n",
      "epoch:29 step:22956 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.005605]\n",
      "epoch:29 step:22957 [D loss: 0.001379, acc.: 100.00%] [G loss: 0.006844]\n",
      "epoch:29 step:22958 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.007004]\n",
      "epoch:29 step:22959 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:29 step:22960 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.013013]\n",
      "epoch:29 step:22961 [D loss: 0.001100, acc.: 100.00%] [G loss: 0.002752]\n",
      "epoch:29 step:22962 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.007021]\n",
      "epoch:29 step:22963 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.009805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22964 [D loss: 0.004330, acc.: 100.00%] [G loss: 0.007235]\n",
      "epoch:29 step:22965 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:29 step:22966 [D loss: 0.001063, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:29 step:22967 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.005817]\n",
      "epoch:29 step:22968 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.007484]\n",
      "epoch:29 step:22969 [D loss: 0.003386, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:29 step:22970 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:29 step:22971 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.002697]\n",
      "epoch:29 step:22972 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:29 step:22973 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.004370]\n",
      "epoch:29 step:22974 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:29 step:22975 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.001890]\n",
      "epoch:29 step:22976 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:29 step:22977 [D loss: 0.010582, acc.: 100.00%] [G loss: 0.003569]\n",
      "epoch:29 step:22978 [D loss: 0.013729, acc.: 99.22%] [G loss: 0.005045]\n",
      "epoch:29 step:22979 [D loss: 0.001715, acc.: 100.00%] [G loss: 0.025414]\n",
      "epoch:29 step:22980 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.010295]\n",
      "epoch:29 step:22981 [D loss: 0.002985, acc.: 100.00%] [G loss: 0.005188]\n",
      "epoch:29 step:22982 [D loss: 0.046989, acc.: 100.00%] [G loss: 0.030368]\n",
      "epoch:29 step:22983 [D loss: 0.007359, acc.: 100.00%] [G loss: 0.030182]\n",
      "epoch:29 step:22984 [D loss: 0.138338, acc.: 96.09%] [G loss: 0.033601]\n",
      "epoch:29 step:22985 [D loss: 0.012708, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:29 step:22986 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:29 step:22987 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:29 step:22988 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:29 step:22989 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000318]\n",
      "epoch:29 step:22990 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.004165]\n",
      "epoch:29 step:22991 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:29 step:22992 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:29 step:22993 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.018041]\n",
      "epoch:29 step:22994 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.011346]\n",
      "epoch:29 step:22995 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:29 step:22996 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:29 step:22997 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:29 step:22998 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:29 step:22999 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.002073]\n",
      "epoch:29 step:23000 [D loss: 0.007252, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:29 step:23001 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:29 step:23002 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.066242]\n",
      "epoch:29 step:23003 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:29 step:23004 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.008995]\n",
      "epoch:29 step:23005 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:29 step:23006 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004196]\n",
      "epoch:29 step:23007 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:29 step:23008 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:29 step:23009 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.006144]\n",
      "epoch:29 step:23010 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:29 step:23011 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:29 step:23012 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.027026]\n",
      "epoch:29 step:23013 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:29 step:23014 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:29 step:23015 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:29 step:23016 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:29 step:23017 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000778]\n",
      "epoch:29 step:23018 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:29 step:23019 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:29 step:23020 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:29 step:23021 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:29 step:23022 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:29 step:23023 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:29 step:23024 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:29 step:23025 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:29 step:23026 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:29 step:23027 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:29 step:23028 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.015133]\n",
      "epoch:29 step:23029 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:29 step:23030 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:29 step:23031 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:29 step:23032 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:29 step:23033 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:29 step:23034 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:29 step:23035 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:29 step:23036 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.002121]\n",
      "epoch:29 step:23037 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:29 step:23038 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:29 step:23039 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:29 step:23040 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:29 step:23041 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:29 step:23042 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:29 step:23043 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:29 step:23044 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000790]\n",
      "epoch:29 step:23045 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:29 step:23046 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:29 step:23047 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:29 step:23048 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:29 step:23049 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:29 step:23050 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:29 step:23051 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000481]\n",
      "epoch:29 step:23052 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:29 step:23053 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:29 step:23054 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:29 step:23055 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:29 step:23056 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.002765]\n",
      "epoch:29 step:23057 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:29 step:23058 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:29 step:23059 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:29 step:23060 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000526]\n",
      "epoch:29 step:23061 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:29 step:23062 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:29 step:23063 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:29 step:23064 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:29 step:23065 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000734]\n",
      "epoch:29 step:23066 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.004489]\n",
      "epoch:29 step:23067 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:29 step:23068 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:29 step:23069 [D loss: 0.001292, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:29 step:23070 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:29 step:23071 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:29 step:23072 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:29 step:23073 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:29 step:23074 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:29 step:23075 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:29 step:23076 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:29 step:23077 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23078 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:29 step:23079 [D loss: 0.001463, acc.: 100.00%] [G loss: 0.005923]\n",
      "epoch:29 step:23080 [D loss: 0.001293, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:29 step:23081 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:29 step:23082 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:29 step:23083 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:29 step:23084 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:29 step:23085 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:29 step:23086 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:29 step:23087 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:29 step:23088 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:29 step:23089 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:29 step:23090 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:29 step:23091 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:29 step:23092 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:29 step:23093 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:29 step:23094 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:29 step:23095 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:29 step:23096 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:29 step:23097 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:29 step:23098 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:29 step:23099 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:29 step:23100 [D loss: 0.002075, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:29 step:23101 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:29 step:23102 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:29 step:23103 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:29 step:23104 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:29 step:23105 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:29 step:23106 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:29 step:23107 [D loss: 0.002095, acc.: 100.00%] [G loss: 0.002932]\n",
      "epoch:29 step:23108 [D loss: 0.001163, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:29 step:23109 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:29 step:23110 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:29 step:23111 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:29 step:23112 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:29 step:23113 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:29 step:23114 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.003602]\n",
      "epoch:29 step:23115 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000565]\n",
      "epoch:29 step:23116 [D loss: 0.007868, acc.: 100.00%] [G loss: 0.003168]\n",
      "epoch:29 step:23117 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:29 step:23118 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:29 step:23119 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:29 step:23120 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000918]\n",
      "epoch:29 step:23121 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.002884]\n",
      "epoch:29 step:23122 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.002740]\n",
      "epoch:29 step:23123 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:29 step:23124 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:29 step:23125 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:29 step:23126 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:29 step:23127 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:29 step:23128 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:29 step:23129 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:29 step:23130 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:29 step:23131 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:29 step:23132 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000920]\n",
      "epoch:29 step:23133 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002867]\n",
      "epoch:29 step:23134 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:29 step:23135 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.008238]\n",
      "epoch:29 step:23136 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.054733]\n",
      "epoch:29 step:23137 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:29 step:23138 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:29 step:23139 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000703]\n",
      "epoch:29 step:23140 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:29 step:23141 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:29 step:23142 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:29 step:23143 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:29 step:23144 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:29 step:23145 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000408]\n",
      "epoch:29 step:23146 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:29 step:23147 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:29 step:23148 [D loss: 0.015965, acc.: 100.00%] [G loss: 0.001372]\n",
      "epoch:29 step:23149 [D loss: 0.007716, acc.: 100.00%] [G loss: 0.003395]\n",
      "epoch:29 step:23150 [D loss: 0.012479, acc.: 100.00%] [G loss: 0.003042]\n",
      "epoch:29 step:23151 [D loss: 0.024334, acc.: 100.00%] [G loss: 0.029740]\n",
      "epoch:29 step:23152 [D loss: 0.001502, acc.: 100.00%] [G loss: 1.366363]\n",
      "epoch:29 step:23153 [D loss: 0.003528, acc.: 100.00%] [G loss: 0.020781]\n",
      "epoch:29 step:23154 [D loss: 0.001459, acc.: 100.00%] [G loss: 0.287959]\n",
      "epoch:29 step:23155 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.108724]\n",
      "epoch:29 step:23156 [D loss: 0.002511, acc.: 100.00%] [G loss: 0.025055]\n",
      "epoch:29 step:23157 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.034779]\n",
      "epoch:29 step:23158 [D loss: 0.006023, acc.: 99.22%] [G loss: 0.006371]\n",
      "epoch:29 step:23159 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.006455]\n",
      "epoch:29 step:23160 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.005896]\n",
      "epoch:29 step:23161 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.005001]\n",
      "epoch:29 step:23162 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.023829]\n",
      "epoch:29 step:23163 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.006389]\n",
      "epoch:29 step:23164 [D loss: 0.001669, acc.: 100.00%] [G loss: 0.027245]\n",
      "epoch:29 step:23165 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.012931]\n",
      "epoch:29 step:23166 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.003673]\n",
      "epoch:29 step:23167 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.002095]\n",
      "epoch:29 step:23168 [D loss: 0.001949, acc.: 100.00%] [G loss: 3.706708]\n",
      "epoch:29 step:23169 [D loss: 0.019769, acc.: 100.00%] [G loss: 3.019988]\n",
      "epoch:29 step:23170 [D loss: 0.104141, acc.: 95.31%] [G loss: 8.841721]\n",
      "epoch:29 step:23171 [D loss: 0.111249, acc.: 95.31%] [G loss: 9.885250]\n",
      "epoch:29 step:23172 [D loss: 0.001317, acc.: 100.00%] [G loss: 9.522926]\n",
      "epoch:29 step:23173 [D loss: 0.000655, acc.: 100.00%] [G loss: 9.278458]\n",
      "epoch:29 step:23174 [D loss: 0.000634, acc.: 100.00%] [G loss: 8.269197]\n",
      "epoch:29 step:23175 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.063064]\n",
      "epoch:29 step:23176 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.154074]\n",
      "epoch:29 step:23177 [D loss: 0.000089, acc.: 100.00%] [G loss: 8.189503]\n",
      "epoch:29 step:23178 [D loss: 0.000843, acc.: 100.00%] [G loss: 1.931650]\n",
      "epoch:29 step:23179 [D loss: 0.001428, acc.: 100.00%] [G loss: 8.116827]\n",
      "epoch:29 step:23180 [D loss: 0.029976, acc.: 99.22%] [G loss: 8.057656]\n",
      "epoch:29 step:23181 [D loss: 0.003710, acc.: 100.00%] [G loss: 8.168687]\n",
      "epoch:29 step:23182 [D loss: 0.019186, acc.: 99.22%] [G loss: 7.203184]\n",
      "epoch:29 step:23183 [D loss: 0.000645, acc.: 100.00%] [G loss: 6.850681]\n",
      "epoch:29 step:23184 [D loss: 0.001460, acc.: 100.00%] [G loss: 6.660026]\n",
      "epoch:29 step:23185 [D loss: 0.000887, acc.: 100.00%] [G loss: 6.570868]\n",
      "epoch:29 step:23186 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.000987]\n",
      "epoch:29 step:23187 [D loss: 0.000399, acc.: 100.00%] [G loss: 5.388379]\n",
      "epoch:29 step:23188 [D loss: 0.000401, acc.: 100.00%] [G loss: 4.870252]\n",
      "epoch:29 step:23189 [D loss: 0.000187, acc.: 100.00%] [G loss: 4.562824]\n",
      "epoch:29 step:23190 [D loss: 0.000144, acc.: 100.00%] [G loss: 3.583705]\n",
      "epoch:29 step:23191 [D loss: 0.001531, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:29 step:23192 [D loss: 0.000502, acc.: 100.00%] [G loss: 2.432832]\n",
      "epoch:29 step:23193 [D loss: 0.000299, acc.: 100.00%] [G loss: 1.942179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23194 [D loss: 0.000305, acc.: 100.00%] [G loss: 1.054848]\n",
      "epoch:29 step:23195 [D loss: 0.006357, acc.: 100.00%] [G loss: 0.015531]\n",
      "epoch:29 step:23196 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.442955]\n",
      "epoch:29 step:23197 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.214937]\n",
      "epoch:29 step:23198 [D loss: 0.002192, acc.: 100.00%] [G loss: 0.268910]\n",
      "epoch:29 step:23199 [D loss: 0.000901, acc.: 100.00%] [G loss: 0.091683]\n",
      "epoch:29 step:23200 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.692303]\n",
      "epoch:29 step:23201 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.153430]\n",
      "epoch:29 step:23202 [D loss: 0.005576, acc.: 100.00%] [G loss: 0.020085]\n",
      "epoch:29 step:23203 [D loss: 0.005060, acc.: 100.00%] [G loss: 0.115604]\n",
      "epoch:29 step:23204 [D loss: 0.000747, acc.: 100.00%] [G loss: 0.131601]\n",
      "epoch:29 step:23205 [D loss: 0.000781, acc.: 100.00%] [G loss: 0.032428]\n",
      "epoch:29 step:23206 [D loss: 0.007268, acc.: 100.00%] [G loss: 0.118281]\n",
      "epoch:29 step:23207 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.119243]\n",
      "epoch:29 step:23208 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.022926]\n",
      "epoch:29 step:23209 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.030685]\n",
      "epoch:29 step:23210 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.301977]\n",
      "epoch:29 step:23211 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.010429]\n",
      "epoch:29 step:23212 [D loss: 0.001981, acc.: 100.00%] [G loss: 0.118270]\n",
      "epoch:29 step:23213 [D loss: 0.001162, acc.: 100.00%] [G loss: 0.089369]\n",
      "epoch:29 step:23214 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.103427]\n",
      "epoch:29 step:23215 [D loss: 0.002926, acc.: 100.00%] [G loss: 0.030890]\n",
      "epoch:29 step:23216 [D loss: 0.002687, acc.: 100.00%] [G loss: 0.024610]\n",
      "epoch:29 step:23217 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.014767]\n",
      "epoch:29 step:23218 [D loss: 0.001546, acc.: 100.00%] [G loss: 0.152873]\n",
      "epoch:29 step:23219 [D loss: 0.003983, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:29 step:23220 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.451429]\n",
      "epoch:29 step:23221 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.627626]\n",
      "epoch:29 step:23222 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.170639]\n",
      "epoch:29 step:23223 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.058249]\n",
      "epoch:29 step:23224 [D loss: 0.001360, acc.: 100.00%] [G loss: 0.429805]\n",
      "epoch:29 step:23225 [D loss: 0.000991, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:29 step:23226 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.479989]\n",
      "epoch:29 step:23227 [D loss: 0.005134, acc.: 100.00%] [G loss: 0.717689]\n",
      "epoch:29 step:23228 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.107288]\n",
      "epoch:29 step:23229 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.184023]\n",
      "epoch:29 step:23230 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:29 step:23231 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:29 step:23232 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.256142]\n",
      "epoch:29 step:23233 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.047638]\n",
      "epoch:29 step:23234 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:29 step:23235 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.062423]\n",
      "epoch:29 step:23236 [D loss: 0.006852, acc.: 99.22%] [G loss: 0.005851]\n",
      "epoch:29 step:23237 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.120118]\n",
      "epoch:29 step:23238 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:29 step:23239 [D loss: 0.003725, acc.: 100.00%] [G loss: 0.007205]\n",
      "epoch:29 step:23240 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.060248]\n",
      "epoch:29 step:23241 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:29 step:23242 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.047428]\n",
      "epoch:29 step:23243 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:29 step:23244 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.001941]\n",
      "epoch:29 step:23245 [D loss: 0.001473, acc.: 100.00%] [G loss: 0.004671]\n",
      "epoch:29 step:23246 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.002373]\n",
      "epoch:29 step:23247 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:29 step:23248 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.009346]\n",
      "epoch:29 step:23249 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:29 step:23250 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.036835]\n",
      "epoch:29 step:23251 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.014117]\n",
      "epoch:29 step:23252 [D loss: 0.010953, acc.: 99.22%] [G loss: 0.001999]\n",
      "epoch:29 step:23253 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:29 step:23254 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.004751]\n",
      "epoch:29 step:23255 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.002630]\n",
      "epoch:29 step:23256 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001723]\n",
      "epoch:29 step:23257 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:29 step:23258 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.009806]\n",
      "epoch:29 step:23259 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:29 step:23260 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:29 step:23261 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:29 step:23262 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:29 step:23263 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001437]\n",
      "epoch:29 step:23264 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.001424]\n",
      "epoch:29 step:23265 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:29 step:23266 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.003342]\n",
      "epoch:29 step:23267 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003304]\n",
      "epoch:29 step:23268 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:29 step:23269 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:29 step:23270 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:29 step:23271 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:29 step:23272 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:29 step:23273 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:29 step:23274 [D loss: 0.487276, acc.: 75.00%] [G loss: 15.040182]\n",
      "epoch:29 step:23275 [D loss: 3.914671, acc.: 50.00%] [G loss: 7.592383]\n",
      "epoch:29 step:23276 [D loss: 0.033813, acc.: 99.22%] [G loss: 4.768484]\n",
      "epoch:29 step:23277 [D loss: 0.017415, acc.: 100.00%] [G loss: 4.727901]\n",
      "epoch:29 step:23278 [D loss: 0.011686, acc.: 100.00%] [G loss: 1.452825]\n",
      "epoch:29 step:23279 [D loss: 0.011557, acc.: 100.00%] [G loss: 2.718051]\n",
      "epoch:29 step:23280 [D loss: 0.009991, acc.: 100.00%] [G loss: 1.963606]\n",
      "epoch:29 step:23281 [D loss: 0.007399, acc.: 100.00%] [G loss: 0.625027]\n",
      "epoch:29 step:23282 [D loss: 0.052945, acc.: 98.44%] [G loss: 0.866493]\n",
      "epoch:29 step:23283 [D loss: 0.042565, acc.: 99.22%] [G loss: 0.423227]\n",
      "epoch:29 step:23284 [D loss: 0.001600, acc.: 100.00%] [G loss: 0.035838]\n",
      "epoch:29 step:23285 [D loss: 0.107660, acc.: 96.88%] [G loss: 1.056113]\n",
      "epoch:29 step:23286 [D loss: 0.452072, acc.: 82.03%] [G loss: 0.062795]\n",
      "epoch:29 step:23287 [D loss: 0.003258, acc.: 100.00%] [G loss: 0.022789]\n",
      "epoch:29 step:23288 [D loss: 0.060315, acc.: 97.66%] [G loss: 0.245281]\n",
      "epoch:29 step:23289 [D loss: 0.860501, acc.: 73.44%] [G loss: 2.933050]\n",
      "epoch:29 step:23290 [D loss: 2.092620, acc.: 50.00%] [G loss: 6.960686]\n",
      "epoch:29 step:23291 [D loss: 0.798329, acc.: 66.41%] [G loss: 3.852712]\n",
      "epoch:29 step:23292 [D loss: 0.110330, acc.: 95.31%] [G loss: 0.182056]\n",
      "epoch:29 step:23293 [D loss: 0.050761, acc.: 99.22%] [G loss: 0.474119]\n",
      "epoch:29 step:23294 [D loss: 0.020499, acc.: 100.00%] [G loss: 0.051324]\n",
      "epoch:29 step:23295 [D loss: 0.048317, acc.: 99.22%] [G loss: 0.042274]\n",
      "epoch:29 step:23296 [D loss: 0.034352, acc.: 99.22%] [G loss: 0.110425]\n",
      "epoch:29 step:23297 [D loss: 0.020515, acc.: 100.00%] [G loss: 0.020644]\n",
      "epoch:29 step:23298 [D loss: 0.045452, acc.: 99.22%] [G loss: 0.019293]\n",
      "epoch:29 step:23299 [D loss: 0.054006, acc.: 100.00%] [G loss: 0.024931]\n",
      "epoch:29 step:23300 [D loss: 0.014146, acc.: 100.00%] [G loss: 0.170040]\n",
      "epoch:29 step:23301 [D loss: 0.047352, acc.: 100.00%] [G loss: 0.106697]\n",
      "epoch:29 step:23302 [D loss: 0.034215, acc.: 100.00%] [G loss: 0.091980]\n",
      "epoch:29 step:23303 [D loss: 0.039696, acc.: 100.00%] [G loss: 0.193749]\n",
      "epoch:29 step:23304 [D loss: 0.016377, acc.: 100.00%] [G loss: 0.190285]\n",
      "epoch:29 step:23305 [D loss: 0.018753, acc.: 100.00%] [G loss: 0.271781]\n",
      "epoch:29 step:23306 [D loss: 0.024377, acc.: 100.00%] [G loss: 3.715570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23307 [D loss: 0.008352, acc.: 100.00%] [G loss: 2.391969]\n",
      "epoch:29 step:23308 [D loss: 0.003009, acc.: 100.00%] [G loss: 0.186741]\n",
      "epoch:29 step:23309 [D loss: 0.019899, acc.: 100.00%] [G loss: 0.122025]\n",
      "epoch:29 step:23310 [D loss: 0.019616, acc.: 100.00%] [G loss: 0.129825]\n",
      "epoch:29 step:23311 [D loss: 0.010139, acc.: 100.00%] [G loss: 0.302759]\n",
      "epoch:29 step:23312 [D loss: 0.004933, acc.: 100.00%] [G loss: 0.103162]\n",
      "epoch:29 step:23313 [D loss: 0.011034, acc.: 100.00%] [G loss: 0.076597]\n",
      "epoch:29 step:23314 [D loss: 0.012720, acc.: 100.00%] [G loss: 0.092950]\n",
      "epoch:29 step:23315 [D loss: 0.014125, acc.: 100.00%] [G loss: 0.062361]\n",
      "epoch:29 step:23316 [D loss: 0.006996, acc.: 100.00%] [G loss: 0.120849]\n",
      "epoch:29 step:23317 [D loss: 0.003184, acc.: 100.00%] [G loss: 0.031460]\n",
      "epoch:29 step:23318 [D loss: 0.002920, acc.: 100.00%] [G loss: 0.031255]\n",
      "epoch:29 step:23319 [D loss: 0.006322, acc.: 100.00%] [G loss: 0.018020]\n",
      "epoch:29 step:23320 [D loss: 0.018547, acc.: 100.00%] [G loss: 0.030586]\n",
      "epoch:29 step:23321 [D loss: 0.003191, acc.: 100.00%] [G loss: 0.111122]\n",
      "epoch:29 step:23322 [D loss: 0.001685, acc.: 100.00%] [G loss: 0.040462]\n",
      "epoch:29 step:23323 [D loss: 0.003975, acc.: 100.00%] [G loss: 0.045439]\n",
      "epoch:29 step:23324 [D loss: 0.004211, acc.: 100.00%] [G loss: 0.021649]\n",
      "epoch:29 step:23325 [D loss: 0.010195, acc.: 100.00%] [G loss: 0.021716]\n",
      "epoch:29 step:23326 [D loss: 0.003303, acc.: 100.00%] [G loss: 0.032135]\n",
      "epoch:29 step:23327 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.019608]\n",
      "epoch:29 step:23328 [D loss: 0.012576, acc.: 100.00%] [G loss: 0.011293]\n",
      "epoch:29 step:23329 [D loss: 0.004571, acc.: 100.00%] [G loss: 0.011924]\n",
      "epoch:29 step:23330 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.019597]\n",
      "epoch:29 step:23331 [D loss: 0.003008, acc.: 100.00%] [G loss: 0.026148]\n",
      "epoch:29 step:23332 [D loss: 0.002108, acc.: 100.00%] [G loss: 0.019577]\n",
      "epoch:29 step:23333 [D loss: 0.014059, acc.: 100.00%] [G loss: 0.003778]\n",
      "epoch:29 step:23334 [D loss: 0.004466, acc.: 100.00%] [G loss: 0.052845]\n",
      "epoch:29 step:23335 [D loss: 0.005738, acc.: 100.00%] [G loss: 0.019544]\n",
      "epoch:29 step:23336 [D loss: 0.004924, acc.: 100.00%] [G loss: 0.021671]\n",
      "epoch:29 step:23337 [D loss: 0.001604, acc.: 100.00%] [G loss: 0.002949]\n",
      "epoch:29 step:23338 [D loss: 0.001884, acc.: 100.00%] [G loss: 0.025976]\n",
      "epoch:29 step:23339 [D loss: 0.001264, acc.: 100.00%] [G loss: 0.011678]\n",
      "epoch:29 step:23340 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.005484]\n",
      "epoch:29 step:23341 [D loss: 0.014650, acc.: 100.00%] [G loss: 0.052311]\n",
      "epoch:29 step:23342 [D loss: 0.002591, acc.: 100.00%] [G loss: 0.007815]\n",
      "epoch:29 step:23343 [D loss: 0.002634, acc.: 100.00%] [G loss: 0.004860]\n",
      "epoch:29 step:23344 [D loss: 0.005707, acc.: 100.00%] [G loss: 0.012947]\n",
      "epoch:29 step:23345 [D loss: 0.005887, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:29 step:23346 [D loss: 0.000204, acc.: 100.00%] [G loss: 2.024331]\n",
      "epoch:29 step:23347 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.017197]\n",
      "epoch:29 step:23348 [D loss: 0.008759, acc.: 100.00%] [G loss: 0.000877]\n",
      "epoch:29 step:23349 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.039809]\n",
      "epoch:29 step:23350 [D loss: 0.006179, acc.: 100.00%] [G loss: 0.026478]\n",
      "epoch:29 step:23351 [D loss: 0.003393, acc.: 100.00%] [G loss: 0.004160]\n",
      "epoch:29 step:23352 [D loss: 0.003218, acc.: 100.00%] [G loss: 0.045439]\n",
      "epoch:29 step:23353 [D loss: 0.058033, acc.: 99.22%] [G loss: 0.022162]\n",
      "epoch:29 step:23354 [D loss: 0.002331, acc.: 100.00%] [G loss: 0.001140]\n",
      "epoch:29 step:23355 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:29 step:23356 [D loss: 0.032663, acc.: 99.22%] [G loss: 0.029860]\n",
      "epoch:29 step:23357 [D loss: 0.005183, acc.: 100.00%] [G loss: 0.005895]\n",
      "epoch:29 step:23358 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.115358]\n",
      "epoch:29 step:23359 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.057774]\n",
      "epoch:29 step:23360 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.050291]\n",
      "epoch:29 step:23361 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.021640]\n",
      "epoch:29 step:23362 [D loss: 0.049780, acc.: 99.22%] [G loss: 0.012863]\n",
      "epoch:29 step:23363 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.382173]\n",
      "epoch:29 step:23364 [D loss: 0.033311, acc.: 100.00%] [G loss: 0.004391]\n",
      "epoch:29 step:23365 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.014056]\n",
      "epoch:29 step:23366 [D loss: 0.001268, acc.: 100.00%] [G loss: 0.030966]\n",
      "epoch:29 step:23367 [D loss: 0.036228, acc.: 100.00%] [G loss: 0.428712]\n",
      "epoch:29 step:23368 [D loss: 0.001389, acc.: 100.00%] [G loss: 0.153508]\n",
      "epoch:29 step:23369 [D loss: 0.018195, acc.: 99.22%] [G loss: 0.012027]\n",
      "epoch:29 step:23370 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.881609]\n",
      "epoch:29 step:23371 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.055373]\n",
      "epoch:29 step:23372 [D loss: 0.003758, acc.: 100.00%] [G loss: 0.485789]\n",
      "epoch:29 step:23373 [D loss: 0.003354, acc.: 100.00%] [G loss: 0.003161]\n",
      "epoch:29 step:23374 [D loss: 0.003442, acc.: 100.00%] [G loss: 0.015891]\n",
      "epoch:29 step:23375 [D loss: 0.001261, acc.: 100.00%] [G loss: 0.014138]\n",
      "epoch:29 step:23376 [D loss: 0.001898, acc.: 100.00%] [G loss: 0.005747]\n",
      "epoch:29 step:23377 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.020839]\n",
      "epoch:29 step:23378 [D loss: 0.001426, acc.: 100.00%] [G loss: 0.009591]\n",
      "epoch:29 step:23379 [D loss: 0.003446, acc.: 100.00%] [G loss: 0.016632]\n",
      "epoch:29 step:23380 [D loss: 0.003585, acc.: 100.00%] [G loss: 0.029685]\n",
      "epoch:29 step:23381 [D loss: 0.004109, acc.: 100.00%] [G loss: 0.023031]\n",
      "epoch:29 step:23382 [D loss: 0.005563, acc.: 100.00%] [G loss: 0.022272]\n",
      "epoch:29 step:23383 [D loss: 0.158323, acc.: 95.31%] [G loss: 3.121352]\n",
      "epoch:29 step:23384 [D loss: 0.225495, acc.: 92.19%] [G loss: 0.859192]\n",
      "epoch:29 step:23385 [D loss: 0.001415, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:29 step:23386 [D loss: 0.001023, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:29 step:23387 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:29 step:23388 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.108811]\n",
      "epoch:29 step:23389 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.114619]\n",
      "epoch:29 step:23390 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.034204]\n",
      "epoch:29 step:23391 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.029481]\n",
      "epoch:29 step:23392 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:29 step:23393 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.021028]\n",
      "epoch:29 step:23394 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.009596]\n",
      "epoch:29 step:23395 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.001851]\n",
      "epoch:29 step:23396 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.207945]\n",
      "epoch:29 step:23397 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.019379]\n",
      "epoch:29 step:23398 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.004765]\n",
      "epoch:29 step:23399 [D loss: 0.003189, acc.: 100.00%] [G loss: 0.006009]\n",
      "epoch:29 step:23400 [D loss: 0.002541, acc.: 100.00%] [G loss: 0.007590]\n",
      "epoch:29 step:23401 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.005653]\n",
      "epoch:29 step:23402 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.262536]\n",
      "epoch:29 step:23403 [D loss: 0.000832, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:29 step:23404 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.004101]\n",
      "epoch:29 step:23405 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:29 step:23406 [D loss: 0.017074, acc.: 100.00%] [G loss: 0.020052]\n",
      "epoch:29 step:23407 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:29 step:23408 [D loss: 0.003558, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:29 step:23409 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:29 step:23410 [D loss: 0.002721, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:29 step:23411 [D loss: 0.290472, acc.: 84.38%] [G loss: 4.483267]\n",
      "epoch:29 step:23412 [D loss: 1.100549, acc.: 67.19%] [G loss: 3.079996]\n",
      "epoch:29 step:23413 [D loss: 0.002088, acc.: 100.00%] [G loss: 0.637302]\n",
      "epoch:29 step:23414 [D loss: 0.109660, acc.: 96.88%] [G loss: 0.453879]\n",
      "epoch:29 step:23415 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.695414]\n",
      "epoch:29 step:23416 [D loss: 0.005385, acc.: 100.00%] [G loss: 4.194655]\n",
      "epoch:29 step:23417 [D loss: 0.016519, acc.: 99.22%] [G loss: 0.139193]\n",
      "epoch:29 step:23418 [D loss: 0.002824, acc.: 100.00%] [G loss: 0.172172]\n",
      "epoch:29 step:23419 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.080466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23420 [D loss: 0.021157, acc.: 99.22%] [G loss: 0.029823]\n",
      "epoch:29 step:23421 [D loss: 0.001902, acc.: 100.00%] [G loss: 2.064739]\n",
      "epoch:29 step:23422 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.078101]\n",
      "epoch:29 step:23423 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.050568]\n",
      "epoch:29 step:23424 [D loss: 0.034559, acc.: 98.44%] [G loss: 0.101123]\n",
      "epoch:29 step:23425 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.804135]\n",
      "epoch:29 step:23426 [D loss: 0.005078, acc.: 100.00%] [G loss: 0.254007]\n",
      "epoch:29 step:23427 [D loss: 0.001561, acc.: 100.00%] [G loss: 0.097408]\n",
      "epoch:29 step:23428 [D loss: 0.008023, acc.: 100.00%] [G loss: 0.144208]\n",
      "epoch:29 step:23429 [D loss: 0.002816, acc.: 100.00%] [G loss: 0.099544]\n",
      "epoch:29 step:23430 [D loss: 0.006297, acc.: 100.00%] [G loss: 1.697912]\n",
      "epoch:30 step:23431 [D loss: 0.006821, acc.: 100.00%] [G loss: 0.060198]\n",
      "epoch:30 step:23432 [D loss: 0.009057, acc.: 100.00%] [G loss: 0.197506]\n",
      "epoch:30 step:23433 [D loss: 0.082666, acc.: 99.22%] [G loss: 0.228065]\n",
      "epoch:30 step:23434 [D loss: 0.001464, acc.: 100.00%] [G loss: 1.848330]\n",
      "epoch:30 step:23435 [D loss: 0.020208, acc.: 99.22%] [G loss: 0.831597]\n",
      "epoch:30 step:23436 [D loss: 0.003768, acc.: 100.00%] [G loss: 0.340487]\n",
      "epoch:30 step:23437 [D loss: 0.006263, acc.: 100.00%] [G loss: 0.159870]\n",
      "epoch:30 step:23438 [D loss: 0.007379, acc.: 100.00%] [G loss: 0.122748]\n",
      "epoch:30 step:23439 [D loss: 0.006910, acc.: 100.00%] [G loss: 3.105912]\n",
      "epoch:30 step:23440 [D loss: 0.093854, acc.: 98.44%] [G loss: 0.242753]\n",
      "epoch:30 step:23441 [D loss: 0.004808, acc.: 100.00%] [G loss: 3.279809]\n",
      "epoch:30 step:23442 [D loss: 0.073742, acc.: 97.66%] [G loss: 0.534593]\n",
      "epoch:30 step:23443 [D loss: 0.002352, acc.: 100.00%] [G loss: 0.284075]\n",
      "epoch:30 step:23444 [D loss: 0.004715, acc.: 100.00%] [G loss: 0.150262]\n",
      "epoch:30 step:23445 [D loss: 0.012741, acc.: 100.00%] [G loss: 0.035549]\n",
      "epoch:30 step:23446 [D loss: 0.015837, acc.: 100.00%] [G loss: 0.097353]\n",
      "epoch:30 step:23447 [D loss: 0.002563, acc.: 100.00%] [G loss: 0.003435]\n",
      "epoch:30 step:23448 [D loss: 0.000753, acc.: 100.00%] [G loss: 3.556499]\n",
      "epoch:30 step:23449 [D loss: 0.005521, acc.: 100.00%] [G loss: 0.256981]\n",
      "epoch:30 step:23450 [D loss: 0.011216, acc.: 100.00%] [G loss: 0.110341]\n",
      "epoch:30 step:23451 [D loss: 0.012805, acc.: 100.00%] [G loss: 0.271370]\n",
      "epoch:30 step:23452 [D loss: 0.013458, acc.: 100.00%] [G loss: 0.759562]\n",
      "epoch:30 step:23453 [D loss: 0.008212, acc.: 100.00%] [G loss: 0.423662]\n",
      "epoch:30 step:23454 [D loss: 0.008600, acc.: 100.00%] [G loss: 0.166417]\n",
      "epoch:30 step:23455 [D loss: 0.009928, acc.: 100.00%] [G loss: 0.595885]\n",
      "epoch:30 step:23456 [D loss: 0.001682, acc.: 100.00%] [G loss: 0.833026]\n",
      "epoch:30 step:23457 [D loss: 0.004043, acc.: 100.00%] [G loss: 0.446557]\n",
      "epoch:30 step:23458 [D loss: 0.010860, acc.: 100.00%] [G loss: 0.153594]\n",
      "epoch:30 step:23459 [D loss: 0.013348, acc.: 100.00%] [G loss: 0.275167]\n",
      "epoch:30 step:23460 [D loss: 0.002718, acc.: 100.00%] [G loss: 1.240045]\n",
      "epoch:30 step:23461 [D loss: 0.006523, acc.: 100.00%] [G loss: 1.206401]\n",
      "epoch:30 step:23462 [D loss: 0.003703, acc.: 100.00%] [G loss: 0.954833]\n",
      "epoch:30 step:23463 [D loss: 0.007787, acc.: 100.00%] [G loss: 1.732260]\n",
      "epoch:30 step:23464 [D loss: 0.005362, acc.: 100.00%] [G loss: 1.675635]\n",
      "epoch:30 step:23465 [D loss: 0.011191, acc.: 100.00%] [G loss: 2.805448]\n",
      "epoch:30 step:23466 [D loss: 0.021045, acc.: 99.22%] [G loss: 0.005463]\n",
      "epoch:30 step:23467 [D loss: 0.002116, acc.: 100.00%] [G loss: 4.214901]\n",
      "epoch:30 step:23468 [D loss: 0.001151, acc.: 100.00%] [G loss: 4.432304]\n",
      "epoch:30 step:23469 [D loss: 0.004157, acc.: 100.00%] [G loss: 3.310889]\n",
      "epoch:30 step:23470 [D loss: 0.002493, acc.: 100.00%] [G loss: 1.592924]\n",
      "epoch:30 step:23471 [D loss: 0.004366, acc.: 100.00%] [G loss: 0.830251]\n",
      "epoch:30 step:23472 [D loss: 0.003512, acc.: 100.00%] [G loss: 0.758570]\n",
      "epoch:30 step:23473 [D loss: 0.015017, acc.: 100.00%] [G loss: 0.026857]\n",
      "epoch:30 step:23474 [D loss: 0.034062, acc.: 99.22%] [G loss: 2.505625]\n",
      "epoch:30 step:23475 [D loss: 0.001055, acc.: 100.00%] [G loss: 1.395648]\n",
      "epoch:30 step:23476 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.000920]\n",
      "epoch:30 step:23477 [D loss: 0.000382, acc.: 100.00%] [G loss: 1.777113]\n",
      "epoch:30 step:23478 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.411934]\n",
      "epoch:30 step:23479 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.010629]\n",
      "epoch:30 step:23480 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.337500]\n",
      "epoch:30 step:23481 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.133102]\n",
      "epoch:30 step:23482 [D loss: 0.000963, acc.: 100.00%] [G loss: 0.689314]\n",
      "epoch:30 step:23483 [D loss: 0.006159, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:30 step:23484 [D loss: 0.046801, acc.: 98.44%] [G loss: 2.750311]\n",
      "epoch:30 step:23485 [D loss: 0.000214, acc.: 100.00%] [G loss: 5.200191]\n",
      "epoch:30 step:23486 [D loss: 0.008583, acc.: 100.00%] [G loss: 1.809735]\n",
      "epoch:30 step:23487 [D loss: 0.027844, acc.: 98.44%] [G loss: 3.035514]\n",
      "epoch:30 step:23488 [D loss: 0.000235, acc.: 100.00%] [G loss: 2.217113]\n",
      "epoch:30 step:23489 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.105006]\n",
      "epoch:30 step:23490 [D loss: 0.001399, acc.: 100.00%] [G loss: 1.797635]\n",
      "epoch:30 step:23491 [D loss: 0.000756, acc.: 100.00%] [G loss: 1.026394]\n",
      "epoch:30 step:23492 [D loss: 0.000379, acc.: 100.00%] [G loss: 1.248104]\n",
      "epoch:30 step:23493 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:30 step:23494 [D loss: 0.002449, acc.: 100.00%] [G loss: 0.244809]\n",
      "epoch:30 step:23495 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.324613]\n",
      "epoch:30 step:23496 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.128494]\n",
      "epoch:30 step:23497 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.144408]\n",
      "epoch:30 step:23498 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.150888]\n",
      "epoch:30 step:23499 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.009053]\n",
      "epoch:30 step:23500 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.074660]\n",
      "epoch:30 step:23501 [D loss: 0.007911, acc.: 100.00%] [G loss: 0.066634]\n",
      "epoch:30 step:23502 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.113363]\n",
      "epoch:30 step:23503 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.051578]\n",
      "epoch:30 step:23504 [D loss: 0.002646, acc.: 100.00%] [G loss: 0.068186]\n",
      "epoch:30 step:23505 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:30 step:23506 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.093567]\n",
      "epoch:30 step:23507 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.139789]\n",
      "epoch:30 step:23508 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.043402]\n",
      "epoch:30 step:23509 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.119586]\n",
      "epoch:30 step:23510 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.054036]\n",
      "epoch:30 step:23511 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.077304]\n",
      "epoch:30 step:23512 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.031461]\n",
      "epoch:30 step:23513 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.074055]\n",
      "epoch:30 step:23514 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.283112]\n",
      "epoch:30 step:23515 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.110795]\n",
      "epoch:30 step:23516 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:30 step:23517 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000897]\n",
      "epoch:30 step:23518 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.084845]\n",
      "epoch:30 step:23519 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:30 step:23520 [D loss: 0.009061, acc.: 100.00%] [G loss: 0.025320]\n",
      "epoch:30 step:23521 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.016264]\n",
      "epoch:30 step:23522 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.053419]\n",
      "epoch:30 step:23523 [D loss: 0.000179, acc.: 100.00%] [G loss: 5.558121]\n",
      "epoch:30 step:23524 [D loss: 0.002181, acc.: 100.00%] [G loss: 0.438188]\n",
      "epoch:30 step:23525 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.995155]\n",
      "epoch:30 step:23526 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:30 step:23527 [D loss: 0.004923, acc.: 100.00%] [G loss: 0.310544]\n",
      "epoch:30 step:23528 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.001604]\n",
      "epoch:30 step:23529 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.005964]\n",
      "epoch:30 step:23530 [D loss: 0.045210, acc.: 100.00%] [G loss: 0.188922]\n",
      "epoch:30 step:23531 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.426395]\n",
      "epoch:30 step:23532 [D loss: 0.021531, acc.: 98.44%] [G loss: 0.057362]\n",
      "epoch:30 step:23533 [D loss: 0.002673, acc.: 100.00%] [G loss: 0.019150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23534 [D loss: 0.000595, acc.: 100.00%] [G loss: 0.018351]\n",
      "epoch:30 step:23535 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.062282]\n",
      "epoch:30 step:23536 [D loss: 0.000707, acc.: 100.00%] [G loss: 1.353290]\n",
      "epoch:30 step:23537 [D loss: 0.003708, acc.: 100.00%] [G loss: 0.047643]\n",
      "epoch:30 step:23538 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.073070]\n",
      "epoch:30 step:23539 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.022850]\n",
      "epoch:30 step:23540 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.070949]\n",
      "epoch:30 step:23541 [D loss: 0.001419, acc.: 100.00%] [G loss: 0.007746]\n",
      "epoch:30 step:23542 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.078664]\n",
      "epoch:30 step:23543 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.011702]\n",
      "epoch:30 step:23544 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:30 step:23545 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.066545]\n",
      "epoch:30 step:23546 [D loss: 0.001337, acc.: 100.00%] [G loss: 4.943123]\n",
      "epoch:30 step:23547 [D loss: 0.032683, acc.: 100.00%] [G loss: 0.851943]\n",
      "epoch:30 step:23548 [D loss: 0.000391, acc.: 100.00%] [G loss: 1.307940]\n",
      "epoch:30 step:23549 [D loss: 1.214421, acc.: 53.12%] [G loss: 13.740404]\n",
      "epoch:30 step:23550 [D loss: 3.735434, acc.: 50.00%] [G loss: 8.608980]\n",
      "epoch:30 step:23551 [D loss: 1.498924, acc.: 58.59%] [G loss: 1.592865]\n",
      "epoch:30 step:23552 [D loss: 0.186212, acc.: 96.09%] [G loss: 3.400548]\n",
      "epoch:30 step:23553 [D loss: 0.063970, acc.: 98.44%] [G loss: 0.558263]\n",
      "epoch:30 step:23554 [D loss: 0.022671, acc.: 100.00%] [G loss: 2.692510]\n",
      "epoch:30 step:23555 [D loss: 0.336784, acc.: 82.03%] [G loss: 0.628810]\n",
      "epoch:30 step:23556 [D loss: 0.031665, acc.: 100.00%] [G loss: 1.555743]\n",
      "epoch:30 step:23557 [D loss: 0.162909, acc.: 92.19%] [G loss: 0.193880]\n",
      "epoch:30 step:23558 [D loss: 0.077914, acc.: 97.66%] [G loss: 0.171335]\n",
      "epoch:30 step:23559 [D loss: 0.068077, acc.: 97.66%] [G loss: 0.023355]\n",
      "epoch:30 step:23560 [D loss: 0.031538, acc.: 100.00%] [G loss: 0.006805]\n",
      "epoch:30 step:23561 [D loss: 0.023082, acc.: 100.00%] [G loss: 0.022184]\n",
      "epoch:30 step:23562 [D loss: 0.009454, acc.: 100.00%] [G loss: 3.269470]\n",
      "epoch:30 step:23563 [D loss: 0.005941, acc.: 100.00%] [G loss: 0.021015]\n",
      "epoch:30 step:23564 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.049369]\n",
      "epoch:30 step:23565 [D loss: 0.026513, acc.: 100.00%] [G loss: 0.011906]\n",
      "epoch:30 step:23566 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.354710]\n",
      "epoch:30 step:23567 [D loss: 0.008964, acc.: 100.00%] [G loss: 0.039070]\n",
      "epoch:30 step:23568 [D loss: 0.003421, acc.: 100.00%] [G loss: 0.001157]\n",
      "epoch:30 step:23569 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.000949]\n",
      "epoch:30 step:23570 [D loss: 0.014886, acc.: 100.00%] [G loss: 3.262507]\n",
      "epoch:30 step:23571 [D loss: 0.011416, acc.: 100.00%] [G loss: 0.004197]\n",
      "epoch:30 step:23572 [D loss: 0.001907, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:30 step:23573 [D loss: 0.017316, acc.: 100.00%] [G loss: 0.025595]\n",
      "epoch:30 step:23574 [D loss: 0.010948, acc.: 100.00%] [G loss: 0.080307]\n",
      "epoch:30 step:23575 [D loss: 0.002967, acc.: 100.00%] [G loss: 0.003107]\n",
      "epoch:30 step:23576 [D loss: 0.001155, acc.: 100.00%] [G loss: 0.003151]\n",
      "epoch:30 step:23577 [D loss: 0.009986, acc.: 100.00%] [G loss: 0.002256]\n",
      "epoch:30 step:23578 [D loss: 0.001917, acc.: 100.00%] [G loss: 0.001500]\n",
      "epoch:30 step:23579 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.003521]\n",
      "epoch:30 step:23580 [D loss: 0.019193, acc.: 100.00%] [G loss: 0.002767]\n",
      "epoch:30 step:23581 [D loss: 0.004507, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:30 step:23582 [D loss: 0.014730, acc.: 100.00%] [G loss: 0.117623]\n",
      "epoch:30 step:23583 [D loss: 0.031670, acc.: 98.44%] [G loss: 0.001885]\n",
      "epoch:30 step:23584 [D loss: 0.001557, acc.: 100.00%] [G loss: 0.005247]\n",
      "epoch:30 step:23585 [D loss: 0.004746, acc.: 100.00%] [G loss: 0.014739]\n",
      "epoch:30 step:23586 [D loss: 0.007995, acc.: 100.00%] [G loss: 0.002322]\n",
      "epoch:30 step:23587 [D loss: 0.005679, acc.: 100.00%] [G loss: 0.002180]\n",
      "epoch:30 step:23588 [D loss: 0.006324, acc.: 100.00%] [G loss: 0.002664]\n",
      "epoch:30 step:23589 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000860]\n",
      "epoch:30 step:23590 [D loss: 0.102986, acc.: 94.53%] [G loss: 0.000006]\n",
      "epoch:30 step:23591 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:30 step:23592 [D loss: 0.023348, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:30 step:23593 [D loss: 0.179581, acc.: 96.88%] [G loss: 0.483761]\n",
      "epoch:30 step:23594 [D loss: 0.007045, acc.: 100.00%] [G loss: 6.615548]\n",
      "epoch:30 step:23595 [D loss: 0.431233, acc.: 74.22%] [G loss: 0.000688]\n",
      "epoch:30 step:23596 [D loss: 0.001679, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:30 step:23597 [D loss: 0.002861, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:30 step:23598 [D loss: 0.004347, acc.: 100.00%] [G loss: 0.045540]\n",
      "epoch:30 step:23599 [D loss: 0.157350, acc.: 94.53%] [G loss: 0.000757]\n",
      "epoch:30 step:23600 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.003136]\n",
      "epoch:30 step:23601 [D loss: 0.048320, acc.: 97.66%] [G loss: 0.000954]\n",
      "epoch:30 step:23602 [D loss: 0.002165, acc.: 100.00%] [G loss: 0.362638]\n",
      "epoch:30 step:23603 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.451327]\n",
      "epoch:30 step:23604 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.152867]\n",
      "epoch:30 step:23605 [D loss: 0.003686, acc.: 100.00%] [G loss: 0.000639]\n",
      "epoch:30 step:23606 [D loss: 0.004349, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:30 step:23607 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:30 step:23608 [D loss: 0.001085, acc.: 100.00%] [G loss: 0.001851]\n",
      "epoch:30 step:23609 [D loss: 0.007536, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:30 step:23610 [D loss: 0.008510, acc.: 100.00%] [G loss: 2.469583]\n",
      "epoch:30 step:23611 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.003006]\n",
      "epoch:30 step:23612 [D loss: 0.004034, acc.: 100.00%] [G loss: 0.048275]\n",
      "epoch:30 step:23613 [D loss: 0.250670, acc.: 85.94%] [G loss: 4.186481]\n",
      "epoch:30 step:23614 [D loss: 0.144659, acc.: 94.53%] [G loss: 0.762956]\n",
      "epoch:30 step:23615 [D loss: 0.006921, acc.: 99.22%] [G loss: 2.788140]\n",
      "epoch:30 step:23616 [D loss: 0.000783, acc.: 100.00%] [G loss: 2.232118]\n",
      "epoch:30 step:23617 [D loss: 0.053121, acc.: 99.22%] [G loss: 0.217007]\n",
      "epoch:30 step:23618 [D loss: 0.065353, acc.: 98.44%] [G loss: 0.205631]\n",
      "epoch:30 step:23619 [D loss: 0.004148, acc.: 100.00%] [G loss: 0.038572]\n",
      "epoch:30 step:23620 [D loss: 0.016509, acc.: 100.00%] [G loss: 0.122680]\n",
      "epoch:30 step:23621 [D loss: 0.007615, acc.: 100.00%] [G loss: 0.042665]\n",
      "epoch:30 step:23622 [D loss: 0.001670, acc.: 100.00%] [G loss: 0.061954]\n",
      "epoch:30 step:23623 [D loss: 0.059401, acc.: 97.66%] [G loss: 0.366890]\n",
      "epoch:30 step:23624 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.003279]\n",
      "epoch:30 step:23625 [D loss: 0.002749, acc.: 100.00%] [G loss: 0.001865]\n",
      "epoch:30 step:23626 [D loss: 0.006239, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:30 step:23627 [D loss: 0.005335, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:30 step:23628 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.006145]\n",
      "epoch:30 step:23629 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:30 step:23630 [D loss: 0.003134, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:30 step:23631 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.007226]\n",
      "epoch:30 step:23632 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.308892]\n",
      "epoch:30 step:23633 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.002024]\n",
      "epoch:30 step:23634 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.001372]\n",
      "epoch:30 step:23635 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:30 step:23636 [D loss: 0.001332, acc.: 100.00%] [G loss: 0.000668]\n",
      "epoch:30 step:23637 [D loss: 0.001709, acc.: 100.00%] [G loss: 0.001048]\n",
      "epoch:30 step:23638 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:30 step:23639 [D loss: 0.010912, acc.: 100.00%] [G loss: 0.001311]\n",
      "epoch:30 step:23640 [D loss: 0.002538, acc.: 100.00%] [G loss: 0.098822]\n",
      "epoch:30 step:23641 [D loss: 0.004538, acc.: 100.00%] [G loss: 0.004885]\n",
      "epoch:30 step:23642 [D loss: 0.001267, acc.: 100.00%] [G loss: 0.002985]\n",
      "epoch:30 step:23643 [D loss: 0.032427, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:30 step:23644 [D loss: 0.008457, acc.: 100.00%] [G loss: 0.009863]\n",
      "epoch:30 step:23645 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.001093]\n",
      "epoch:30 step:23646 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:30 step:23647 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.020139]\n",
      "epoch:30 step:23648 [D loss: 0.001445, acc.: 100.00%] [G loss: 0.000657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23649 [D loss: 0.003266, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:30 step:23650 [D loss: 0.003843, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:30 step:23651 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000572]\n",
      "epoch:30 step:23652 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.008788]\n",
      "epoch:30 step:23653 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.020806]\n",
      "epoch:30 step:23654 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:30 step:23655 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.012196]\n",
      "epoch:30 step:23656 [D loss: 0.001319, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:30 step:23657 [D loss: 0.000685, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:30 step:23658 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:30 step:23659 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.001509]\n",
      "epoch:30 step:23660 [D loss: 0.007900, acc.: 99.22%] [G loss: 0.000159]\n",
      "epoch:30 step:23661 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:30 step:23662 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.008856]\n",
      "epoch:30 step:23663 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:30 step:23664 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:30 step:23665 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.004390]\n",
      "epoch:30 step:23666 [D loss: 0.004284, acc.: 100.00%] [G loss: 0.124281]\n",
      "epoch:30 step:23667 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:30 step:23668 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:30 step:23669 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:30 step:23670 [D loss: 0.045688, acc.: 99.22%] [G loss: 0.000306]\n",
      "epoch:30 step:23671 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.001047]\n",
      "epoch:30 step:23672 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.019011]\n",
      "epoch:30 step:23673 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.127612]\n",
      "epoch:30 step:23674 [D loss: 0.001216, acc.: 100.00%] [G loss: 0.036507]\n",
      "epoch:30 step:23675 [D loss: 0.001517, acc.: 100.00%] [G loss: 0.021668]\n",
      "epoch:30 step:23676 [D loss: 0.003863, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:30 step:23677 [D loss: 0.014615, acc.: 100.00%] [G loss: 0.007493]\n",
      "epoch:30 step:23678 [D loss: 0.001340, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:30 step:23679 [D loss: 0.002119, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:30 step:23680 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:30 step:23681 [D loss: 0.003980, acc.: 100.00%] [G loss: 0.000742]\n",
      "epoch:30 step:23682 [D loss: 0.017494, acc.: 100.00%] [G loss: 0.044868]\n",
      "epoch:30 step:23683 [D loss: 0.004470, acc.: 100.00%] [G loss: 0.009374]\n",
      "epoch:30 step:23684 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.002535]\n",
      "epoch:30 step:23685 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.003270]\n",
      "epoch:30 step:23686 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.001368]\n",
      "epoch:30 step:23687 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.021046]\n",
      "epoch:30 step:23688 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000783]\n",
      "epoch:30 step:23689 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.004116]\n",
      "epoch:30 step:23690 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.984564]\n",
      "epoch:30 step:23691 [D loss: 0.075599, acc.: 97.66%] [G loss: 0.000158]\n",
      "epoch:30 step:23692 [D loss: 0.015696, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:30 step:23693 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.004095]\n",
      "epoch:30 step:23694 [D loss: 0.002206, acc.: 100.00%] [G loss: 0.008982]\n",
      "epoch:30 step:23695 [D loss: 0.000955, acc.: 100.00%] [G loss: 0.004010]\n",
      "epoch:30 step:23696 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.020809]\n",
      "epoch:30 step:23697 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.025644]\n",
      "epoch:30 step:23698 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:30 step:23699 [D loss: 0.000788, acc.: 100.00%] [G loss: 0.000411]\n",
      "epoch:30 step:23700 [D loss: 0.000459, acc.: 100.00%] [G loss: 0.002789]\n",
      "epoch:30 step:23701 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:30 step:23702 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:30 step:23703 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.001039]\n",
      "epoch:30 step:23704 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.010969]\n",
      "epoch:30 step:23705 [D loss: 0.001470, acc.: 100.00%] [G loss: 0.001064]\n",
      "epoch:30 step:23706 [D loss: 0.001918, acc.: 100.00%] [G loss: 0.075688]\n",
      "epoch:30 step:23707 [D loss: 0.060075, acc.: 100.00%] [G loss: 0.001135]\n",
      "epoch:30 step:23708 [D loss: 0.000102, acc.: 100.00%] [G loss: 1.204647]\n",
      "epoch:30 step:23709 [D loss: 0.012743, acc.: 100.00%] [G loss: 0.012824]\n",
      "epoch:30 step:23710 [D loss: 0.001803, acc.: 100.00%] [G loss: 0.111519]\n",
      "epoch:30 step:23711 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.043784]\n",
      "epoch:30 step:23712 [D loss: 0.009639, acc.: 100.00%] [G loss: 0.009061]\n",
      "epoch:30 step:23713 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.360058]\n",
      "epoch:30 step:23714 [D loss: 0.008645, acc.: 100.00%] [G loss: 0.503283]\n",
      "epoch:30 step:23715 [D loss: 0.090528, acc.: 98.44%] [G loss: 2.765604]\n",
      "epoch:30 step:23716 [D loss: 0.557495, acc.: 79.69%] [G loss: 0.324329]\n",
      "epoch:30 step:23717 [D loss: 0.466452, acc.: 83.59%] [G loss: 6.243392]\n",
      "epoch:30 step:23718 [D loss: 0.430401, acc.: 79.69%] [G loss: 5.356152]\n",
      "epoch:30 step:23719 [D loss: 0.072582, acc.: 98.44%] [G loss: 3.890888]\n",
      "epoch:30 step:23720 [D loss: 0.013722, acc.: 100.00%] [G loss: 3.715044]\n",
      "epoch:30 step:23721 [D loss: 0.017293, acc.: 99.22%] [G loss: 2.754858]\n",
      "epoch:30 step:23722 [D loss: 0.002760, acc.: 100.00%] [G loss: 0.269420]\n",
      "epoch:30 step:23723 [D loss: 0.012249, acc.: 100.00%] [G loss: 1.221366]\n",
      "epoch:30 step:23724 [D loss: 0.025109, acc.: 100.00%] [G loss: 0.556235]\n",
      "epoch:30 step:23725 [D loss: 0.033183, acc.: 100.00%] [G loss: 1.335793]\n",
      "epoch:30 step:23726 [D loss: 0.005323, acc.: 100.00%] [G loss: 4.121950]\n",
      "epoch:30 step:23727 [D loss: 0.001985, acc.: 100.00%] [G loss: 1.472754]\n",
      "epoch:30 step:23728 [D loss: 0.000531, acc.: 100.00%] [G loss: 1.487279]\n",
      "epoch:30 step:23729 [D loss: 0.008318, acc.: 99.22%] [G loss: 0.275255]\n",
      "epoch:30 step:23730 [D loss: 0.002576, acc.: 100.00%] [G loss: 1.928036]\n",
      "epoch:30 step:23731 [D loss: 0.002333, acc.: 100.00%] [G loss: 0.313652]\n",
      "epoch:30 step:23732 [D loss: 0.002512, acc.: 100.00%] [G loss: 0.335261]\n",
      "epoch:30 step:23733 [D loss: 0.001802, acc.: 100.00%] [G loss: 0.326237]\n",
      "epoch:30 step:23734 [D loss: 0.004557, acc.: 100.00%] [G loss: 0.559256]\n",
      "epoch:30 step:23735 [D loss: 0.001986, acc.: 100.00%] [G loss: 0.116336]\n",
      "epoch:30 step:23736 [D loss: 0.012248, acc.: 100.00%] [G loss: 0.009129]\n",
      "epoch:30 step:23737 [D loss: 0.005095, acc.: 100.00%] [G loss: 5.072576]\n",
      "epoch:30 step:23738 [D loss: 0.841834, acc.: 70.31%] [G loss: 7.756505]\n",
      "epoch:30 step:23739 [D loss: 1.290016, acc.: 60.16%] [G loss: 5.837955]\n",
      "epoch:30 step:23740 [D loss: 0.194518, acc.: 89.06%] [G loss: 4.204147]\n",
      "epoch:30 step:23741 [D loss: 0.024599, acc.: 98.44%] [G loss: 0.732575]\n",
      "epoch:30 step:23742 [D loss: 0.006549, acc.: 100.00%] [G loss: 3.141095]\n",
      "epoch:30 step:23743 [D loss: 0.013025, acc.: 100.00%] [G loss: 0.353262]\n",
      "epoch:30 step:23744 [D loss: 0.005857, acc.: 100.00%] [G loss: 0.001963]\n",
      "epoch:30 step:23745 [D loss: 0.011572, acc.: 100.00%] [G loss: 2.172358]\n",
      "epoch:30 step:23746 [D loss: 0.007061, acc.: 100.00%] [G loss: 0.000923]\n",
      "epoch:30 step:23747 [D loss: 0.062270, acc.: 98.44%] [G loss: 1.320775]\n",
      "epoch:30 step:23748 [D loss: 0.010457, acc.: 100.00%] [G loss: 1.212617]\n",
      "epoch:30 step:23749 [D loss: 0.005306, acc.: 100.00%] [G loss: 0.678681]\n",
      "epoch:30 step:23750 [D loss: 0.004769, acc.: 100.00%] [G loss: 0.139783]\n",
      "epoch:30 step:23751 [D loss: 0.003392, acc.: 100.00%] [G loss: 0.133018]\n",
      "epoch:30 step:23752 [D loss: 0.006132, acc.: 100.00%] [G loss: 0.071288]\n",
      "epoch:30 step:23753 [D loss: 0.002456, acc.: 100.00%] [G loss: 1.270439]\n",
      "epoch:30 step:23754 [D loss: 0.008443, acc.: 100.00%] [G loss: 0.073103]\n",
      "epoch:30 step:23755 [D loss: 0.006737, acc.: 100.00%] [G loss: 0.004095]\n",
      "epoch:30 step:23756 [D loss: 0.064336, acc.: 98.44%] [G loss: 0.141747]\n",
      "epoch:30 step:23757 [D loss: 0.009014, acc.: 100.00%] [G loss: 0.401582]\n",
      "epoch:30 step:23758 [D loss: 0.006633, acc.: 100.00%] [G loss: 0.331958]\n",
      "epoch:30 step:23759 [D loss: 0.006582, acc.: 100.00%] [G loss: 0.073087]\n",
      "epoch:30 step:23760 [D loss: 0.014188, acc.: 99.22%] [G loss: 0.018673]\n",
      "epoch:30 step:23761 [D loss: 0.020136, acc.: 99.22%] [G loss: 0.022888]\n",
      "epoch:30 step:23762 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.118159]\n",
      "epoch:30 step:23763 [D loss: 0.001750, acc.: 100.00%] [G loss: 0.131102]\n",
      "epoch:30 step:23764 [D loss: 0.002601, acc.: 100.00%] [G loss: 0.003057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23765 [D loss: 0.000960, acc.: 100.00%] [G loss: 0.003358]\n",
      "epoch:30 step:23766 [D loss: 0.002231, acc.: 100.00%] [G loss: 0.007587]\n",
      "epoch:30 step:23767 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.004327]\n",
      "epoch:30 step:23768 [D loss: 0.005219, acc.: 100.00%] [G loss: 0.002158]\n",
      "epoch:30 step:23769 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.002228]\n",
      "epoch:30 step:23770 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.006170]\n",
      "epoch:30 step:23771 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.005874]\n",
      "epoch:30 step:23772 [D loss: 0.000413, acc.: 100.00%] [G loss: 0.001637]\n",
      "epoch:30 step:23773 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.002445]\n",
      "epoch:30 step:23774 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.001675]\n",
      "epoch:30 step:23775 [D loss: 0.000912, acc.: 100.00%] [G loss: 0.000577]\n",
      "epoch:30 step:23776 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.003225]\n",
      "epoch:30 step:23777 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.002418]\n",
      "epoch:30 step:23778 [D loss: 0.001126, acc.: 100.00%] [G loss: 0.004723]\n",
      "epoch:30 step:23779 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:30 step:23780 [D loss: 0.005123, acc.: 100.00%] [G loss: 0.036584]\n",
      "epoch:30 step:23781 [D loss: 0.006249, acc.: 100.00%] [G loss: 0.000910]\n",
      "epoch:30 step:23782 [D loss: 0.018419, acc.: 99.22%] [G loss: 0.008709]\n",
      "epoch:30 step:23783 [D loss: 0.004184, acc.: 100.00%] [G loss: 0.008898]\n",
      "epoch:30 step:23784 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.030500]\n",
      "epoch:30 step:23785 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.007945]\n",
      "epoch:30 step:23786 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.007113]\n",
      "epoch:30 step:23787 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.016908]\n",
      "epoch:30 step:23788 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.018328]\n",
      "epoch:30 step:23789 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.196160]\n",
      "epoch:30 step:23790 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.001974]\n",
      "epoch:30 step:23791 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.007244]\n",
      "epoch:30 step:23792 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.005350]\n",
      "epoch:30 step:23793 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003340]\n",
      "epoch:30 step:23794 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:30 step:23795 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.002403]\n",
      "epoch:30 step:23796 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.003655]\n",
      "epoch:30 step:23797 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.001059]\n",
      "epoch:30 step:23798 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.004482]\n",
      "epoch:30 step:23799 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.003018]\n",
      "epoch:30 step:23800 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.002622]\n",
      "epoch:30 step:23801 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001611]\n",
      "epoch:30 step:23802 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.002695]\n",
      "epoch:30 step:23803 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.001341]\n",
      "epoch:30 step:23804 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.001367]\n",
      "epoch:30 step:23805 [D loss: 0.008937, acc.: 99.22%] [G loss: 0.002526]\n",
      "epoch:30 step:23806 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000866]\n",
      "epoch:30 step:23807 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:30 step:23808 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000612]\n",
      "epoch:30 step:23809 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.003872]\n",
      "epoch:30 step:23810 [D loss: 0.000273, acc.: 100.00%] [G loss: 1.278561]\n",
      "epoch:30 step:23811 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:30 step:23812 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:30 step:23813 [D loss: 0.007278, acc.: 100.00%] [G loss: 0.001686]\n",
      "epoch:30 step:23814 [D loss: 0.002882, acc.: 100.00%] [G loss: 0.003061]\n",
      "epoch:30 step:23815 [D loss: 0.001349, acc.: 100.00%] [G loss: 0.004347]\n",
      "epoch:30 step:23816 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.002460]\n",
      "epoch:30 step:23817 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.003181]\n",
      "epoch:30 step:23818 [D loss: 0.007313, acc.: 100.00%] [G loss: 0.003953]\n",
      "epoch:30 step:23819 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.001009]\n",
      "epoch:30 step:23820 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.002001]\n",
      "epoch:30 step:23821 [D loss: 0.002018, acc.: 100.00%] [G loss: 0.001163]\n",
      "epoch:30 step:23822 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.001145]\n",
      "epoch:30 step:23823 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.005580]\n",
      "epoch:30 step:23824 [D loss: 0.001246, acc.: 100.00%] [G loss: 0.001936]\n",
      "epoch:30 step:23825 [D loss: 0.008693, acc.: 100.00%] [G loss: 0.001006]\n",
      "epoch:30 step:23826 [D loss: 0.007114, acc.: 100.00%] [G loss: 0.005646]\n",
      "epoch:30 step:23827 [D loss: 0.002164, acc.: 100.00%] [G loss: 0.004655]\n",
      "epoch:30 step:23828 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.001876]\n",
      "epoch:30 step:23829 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.001228]\n",
      "epoch:30 step:23830 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.006158]\n",
      "epoch:30 step:23831 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.024096]\n",
      "epoch:30 step:23832 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.045890]\n",
      "epoch:30 step:23833 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.028718]\n",
      "epoch:30 step:23834 [D loss: 0.001446, acc.: 100.00%] [G loss: 0.031570]\n",
      "epoch:30 step:23835 [D loss: 0.048859, acc.: 99.22%] [G loss: 0.110106]\n",
      "epoch:30 step:23836 [D loss: 0.001652, acc.: 100.00%] [G loss: 0.202219]\n",
      "epoch:30 step:23837 [D loss: 0.000817, acc.: 100.00%] [G loss: 3.603272]\n",
      "epoch:30 step:23838 [D loss: 0.008293, acc.: 100.00%] [G loss: 0.076374]\n",
      "epoch:30 step:23839 [D loss: 0.017506, acc.: 99.22%] [G loss: 0.008698]\n",
      "epoch:30 step:23840 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.007053]\n",
      "epoch:30 step:23841 [D loss: 0.031479, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:30 step:23842 [D loss: 0.021832, acc.: 100.00%] [G loss: 0.020208]\n",
      "epoch:30 step:23843 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.007779]\n",
      "epoch:30 step:23844 [D loss: 0.007644, acc.: 100.00%] [G loss: 0.006602]\n",
      "epoch:30 step:23845 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.002877]\n",
      "epoch:30 step:23846 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.046181]\n",
      "epoch:30 step:23847 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.001454]\n",
      "epoch:30 step:23848 [D loss: 0.008344, acc.: 100.00%] [G loss: 0.035277]\n",
      "epoch:30 step:23849 [D loss: 0.000901, acc.: 100.00%] [G loss: 0.029086]\n",
      "epoch:30 step:23850 [D loss: 0.005964, acc.: 100.00%] [G loss: 0.002047]\n",
      "epoch:30 step:23851 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.050369]\n",
      "epoch:30 step:23852 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.002385]\n",
      "epoch:30 step:23853 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.008931]\n",
      "epoch:30 step:23854 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.010131]\n",
      "epoch:30 step:23855 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002352]\n",
      "epoch:30 step:23856 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001602]\n",
      "epoch:30 step:23857 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000660]\n",
      "epoch:30 step:23858 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:30 step:23859 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.001594]\n",
      "epoch:30 step:23860 [D loss: 0.015912, acc.: 99.22%] [G loss: 0.000162]\n",
      "epoch:30 step:23861 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:30 step:23862 [D loss: 0.000952, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:30 step:23863 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:30 step:23864 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:30 step:23865 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000686]\n",
      "epoch:30 step:23866 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:30 step:23867 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000847]\n",
      "epoch:30 step:23868 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:30 step:23869 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001708]\n",
      "epoch:30 step:23870 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:30 step:23871 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:30 step:23872 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:30 step:23873 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:30 step:23874 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:30 step:23875 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000779]\n",
      "epoch:30 step:23876 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:30 step:23877 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23878 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.002226]\n",
      "epoch:30 step:23879 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:30 step:23880 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:30 step:23881 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:30 step:23882 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:30 step:23883 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:30 step:23884 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:30 step:23885 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:30 step:23886 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:30 step:23887 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001336]\n",
      "epoch:30 step:23888 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:30 step:23889 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000466]\n",
      "epoch:30 step:23890 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:30 step:23891 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:30 step:23892 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:30 step:23893 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:30 step:23894 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:30 step:23895 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:30 step:23896 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:30 step:23897 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:30 step:23898 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:30 step:23899 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000854]\n",
      "epoch:30 step:23900 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:30 step:23901 [D loss: 0.036829, acc.: 99.22%] [G loss: 0.415549]\n",
      "epoch:30 step:23902 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.131395]\n",
      "epoch:30 step:23903 [D loss: 0.003621, acc.: 100.00%] [G loss: 0.798715]\n",
      "epoch:30 step:23904 [D loss: 0.001421, acc.: 100.00%] [G loss: 0.079558]\n",
      "epoch:30 step:23905 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.118194]\n",
      "epoch:30 step:23906 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.019599]\n",
      "epoch:30 step:23907 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.071904]\n",
      "epoch:30 step:23908 [D loss: 0.006395, acc.: 100.00%] [G loss: 0.010151]\n",
      "epoch:30 step:23909 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001890]\n",
      "epoch:30 step:23910 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001099]\n",
      "epoch:30 step:23911 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.006211]\n",
      "epoch:30 step:23912 [D loss: 0.002321, acc.: 100.00%] [G loss: 0.002626]\n",
      "epoch:30 step:23913 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000557]\n",
      "epoch:30 step:23914 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001610]\n",
      "epoch:30 step:23915 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.012934]\n",
      "epoch:30 step:23916 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.001114]\n",
      "epoch:30 step:23917 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002312]\n",
      "epoch:30 step:23918 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:30 step:23919 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001889]\n",
      "epoch:30 step:23920 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.001800]\n",
      "epoch:30 step:23921 [D loss: 0.002807, acc.: 100.00%] [G loss: 0.000953]\n",
      "epoch:30 step:23922 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.002122]\n",
      "epoch:30 step:23923 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:30 step:23924 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000520]\n",
      "epoch:30 step:23925 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:30 step:23926 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:30 step:23927 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000730]\n",
      "epoch:30 step:23928 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:30 step:23929 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.001049]\n",
      "epoch:30 step:23930 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:30 step:23931 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:30 step:23932 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:30 step:23933 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:30 step:23934 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:30 step:23935 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:30 step:23936 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:30 step:23937 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:30 step:23938 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001242]\n",
      "epoch:30 step:23939 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.001055]\n",
      "epoch:30 step:23940 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:30 step:23941 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:30 step:23942 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:30 step:23943 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:30 step:23944 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:30 step:23945 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:30 step:23946 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:30 step:23947 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:30 step:23948 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:30 step:23949 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000859]\n",
      "epoch:30 step:23950 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:30 step:23951 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003085]\n",
      "epoch:30 step:23952 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000687]\n",
      "epoch:30 step:23953 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:30 step:23954 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:30 step:23955 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:30 step:23956 [D loss: 0.009871, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:30 step:23957 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:30 step:23958 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:30 step:23959 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:30 step:23960 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:23961 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:30 step:23962 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:30 step:23963 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:30 step:23964 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:30 step:23965 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:30 step:23966 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:30 step:23967 [D loss: 0.003562, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:30 step:23968 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:30 step:23969 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:30 step:23970 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:30 step:23971 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:30 step:23972 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:30 step:23973 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:30 step:23974 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:30 step:23975 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:30 step:23976 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:30 step:23977 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:30 step:23978 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:23979 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.074050]\n",
      "epoch:30 step:23980 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000612]\n",
      "epoch:30 step:23981 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:30 step:23982 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:30 step:23983 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:30 step:23984 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:30 step:23985 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:30 step:23986 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:30 step:23987 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:30 step:23988 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:30 step:23989 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:30 step:23990 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23991 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:30 step:23992 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:30 step:23993 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:30 step:23994 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:30 step:23995 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:30 step:23996 [D loss: 0.016563, acc.: 100.00%] [G loss: 0.026153]\n",
      "epoch:30 step:23997 [D loss: 0.009405, acc.: 100.00%] [G loss: 0.000524]\n",
      "epoch:30 step:23998 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.003386]\n",
      "epoch:30 step:23999 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.003687]\n",
      "epoch:30 step:24000 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.004146]\n",
      "epoch:30 step:24001 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.666767]\n",
      "epoch:30 step:24002 [D loss: 0.000990, acc.: 100.00%] [G loss: 0.007243]\n",
      "epoch:30 step:24003 [D loss: 0.005961, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:30 step:24004 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:30 step:24005 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000953]\n",
      "epoch:30 step:24006 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.098952]\n",
      "epoch:30 step:24007 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:30 step:24008 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.042222]\n",
      "epoch:30 step:24009 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.118331]\n",
      "epoch:30 step:24010 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000416]\n",
      "epoch:30 step:24011 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.013288]\n",
      "epoch:30 step:24012 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.004580]\n",
      "epoch:30 step:24013 [D loss: 0.001933, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:30 step:24014 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.010491]\n",
      "epoch:30 step:24015 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:30 step:24016 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:30 step:24017 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:30 step:24018 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.020944]\n",
      "epoch:30 step:24019 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:30 step:24020 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:30 step:24021 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:30 step:24022 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.013505]\n",
      "epoch:30 step:24023 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.000478]\n",
      "epoch:30 step:24024 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:30 step:24025 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:30 step:24026 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:30 step:24027 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:30 step:24028 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:30 step:24029 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:30 step:24030 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:30 step:24031 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:30 step:24032 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:30 step:24033 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:30 step:24034 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:30 step:24035 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:30 step:24036 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:30 step:24037 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:30 step:24038 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.007240]\n",
      "epoch:30 step:24039 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:30 step:24040 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000716]\n",
      "epoch:30 step:24041 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:30 step:24042 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:30 step:24043 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:30 step:24044 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:30 step:24045 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002824]\n",
      "epoch:30 step:24046 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:30 step:24047 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:30 step:24048 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:30 step:24049 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:30 step:24050 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:30 step:24051 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:30 step:24052 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:30 step:24053 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000859]\n",
      "epoch:30 step:24054 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:30 step:24055 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:30 step:24056 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.005471]\n",
      "epoch:30 step:24057 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:30 step:24058 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:30 step:24059 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:30 step:24060 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:30 step:24061 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:30 step:24062 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:30 step:24063 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000876]\n",
      "epoch:30 step:24064 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001006]\n",
      "epoch:30 step:24065 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:30 step:24066 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:30 step:24067 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:30 step:24068 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:30 step:24069 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:30 step:24070 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:30 step:24071 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:30 step:24072 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:30 step:24073 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:30 step:24074 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:30 step:24075 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:30 step:24076 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:30 step:24077 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:30 step:24078 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:30 step:24079 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:30 step:24080 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:30 step:24081 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:30 step:24082 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:30 step:24083 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:30 step:24084 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:30 step:24085 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:30 step:24086 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:30 step:24087 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:30 step:24088 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:30 step:24089 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:30 step:24090 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:30 step:24091 [D loss: 0.001075, acc.: 100.00%] [G loss: 0.004199]\n",
      "epoch:30 step:24092 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:30 step:24093 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:30 step:24094 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:30 step:24095 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:30 step:24096 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:30 step:24097 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:30 step:24098 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:30 step:24099 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:30 step:24100 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:30 step:24101 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:30 step:24102 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:30 step:24103 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24104 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:30 step:24105 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:30 step:24106 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:30 step:24107 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:30 step:24108 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:30 step:24109 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:30 step:24110 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:30 step:24111 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:30 step:24112 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:30 step:24113 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:30 step:24114 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:30 step:24115 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:30 step:24116 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:24117 [D loss: 0.002094, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:30 step:24118 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:30 step:24119 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:30 step:24120 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:30 step:24121 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:30 step:24122 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:30 step:24123 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:30 step:24124 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:30 step:24125 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:30 step:24126 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:30 step:24127 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:30 step:24128 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:30 step:24129 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:30 step:24130 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:30 step:24131 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:30 step:24132 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:30 step:24133 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:30 step:24134 [D loss: 0.004606, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:30 step:24135 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:30 step:24136 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004239]\n",
      "epoch:30 step:24137 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:30 step:24138 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:30 step:24139 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:30 step:24140 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:30 step:24141 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:30 step:24142 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:30 step:24143 [D loss: 0.008396, acc.: 99.22%] [G loss: 0.000059]\n",
      "epoch:30 step:24144 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:30 step:24145 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:30 step:24146 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:30 step:24147 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:30 step:24148 [D loss: 0.009849, acc.: 99.22%] [G loss: 0.000310]\n",
      "epoch:30 step:24149 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.003043]\n",
      "epoch:30 step:24150 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000965]\n",
      "epoch:30 step:24151 [D loss: 0.004433, acc.: 100.00%] [G loss: 0.004084]\n",
      "epoch:30 step:24152 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:30 step:24153 [D loss: 0.002294, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:30 step:24154 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:30 step:24155 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:30 step:24156 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:30 step:24157 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000557]\n",
      "epoch:30 step:24158 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:30 step:24159 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:30 step:24160 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:30 step:24161 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.036866]\n",
      "epoch:30 step:24162 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.003427]\n",
      "epoch:30 step:24163 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:24164 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:30 step:24165 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:30 step:24166 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000950]\n",
      "epoch:30 step:24167 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:30 step:24168 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.023998]\n",
      "epoch:30 step:24169 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000805]\n",
      "epoch:30 step:24170 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002006]\n",
      "epoch:30 step:24171 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:30 step:24172 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:30 step:24173 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:30 step:24174 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:30 step:24175 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:24176 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:30 step:24177 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:30 step:24178 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:30 step:24179 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:30 step:24180 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:30 step:24181 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:30 step:24182 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:30 step:24183 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:30 step:24184 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:30 step:24185 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:30 step:24186 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:30 step:24187 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:30 step:24188 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:30 step:24189 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:30 step:24190 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000878]\n",
      "epoch:30 step:24191 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:30 step:24192 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:30 step:24193 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:24194 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:30 step:24195 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:30 step:24196 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:30 step:24197 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.003161]\n",
      "epoch:30 step:24198 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:30 step:24199 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:30 step:24200 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:30 step:24201 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:30 step:24202 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:30 step:24203 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:30 step:24204 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:30 step:24205 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:30 step:24206 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:30 step:24207 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:30 step:24208 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:30 step:24209 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:30 step:24210 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:30 step:24211 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24212 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:31 step:24213 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:31 step:24214 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24215 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:31 step:24216 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24217 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24218 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:31 step:24219 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24220 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:31 step:24221 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:31 step:24222 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24223 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24224 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24225 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:31 step:24226 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.002082]\n",
      "epoch:31 step:24227 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.002762]\n",
      "epoch:31 step:24228 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:31 step:24229 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:31 step:24230 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001142]\n",
      "epoch:31 step:24231 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001123]\n",
      "epoch:31 step:24232 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:31 step:24233 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:31 step:24234 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.049852]\n",
      "epoch:31 step:24235 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:31 step:24236 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24237 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:31 step:24238 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:31 step:24239 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:31 step:24240 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:31 step:24241 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:31 step:24242 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:31 step:24243 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:31 step:24244 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24245 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24246 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002390]\n",
      "epoch:31 step:24247 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24248 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:31 step:24249 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:31 step:24250 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24251 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:31 step:24252 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:31 step:24253 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:31 step:24254 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:31 step:24255 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:31 step:24256 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:31 step:24257 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:31 step:24258 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24259 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:31 step:24260 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:31 step:24261 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24262 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:31 step:24263 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24264 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000859]\n",
      "epoch:31 step:24265 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24266 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24267 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24268 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24269 [D loss: 0.001360, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:31 step:24270 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24271 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:31 step:24272 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:31 step:24273 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24274 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:31 step:24275 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:31 step:24276 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24277 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:31 step:24278 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:31 step:24279 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:31 step:24280 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:31 step:24281 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24282 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24283 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:31 step:24284 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:31 step:24285 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:31 step:24286 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:31 step:24287 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:31 step:24288 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24289 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002140]\n",
      "epoch:31 step:24290 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:31 step:24291 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:31 step:24292 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24293 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:31 step:24294 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24295 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24296 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24297 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24298 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:31 step:24299 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:31 step:24301 [D loss: 0.009139, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24302 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24303 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24304 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24305 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24306 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24307 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24308 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24309 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24310 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24311 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24312 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24313 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24314 [D loss: 0.005846, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24315 [D loss: 0.001290, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24316 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24317 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24318 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24319 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24320 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:31 step:24321 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:31 step:24322 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24323 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24324 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24325 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:31 step:24326 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:31 step:24327 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24328 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:31 step:24329 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24330 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24331 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24332 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24333 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24334 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24335 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24336 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24337 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24338 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24339 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24340 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24341 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24342 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002366]\n",
      "epoch:31 step:24343 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:31 step:24344 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24345 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:31 step:24346 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:31 step:24347 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24348 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24349 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24350 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24351 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24352 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:31 step:24353 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24354 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24355 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24356 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24357 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:31 step:24358 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:31 step:24359 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24360 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24361 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24362 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:31 step:24363 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24364 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24365 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24366 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24367 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24368 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24369 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24370 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24371 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24372 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002411]\n",
      "epoch:31 step:24373 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24374 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24375 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24376 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24377 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24378 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:31 step:24379 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24380 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24381 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24382 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24383 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:31 step:24384 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24385 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24386 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24387 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24388 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24389 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:31 step:24390 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24391 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24392 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000857]\n",
      "epoch:31 step:24393 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:31 step:24394 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24395 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24396 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:31 step:24397 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24398 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24399 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:31 step:24401 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24402 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:31 step:24403 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24404 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24405 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24406 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24407 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24408 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24409 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24410 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:31 step:24411 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24412 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24413 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24414 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24415 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24416 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24417 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24418 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24419 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24420 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24421 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24422 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24423 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:31 step:24424 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24425 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24426 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24427 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24428 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.005588]\n",
      "epoch:31 step:24429 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24430 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24431 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24432 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24433 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:31 step:24434 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:31 step:24435 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:31 step:24436 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24437 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24438 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24439 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:31 step:24440 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24441 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24442 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24443 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:31 step:24444 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24445 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24446 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24447 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24448 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:31 step:24449 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:31 step:24450 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24451 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:31 step:24452 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24453 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:31 step:24454 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24455 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24456 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24457 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24458 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:31 step:24459 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24460 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24461 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24462 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24463 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24464 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24465 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24466 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24467 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24468 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:31 step:24469 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:31 step:24470 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24471 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24472 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24473 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24474 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24475 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24476 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:31 step:24477 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24478 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24479 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24480 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24481 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24482 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24483 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24484 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24485 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24486 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24487 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:31 step:24488 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24489 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24490 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24491 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24492 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24493 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24494 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000958]\n",
      "epoch:31 step:24495 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24496 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24497 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000722]\n",
      "epoch:31 step:24498 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:31 step:24499 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24500 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24501 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24502 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24503 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24504 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24505 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24506 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24507 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24508 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24509 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24510 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24511 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24512 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24513 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:31 step:24514 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:31 step:24515 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24516 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24517 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24518 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24519 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24520 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24521 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:31 step:24522 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24523 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24524 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24525 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24526 [D loss: 0.014685, acc.: 99.22%] [G loss: 0.000000]\n",
      "epoch:31 step:24527 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24528 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24529 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24530 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24531 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24532 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24533 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24534 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24535 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24536 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24537 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24538 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24539 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24540 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:31 step:24541 [D loss: 0.037180, acc.: 99.22%] [G loss: 0.711652]\n",
      "epoch:31 step:24542 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.065367]\n",
      "epoch:31 step:24543 [D loss: 0.023192, acc.: 99.22%] [G loss: 0.019731]\n",
      "epoch:31 step:24544 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.002934]\n",
      "epoch:31 step:24545 [D loss: 0.005955, acc.: 100.00%] [G loss: 0.003795]\n",
      "epoch:31 step:24546 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000477]\n",
      "epoch:31 step:24547 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:31 step:24548 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:31 step:24549 [D loss: 0.001247, acc.: 100.00%] [G loss: 0.747212]\n",
      "epoch:31 step:24550 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.077285]\n",
      "epoch:31 step:24551 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:31 step:24552 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000524]\n",
      "epoch:31 step:24553 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.007220]\n",
      "epoch:31 step:24554 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.083490]\n",
      "epoch:31 step:24555 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:31 step:24556 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:31 step:24557 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:31 step:24558 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24559 [D loss: 0.000561, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:31 step:24560 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003885]\n",
      "epoch:31 step:24561 [D loss: 0.004962, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24562 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:31 step:24563 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:31 step:24564 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:31 step:24565 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24566 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:31 step:24567 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:31 step:24568 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24569 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:31 step:24570 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24571 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:31 step:24572 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:31 step:24573 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24574 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24575 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:31 step:24576 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24577 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24578 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:31 step:24579 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24580 [D loss: 0.044602, acc.: 97.66%] [G loss: 0.408895]\n",
      "epoch:31 step:24581 [D loss: 0.004880, acc.: 100.00%] [G loss: 0.003678]\n",
      "epoch:31 step:24582 [D loss: 0.171207, acc.: 96.09%] [G loss: 0.000076]\n",
      "epoch:31 step:24583 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:31 step:24584 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:31 step:24585 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.004864]\n",
      "epoch:31 step:24586 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:31 step:24587 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.036618]\n",
      "epoch:31 step:24588 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.050765]\n",
      "epoch:31 step:24589 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.202866]\n",
      "epoch:31 step:24590 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.001608]\n",
      "epoch:31 step:24591 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:31 step:24592 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:31 step:24593 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.019953]\n",
      "epoch:31 step:24594 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:31 step:24595 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001111]\n",
      "epoch:31 step:24596 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:31 step:24597 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.109242]\n",
      "epoch:31 step:24598 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:31 step:24599 [D loss: 0.002140, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:31 step:24600 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:31 step:24601 [D loss: 0.003400, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:31 step:24602 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.005122]\n",
      "epoch:31 step:24603 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:31 step:24604 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:31 step:24605 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:31 step:24606 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.002929]\n",
      "epoch:31 step:24607 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:31 step:24608 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:31 step:24609 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:31 step:24610 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:31 step:24611 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:31 step:24612 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:31 step:24613 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:31 step:24614 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:31 step:24615 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.003796]\n",
      "epoch:31 step:24616 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:31 step:24617 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:31 step:24618 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:31 step:24619 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:31 step:24620 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.044989]\n",
      "epoch:31 step:24621 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:31 step:24622 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:31 step:24623 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001352]\n",
      "epoch:31 step:24624 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:31 step:24625 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:31 step:24626 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:31 step:24627 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003752]\n",
      "epoch:31 step:24628 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:31 step:24629 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.004498]\n",
      "epoch:31 step:24630 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:31 step:24631 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:31 step:24632 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.004224]\n",
      "epoch:31 step:24633 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:31 step:24634 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:31 step:24635 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:31 step:24636 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24637 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:31 step:24638 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:31 step:24639 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:31 step:24640 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000713]\n",
      "epoch:31 step:24641 [D loss: 0.002086, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:31 step:24642 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000895]\n",
      "epoch:31 step:24643 [D loss: 0.007624, acc.: 99.22%] [G loss: 0.000014]\n",
      "epoch:31 step:24644 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24645 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:31 step:24646 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24647 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:31 step:24648 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24649 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:31 step:24650 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24651 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:31 step:24652 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:31 step:24653 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24654 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24655 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:31 step:24656 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24657 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24658 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24659 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24660 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24661 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24662 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24663 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:31 step:24664 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24665 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24666 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:31 step:24667 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24668 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:31 step:24669 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24670 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24671 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24672 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:31 step:24673 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:31 step:24674 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24675 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24676 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24677 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24678 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24679 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24680 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24681 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24682 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:31 step:24683 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:31 step:24684 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24685 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:31 step:24686 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24687 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24688 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24689 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:31 step:24690 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:31 step:24691 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24692 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24693 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24694 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24695 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24696 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:31 step:24697 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.005838]\n",
      "epoch:31 step:24698 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.004118]\n",
      "epoch:31 step:24699 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24700 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24701 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:31 step:24702 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24703 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:31 step:24704 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24705 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:31 step:24706 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:31 step:24707 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24708 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24709 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24710 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24711 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:31 step:24712 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24713 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:31 step:24714 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24715 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24716 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24717 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:31 step:24718 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:31 step:24719 [D loss: 0.004069, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24720 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:31 step:24721 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:31 step:24722 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:31 step:24723 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.014150]\n",
      "epoch:31 step:24724 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:31 step:24725 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24726 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:31 step:24727 [D loss: 0.009543, acc.: 100.00%] [G loss: 0.025931]\n",
      "epoch:31 step:24728 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.368509]\n",
      "epoch:31 step:24729 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.003386]\n",
      "epoch:31 step:24730 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.009093]\n",
      "epoch:31 step:24731 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001172]\n",
      "epoch:31 step:24732 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.208124]\n",
      "epoch:31 step:24733 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:31 step:24734 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.010283]\n",
      "epoch:31 step:24735 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:31 step:24736 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:31 step:24737 [D loss: 0.007323, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:31 step:24738 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:31 step:24739 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24740 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:31 step:24741 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:31 step:24742 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:31 step:24743 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24744 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:31 step:24745 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:31 step:24746 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24747 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24748 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24749 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:31 step:24750 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:31 step:24751 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:31 step:24752 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:31 step:24753 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24754 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:31 step:24755 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24756 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:31 step:24757 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:31 step:24758 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24759 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24760 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:31 step:24761 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:31 step:24762 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:31 step:24763 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:31 step:24764 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:31 step:24765 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:31 step:24766 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24767 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:31 step:24768 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24769 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:31 step:24770 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:31 step:24771 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:31 step:24772 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24773 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:31 step:24774 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:31 step:24775 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:31 step:24776 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:31 step:24777 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:31 step:24778 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24779 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24780 [D loss: 0.003853, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:31 step:24781 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:31 step:24782 [D loss: 0.011566, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:31 step:24783 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.004831]\n",
      "epoch:31 step:24784 [D loss: 0.008007, acc.: 100.00%] [G loss: 0.004939]\n",
      "epoch:31 step:24785 [D loss: 0.000750, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:31 step:24786 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.002416]\n",
      "epoch:31 step:24787 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24788 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001435]\n",
      "epoch:31 step:24789 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:31 step:24790 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:31 step:24791 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000811]\n",
      "epoch:31 step:24792 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:31 step:24793 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000608]\n",
      "epoch:31 step:24794 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.001461]\n",
      "epoch:31 step:24795 [D loss: 0.000023, acc.: 100.00%] [G loss: 1.912857]\n",
      "epoch:31 step:24796 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:31 step:24797 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.007996]\n",
      "epoch:31 step:24798 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.078755]\n",
      "epoch:31 step:24799 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.099557]\n",
      "epoch:31 step:24800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.003570]\n",
      "epoch:31 step:24801 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.004465]\n",
      "epoch:31 step:24802 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.007351]\n",
      "epoch:31 step:24803 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.004509]\n",
      "epoch:31 step:24804 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.003340]\n",
      "epoch:31 step:24805 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001273]\n",
      "epoch:31 step:24806 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.003121]\n",
      "epoch:31 step:24807 [D loss: 0.025880, acc.: 100.00%] [G loss: 2.499687]\n",
      "epoch:31 step:24808 [D loss: 0.001006, acc.: 100.00%] [G loss: 2.345656]\n",
      "epoch:31 step:24809 [D loss: 3.037271, acc.: 42.19%] [G loss: 14.819153]\n",
      "epoch:31 step:24810 [D loss: 5.084116, acc.: 50.00%] [G loss: 8.113148]\n",
      "epoch:31 step:24811 [D loss: 2.626408, acc.: 50.78%] [G loss: 1.364579]\n",
      "epoch:31 step:24812 [D loss: 1.051270, acc.: 52.34%] [G loss: 1.272586]\n",
      "epoch:31 step:24813 [D loss: 0.640528, acc.: 72.66%] [G loss: 1.369994]\n",
      "epoch:31 step:24814 [D loss: 0.354406, acc.: 93.75%] [G loss: 1.585673]\n",
      "epoch:31 step:24815 [D loss: 0.395474, acc.: 85.94%] [G loss: 1.608313]\n",
      "epoch:31 step:24816 [D loss: 0.270203, acc.: 96.09%] [G loss: 0.648553]\n",
      "epoch:31 step:24817 [D loss: 0.207004, acc.: 96.88%] [G loss: 0.736314]\n",
      "epoch:31 step:24818 [D loss: 0.204702, acc.: 95.31%] [G loss: 0.503084]\n",
      "epoch:31 step:24819 [D loss: 0.137869, acc.: 98.44%] [G loss: 2.151172]\n",
      "epoch:31 step:24820 [D loss: 0.223424, acc.: 96.09%] [G loss: 3.607886]\n",
      "epoch:31 step:24821 [D loss: 0.213913, acc.: 93.75%] [G loss: 0.826704]\n",
      "epoch:31 step:24822 [D loss: 0.072346, acc.: 100.00%] [G loss: 0.669046]\n",
      "epoch:31 step:24823 [D loss: 0.138912, acc.: 95.31%] [G loss: 0.605055]\n",
      "epoch:31 step:24824 [D loss: 0.165863, acc.: 98.44%] [G loss: 3.684193]\n",
      "epoch:31 step:24825 [D loss: 0.264543, acc.: 89.06%] [G loss: 1.291716]\n",
      "epoch:31 step:24826 [D loss: 0.335185, acc.: 90.62%] [G loss: 0.683753]\n",
      "epoch:31 step:24827 [D loss: 0.340277, acc.: 89.06%] [G loss: 0.646649]\n",
      "epoch:31 step:24828 [D loss: 0.084786, acc.: 99.22%] [G loss: 4.574889]\n",
      "epoch:31 step:24829 [D loss: 0.058189, acc.: 99.22%] [G loss: 0.474555]\n",
      "epoch:31 step:24830 [D loss: 0.257658, acc.: 90.62%] [G loss: 3.430330]\n",
      "epoch:31 step:24831 [D loss: 0.074680, acc.: 97.66%] [G loss: 0.490740]\n",
      "epoch:31 step:24832 [D loss: 0.083421, acc.: 98.44%] [G loss: 0.194228]\n",
      "epoch:31 step:24833 [D loss: 0.024714, acc.: 100.00%] [G loss: 0.164626]\n",
      "epoch:31 step:24834 [D loss: 0.126686, acc.: 96.09%] [G loss: 0.013205]\n",
      "epoch:31 step:24835 [D loss: 0.038072, acc.: 100.00%] [G loss: 0.023699]\n",
      "epoch:31 step:24836 [D loss: 0.028805, acc.: 99.22%] [G loss: 0.331239]\n",
      "epoch:31 step:24837 [D loss: 0.021118, acc.: 100.00%] [G loss: 0.006509]\n",
      "epoch:31 step:24838 [D loss: 0.026124, acc.: 100.00%] [G loss: 0.017032]\n",
      "epoch:31 step:24839 [D loss: 0.058000, acc.: 98.44%] [G loss: 5.162193]\n",
      "epoch:31 step:24840 [D loss: 0.072738, acc.: 97.66%] [G loss: 0.247906]\n",
      "epoch:31 step:24841 [D loss: 0.038019, acc.: 100.00%] [G loss: 0.304830]\n",
      "epoch:31 step:24842 [D loss: 0.033035, acc.: 99.22%] [G loss: 2.376467]\n",
      "epoch:31 step:24843 [D loss: 0.026759, acc.: 100.00%] [G loss: 0.197343]\n",
      "epoch:31 step:24844 [D loss: 0.009852, acc.: 100.00%] [G loss: 0.094875]\n",
      "epoch:31 step:24845 [D loss: 0.076196, acc.: 99.22%] [G loss: 0.076480]\n",
      "epoch:31 step:24846 [D loss: 0.124231, acc.: 96.88%] [G loss: 0.062143]\n",
      "epoch:31 step:24847 [D loss: 0.040617, acc.: 99.22%] [G loss: 0.496565]\n",
      "epoch:31 step:24848 [D loss: 0.017567, acc.: 99.22%] [G loss: 1.035307]\n",
      "epoch:31 step:24849 [D loss: 0.089247, acc.: 96.09%] [G loss: 9.022089]\n",
      "epoch:31 step:24850 [D loss: 0.131304, acc.: 95.31%] [G loss: 7.049829]\n",
      "epoch:31 step:24851 [D loss: 0.118023, acc.: 92.97%] [G loss: 7.535586]\n",
      "epoch:31 step:24852 [D loss: 0.047128, acc.: 97.66%] [G loss: 8.688614]\n",
      "epoch:31 step:24853 [D loss: 0.125175, acc.: 95.31%] [G loss: 6.167958]\n",
      "epoch:31 step:24854 [D loss: 0.014383, acc.: 100.00%] [G loss: 6.260198]\n",
      "epoch:31 step:24855 [D loss: 0.016878, acc.: 100.00%] [G loss: 6.634523]\n",
      "epoch:31 step:24856 [D loss: 0.182813, acc.: 93.75%] [G loss: 9.920214]\n",
      "epoch:31 step:24857 [D loss: 0.167845, acc.: 92.19%] [G loss: 8.481725]\n",
      "epoch:31 step:24858 [D loss: 0.079240, acc.: 96.09%] [G loss: 0.909421]\n",
      "epoch:31 step:24859 [D loss: 0.083255, acc.: 99.22%] [G loss: 6.683884]\n",
      "epoch:31 step:24860 [D loss: 0.013867, acc.: 100.00%] [G loss: 6.277851]\n",
      "epoch:31 step:24861 [D loss: 0.013179, acc.: 100.00%] [G loss: 5.366073]\n",
      "epoch:31 step:24862 [D loss: 0.008604, acc.: 100.00%] [G loss: 4.898168]\n",
      "epoch:31 step:24863 [D loss: 0.018919, acc.: 100.00%] [G loss: 4.546476]\n",
      "epoch:31 step:24864 [D loss: 0.027328, acc.: 100.00%] [G loss: 3.811387]\n",
      "epoch:31 step:24865 [D loss: 0.009108, acc.: 100.00%] [G loss: 0.376973]\n",
      "epoch:31 step:24866 [D loss: 0.000906, acc.: 100.00%] [G loss: 3.493840]\n",
      "epoch:31 step:24867 [D loss: 0.010684, acc.: 100.00%] [G loss: 0.412304]\n",
      "epoch:31 step:24868 [D loss: 0.002980, acc.: 100.00%] [G loss: 2.952601]\n",
      "epoch:31 step:24869 [D loss: 0.002086, acc.: 100.00%] [G loss: 2.978545]\n",
      "epoch:31 step:24870 [D loss: 0.001150, acc.: 100.00%] [G loss: 1.730783]\n",
      "epoch:31 step:24871 [D loss: 0.008508, acc.: 100.00%] [G loss: 1.429559]\n",
      "epoch:31 step:24872 [D loss: 0.001339, acc.: 100.00%] [G loss: 0.126502]\n",
      "epoch:31 step:24873 [D loss: 0.001404, acc.: 100.00%] [G loss: 0.642620]\n",
      "epoch:31 step:24874 [D loss: 0.001264, acc.: 100.00%] [G loss: 0.267133]\n",
      "epoch:31 step:24875 [D loss: 0.001154, acc.: 100.00%] [G loss: 0.027758]\n",
      "epoch:31 step:24876 [D loss: 0.003063, acc.: 100.00%] [G loss: 0.122180]\n",
      "epoch:31 step:24877 [D loss: 0.008176, acc.: 100.00%] [G loss: 0.041420]\n",
      "epoch:31 step:24878 [D loss: 0.002897, acc.: 100.00%] [G loss: 0.092765]\n",
      "epoch:31 step:24879 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.020386]\n",
      "epoch:31 step:24880 [D loss: 0.001437, acc.: 100.00%] [G loss: 0.072911]\n",
      "epoch:31 step:24881 [D loss: 0.001640, acc.: 100.00%] [G loss: 0.019185]\n",
      "epoch:31 step:24882 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.005955]\n",
      "epoch:31 step:24883 [D loss: 0.006244, acc.: 100.00%] [G loss: 0.010363]\n",
      "epoch:31 step:24884 [D loss: 0.001850, acc.: 100.00%] [G loss: 0.004420]\n",
      "epoch:31 step:24885 [D loss: 0.000878, acc.: 100.00%] [G loss: 0.083157]\n",
      "epoch:31 step:24886 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.012913]\n",
      "epoch:31 step:24887 [D loss: 0.002037, acc.: 100.00%] [G loss: 0.002017]\n",
      "epoch:31 step:24888 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.010785]\n",
      "epoch:31 step:24889 [D loss: 0.003409, acc.: 100.00%] [G loss: 0.004693]\n",
      "epoch:31 step:24890 [D loss: 0.007907, acc.: 100.00%] [G loss: 1.023437]\n",
      "epoch:31 step:24891 [D loss: 0.535392, acc.: 75.00%] [G loss: 7.917848]\n",
      "epoch:31 step:24892 [D loss: 1.815684, acc.: 50.78%] [G loss: 0.780153]\n",
      "epoch:31 step:24893 [D loss: 0.317380, acc.: 86.72%] [G loss: 1.579435]\n",
      "epoch:31 step:24894 [D loss: 0.080355, acc.: 96.88%] [G loss: 0.550119]\n",
      "epoch:31 step:24895 [D loss: 0.183053, acc.: 94.53%] [G loss: 0.174870]\n",
      "epoch:31 step:24896 [D loss: 0.049913, acc.: 96.88%] [G loss: 0.017708]\n",
      "epoch:31 step:24897 [D loss: 0.090984, acc.: 97.66%] [G loss: 0.035974]\n",
      "epoch:31 step:24898 [D loss: 0.008290, acc.: 100.00%] [G loss: 0.010994]\n",
      "epoch:31 step:24899 [D loss: 0.001827, acc.: 100.00%] [G loss: 0.017226]\n",
      "epoch:31 step:24900 [D loss: 0.021985, acc.: 98.44%] [G loss: 0.007705]\n",
      "epoch:31 step:24901 [D loss: 0.006414, acc.: 100.00%] [G loss: 0.406051]\n",
      "epoch:31 step:24902 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.006690]\n",
      "epoch:31 step:24903 [D loss: 0.100577, acc.: 96.88%] [G loss: 0.001628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24904 [D loss: 0.004038, acc.: 100.00%] [G loss: 0.184328]\n",
      "epoch:31 step:24905 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.066049]\n",
      "epoch:31 step:24906 [D loss: 0.018273, acc.: 100.00%] [G loss: 0.074685]\n",
      "epoch:31 step:24907 [D loss: 0.123859, acc.: 94.53%] [G loss: 0.006074]\n",
      "epoch:31 step:24908 [D loss: 0.052594, acc.: 98.44%] [G loss: 1.680342]\n",
      "epoch:31 step:24909 [D loss: 0.018078, acc.: 99.22%] [G loss: 0.003714]\n",
      "epoch:31 step:24910 [D loss: 0.004590, acc.: 100.00%] [G loss: 0.074069]\n",
      "epoch:31 step:24911 [D loss: 0.005613, acc.: 100.00%] [G loss: 0.078198]\n",
      "epoch:31 step:24912 [D loss: 0.008805, acc.: 99.22%] [G loss: 0.002777]\n",
      "epoch:31 step:24913 [D loss: 0.005246, acc.: 100.00%] [G loss: 0.001419]\n",
      "epoch:31 step:24914 [D loss: 0.023470, acc.: 100.00%] [G loss: 0.007015]\n",
      "epoch:31 step:24915 [D loss: 0.200083, acc.: 92.19%] [G loss: 0.000515]\n",
      "epoch:31 step:24916 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.001133]\n",
      "epoch:31 step:24917 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000971]\n",
      "epoch:31 step:24918 [D loss: 0.010001, acc.: 100.00%] [G loss: 0.000667]\n",
      "epoch:31 step:24919 [D loss: 0.004125, acc.: 100.00%] [G loss: 0.046220]\n",
      "epoch:31 step:24920 [D loss: 0.003053, acc.: 100.00%] [G loss: 0.002306]\n",
      "epoch:31 step:24921 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.001518]\n",
      "epoch:31 step:24922 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.104720]\n",
      "epoch:31 step:24923 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.007675]\n",
      "epoch:31 step:24924 [D loss: 0.016392, acc.: 99.22%] [G loss: 0.000743]\n",
      "epoch:31 step:24925 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:31 step:24926 [D loss: 0.004876, acc.: 100.00%] [G loss: 0.010061]\n",
      "epoch:31 step:24927 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.000536]\n",
      "epoch:31 step:24928 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.003655]\n",
      "epoch:31 step:24929 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.001063]\n",
      "epoch:31 step:24930 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:31 step:24931 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.001996]\n",
      "epoch:31 step:24932 [D loss: 0.001552, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:31 step:24933 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000543]\n",
      "epoch:31 step:24934 [D loss: 0.004338, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:31 step:24935 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:31 step:24936 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.019121]\n",
      "epoch:31 step:24937 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:31 step:24938 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000536]\n",
      "epoch:31 step:24939 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.003457]\n",
      "epoch:31 step:24940 [D loss: 0.000476, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:31 step:24941 [D loss: 0.004826, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:31 step:24942 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:31 step:24943 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000641]\n",
      "epoch:31 step:24944 [D loss: 0.009351, acc.: 100.00%] [G loss: 0.001186]\n",
      "epoch:31 step:24945 [D loss: 0.008228, acc.: 99.22%] [G loss: 0.000296]\n",
      "epoch:31 step:24946 [D loss: 0.006155, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:31 step:24947 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000814]\n",
      "epoch:31 step:24948 [D loss: 0.004955, acc.: 100.00%] [G loss: 0.001062]\n",
      "epoch:31 step:24949 [D loss: 0.004219, acc.: 100.00%] [G loss: 0.000763]\n",
      "epoch:31 step:24950 [D loss: 0.010368, acc.: 100.00%] [G loss: 0.000946]\n",
      "epoch:31 step:24951 [D loss: 0.012383, acc.: 100.00%] [G loss: 0.002838]\n",
      "epoch:31 step:24952 [D loss: 0.057159, acc.: 100.00%] [G loss: 0.169784]\n",
      "epoch:31 step:24953 [D loss: 0.024660, acc.: 99.22%] [G loss: 0.950425]\n",
      "epoch:31 step:24954 [D loss: 2.085936, acc.: 52.34%] [G loss: 9.898748]\n",
      "epoch:31 step:24955 [D loss: 1.630424, acc.: 57.03%] [G loss: 2.005326]\n",
      "epoch:31 step:24956 [D loss: 0.318562, acc.: 87.50%] [G loss: 3.473619]\n",
      "epoch:31 step:24957 [D loss: 0.190338, acc.: 96.09%] [G loss: 3.543425]\n",
      "epoch:31 step:24958 [D loss: 0.190599, acc.: 96.88%] [G loss: 3.656211]\n",
      "epoch:31 step:24959 [D loss: 0.086870, acc.: 98.44%] [G loss: 4.151872]\n",
      "epoch:31 step:24960 [D loss: 0.101618, acc.: 100.00%] [G loss: 3.959217]\n",
      "epoch:31 step:24961 [D loss: 0.114128, acc.: 96.88%] [G loss: 0.919772]\n",
      "epoch:31 step:24962 [D loss: 0.307152, acc.: 86.72%] [G loss: 4.264258]\n",
      "epoch:31 step:24963 [D loss: 0.058529, acc.: 98.44%] [G loss: 4.419711]\n",
      "epoch:31 step:24964 [D loss: 0.171478, acc.: 93.75%] [G loss: 3.981591]\n",
      "epoch:31 step:24965 [D loss: 0.029645, acc.: 100.00%] [G loss: 0.499910]\n",
      "epoch:31 step:24966 [D loss: 0.141813, acc.: 96.09%] [G loss: 4.101845]\n",
      "epoch:31 step:24967 [D loss: 0.040297, acc.: 99.22%] [G loss: 4.549688]\n",
      "epoch:31 step:24968 [D loss: 0.134111, acc.: 94.53%] [G loss: 1.735612]\n",
      "epoch:31 step:24969 [D loss: 0.066340, acc.: 99.22%] [G loss: 3.368472]\n",
      "epoch:31 step:24970 [D loss: 0.034005, acc.: 100.00%] [G loss: 3.434152]\n",
      "epoch:31 step:24971 [D loss: 0.077750, acc.: 98.44%] [G loss: 3.348658]\n",
      "epoch:31 step:24972 [D loss: 0.018712, acc.: 100.00%] [G loss: 0.012223]\n",
      "epoch:31 step:24973 [D loss: 0.150255, acc.: 96.09%] [G loss: 0.799175]\n",
      "epoch:31 step:24974 [D loss: 0.160166, acc.: 93.75%] [G loss: 3.799952]\n",
      "epoch:31 step:24975 [D loss: 0.692164, acc.: 67.19%] [G loss: 6.658388]\n",
      "epoch:31 step:24976 [D loss: 0.781823, acc.: 61.72%] [G loss: 2.263222]\n",
      "epoch:31 step:24977 [D loss: 0.509751, acc.: 76.56%] [G loss: 1.406192]\n",
      "epoch:31 step:24978 [D loss: 0.065607, acc.: 98.44%] [G loss: 0.624951]\n",
      "epoch:31 step:24979 [D loss: 0.027736, acc.: 99.22%] [G loss: 1.095148]\n",
      "epoch:31 step:24980 [D loss: 0.046451, acc.: 99.22%] [G loss: 0.336820]\n",
      "epoch:31 step:24981 [D loss: 0.004556, acc.: 100.00%] [G loss: 0.360305]\n",
      "epoch:31 step:24982 [D loss: 0.063021, acc.: 98.44%] [G loss: 0.219045]\n",
      "epoch:31 step:24983 [D loss: 0.005208, acc.: 100.00%] [G loss: 0.071088]\n",
      "epoch:31 step:24984 [D loss: 0.009287, acc.: 100.00%] [G loss: 0.114262]\n",
      "epoch:31 step:24985 [D loss: 0.004175, acc.: 100.00%] [G loss: 0.033684]\n",
      "epoch:31 step:24986 [D loss: 0.005211, acc.: 100.00%] [G loss: 0.026054]\n",
      "epoch:31 step:24987 [D loss: 0.002130, acc.: 100.00%] [G loss: 0.080483]\n",
      "epoch:31 step:24988 [D loss: 0.001689, acc.: 100.00%] [G loss: 0.042747]\n",
      "epoch:31 step:24989 [D loss: 0.001784, acc.: 100.00%] [G loss: 0.044092]\n",
      "epoch:31 step:24990 [D loss: 0.010662, acc.: 100.00%] [G loss: 0.018169]\n",
      "epoch:31 step:24991 [D loss: 0.002066, acc.: 100.00%] [G loss: 0.089687]\n",
      "epoch:31 step:24992 [D loss: 0.004102, acc.: 100.00%] [G loss: 0.561187]\n",
      "epoch:32 step:24993 [D loss: 0.002830, acc.: 100.00%] [G loss: 0.032466]\n",
      "epoch:32 step:24994 [D loss: 0.015377, acc.: 100.00%] [G loss: 0.094426]\n",
      "epoch:32 step:24995 [D loss: 0.002050, acc.: 100.00%] [G loss: 0.004996]\n",
      "epoch:32 step:24996 [D loss: 0.005205, acc.: 100.00%] [G loss: 0.016358]\n",
      "epoch:32 step:24997 [D loss: 0.001518, acc.: 100.00%] [G loss: 0.002777]\n",
      "epoch:32 step:24998 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.018289]\n",
      "epoch:32 step:24999 [D loss: 0.003144, acc.: 100.00%] [G loss: 0.009982]\n",
      "epoch:32 step:25000 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.001907]\n",
      "epoch:32 step:25001 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.016968]\n",
      "epoch:32 step:25002 [D loss: 0.000738, acc.: 100.00%] [G loss: 0.009527]\n",
      "epoch:32 step:25003 [D loss: 0.001326, acc.: 100.00%] [G loss: 0.029103]\n",
      "epoch:32 step:25004 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:32 step:25005 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.005555]\n",
      "epoch:32 step:25006 [D loss: 0.001733, acc.: 100.00%] [G loss: 0.000807]\n",
      "epoch:32 step:25007 [D loss: 0.005824, acc.: 100.00%] [G loss: 0.009039]\n",
      "epoch:32 step:25008 [D loss: 0.001558, acc.: 100.00%] [G loss: 0.004481]\n",
      "epoch:32 step:25009 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.004256]\n",
      "epoch:32 step:25010 [D loss: 0.003280, acc.: 100.00%] [G loss: 0.007757]\n",
      "epoch:32 step:25011 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.008719]\n",
      "epoch:32 step:25012 [D loss: 0.012259, acc.: 100.00%] [G loss: 0.009457]\n",
      "epoch:32 step:25013 [D loss: 0.000887, acc.: 100.00%] [G loss: 0.061803]\n",
      "epoch:32 step:25014 [D loss: 0.001861, acc.: 100.00%] [G loss: 0.004483]\n",
      "epoch:32 step:25015 [D loss: 0.001363, acc.: 100.00%] [G loss: 0.004811]\n",
      "epoch:32 step:25016 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.016437]\n",
      "epoch:32 step:25017 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.006210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25018 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.002917]\n",
      "epoch:32 step:25019 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.001472]\n",
      "epoch:32 step:25020 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.004975]\n",
      "epoch:32 step:25021 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.004490]\n",
      "epoch:32 step:25022 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.003098]\n",
      "epoch:32 step:25023 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.002001]\n",
      "epoch:32 step:25024 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.005030]\n",
      "epoch:32 step:25025 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.002129]\n",
      "epoch:32 step:25026 [D loss: 0.000832, acc.: 100.00%] [G loss: 0.003394]\n",
      "epoch:32 step:25027 [D loss: 0.001503, acc.: 100.00%] [G loss: 0.001752]\n",
      "epoch:32 step:25028 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.002254]\n",
      "epoch:32 step:25029 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.004453]\n",
      "epoch:32 step:25030 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.003080]\n",
      "epoch:32 step:25031 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.003320]\n",
      "epoch:32 step:25032 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:32 step:25033 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.001998]\n",
      "epoch:32 step:25034 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000957]\n",
      "epoch:32 step:25035 [D loss: 0.004092, acc.: 100.00%] [G loss: 0.006106]\n",
      "epoch:32 step:25036 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.001455]\n",
      "epoch:32 step:25037 [D loss: 0.001675, acc.: 100.00%] [G loss: 0.002364]\n",
      "epoch:32 step:25038 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.001981]\n",
      "epoch:32 step:25039 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.001334]\n",
      "epoch:32 step:25040 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.002555]\n",
      "epoch:32 step:25041 [D loss: 0.003743, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:32 step:25042 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.001274]\n",
      "epoch:32 step:25043 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001882]\n",
      "epoch:32 step:25044 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.003916]\n",
      "epoch:32 step:25045 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:32 step:25046 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:32 step:25047 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.001896]\n",
      "epoch:32 step:25048 [D loss: 0.018801, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:32 step:25049 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:32 step:25050 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001201]\n",
      "epoch:32 step:25051 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:32 step:25052 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:32 step:25053 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:32 step:25054 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:32 step:25055 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:32 step:25056 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:32 step:25057 [D loss: 0.000927, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:32 step:25058 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:32 step:25059 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:32 step:25060 [D loss: 0.001869, acc.: 100.00%] [G loss: 0.004783]\n",
      "epoch:32 step:25061 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:32 step:25062 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.001978]\n",
      "epoch:32 step:25063 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:32 step:25064 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000674]\n",
      "epoch:32 step:25065 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:32 step:25066 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.002128]\n",
      "epoch:32 step:25067 [D loss: 0.001132, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:32 step:25068 [D loss: 0.001141, acc.: 100.00%] [G loss: 0.001125]\n",
      "epoch:32 step:25069 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.001767]\n",
      "epoch:32 step:25070 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:32 step:25071 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:32 step:25072 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:32 step:25073 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.001430]\n",
      "epoch:32 step:25074 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:32 step:25075 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.001254]\n",
      "epoch:32 step:25076 [D loss: 0.001204, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:32 step:25077 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:32 step:25078 [D loss: 0.000733, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:32 step:25079 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:32 step:25080 [D loss: 0.003581, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:32 step:25081 [D loss: 0.040909, acc.: 100.00%] [G loss: 0.001093]\n",
      "epoch:32 step:25082 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.004113]\n",
      "epoch:32 step:25083 [D loss: 0.002921, acc.: 100.00%] [G loss: 0.002779]\n",
      "epoch:32 step:25084 [D loss: 0.005662, acc.: 100.00%] [G loss: 0.099508]\n",
      "epoch:32 step:25085 [D loss: 0.001125, acc.: 100.00%] [G loss: 0.004258]\n",
      "epoch:32 step:25086 [D loss: 0.010653, acc.: 100.00%] [G loss: 0.004914]\n",
      "epoch:32 step:25087 [D loss: 0.011522, acc.: 100.00%] [G loss: 0.068850]\n",
      "epoch:32 step:25088 [D loss: 0.003584, acc.: 100.00%] [G loss: 0.023557]\n",
      "epoch:32 step:25089 [D loss: 0.019103, acc.: 99.22%] [G loss: 0.003286]\n",
      "epoch:32 step:25090 [D loss: 0.002668, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:32 step:25091 [D loss: 0.003530, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:32 step:25092 [D loss: 0.065130, acc.: 96.09%] [G loss: 0.011112]\n",
      "epoch:32 step:25093 [D loss: 0.028976, acc.: 97.66%] [G loss: 0.414338]\n",
      "epoch:32 step:25094 [D loss: 0.117384, acc.: 97.66%] [G loss: 0.005811]\n",
      "epoch:32 step:25095 [D loss: 0.003217, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:32 step:25096 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:32 step:25097 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.001002]\n",
      "epoch:32 step:25098 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001961]\n",
      "epoch:32 step:25099 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:32 step:25100 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:32 step:25101 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000981]\n",
      "epoch:32 step:25102 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:32 step:25103 [D loss: 0.002609, acc.: 100.00%] [G loss: 0.000455]\n",
      "epoch:32 step:25104 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000384]\n",
      "epoch:32 step:25105 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:32 step:25106 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001855]\n",
      "epoch:32 step:25107 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:32 step:25108 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:32 step:25109 [D loss: 0.009488, acc.: 99.22%] [G loss: 0.001665]\n",
      "epoch:32 step:25110 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.001772]\n",
      "epoch:32 step:25111 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:32 step:25112 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.019640]\n",
      "epoch:32 step:25113 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.001137]\n",
      "epoch:32 step:25114 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.024490]\n",
      "epoch:32 step:25115 [D loss: 0.004172, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:32 step:25116 [D loss: 0.002078, acc.: 100.00%] [G loss: 0.001127]\n",
      "epoch:32 step:25117 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:32 step:25118 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:32 step:25119 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:32 step:25120 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:32 step:25121 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:32 step:25122 [D loss: 0.002462, acc.: 100.00%] [G loss: 0.001472]\n",
      "epoch:32 step:25123 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001435]\n",
      "epoch:32 step:25124 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:32 step:25125 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:32 step:25126 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:32 step:25127 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:32 step:25128 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.006186]\n",
      "epoch:32 step:25129 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.001130]\n",
      "epoch:32 step:25130 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.002149]\n",
      "epoch:32 step:25131 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:32 step:25132 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25133 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001710]\n",
      "epoch:32 step:25134 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:32 step:25135 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:32 step:25136 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:32 step:25137 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:32 step:25138 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:32 step:25139 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000572]\n",
      "epoch:32 step:25140 [D loss: 0.007609, acc.: 100.00%] [G loss: 0.002416]\n",
      "epoch:32 step:25141 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:32 step:25142 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:32 step:25143 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:32 step:25144 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.011013]\n",
      "epoch:32 step:25145 [D loss: 0.004326, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:32 step:25146 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:32 step:25147 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000489]\n",
      "epoch:32 step:25148 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:32 step:25149 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:32 step:25150 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:32 step:25151 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:32 step:25152 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:32 step:25153 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:32 step:25154 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:32 step:25155 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:32 step:25156 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:32 step:25157 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:32 step:25158 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:32 step:25159 [D loss: 0.037805, acc.: 100.00%] [G loss: 0.000924]\n",
      "epoch:32 step:25160 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.004364]\n",
      "epoch:32 step:25161 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.002797]\n",
      "epoch:32 step:25162 [D loss: 0.002862, acc.: 100.00%] [G loss: 0.004048]\n",
      "epoch:32 step:25163 [D loss: 0.016791, acc.: 100.00%] [G loss: 0.003401]\n",
      "epoch:32 step:25164 [D loss: 0.115959, acc.: 95.31%] [G loss: 0.418518]\n",
      "epoch:32 step:25165 [D loss: 0.003716, acc.: 100.00%] [G loss: 1.477412]\n",
      "epoch:32 step:25166 [D loss: 1.786299, acc.: 37.50%] [G loss: 2.856195]\n",
      "epoch:32 step:25167 [D loss: 0.102692, acc.: 96.09%] [G loss: 2.461017]\n",
      "epoch:32 step:25168 [D loss: 0.699822, acc.: 67.19%] [G loss: 0.199546]\n",
      "epoch:32 step:25169 [D loss: 0.054641, acc.: 97.66%] [G loss: 0.231715]\n",
      "epoch:32 step:25170 [D loss: 0.073642, acc.: 97.66%] [G loss: 0.027066]\n",
      "epoch:32 step:25171 [D loss: 0.006965, acc.: 100.00%] [G loss: 0.041973]\n",
      "epoch:32 step:25172 [D loss: 0.002821, acc.: 100.00%] [G loss: 0.058317]\n",
      "epoch:32 step:25173 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.054875]\n",
      "epoch:32 step:25174 [D loss: 0.003490, acc.: 100.00%] [G loss: 0.052610]\n",
      "epoch:32 step:25175 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.038723]\n",
      "epoch:32 step:25176 [D loss: 0.010834, acc.: 100.00%] [G loss: 0.080840]\n",
      "epoch:32 step:25177 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.025641]\n",
      "epoch:32 step:25178 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.017331]\n",
      "epoch:32 step:25179 [D loss: 0.002371, acc.: 100.00%] [G loss: 0.020568]\n",
      "epoch:32 step:25180 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.022865]\n",
      "epoch:32 step:25181 [D loss: 0.001802, acc.: 100.00%] [G loss: 0.018226]\n",
      "epoch:32 step:25182 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.006774]\n",
      "epoch:32 step:25183 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.008183]\n",
      "epoch:32 step:25184 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.011590]\n",
      "epoch:32 step:25185 [D loss: 0.003636, acc.: 100.00%] [G loss: 0.007444]\n",
      "epoch:32 step:25186 [D loss: 0.007206, acc.: 100.00%] [G loss: 0.013525]\n",
      "epoch:32 step:25187 [D loss: 0.011844, acc.: 100.00%] [G loss: 0.011594]\n",
      "epoch:32 step:25188 [D loss: 0.020932, acc.: 100.00%] [G loss: 0.053400]\n",
      "epoch:32 step:25189 [D loss: 0.027645, acc.: 100.00%] [G loss: 0.014256]\n",
      "epoch:32 step:25190 [D loss: 0.016962, acc.: 100.00%] [G loss: 0.044513]\n",
      "epoch:32 step:25191 [D loss: 0.005428, acc.: 100.00%] [G loss: 0.037053]\n",
      "epoch:32 step:25192 [D loss: 0.002384, acc.: 100.00%] [G loss: 0.055969]\n",
      "epoch:32 step:25193 [D loss: 0.002120, acc.: 100.00%] [G loss: 0.024679]\n",
      "epoch:32 step:25194 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.001139]\n",
      "epoch:32 step:25195 [D loss: 0.001282, acc.: 100.00%] [G loss: 0.259709]\n",
      "epoch:32 step:25196 [D loss: 0.001599, acc.: 100.00%] [G loss: 0.026189]\n",
      "epoch:32 step:25197 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.493758]\n",
      "epoch:32 step:25198 [D loss: 0.003351, acc.: 100.00%] [G loss: 0.013856]\n",
      "epoch:32 step:25199 [D loss: 0.021028, acc.: 100.00%] [G loss: 0.002439]\n",
      "epoch:32 step:25200 [D loss: 0.061688, acc.: 98.44%] [G loss: 0.029754]\n",
      "epoch:32 step:25201 [D loss: 0.075692, acc.: 97.66%] [G loss: 0.006497]\n",
      "epoch:32 step:25202 [D loss: 0.008690, acc.: 100.00%] [G loss: 0.004937]\n",
      "epoch:32 step:25203 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.008817]\n",
      "epoch:32 step:25204 [D loss: 0.000776, acc.: 100.00%] [G loss: 2.372909]\n",
      "epoch:32 step:25205 [D loss: 0.009437, acc.: 100.00%] [G loss: 0.009934]\n",
      "epoch:32 step:25206 [D loss: 0.325078, acc.: 86.72%] [G loss: 0.381982]\n",
      "epoch:32 step:25207 [D loss: 0.213255, acc.: 89.06%] [G loss: 0.602289]\n",
      "epoch:32 step:25208 [D loss: 0.323322, acc.: 86.72%] [G loss: 0.030644]\n",
      "epoch:32 step:25209 [D loss: 0.268211, acc.: 89.84%] [G loss: 0.125603]\n",
      "epoch:32 step:25210 [D loss: 0.002443, acc.: 100.00%] [G loss: 0.439469]\n",
      "epoch:32 step:25211 [D loss: 0.049094, acc.: 97.66%] [G loss: 0.189825]\n",
      "epoch:32 step:25212 [D loss: 0.012960, acc.: 100.00%] [G loss: 0.041713]\n",
      "epoch:32 step:25213 [D loss: 0.005715, acc.: 100.00%] [G loss: 0.082206]\n",
      "epoch:32 step:25214 [D loss: 0.003910, acc.: 100.00%] [G loss: 0.015983]\n",
      "epoch:32 step:25215 [D loss: 0.005763, acc.: 100.00%] [G loss: 0.022944]\n",
      "epoch:32 step:25216 [D loss: 0.004904, acc.: 100.00%] [G loss: 0.011597]\n",
      "epoch:32 step:25217 [D loss: 0.010473, acc.: 100.00%] [G loss: 0.023450]\n",
      "epoch:32 step:25218 [D loss: 0.010642, acc.: 100.00%] [G loss: 0.014173]\n",
      "epoch:32 step:25219 [D loss: 0.003549, acc.: 100.00%] [G loss: 0.016523]\n",
      "epoch:32 step:25220 [D loss: 0.006249, acc.: 100.00%] [G loss: 0.020002]\n",
      "epoch:32 step:25221 [D loss: 0.075739, acc.: 97.66%] [G loss: 0.022251]\n",
      "epoch:32 step:25222 [D loss: 0.092943, acc.: 97.66%] [G loss: 0.171869]\n",
      "epoch:32 step:25223 [D loss: 0.014816, acc.: 100.00%] [G loss: 0.021149]\n",
      "epoch:32 step:25224 [D loss: 0.007782, acc.: 100.00%] [G loss: 0.066590]\n",
      "epoch:32 step:25225 [D loss: 0.004686, acc.: 100.00%] [G loss: 0.619826]\n",
      "epoch:32 step:25226 [D loss: 0.412366, acc.: 79.69%] [G loss: 3.620239]\n",
      "epoch:32 step:25227 [D loss: 0.195283, acc.: 93.75%] [G loss: 0.172702]\n",
      "epoch:32 step:25228 [D loss: 0.552434, acc.: 78.12%] [G loss: 0.004594]\n",
      "epoch:32 step:25229 [D loss: 0.093879, acc.: 96.88%] [G loss: 2.804108]\n",
      "epoch:32 step:25230 [D loss: 0.019661, acc.: 100.00%] [G loss: 0.074247]\n",
      "epoch:32 step:25231 [D loss: 0.010417, acc.: 100.00%] [G loss: 1.832144]\n",
      "epoch:32 step:25232 [D loss: 0.035712, acc.: 100.00%] [G loss: 1.537104]\n",
      "epoch:32 step:25233 [D loss: 0.104134, acc.: 96.88%] [G loss: 0.283240]\n",
      "epoch:32 step:25234 [D loss: 0.050619, acc.: 97.66%] [G loss: 0.047827]\n",
      "epoch:32 step:25235 [D loss: 0.023270, acc.: 99.22%] [G loss: 0.022979]\n",
      "epoch:32 step:25236 [D loss: 0.005637, acc.: 100.00%] [G loss: 0.010677]\n",
      "epoch:32 step:25237 [D loss: 0.011077, acc.: 100.00%] [G loss: 1.099554]\n",
      "epoch:32 step:25238 [D loss: 0.033357, acc.: 99.22%] [G loss: 4.158617]\n",
      "epoch:32 step:25239 [D loss: 0.021552, acc.: 100.00%] [G loss: 0.001620]\n",
      "epoch:32 step:25240 [D loss: 0.004978, acc.: 100.00%] [G loss: 0.002537]\n",
      "epoch:32 step:25241 [D loss: 0.037866, acc.: 99.22%] [G loss: 1.083094]\n",
      "epoch:32 step:25242 [D loss: 0.042053, acc.: 98.44%] [G loss: 0.330136]\n",
      "epoch:32 step:25243 [D loss: 0.038731, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:32 step:25244 [D loss: 0.016720, acc.: 99.22%] [G loss: 0.000527]\n",
      "epoch:32 step:25245 [D loss: 0.001935, acc.: 100.00%] [G loss: 0.001825]\n",
      "epoch:32 step:25246 [D loss: 0.002425, acc.: 100.00%] [G loss: 0.003818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25247 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.006332]\n",
      "epoch:32 step:25248 [D loss: 0.019636, acc.: 99.22%] [G loss: 0.000678]\n",
      "epoch:32 step:25249 [D loss: 0.000935, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:32 step:25250 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.001210]\n",
      "epoch:32 step:25251 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.250527]\n",
      "epoch:32 step:25252 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:32 step:25253 [D loss: 0.023105, acc.: 100.00%] [G loss: 0.000850]\n",
      "epoch:32 step:25254 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.020843]\n",
      "epoch:32 step:25255 [D loss: 0.001006, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:32 step:25256 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.010093]\n",
      "epoch:32 step:25257 [D loss: 0.001196, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:32 step:25258 [D loss: 0.001377, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:32 step:25259 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.110686]\n",
      "epoch:32 step:25260 [D loss: 0.002454, acc.: 100.00%] [G loss: 0.012719]\n",
      "epoch:32 step:25261 [D loss: 0.003370, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:32 step:25262 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:32 step:25263 [D loss: 0.001225, acc.: 100.00%] [G loss: 0.000739]\n",
      "epoch:32 step:25264 [D loss: 0.002248, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:32 step:25265 [D loss: 0.001102, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:32 step:25266 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:32 step:25267 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:32 step:25268 [D loss: 0.000719, acc.: 100.00%] [G loss: 0.003152]\n",
      "epoch:32 step:25269 [D loss: 0.002565, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:32 step:25270 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.036395]\n",
      "epoch:32 step:25271 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:32 step:25272 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:32 step:25273 [D loss: 0.003166, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:32 step:25274 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:32 step:25275 [D loss: 0.003343, acc.: 100.00%] [G loss: 0.005424]\n",
      "epoch:32 step:25276 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:32 step:25277 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:32 step:25278 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:32 step:25279 [D loss: 0.003915, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:32 step:25280 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:32 step:25281 [D loss: 0.003671, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:32 step:25282 [D loss: 0.006110, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:32 step:25283 [D loss: 0.018476, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:32 step:25284 [D loss: 0.019388, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:32 step:25285 [D loss: 0.122673, acc.: 96.09%] [G loss: 0.340183]\n",
      "epoch:32 step:25286 [D loss: 0.026024, acc.: 99.22%] [G loss: 0.552484]\n",
      "epoch:32 step:25287 [D loss: 0.185189, acc.: 92.19%] [G loss: 0.000187]\n",
      "epoch:32 step:25288 [D loss: 0.055294, acc.: 98.44%] [G loss: 0.277958]\n",
      "epoch:32 step:25289 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.040589]\n",
      "epoch:32 step:25290 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.151364]\n",
      "epoch:32 step:25291 [D loss: 0.011322, acc.: 100.00%] [G loss: 0.014554]\n",
      "epoch:32 step:25292 [D loss: 0.104979, acc.: 96.88%] [G loss: 0.001692]\n",
      "epoch:32 step:25293 [D loss: 0.140241, acc.: 99.22%] [G loss: 0.002344]\n",
      "epoch:32 step:25294 [D loss: 0.001360, acc.: 100.00%] [G loss: 0.005294]\n",
      "epoch:32 step:25295 [D loss: 0.002316, acc.: 100.00%] [G loss: 0.001561]\n",
      "epoch:32 step:25296 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.001489]\n",
      "epoch:32 step:25297 [D loss: 0.000443, acc.: 100.00%] [G loss: 0.004128]\n",
      "epoch:32 step:25298 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.002012]\n",
      "epoch:32 step:25299 [D loss: 0.000827, acc.: 100.00%] [G loss: 0.002124]\n",
      "epoch:32 step:25300 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.004819]\n",
      "epoch:32 step:25301 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:32 step:25302 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.001825]\n",
      "epoch:32 step:25303 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.001575]\n",
      "epoch:32 step:25304 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:32 step:25305 [D loss: 0.000833, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:32 step:25306 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.001986]\n",
      "epoch:32 step:25307 [D loss: 0.017881, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:32 step:25308 [D loss: 0.002834, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:32 step:25309 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:32 step:25310 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:32 step:25311 [D loss: 0.004003, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:32 step:25312 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.009660]\n",
      "epoch:32 step:25313 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.000712]\n",
      "epoch:32 step:25314 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:32 step:25315 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:32 step:25316 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:32 step:25317 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000840]\n",
      "epoch:32 step:25318 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.003952]\n",
      "epoch:32 step:25319 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:32 step:25320 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:32 step:25321 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:32 step:25322 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:32 step:25323 [D loss: 0.002508, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:32 step:25324 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:32 step:25325 [D loss: 0.005414, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:32 step:25326 [D loss: 0.002794, acc.: 100.00%] [G loss: 0.000442]\n",
      "epoch:32 step:25327 [D loss: 0.006068, acc.: 100.00%] [G loss: 0.007828]\n",
      "epoch:32 step:25328 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.005205]\n",
      "epoch:32 step:25329 [D loss: 0.004329, acc.: 100.00%] [G loss: 0.016977]\n",
      "epoch:32 step:25330 [D loss: 0.005138, acc.: 100.00%] [G loss: 0.000701]\n",
      "epoch:32 step:25331 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.007049]\n",
      "epoch:32 step:25332 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:32 step:25333 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.011555]\n",
      "epoch:32 step:25334 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.005841]\n",
      "epoch:32 step:25335 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.001021]\n",
      "epoch:32 step:25336 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.014899]\n",
      "epoch:32 step:25337 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.012101]\n",
      "epoch:32 step:25338 [D loss: 0.002325, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:32 step:25339 [D loss: 0.002280, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:32 step:25340 [D loss: 0.027676, acc.: 100.00%] [G loss: 0.014786]\n",
      "epoch:32 step:25341 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.017668]\n",
      "epoch:32 step:25342 [D loss: 0.013694, acc.: 99.22%] [G loss: 0.039623]\n",
      "epoch:32 step:25343 [D loss: 0.027898, acc.: 100.00%] [G loss: 0.039335]\n",
      "epoch:32 step:25344 [D loss: 0.014436, acc.: 99.22%] [G loss: 0.082595]\n",
      "epoch:32 step:25345 [D loss: 0.014213, acc.: 99.22%] [G loss: 0.016258]\n",
      "epoch:32 step:25346 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.475436]\n",
      "epoch:32 step:25347 [D loss: 0.002295, acc.: 100.00%] [G loss: 0.109834]\n",
      "epoch:32 step:25348 [D loss: 0.089763, acc.: 97.66%] [G loss: 2.952718]\n",
      "epoch:32 step:25349 [D loss: 0.002863, acc.: 100.00%] [G loss: 1.064301]\n",
      "epoch:32 step:25350 [D loss: 0.075586, acc.: 98.44%] [G loss: 0.801495]\n",
      "epoch:32 step:25351 [D loss: 0.042335, acc.: 98.44%] [G loss: 0.884027]\n",
      "epoch:32 step:25352 [D loss: 0.020333, acc.: 100.00%] [G loss: 0.791124]\n",
      "epoch:32 step:25353 [D loss: 0.266080, acc.: 87.50%] [G loss: 1.466647]\n",
      "epoch:32 step:25354 [D loss: 0.546724, acc.: 79.69%] [G loss: 0.000001]\n",
      "epoch:32 step:25355 [D loss: 0.000778, acc.: 100.00%] [G loss: 3.754886]\n",
      "epoch:32 step:25356 [D loss: 0.007321, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25357 [D loss: 0.002085, acc.: 100.00%] [G loss: 2.183532]\n",
      "epoch:32 step:25358 [D loss: 0.001703, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25359 [D loss: 0.002143, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25360 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25361 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25362 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25363 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25364 [D loss: 0.000782, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:32 step:25365 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25366 [D loss: 0.000816, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25367 [D loss: 0.002357, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25368 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25369 [D loss: 0.022789, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25370 [D loss: 0.015997, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25371 [D loss: 0.001627, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:32 step:25372 [D loss: 0.100613, acc.: 95.31%] [G loss: 0.014195]\n",
      "epoch:32 step:25373 [D loss: 0.039104, acc.: 98.44%] [G loss: 0.402949]\n",
      "epoch:32 step:25374 [D loss: 0.018268, acc.: 99.22%] [G loss: 0.043250]\n",
      "epoch:32 step:25375 [D loss: 0.024062, acc.: 99.22%] [G loss: 1.256523]\n",
      "epoch:32 step:25376 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.250986]\n",
      "epoch:32 step:25377 [D loss: 0.002041, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:32 step:25378 [D loss: 0.003096, acc.: 100.00%] [G loss: 0.372332]\n",
      "epoch:32 step:25379 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:32 step:25380 [D loss: 0.558791, acc.: 78.91%] [G loss: 0.000000]\n",
      "epoch:32 step:25381 [D loss: 0.114148, acc.: 94.53%] [G loss: 0.000001]\n",
      "epoch:32 step:25382 [D loss: 0.008159, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25383 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25384 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25385 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.024441]\n",
      "epoch:32 step:25386 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25387 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25388 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25389 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.007546]\n",
      "epoch:32 step:25390 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:32 step:25391 [D loss: 0.009986, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25392 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25393 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25394 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25395 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25396 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:32 step:25397 [D loss: 0.000770, acc.: 100.00%] [G loss: 0.001204]\n",
      "epoch:32 step:25398 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.002449]\n",
      "epoch:32 step:25399 [D loss: 0.283993, acc.: 82.03%] [G loss: 0.030989]\n",
      "epoch:32 step:25400 [D loss: 0.000066, acc.: 100.00%] [G loss: 1.024776]\n",
      "epoch:32 step:25401 [D loss: 0.202298, acc.: 92.97%] [G loss: 5.487144]\n",
      "epoch:32 step:25402 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.001589]\n",
      "epoch:32 step:25403 [D loss: 0.308764, acc.: 87.50%] [G loss: 0.000009]\n",
      "epoch:32 step:25404 [D loss: 0.282842, acc.: 94.53%] [G loss: 0.000006]\n",
      "epoch:32 step:25405 [D loss: 0.005446, acc.: 100.00%] [G loss: 0.005052]\n",
      "epoch:32 step:25406 [D loss: 0.074127, acc.: 96.88%] [G loss: 0.173566]\n",
      "epoch:32 step:25407 [D loss: 0.034757, acc.: 99.22%] [G loss: 0.000019]\n",
      "epoch:32 step:25408 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:32 step:25409 [D loss: 0.140414, acc.: 98.44%] [G loss: 0.001338]\n",
      "epoch:32 step:25410 [D loss: 0.003059, acc.: 100.00%] [G loss: 0.431714]\n",
      "epoch:32 step:25411 [D loss: 0.004776, acc.: 100.00%] [G loss: 3.890080]\n",
      "epoch:32 step:25412 [D loss: 0.028888, acc.: 98.44%] [G loss: 0.113257]\n",
      "epoch:32 step:25413 [D loss: 0.002135, acc.: 100.00%] [G loss: 0.018030]\n",
      "epoch:32 step:25414 [D loss: 0.000058, acc.: 100.00%] [G loss: 1.234923]\n",
      "epoch:32 step:25415 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.005985]\n",
      "epoch:32 step:25416 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:32 step:25417 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:32 step:25418 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.361196]\n",
      "epoch:32 step:25419 [D loss: 0.000016, acc.: 100.00%] [G loss: 2.183873]\n",
      "epoch:32 step:25420 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:32 step:25421 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.064454]\n",
      "epoch:32 step:25422 [D loss: 0.068758, acc.: 97.66%] [G loss: 0.003487]\n",
      "epoch:32 step:25423 [D loss: 0.001951, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:32 step:25424 [D loss: 0.091339, acc.: 96.88%] [G loss: 0.003696]\n",
      "epoch:32 step:25425 [D loss: 0.000006, acc.: 100.00%] [G loss: 1.223824]\n",
      "epoch:32 step:25426 [D loss: 0.000889, acc.: 100.00%] [G loss: 0.098079]\n",
      "epoch:32 step:25427 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.129889]\n",
      "epoch:32 step:25428 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.005643]\n",
      "epoch:32 step:25429 [D loss: 0.002478, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:32 step:25430 [D loss: 0.009895, acc.: 100.00%] [G loss: 0.001149]\n",
      "epoch:32 step:25431 [D loss: 0.002009, acc.: 100.00%] [G loss: 0.003379]\n",
      "epoch:32 step:25432 [D loss: 0.024844, acc.: 100.00%] [G loss: 0.017105]\n",
      "epoch:32 step:25433 [D loss: 0.002957, acc.: 100.00%] [G loss: 1.579714]\n",
      "epoch:32 step:25434 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.508301]\n",
      "epoch:32 step:25435 [D loss: 0.000927, acc.: 100.00%] [G loss: 0.007861]\n",
      "epoch:32 step:25436 [D loss: 0.069825, acc.: 98.44%] [G loss: 0.777417]\n",
      "epoch:32 step:25437 [D loss: 0.001939, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:32 step:25438 [D loss: 0.544441, acc.: 84.38%] [G loss: 6.001056]\n",
      "epoch:32 step:25439 [D loss: 1.406301, acc.: 54.69%] [G loss: 0.000072]\n",
      "epoch:32 step:25440 [D loss: 0.511192, acc.: 81.25%] [G loss: 4.254612]\n",
      "epoch:32 step:25441 [D loss: 0.000734, acc.: 100.00%] [G loss: 1.323295]\n",
      "epoch:32 step:25442 [D loss: 0.139234, acc.: 93.75%] [G loss: 0.203462]\n",
      "epoch:32 step:25443 [D loss: 0.043780, acc.: 99.22%] [G loss: 0.001744]\n",
      "epoch:32 step:25444 [D loss: 0.001290, acc.: 100.00%] [G loss: 0.003353]\n",
      "epoch:32 step:25445 [D loss: 0.021723, acc.: 100.00%] [G loss: 0.000688]\n",
      "epoch:32 step:25446 [D loss: 0.019858, acc.: 98.44%] [G loss: 0.002012]\n",
      "epoch:32 step:25447 [D loss: 0.001060, acc.: 100.00%] [G loss: 3.144269]\n",
      "epoch:32 step:25448 [D loss: 0.003766, acc.: 100.00%] [G loss: 0.011908]\n",
      "epoch:32 step:25449 [D loss: 0.010939, acc.: 99.22%] [G loss: 0.014909]\n",
      "epoch:32 step:25450 [D loss: 0.064904, acc.: 96.09%] [G loss: 0.006863]\n",
      "epoch:32 step:25451 [D loss: 0.066178, acc.: 98.44%] [G loss: 0.080201]\n",
      "epoch:32 step:25452 [D loss: 0.035056, acc.: 99.22%] [G loss: 3.698611]\n",
      "epoch:32 step:25453 [D loss: 0.020746, acc.: 99.22%] [G loss: 1.879894]\n",
      "epoch:32 step:25454 [D loss: 0.012569, acc.: 100.00%] [G loss: 0.012323]\n",
      "epoch:32 step:25455 [D loss: 0.030452, acc.: 100.00%] [G loss: 0.009661]\n",
      "epoch:32 step:25456 [D loss: 0.041369, acc.: 100.00%] [G loss: 0.003352]\n",
      "epoch:32 step:25457 [D loss: 0.016390, acc.: 100.00%] [G loss: 0.609614]\n",
      "epoch:32 step:25458 [D loss: 0.002239, acc.: 100.00%] [G loss: 0.087220]\n",
      "epoch:32 step:25459 [D loss: 0.005100, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:32 step:25460 [D loss: 0.002165, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:32 step:25461 [D loss: 0.002060, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:32 step:25462 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000932]\n",
      "epoch:32 step:25463 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.002840]\n",
      "epoch:32 step:25464 [D loss: 0.003885, acc.: 100.00%] [G loss: 0.004405]\n",
      "epoch:32 step:25465 [D loss: 0.017007, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:32 step:25466 [D loss: 0.001927, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:32 step:25467 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:32 step:25468 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.002547]\n",
      "epoch:32 step:25469 [D loss: 0.001261, acc.: 100.00%] [G loss: 0.001764]\n",
      "epoch:32 step:25470 [D loss: 0.005343, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:32 step:25471 [D loss: 0.075424, acc.: 97.66%] [G loss: 0.018401]\n",
      "epoch:32 step:25472 [D loss: 0.002721, acc.: 100.00%] [G loss: 0.112261]\n",
      "epoch:32 step:25473 [D loss: 0.085615, acc.: 96.88%] [G loss: 0.000457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25474 [D loss: 0.027131, acc.: 99.22%] [G loss: 0.000067]\n",
      "epoch:32 step:25475 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.072328]\n",
      "epoch:32 step:25476 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:32 step:25477 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.067256]\n",
      "epoch:32 step:25478 [D loss: 0.001903, acc.: 100.00%] [G loss: 0.008133]\n",
      "epoch:32 step:25479 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:32 step:25480 [D loss: 0.000835, acc.: 100.00%] [G loss: 0.009658]\n",
      "epoch:32 step:25481 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.014594]\n",
      "epoch:32 step:25482 [D loss: 0.007046, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:32 step:25483 [D loss: 0.003926, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:32 step:25484 [D loss: 0.001281, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:32 step:25485 [D loss: 0.000634, acc.: 100.00%] [G loss: 0.031998]\n",
      "epoch:32 step:25486 [D loss: 0.002044, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:32 step:25487 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:32 step:25488 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:32 step:25489 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:32 step:25490 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:32 step:25491 [D loss: 0.025855, acc.: 98.44%] [G loss: 0.003156]\n",
      "epoch:32 step:25492 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:32 step:25493 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003469]\n",
      "epoch:32 step:25494 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.097402]\n",
      "epoch:32 step:25495 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:32 step:25496 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:32 step:25497 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:32 step:25498 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.042365]\n",
      "epoch:32 step:25499 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:32 step:25500 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:32 step:25501 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:32 step:25502 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:32 step:25503 [D loss: 0.000886, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:32 step:25504 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:32 step:25505 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:32 step:25506 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:32 step:25507 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.001066]\n",
      "epoch:32 step:25508 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000752]\n",
      "epoch:32 step:25509 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001811]\n",
      "epoch:32 step:25510 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:32 step:25511 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:32 step:25512 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001401]\n",
      "epoch:32 step:25513 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:32 step:25514 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:32 step:25515 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:32 step:25516 [D loss: 0.015109, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:32 step:25517 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:32 step:25518 [D loss: 0.058541, acc.: 96.88%] [G loss: 0.000001]\n",
      "epoch:32 step:25519 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25520 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25521 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001008]\n",
      "epoch:32 step:25522 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25523 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25524 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25525 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25526 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:32 step:25527 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25528 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25529 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:32 step:25530 [D loss: 0.001316, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25531 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:32 step:25532 [D loss: 0.002343, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:32 step:25533 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25534 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:32 step:25535 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25536 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25537 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:32 step:25538 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25539 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25540 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25541 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25542 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.002157]\n",
      "epoch:32 step:25543 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:32 step:25544 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:32 step:25545 [D loss: 0.014940, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25546 [D loss: 0.003030, acc.: 100.00%] [G loss: 0.018325]\n",
      "epoch:32 step:25547 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25548 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:32 step:25549 [D loss: 0.051820, acc.: 98.44%] [G loss: 0.183372]\n",
      "epoch:32 step:25550 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:32 step:25551 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:32 step:25552 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:32 step:25553 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:32 step:25554 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:32 step:25555 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:32 step:25556 [D loss: 0.001419, acc.: 100.00%] [G loss: 0.840538]\n",
      "epoch:32 step:25557 [D loss: 0.004357, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:32 step:25558 [D loss: 0.002117, acc.: 100.00%] [G loss: 0.111434]\n",
      "epoch:32 step:25559 [D loss: 0.002960, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:32 step:25560 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:32 step:25561 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:32 step:25562 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:32 step:25563 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:32 step:25564 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:32 step:25565 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:32 step:25566 [D loss: 0.001606, acc.: 100.00%] [G loss: 0.139930]\n",
      "epoch:32 step:25567 [D loss: 0.001945, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:32 step:25568 [D loss: 0.005313, acc.: 100.00%] [G loss: 0.001217]\n",
      "epoch:32 step:25569 [D loss: 0.010492, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:32 step:25570 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:32 step:25571 [D loss: 0.129951, acc.: 95.31%] [G loss: 0.567242]\n",
      "epoch:32 step:25572 [D loss: 0.507636, acc.: 77.34%] [G loss: 0.000349]\n",
      "epoch:32 step:25573 [D loss: 0.002330, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:32 step:25574 [D loss: 0.194960, acc.: 91.41%] [G loss: 0.100309]\n",
      "epoch:32 step:25575 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.966968]\n",
      "epoch:32 step:25576 [D loss: 0.038713, acc.: 99.22%] [G loss: 0.711848]\n",
      "epoch:32 step:25577 [D loss: 0.001248, acc.: 100.00%] [G loss: 0.370569]\n",
      "epoch:32 step:25578 [D loss: 0.016481, acc.: 98.44%] [G loss: 0.181521]\n",
      "epoch:32 step:25579 [D loss: 0.003261, acc.: 100.00%] [G loss: 0.008900]\n",
      "epoch:32 step:25580 [D loss: 0.006093, acc.: 100.00%] [G loss: 0.007306]\n",
      "epoch:32 step:25581 [D loss: 0.001455, acc.: 100.00%] [G loss: 0.016548]\n",
      "epoch:32 step:25582 [D loss: 0.002423, acc.: 100.00%] [G loss: 0.006132]\n",
      "epoch:32 step:25583 [D loss: 0.034008, acc.: 99.22%] [G loss: 0.005565]\n",
      "epoch:32 step:25584 [D loss: 0.002570, acc.: 100.00%] [G loss: 0.006874]\n",
      "epoch:32 step:25585 [D loss: 0.015720, acc.: 99.22%] [G loss: 0.008317]\n",
      "epoch:32 step:25586 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.085040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25587 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.053793]\n",
      "epoch:32 step:25588 [D loss: 0.020977, acc.: 100.00%] [G loss: 0.002703]\n",
      "epoch:32 step:25589 [D loss: 0.004582, acc.: 100.00%] [G loss: 0.007605]\n",
      "epoch:32 step:25590 [D loss: 0.001460, acc.: 100.00%] [G loss: 4.084541]\n",
      "epoch:32 step:25591 [D loss: 0.064630, acc.: 97.66%] [G loss: 0.040687]\n",
      "epoch:32 step:25592 [D loss: 0.199187, acc.: 92.19%] [G loss: 2.583110]\n",
      "epoch:32 step:25593 [D loss: 1.965683, acc.: 33.59%] [G loss: 1.977086]\n",
      "epoch:32 step:25594 [D loss: 0.004607, acc.: 100.00%] [G loss: 7.179172]\n",
      "epoch:32 step:25595 [D loss: 0.601543, acc.: 81.25%] [G loss: 0.125457]\n",
      "epoch:32 step:25596 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.014552]\n",
      "epoch:32 step:25597 [D loss: 0.068612, acc.: 98.44%] [G loss: 0.985620]\n",
      "epoch:32 step:25598 [D loss: 0.003416, acc.: 100.00%] [G loss: 0.001045]\n",
      "epoch:32 step:25599 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.619310]\n",
      "epoch:32 step:25600 [D loss: 0.000824, acc.: 100.00%] [G loss: 0.000576]\n",
      "epoch:32 step:25601 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.016489]\n",
      "epoch:32 step:25602 [D loss: 0.014888, acc.: 100.00%] [G loss: 0.026816]\n",
      "epoch:32 step:25603 [D loss: 0.002637, acc.: 100.00%] [G loss: 0.048319]\n",
      "epoch:32 step:25604 [D loss: 0.017823, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:32 step:25605 [D loss: 0.002396, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:32 step:25606 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.002552]\n",
      "epoch:32 step:25607 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000512]\n",
      "epoch:32 step:25608 [D loss: 0.001774, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:32 step:25609 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.005501]\n",
      "epoch:32 step:25610 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:32 step:25611 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.016787]\n",
      "epoch:32 step:25612 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:32 step:25613 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000949]\n",
      "epoch:32 step:25614 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:32 step:25615 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.015255]\n",
      "epoch:32 step:25616 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:32 step:25617 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:32 step:25618 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.001430]\n",
      "epoch:32 step:25619 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000482]\n",
      "epoch:32 step:25620 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:32 step:25621 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.005841]\n",
      "epoch:32 step:25622 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.007536]\n",
      "epoch:32 step:25623 [D loss: 0.010524, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:32 step:25624 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.021467]\n",
      "epoch:32 step:25625 [D loss: 0.000863, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:32 step:25626 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.001980]\n",
      "epoch:32 step:25627 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:32 step:25628 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000391]\n",
      "epoch:32 step:25629 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:32 step:25630 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000694]\n",
      "epoch:32 step:25631 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.048796]\n",
      "epoch:32 step:25632 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000958]\n",
      "epoch:32 step:25633 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.046099]\n",
      "epoch:32 step:25634 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:32 step:25635 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:32 step:25636 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:32 step:25637 [D loss: 0.001540, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:32 step:25638 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.154624]\n",
      "epoch:32 step:25639 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:32 step:25640 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:32 step:25641 [D loss: 0.004378, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:32 step:25642 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:32 step:25643 [D loss: 0.002654, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:32 step:25644 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.001803]\n",
      "epoch:32 step:25645 [D loss: 0.035779, acc.: 99.22%] [G loss: 0.002133]\n",
      "epoch:32 step:25646 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.042683]\n",
      "epoch:32 step:25647 [D loss: 0.000426, acc.: 100.00%] [G loss: 1.462369]\n",
      "epoch:32 step:25648 [D loss: 0.458023, acc.: 81.25%] [G loss: 5.810235]\n",
      "epoch:32 step:25649 [D loss: 0.489646, acc.: 77.34%] [G loss: 2.452003]\n",
      "epoch:32 step:25650 [D loss: 0.077154, acc.: 96.88%] [G loss: 0.008154]\n",
      "epoch:32 step:25651 [D loss: 0.031136, acc.: 100.00%] [G loss: 2.057253]\n",
      "epoch:32 step:25652 [D loss: 0.191009, acc.: 93.75%] [G loss: 0.071637]\n",
      "epoch:32 step:25653 [D loss: 0.413660, acc.: 79.69%] [G loss: 1.623769]\n",
      "epoch:32 step:25654 [D loss: 0.247409, acc.: 89.06%] [G loss: 0.000999]\n",
      "epoch:32 step:25655 [D loss: 0.077857, acc.: 96.88%] [G loss: 7.147449]\n",
      "epoch:32 step:25656 [D loss: 0.127247, acc.: 96.88%] [G loss: 0.000062]\n",
      "epoch:32 step:25657 [D loss: 0.048402, acc.: 98.44%] [G loss: 2.937880]\n",
      "epoch:32 step:25658 [D loss: 0.054904, acc.: 98.44%] [G loss: 0.015697]\n",
      "epoch:32 step:25659 [D loss: 0.008070, acc.: 100.00%] [G loss: 1.317304]\n",
      "epoch:32 step:25660 [D loss: 0.038053, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:32 step:25661 [D loss: 0.001497, acc.: 100.00%] [G loss: 0.770039]\n",
      "epoch:32 step:25662 [D loss: 0.160733, acc.: 96.09%] [G loss: 0.799869]\n",
      "epoch:32 step:25663 [D loss: 0.025803, acc.: 100.00%] [G loss: 0.001629]\n",
      "epoch:32 step:25664 [D loss: 0.016523, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:32 step:25665 [D loss: 0.039434, acc.: 99.22%] [G loss: 0.002280]\n",
      "epoch:32 step:25666 [D loss: 0.118270, acc.: 96.09%] [G loss: 1.554935]\n",
      "epoch:32 step:25667 [D loss: 0.037696, acc.: 100.00%] [G loss: 2.556439]\n",
      "epoch:32 step:25668 [D loss: 4.185620, acc.: 28.91%] [G loss: 3.902499]\n",
      "epoch:32 step:25669 [D loss: 1.652740, acc.: 54.69%] [G loss: 5.996481]\n",
      "epoch:32 step:25670 [D loss: 0.680098, acc.: 72.66%] [G loss: 3.811220]\n",
      "epoch:32 step:25671 [D loss: 0.099195, acc.: 97.66%] [G loss: 0.519161]\n",
      "epoch:32 step:25672 [D loss: 0.049098, acc.: 100.00%] [G loss: 3.245167]\n",
      "epoch:32 step:25673 [D loss: 0.074266, acc.: 98.44%] [G loss: 0.380093]\n",
      "epoch:32 step:25674 [D loss: 0.025682, acc.: 100.00%] [G loss: 0.236307]\n",
      "epoch:32 step:25675 [D loss: 0.046979, acc.: 100.00%] [G loss: 0.111147]\n",
      "epoch:32 step:25676 [D loss: 0.016774, acc.: 100.00%] [G loss: 0.066851]\n",
      "epoch:32 step:25677 [D loss: 0.015561, acc.: 100.00%] [G loss: 0.054347]\n",
      "epoch:32 step:25678 [D loss: 0.018009, acc.: 100.00%] [G loss: 0.044256]\n",
      "epoch:32 step:25679 [D loss: 0.020002, acc.: 99.22%] [G loss: 0.020221]\n",
      "epoch:32 step:25680 [D loss: 0.018653, acc.: 100.00%] [G loss: 3.602544]\n",
      "epoch:32 step:25681 [D loss: 0.015227, acc.: 100.00%] [G loss: 0.012272]\n",
      "epoch:32 step:25682 [D loss: 0.016898, acc.: 100.00%] [G loss: 3.482253]\n",
      "epoch:32 step:25683 [D loss: 0.031518, acc.: 100.00%] [G loss: 0.743948]\n",
      "epoch:32 step:25684 [D loss: 0.023175, acc.: 100.00%] [G loss: 0.004115]\n",
      "epoch:32 step:25685 [D loss: 0.027680, acc.: 100.00%] [G loss: 0.003759]\n",
      "epoch:32 step:25686 [D loss: 0.009894, acc.: 100.00%] [G loss: 2.982582]\n",
      "epoch:32 step:25687 [D loss: 0.021618, acc.: 100.00%] [G loss: 2.745317]\n",
      "epoch:32 step:25688 [D loss: 0.180602, acc.: 91.41%] [G loss: 0.015718]\n",
      "epoch:32 step:25689 [D loss: 0.050259, acc.: 99.22%] [G loss: 0.072703]\n",
      "epoch:32 step:25690 [D loss: 0.036769, acc.: 98.44%] [G loss: 3.812608]\n",
      "epoch:32 step:25691 [D loss: 0.014936, acc.: 100.00%] [G loss: 2.927205]\n",
      "epoch:32 step:25692 [D loss: 0.114104, acc.: 96.88%] [G loss: 2.507985]\n",
      "epoch:32 step:25693 [D loss: 0.153555, acc.: 96.88%] [G loss: 0.035432]\n",
      "epoch:32 step:25694 [D loss: 0.255181, acc.: 89.84%] [G loss: 0.044571]\n",
      "epoch:32 step:25695 [D loss: 0.069653, acc.: 100.00%] [G loss: 0.037358]\n",
      "epoch:32 step:25696 [D loss: 0.064517, acc.: 99.22%] [G loss: 0.022382]\n",
      "epoch:32 step:25697 [D loss: 0.032624, acc.: 100.00%] [G loss: 3.591397]\n",
      "epoch:32 step:25698 [D loss: 0.133137, acc.: 98.44%] [G loss: 0.039543]\n",
      "epoch:32 step:25699 [D loss: 0.038538, acc.: 98.44%] [G loss: 5.053692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25700 [D loss: 0.115456, acc.: 94.53%] [G loss: 0.011393]\n",
      "epoch:32 step:25701 [D loss: 0.070268, acc.: 99.22%] [G loss: 1.564254]\n",
      "epoch:32 step:25702 [D loss: 0.036962, acc.: 100.00%] [G loss: 0.021720]\n",
      "epoch:32 step:25703 [D loss: 0.024510, acc.: 100.00%] [G loss: 0.025346]\n",
      "epoch:32 step:25704 [D loss: 0.076408, acc.: 98.44%] [G loss: 0.057418]\n",
      "epoch:32 step:25705 [D loss: 0.098357, acc.: 96.88%] [G loss: 1.206680]\n",
      "epoch:32 step:25706 [D loss: 0.001703, acc.: 100.00%] [G loss: 0.001683]\n",
      "epoch:32 step:25707 [D loss: 0.029969, acc.: 99.22%] [G loss: 0.186033]\n",
      "epoch:32 step:25708 [D loss: 0.201658, acc.: 92.19%] [G loss: 0.410307]\n",
      "epoch:32 step:25709 [D loss: 0.038575, acc.: 99.22%] [G loss: 0.664871]\n",
      "epoch:32 step:25710 [D loss: 0.550401, acc.: 73.44%] [G loss: 0.000987]\n",
      "epoch:32 step:25711 [D loss: 0.020963, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:32 step:25712 [D loss: 0.003425, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:32 step:25713 [D loss: 0.001593, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:32 step:25714 [D loss: 0.013924, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:32 step:25715 [D loss: 0.002516, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:32 step:25716 [D loss: 0.133264, acc.: 94.53%] [G loss: 0.009489]\n",
      "epoch:32 step:25717 [D loss: 0.002286, acc.: 100.00%] [G loss: 0.146928]\n",
      "epoch:32 step:25718 [D loss: 0.001057, acc.: 100.00%] [G loss: 0.171430]\n",
      "epoch:32 step:25719 [D loss: 0.075382, acc.: 95.31%] [G loss: 0.039238]\n",
      "epoch:32 step:25720 [D loss: 0.004133, acc.: 100.00%] [G loss: 4.224051]\n",
      "epoch:32 step:25721 [D loss: 0.510099, acc.: 72.66%] [G loss: 2.615781]\n",
      "epoch:32 step:25722 [D loss: 0.370756, acc.: 80.47%] [G loss: 1.956202]\n",
      "epoch:32 step:25723 [D loss: 0.071237, acc.: 97.66%] [G loss: 4.552024]\n",
      "epoch:32 step:25724 [D loss: 0.006631, acc.: 100.00%] [G loss: 3.445990]\n",
      "epoch:32 step:25725 [D loss: 0.012268, acc.: 100.00%] [G loss: 1.056422]\n",
      "epoch:32 step:25726 [D loss: 0.034027, acc.: 99.22%] [G loss: 2.686520]\n",
      "epoch:32 step:25727 [D loss: 0.075269, acc.: 97.66%] [G loss: 0.717823]\n",
      "epoch:32 step:25728 [D loss: 0.031527, acc.: 100.00%] [G loss: 1.762855]\n",
      "epoch:32 step:25729 [D loss: 0.032673, acc.: 100.00%] [G loss: 0.158559]\n",
      "epoch:32 step:25730 [D loss: 0.081868, acc.: 98.44%] [G loss: 0.085075]\n",
      "epoch:32 step:25731 [D loss: 0.086923, acc.: 99.22%] [G loss: 0.170260]\n",
      "epoch:32 step:25732 [D loss: 0.023866, acc.: 100.00%] [G loss: 0.144518]\n",
      "epoch:32 step:25733 [D loss: 0.052882, acc.: 99.22%] [G loss: 0.146397]\n",
      "epoch:32 step:25734 [D loss: 0.041160, acc.: 99.22%] [G loss: 0.079465]\n",
      "epoch:32 step:25735 [D loss: 0.060432, acc.: 98.44%] [G loss: 3.574790]\n",
      "epoch:32 step:25736 [D loss: 0.008262, acc.: 100.00%] [G loss: 0.821163]\n",
      "epoch:32 step:25737 [D loss: 0.013838, acc.: 100.00%] [G loss: 0.410805]\n",
      "epoch:32 step:25738 [D loss: 0.029263, acc.: 99.22%] [G loss: 0.193564]\n",
      "epoch:32 step:25739 [D loss: 0.009794, acc.: 100.00%] [G loss: 1.395525]\n",
      "epoch:32 step:25740 [D loss: 0.018562, acc.: 100.00%] [G loss: 4.012600]\n",
      "epoch:32 step:25741 [D loss: 0.019768, acc.: 100.00%] [G loss: 0.212164]\n",
      "epoch:32 step:25742 [D loss: 0.077438, acc.: 97.66%] [G loss: 0.050604]\n",
      "epoch:32 step:25743 [D loss: 0.009264, acc.: 100.00%] [G loss: 0.008516]\n",
      "epoch:32 step:25744 [D loss: 0.007059, acc.: 100.00%] [G loss: 0.032725]\n",
      "epoch:32 step:25745 [D loss: 0.012994, acc.: 100.00%] [G loss: 0.021560]\n",
      "epoch:32 step:25746 [D loss: 0.007454, acc.: 100.00%] [G loss: 0.500136]\n",
      "epoch:32 step:25747 [D loss: 0.004613, acc.: 100.00%] [G loss: 0.006411]\n",
      "epoch:32 step:25748 [D loss: 0.002050, acc.: 100.00%] [G loss: 0.003542]\n",
      "epoch:32 step:25749 [D loss: 0.003501, acc.: 100.00%] [G loss: 0.010411]\n",
      "epoch:32 step:25750 [D loss: 0.002210, acc.: 100.00%] [G loss: 0.017326]\n",
      "epoch:32 step:25751 [D loss: 0.018654, acc.: 99.22%] [G loss: 0.002855]\n",
      "epoch:32 step:25752 [D loss: 0.020057, acc.: 100.00%] [G loss: 0.021429]\n",
      "epoch:32 step:25753 [D loss: 0.003715, acc.: 100.00%] [G loss: 0.008050]\n",
      "epoch:32 step:25754 [D loss: 0.040909, acc.: 97.66%] [G loss: 0.003543]\n",
      "epoch:32 step:25755 [D loss: 0.018841, acc.: 100.00%] [G loss: 0.004046]\n",
      "epoch:32 step:25756 [D loss: 0.006339, acc.: 100.00%] [G loss: 0.094627]\n",
      "epoch:32 step:25757 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.090206]\n",
      "epoch:32 step:25758 [D loss: 0.005132, acc.: 100.00%] [G loss: 0.116609]\n",
      "epoch:32 step:25759 [D loss: 0.001563, acc.: 100.00%] [G loss: 0.043127]\n",
      "epoch:32 step:25760 [D loss: 0.002490, acc.: 100.00%] [G loss: 1.889783]\n",
      "epoch:32 step:25761 [D loss: 0.013704, acc.: 100.00%] [G loss: 0.103118]\n",
      "epoch:32 step:25762 [D loss: 0.003509, acc.: 100.00%] [G loss: 0.046189]\n",
      "epoch:32 step:25763 [D loss: 0.084570, acc.: 96.88%] [G loss: 0.009308]\n",
      "epoch:32 step:25764 [D loss: 0.150436, acc.: 94.53%] [G loss: 0.128327]\n",
      "epoch:32 step:25765 [D loss: 0.019796, acc.: 99.22%] [G loss: 0.541617]\n",
      "epoch:32 step:25766 [D loss: 0.179196, acc.: 93.75%] [G loss: 0.012351]\n",
      "epoch:32 step:25767 [D loss: 0.008617, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:32 step:25768 [D loss: 0.003011, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:32 step:25769 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.001820]\n",
      "epoch:32 step:25770 [D loss: 0.006034, acc.: 100.00%] [G loss: 1.686631]\n",
      "epoch:32 step:25771 [D loss: 0.438921, acc.: 80.47%] [G loss: 7.007959]\n",
      "epoch:32 step:25772 [D loss: 0.111444, acc.: 97.66%] [G loss: 9.211206]\n",
      "epoch:32 step:25773 [D loss: 0.600384, acc.: 79.69%] [G loss: 7.486729]\n",
      "epoch:33 step:25774 [D loss: 0.082052, acc.: 98.44%] [G loss: 6.132745]\n",
      "epoch:33 step:25775 [D loss: 0.001020, acc.: 100.00%] [G loss: 6.377057]\n",
      "epoch:33 step:25776 [D loss: 0.017075, acc.: 100.00%] [G loss: 5.205352]\n",
      "epoch:33 step:25777 [D loss: 0.003571, acc.: 100.00%] [G loss: 4.492224]\n",
      "epoch:33 step:25778 [D loss: 0.006400, acc.: 100.00%] [G loss: 4.467595]\n",
      "epoch:33 step:25779 [D loss: 0.002953, acc.: 100.00%] [G loss: 3.721129]\n",
      "epoch:33 step:25780 [D loss: 0.019770, acc.: 99.22%] [G loss: 2.829969]\n",
      "epoch:33 step:25781 [D loss: 0.002228, acc.: 100.00%] [G loss: 2.766613]\n",
      "epoch:33 step:25782 [D loss: 0.005616, acc.: 100.00%] [G loss: 1.953000]\n",
      "epoch:33 step:25783 [D loss: 0.018798, acc.: 100.00%] [G loss: 1.764642]\n",
      "epoch:33 step:25784 [D loss: 0.007769, acc.: 100.00%] [G loss: 0.429288]\n",
      "epoch:33 step:25785 [D loss: 0.034596, acc.: 100.00%] [G loss: 2.382559]\n",
      "epoch:33 step:25786 [D loss: 0.005402, acc.: 100.00%] [G loss: 1.292019]\n",
      "epoch:33 step:25787 [D loss: 0.445693, acc.: 79.69%] [G loss: 5.393589]\n",
      "epoch:33 step:25788 [D loss: 1.286146, acc.: 58.59%] [G loss: 0.746968]\n",
      "epoch:33 step:25789 [D loss: 0.002518, acc.: 100.00%] [G loss: 0.179915]\n",
      "epoch:33 step:25790 [D loss: 0.003097, acc.: 100.00%] [G loss: 0.164828]\n",
      "epoch:33 step:25791 [D loss: 0.000878, acc.: 100.00%] [G loss: 0.059439]\n",
      "epoch:33 step:25792 [D loss: 0.002036, acc.: 100.00%] [G loss: 1.663123]\n",
      "epoch:33 step:25793 [D loss: 0.003311, acc.: 100.00%] [G loss: 0.053177]\n",
      "epoch:33 step:25794 [D loss: 0.000817, acc.: 100.00%] [G loss: 2.831346]\n",
      "epoch:33 step:25795 [D loss: 0.003372, acc.: 100.00%] [G loss: 0.031606]\n",
      "epoch:33 step:25796 [D loss: 0.004026, acc.: 100.00%] [G loss: 0.025256]\n",
      "epoch:33 step:25797 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.037121]\n",
      "epoch:33 step:25798 [D loss: 0.001827, acc.: 100.00%] [G loss: 0.006096]\n",
      "epoch:33 step:25799 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.026125]\n",
      "epoch:33 step:25800 [D loss: 0.005311, acc.: 100.00%] [G loss: 0.018698]\n",
      "epoch:33 step:25801 [D loss: 0.002338, acc.: 100.00%] [G loss: 0.015923]\n",
      "epoch:33 step:25802 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.012608]\n",
      "epoch:33 step:25803 [D loss: 0.003119, acc.: 100.00%] [G loss: 0.030363]\n",
      "epoch:33 step:25804 [D loss: 0.004416, acc.: 100.00%] [G loss: 0.015808]\n",
      "epoch:33 step:25805 [D loss: 0.025250, acc.: 100.00%] [G loss: 0.057561]\n",
      "epoch:33 step:25806 [D loss: 0.005506, acc.: 100.00%] [G loss: 0.089983]\n",
      "epoch:33 step:25807 [D loss: 0.007058, acc.: 100.00%] [G loss: 0.073144]\n",
      "epoch:33 step:25808 [D loss: 0.047303, acc.: 100.00%] [G loss: 0.333583]\n",
      "epoch:33 step:25809 [D loss: 0.031092, acc.: 100.00%] [G loss: 0.414821]\n",
      "epoch:33 step:25810 [D loss: 0.003842, acc.: 100.00%] [G loss: 0.313015]\n",
      "epoch:33 step:25811 [D loss: 0.024772, acc.: 100.00%] [G loss: 0.526143]\n",
      "epoch:33 step:25812 [D loss: 0.006405, acc.: 100.00%] [G loss: 1.064549]\n",
      "epoch:33 step:25813 [D loss: 0.007448, acc.: 100.00%] [G loss: 1.177414]\n",
      "epoch:33 step:25814 [D loss: 0.005827, acc.: 100.00%] [G loss: 1.074353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25815 [D loss: 0.018028, acc.: 100.00%] [G loss: 1.272759]\n",
      "epoch:33 step:25816 [D loss: 0.017187, acc.: 100.00%] [G loss: 2.019808]\n",
      "epoch:33 step:25817 [D loss: 0.030639, acc.: 99.22%] [G loss: 1.316637]\n",
      "epoch:33 step:25818 [D loss: 0.001206, acc.: 100.00%] [G loss: 0.832119]\n",
      "epoch:33 step:25819 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.443944]\n",
      "epoch:33 step:25820 [D loss: 0.001403, acc.: 100.00%] [G loss: 0.398012]\n",
      "epoch:33 step:25821 [D loss: 0.004232, acc.: 100.00%] [G loss: 0.330246]\n",
      "epoch:33 step:25822 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.322200]\n",
      "epoch:33 step:25823 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.080592]\n",
      "epoch:33 step:25824 [D loss: 0.001366, acc.: 100.00%] [G loss: 0.178487]\n",
      "epoch:33 step:25825 [D loss: 0.001325, acc.: 100.00%] [G loss: 0.060628]\n",
      "epoch:33 step:25826 [D loss: 0.000611, acc.: 100.00%] [G loss: 1.507042]\n",
      "epoch:33 step:25827 [D loss: 0.147414, acc.: 96.09%] [G loss: 0.048987]\n",
      "epoch:33 step:25828 [D loss: 0.009470, acc.: 100.00%] [G loss: 0.516287]\n",
      "epoch:33 step:25829 [D loss: 0.145729, acc.: 96.09%] [G loss: 0.945159]\n",
      "epoch:33 step:25830 [D loss: 0.015806, acc.: 100.00%] [G loss: 3.923177]\n",
      "epoch:33 step:25831 [D loss: 0.026214, acc.: 99.22%] [G loss: 3.757939]\n",
      "epoch:33 step:25832 [D loss: 0.004145, acc.: 100.00%] [G loss: 2.632850]\n",
      "epoch:33 step:25833 [D loss: 0.006783, acc.: 100.00%] [G loss: 1.827275]\n",
      "epoch:33 step:25834 [D loss: 0.074683, acc.: 96.88%] [G loss: 0.013917]\n",
      "epoch:33 step:25835 [D loss: 0.026848, acc.: 100.00%] [G loss: 0.194146]\n",
      "epoch:33 step:25836 [D loss: 0.008436, acc.: 100.00%] [G loss: 0.390243]\n",
      "epoch:33 step:25837 [D loss: 0.001575, acc.: 100.00%] [G loss: 0.165676]\n",
      "epoch:33 step:25838 [D loss: 0.082638, acc.: 96.88%] [G loss: 0.011184]\n",
      "epoch:33 step:25839 [D loss: 0.003455, acc.: 100.00%] [G loss: 0.021275]\n",
      "epoch:33 step:25840 [D loss: 0.020455, acc.: 100.00%] [G loss: 0.005620]\n",
      "epoch:33 step:25841 [D loss: 0.000819, acc.: 100.00%] [G loss: 0.005567]\n",
      "epoch:33 step:25842 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.023914]\n",
      "epoch:33 step:25843 [D loss: 0.003338, acc.: 100.00%] [G loss: 0.010123]\n",
      "epoch:33 step:25844 [D loss: 0.072433, acc.: 98.44%] [G loss: 0.098229]\n",
      "epoch:33 step:25845 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.631856]\n",
      "epoch:33 step:25846 [D loss: 0.002412, acc.: 100.00%] [G loss: 0.275216]\n",
      "epoch:33 step:25847 [D loss: 0.006782, acc.: 100.00%] [G loss: 0.121959]\n",
      "epoch:33 step:25848 [D loss: 0.022374, acc.: 99.22%] [G loss: 2.126417]\n",
      "epoch:33 step:25849 [D loss: 0.015398, acc.: 100.00%] [G loss: 0.269763]\n",
      "epoch:33 step:25850 [D loss: 0.419845, acc.: 77.34%] [G loss: 6.250311]\n",
      "epoch:33 step:25851 [D loss: 0.516794, acc.: 75.00%] [G loss: 0.197321]\n",
      "epoch:33 step:25852 [D loss: 0.016977, acc.: 100.00%] [G loss: 1.677120]\n",
      "epoch:33 step:25853 [D loss: 0.108818, acc.: 95.31%] [G loss: 0.075692]\n",
      "epoch:33 step:25854 [D loss: 0.002046, acc.: 100.00%] [G loss: 0.099658]\n",
      "epoch:33 step:25855 [D loss: 0.005478, acc.: 100.00%] [G loss: 0.457314]\n",
      "epoch:33 step:25856 [D loss: 0.003092, acc.: 100.00%] [G loss: 0.019654]\n",
      "epoch:33 step:25857 [D loss: 0.004913, acc.: 100.00%] [G loss: 4.301074]\n",
      "epoch:33 step:25858 [D loss: 0.022196, acc.: 100.00%] [G loss: 1.698969]\n",
      "epoch:33 step:25859 [D loss: 0.024654, acc.: 99.22%] [G loss: 1.344112]\n",
      "epoch:33 step:25860 [D loss: 0.030270, acc.: 100.00%] [G loss: 0.284054]\n",
      "epoch:33 step:25861 [D loss: 0.030067, acc.: 99.22%] [G loss: 0.007076]\n",
      "epoch:33 step:25862 [D loss: 0.062192, acc.: 99.22%] [G loss: 0.028505]\n",
      "epoch:33 step:25863 [D loss: 0.103607, acc.: 96.09%] [G loss: 0.012007]\n",
      "epoch:33 step:25864 [D loss: 0.002269, acc.: 100.00%] [G loss: 0.013238]\n",
      "epoch:33 step:25865 [D loss: 0.001208, acc.: 100.00%] [G loss: 0.009573]\n",
      "epoch:33 step:25866 [D loss: 0.006002, acc.: 100.00%] [G loss: 0.010496]\n",
      "epoch:33 step:25867 [D loss: 0.005040, acc.: 100.00%] [G loss: 0.017896]\n",
      "epoch:33 step:25868 [D loss: 0.004069, acc.: 100.00%] [G loss: 0.002628]\n",
      "epoch:33 step:25869 [D loss: 0.002546, acc.: 100.00%] [G loss: 0.010757]\n",
      "epoch:33 step:25870 [D loss: 0.001466, acc.: 100.00%] [G loss: 0.008421]\n",
      "epoch:33 step:25871 [D loss: 0.011517, acc.: 99.22%] [G loss: 0.017500]\n",
      "epoch:33 step:25872 [D loss: 0.238025, acc.: 90.62%] [G loss: 3.753927]\n",
      "epoch:33 step:25873 [D loss: 0.005086, acc.: 100.00%] [G loss: 4.432594]\n",
      "epoch:33 step:25874 [D loss: 0.947776, acc.: 63.28%] [G loss: 0.000172]\n",
      "epoch:33 step:25875 [D loss: 4.284900, acc.: 50.00%] [G loss: 7.866993]\n",
      "epoch:33 step:25876 [D loss: 1.382370, acc.: 52.34%] [G loss: 4.426828]\n",
      "epoch:33 step:25877 [D loss: 0.473214, acc.: 76.56%] [G loss: 1.380011]\n",
      "epoch:33 step:25878 [D loss: 0.010023, acc.: 100.00%] [G loss: 1.002022]\n",
      "epoch:33 step:25879 [D loss: 0.007556, acc.: 100.00%] [G loss: 0.493022]\n",
      "epoch:33 step:25880 [D loss: 0.004845, acc.: 100.00%] [G loss: 0.470967]\n",
      "epoch:33 step:25881 [D loss: 0.021767, acc.: 100.00%] [G loss: 0.504135]\n",
      "epoch:33 step:25882 [D loss: 0.005246, acc.: 100.00%] [G loss: 0.227302]\n",
      "epoch:33 step:25883 [D loss: 0.018540, acc.: 99.22%] [G loss: 0.222695]\n",
      "epoch:33 step:25884 [D loss: 0.017399, acc.: 100.00%] [G loss: 0.160132]\n",
      "epoch:33 step:25885 [D loss: 0.029341, acc.: 100.00%] [G loss: 4.014905]\n",
      "epoch:33 step:25886 [D loss: 0.034293, acc.: 100.00%] [G loss: 3.651409]\n",
      "epoch:33 step:25887 [D loss: 0.032528, acc.: 100.00%] [G loss: 0.276384]\n",
      "epoch:33 step:25888 [D loss: 0.043639, acc.: 100.00%] [G loss: 3.275063]\n",
      "epoch:33 step:25889 [D loss: 0.033710, acc.: 100.00%] [G loss: 0.359563]\n",
      "epoch:33 step:25890 [D loss: 0.011903, acc.: 100.00%] [G loss: 2.939616]\n",
      "epoch:33 step:25891 [D loss: 0.102068, acc.: 96.88%] [G loss: 0.557806]\n",
      "epoch:33 step:25892 [D loss: 0.046837, acc.: 99.22%] [G loss: 0.698406]\n",
      "epoch:33 step:25893 [D loss: 0.006597, acc.: 100.00%] [G loss: 0.291613]\n",
      "epoch:33 step:25894 [D loss: 0.004795, acc.: 100.00%] [G loss: 0.212366]\n",
      "epoch:33 step:25895 [D loss: 0.004913, acc.: 100.00%] [G loss: 0.107006]\n",
      "epoch:33 step:25896 [D loss: 0.023630, acc.: 100.00%] [G loss: 0.053694]\n",
      "epoch:33 step:25897 [D loss: 0.063868, acc.: 96.88%] [G loss: 0.027585]\n",
      "epoch:33 step:25898 [D loss: 0.040957, acc.: 100.00%] [G loss: 1.363915]\n",
      "epoch:33 step:25899 [D loss: 0.032369, acc.: 100.00%] [G loss: 0.035353]\n",
      "epoch:33 step:25900 [D loss: 0.004018, acc.: 100.00%] [G loss: 0.090563]\n",
      "epoch:33 step:25901 [D loss: 0.014984, acc.: 100.00%] [G loss: 0.050592]\n",
      "epoch:33 step:25902 [D loss: 0.009917, acc.: 100.00%] [G loss: 0.051654]\n",
      "epoch:33 step:25903 [D loss: 0.004488, acc.: 100.00%] [G loss: 0.091055]\n",
      "epoch:33 step:25904 [D loss: 0.004970, acc.: 100.00%] [G loss: 0.044994]\n",
      "epoch:33 step:25905 [D loss: 0.021652, acc.: 99.22%] [G loss: 0.029048]\n",
      "epoch:33 step:25906 [D loss: 0.007089, acc.: 100.00%] [G loss: 0.106469]\n",
      "epoch:33 step:25907 [D loss: 0.013287, acc.: 100.00%] [G loss: 0.040768]\n",
      "epoch:33 step:25908 [D loss: 0.036392, acc.: 98.44%] [G loss: 0.021885]\n",
      "epoch:33 step:25909 [D loss: 0.026668, acc.: 100.00%] [G loss: 0.098198]\n",
      "epoch:33 step:25910 [D loss: 0.007575, acc.: 100.00%] [G loss: 0.142785]\n",
      "epoch:33 step:25911 [D loss: 0.027136, acc.: 99.22%] [G loss: 0.083396]\n",
      "epoch:33 step:25912 [D loss: 0.034497, acc.: 100.00%] [G loss: 0.652566]\n",
      "epoch:33 step:25913 [D loss: 0.043441, acc.: 99.22%] [G loss: 0.676181]\n",
      "epoch:33 step:25914 [D loss: 0.026424, acc.: 99.22%] [G loss: 0.614190]\n",
      "epoch:33 step:25915 [D loss: 0.046542, acc.: 99.22%] [G loss: 2.026337]\n",
      "epoch:33 step:25916 [D loss: 0.014855, acc.: 100.00%] [G loss: 2.180491]\n",
      "epoch:33 step:25917 [D loss: 0.031032, acc.: 100.00%] [G loss: 1.909883]\n",
      "epoch:33 step:25918 [D loss: 0.020505, acc.: 100.00%] [G loss: 0.734154]\n",
      "epoch:33 step:25919 [D loss: 0.001479, acc.: 100.00%] [G loss: 3.433870]\n",
      "epoch:33 step:25920 [D loss: 0.008028, acc.: 100.00%] [G loss: 3.413644]\n",
      "epoch:33 step:25921 [D loss: 0.003132, acc.: 100.00%] [G loss: 2.337311]\n",
      "epoch:33 step:25922 [D loss: 0.009976, acc.: 100.00%] [G loss: 1.897653]\n",
      "epoch:33 step:25923 [D loss: 0.004084, acc.: 100.00%] [G loss: 1.221421]\n",
      "epoch:33 step:25924 [D loss: 0.020159, acc.: 99.22%] [G loss: 5.664566]\n",
      "epoch:33 step:25925 [D loss: 0.011280, acc.: 100.00%] [G loss: 2.804550]\n",
      "epoch:33 step:25926 [D loss: 0.127289, acc.: 94.53%] [G loss: 4.989293]\n",
      "epoch:33 step:25927 [D loss: 0.005166, acc.: 100.00%] [G loss: 1.546380]\n",
      "epoch:33 step:25928 [D loss: 0.021740, acc.: 100.00%] [G loss: 3.019495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25929 [D loss: 0.645732, acc.: 71.88%] [G loss: 7.365736]\n",
      "epoch:33 step:25930 [D loss: 0.134182, acc.: 93.75%] [G loss: 1.466897]\n",
      "epoch:33 step:25931 [D loss: 0.159683, acc.: 92.19%] [G loss: 5.587973]\n",
      "epoch:33 step:25932 [D loss: 0.005231, acc.: 100.00%] [G loss: 4.675791]\n",
      "epoch:33 step:25933 [D loss: 0.006749, acc.: 100.00%] [G loss: 2.694561]\n",
      "epoch:33 step:25934 [D loss: 0.011139, acc.: 100.00%] [G loss: 2.743957]\n",
      "epoch:33 step:25935 [D loss: 0.002404, acc.: 100.00%] [G loss: 1.209368]\n",
      "epoch:33 step:25936 [D loss: 0.006522, acc.: 100.00%] [G loss: 1.266465]\n",
      "epoch:33 step:25937 [D loss: 0.002598, acc.: 100.00%] [G loss: 5.342063]\n",
      "epoch:33 step:25938 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.141609]\n",
      "epoch:33 step:25939 [D loss: 0.001309, acc.: 100.00%] [G loss: 0.386690]\n",
      "epoch:33 step:25940 [D loss: 0.051955, acc.: 98.44%] [G loss: 0.666711]\n",
      "epoch:33 step:25941 [D loss: 0.023884, acc.: 99.22%] [G loss: 0.287450]\n",
      "epoch:33 step:25942 [D loss: 0.049354, acc.: 99.22%] [G loss: 0.345599]\n",
      "epoch:33 step:25943 [D loss: 0.004390, acc.: 100.00%] [G loss: 0.311355]\n",
      "epoch:33 step:25944 [D loss: 0.016429, acc.: 100.00%] [G loss: 0.204460]\n",
      "epoch:33 step:25945 [D loss: 0.003870, acc.: 100.00%] [G loss: 0.023871]\n",
      "epoch:33 step:25946 [D loss: 0.000796, acc.: 100.00%] [G loss: 0.101215]\n",
      "epoch:33 step:25947 [D loss: 0.011585, acc.: 100.00%] [G loss: 0.028051]\n",
      "epoch:33 step:25948 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.072039]\n",
      "epoch:33 step:25949 [D loss: 0.004374, acc.: 100.00%] [G loss: 0.104954]\n",
      "epoch:33 step:25950 [D loss: 0.000634, acc.: 100.00%] [G loss: 0.021605]\n",
      "epoch:33 step:25951 [D loss: 0.007503, acc.: 100.00%] [G loss: 0.073369]\n",
      "epoch:33 step:25952 [D loss: 0.003625, acc.: 100.00%] [G loss: 0.027874]\n",
      "epoch:33 step:25953 [D loss: 0.004871, acc.: 100.00%] [G loss: 0.032036]\n",
      "epoch:33 step:25954 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.024999]\n",
      "epoch:33 step:25955 [D loss: 0.001934, acc.: 100.00%] [G loss: 0.007474]\n",
      "epoch:33 step:25956 [D loss: 0.004684, acc.: 100.00%] [G loss: 0.042353]\n",
      "epoch:33 step:25957 [D loss: 0.004850, acc.: 100.00%] [G loss: 0.021374]\n",
      "epoch:33 step:25958 [D loss: 0.017579, acc.: 100.00%] [G loss: 0.017169]\n",
      "epoch:33 step:25959 [D loss: 0.004973, acc.: 100.00%] [G loss: 0.077139]\n",
      "epoch:33 step:25960 [D loss: 0.018968, acc.: 100.00%] [G loss: 0.066667]\n",
      "epoch:33 step:25961 [D loss: 0.018698, acc.: 100.00%] [G loss: 0.259099]\n",
      "epoch:33 step:25962 [D loss: 0.023973, acc.: 100.00%] [G loss: 0.005777]\n",
      "epoch:33 step:25963 [D loss: 0.001176, acc.: 100.00%] [G loss: 5.530846]\n",
      "epoch:33 step:25964 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.189816]\n",
      "epoch:33 step:25965 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.216532]\n",
      "epoch:33 step:25966 [D loss: 0.005145, acc.: 100.00%] [G loss: 0.287275]\n",
      "epoch:33 step:25967 [D loss: 0.002479, acc.: 100.00%] [G loss: 0.406550]\n",
      "epoch:33 step:25968 [D loss: 0.030984, acc.: 99.22%] [G loss: 0.455537]\n",
      "epoch:33 step:25969 [D loss: 0.001158, acc.: 100.00%] [G loss: 0.346578]\n",
      "epoch:33 step:25970 [D loss: 0.087298, acc.: 97.66%] [G loss: 0.016368]\n",
      "epoch:33 step:25971 [D loss: 0.550140, acc.: 72.66%] [G loss: 8.343280]\n",
      "epoch:33 step:25972 [D loss: 1.044352, acc.: 57.03%] [G loss: 0.068910]\n",
      "epoch:33 step:25973 [D loss: 0.034888, acc.: 99.22%] [G loss: 2.984468]\n",
      "epoch:33 step:25974 [D loss: 0.021289, acc.: 99.22%] [G loss: 0.000750]\n",
      "epoch:33 step:25975 [D loss: 0.005069, acc.: 100.00%] [G loss: 2.410594]\n",
      "epoch:33 step:25976 [D loss: 0.001686, acc.: 100.00%] [G loss: 0.000532]\n",
      "epoch:33 step:25977 [D loss: 0.000426, acc.: 100.00%] [G loss: 1.201895]\n",
      "epoch:33 step:25978 [D loss: 0.006557, acc.: 100.00%] [G loss: 1.031678]\n",
      "epoch:33 step:25979 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.629526]\n",
      "epoch:33 step:25980 [D loss: 0.001304, acc.: 100.00%] [G loss: 0.155066]\n",
      "epoch:33 step:25981 [D loss: 0.000781, acc.: 100.00%] [G loss: 0.266020]\n",
      "epoch:33 step:25982 [D loss: 0.003713, acc.: 100.00%] [G loss: 0.211404]\n",
      "epoch:33 step:25983 [D loss: 0.003195, acc.: 100.00%] [G loss: 0.045490]\n",
      "epoch:33 step:25984 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.087437]\n",
      "epoch:33 step:25985 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.001269]\n",
      "epoch:33 step:25986 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.048344]\n",
      "epoch:33 step:25987 [D loss: 0.001293, acc.: 100.00%] [G loss: 0.039244]\n",
      "epoch:33 step:25988 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.066798]\n",
      "epoch:33 step:25989 [D loss: 0.000642, acc.: 100.00%] [G loss: 0.137210]\n",
      "epoch:33 step:25990 [D loss: 0.002950, acc.: 100.00%] [G loss: 0.034510]\n",
      "epoch:33 step:25991 [D loss: 0.026521, acc.: 99.22%] [G loss: 0.000588]\n",
      "epoch:33 step:25992 [D loss: 0.000939, acc.: 100.00%] [G loss: 0.013392]\n",
      "epoch:33 step:25993 [D loss: 0.057575, acc.: 99.22%] [G loss: 0.103672]\n",
      "epoch:33 step:25994 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.246515]\n",
      "epoch:33 step:25995 [D loss: 0.001670, acc.: 100.00%] [G loss: 0.199082]\n",
      "epoch:33 step:25996 [D loss: 0.004877, acc.: 100.00%] [G loss: 0.088390]\n",
      "epoch:33 step:25997 [D loss: 0.003710, acc.: 100.00%] [G loss: 0.028357]\n",
      "epoch:33 step:25998 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.001389]\n",
      "epoch:33 step:25999 [D loss: 0.003643, acc.: 100.00%] [G loss: 0.064990]\n",
      "epoch:33 step:26000 [D loss: 0.001331, acc.: 100.00%] [G loss: 0.086518]\n",
      "epoch:33 step:26001 [D loss: 0.003493, acc.: 100.00%] [G loss: 0.029395]\n",
      "epoch:33 step:26002 [D loss: 0.007533, acc.: 100.00%] [G loss: 0.017567]\n",
      "epoch:33 step:26003 [D loss: 0.016906, acc.: 100.00%] [G loss: 0.094629]\n",
      "epoch:33 step:26004 [D loss: 0.005413, acc.: 100.00%] [G loss: 0.093966]\n",
      "epoch:33 step:26005 [D loss: 0.028534, acc.: 100.00%] [G loss: 0.142995]\n",
      "epoch:33 step:26006 [D loss: 0.029354, acc.: 100.00%] [G loss: 0.593429]\n",
      "epoch:33 step:26007 [D loss: 0.021512, acc.: 100.00%] [G loss: 0.236259]\n",
      "epoch:33 step:26008 [D loss: 0.017900, acc.: 100.00%] [G loss: 0.001322]\n",
      "epoch:33 step:26009 [D loss: 0.015956, acc.: 99.22%] [G loss: 0.374796]\n",
      "epoch:33 step:26010 [D loss: 0.005602, acc.: 100.00%] [G loss: 0.317549]\n",
      "epoch:33 step:26011 [D loss: 0.001151, acc.: 100.00%] [G loss: 0.125878]\n",
      "epoch:33 step:26012 [D loss: 0.003377, acc.: 100.00%] [G loss: 0.318737]\n",
      "epoch:33 step:26013 [D loss: 0.020551, acc.: 100.00%] [G loss: 0.043330]\n",
      "epoch:33 step:26014 [D loss: 0.002062, acc.: 100.00%] [G loss: 0.018813]\n",
      "epoch:33 step:26015 [D loss: 0.015443, acc.: 100.00%] [G loss: 0.012583]\n",
      "epoch:33 step:26016 [D loss: 0.002421, acc.: 100.00%] [G loss: 0.032171]\n",
      "epoch:33 step:26017 [D loss: 0.009816, acc.: 100.00%] [G loss: 0.010355]\n",
      "epoch:33 step:26018 [D loss: 0.003167, acc.: 100.00%] [G loss: 0.017210]\n",
      "epoch:33 step:26019 [D loss: 0.003618, acc.: 100.00%] [G loss: 0.016446]\n",
      "epoch:33 step:26020 [D loss: 0.006531, acc.: 100.00%] [G loss: 0.027571]\n",
      "epoch:33 step:26021 [D loss: 0.084842, acc.: 98.44%] [G loss: 4.198769]\n",
      "epoch:33 step:26022 [D loss: 0.008796, acc.: 100.00%] [G loss: 0.055149]\n",
      "epoch:33 step:26023 [D loss: 0.308773, acc.: 85.16%] [G loss: 2.214153]\n",
      "epoch:33 step:26024 [D loss: 0.441282, acc.: 76.56%] [G loss: 6.458340]\n",
      "epoch:33 step:26025 [D loss: 0.013681, acc.: 100.00%] [G loss: 8.564580]\n",
      "epoch:33 step:26026 [D loss: 0.357615, acc.: 83.59%] [G loss: 5.628107]\n",
      "epoch:33 step:26027 [D loss: 0.002911, acc.: 100.00%] [G loss: 4.488468]\n",
      "epoch:33 step:26028 [D loss: 0.004168, acc.: 100.00%] [G loss: 3.443645]\n",
      "epoch:33 step:26029 [D loss: 0.025307, acc.: 100.00%] [G loss: 0.002340]\n",
      "epoch:33 step:26030 [D loss: 0.008819, acc.: 100.00%] [G loss: 3.165186]\n",
      "epoch:33 step:26031 [D loss: 0.001831, acc.: 100.00%] [G loss: 0.002268]\n",
      "epoch:33 step:26032 [D loss: 0.000396, acc.: 100.00%] [G loss: 1.610466]\n",
      "epoch:33 step:26033 [D loss: 0.001733, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:33 step:26034 [D loss: 0.009468, acc.: 100.00%] [G loss: 1.027265]\n",
      "epoch:33 step:26035 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.988307]\n",
      "epoch:33 step:26036 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.557500]\n",
      "epoch:33 step:26037 [D loss: 0.004388, acc.: 100.00%] [G loss: 0.690723]\n",
      "epoch:33 step:26038 [D loss: 0.002912, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:33 step:26039 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.000622]\n",
      "epoch:33 step:26040 [D loss: 0.006842, acc.: 100.00%] [G loss: 0.053176]\n",
      "epoch:33 step:26041 [D loss: 0.001178, acc.: 100.00%] [G loss: 0.083304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26042 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:33 step:26043 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.063478]\n",
      "epoch:33 step:26044 [D loss: 0.002620, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:33 step:26045 [D loss: 0.008370, acc.: 100.00%] [G loss: 0.043440]\n",
      "epoch:33 step:26046 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.011687]\n",
      "epoch:33 step:26047 [D loss: 0.001199, acc.: 100.00%] [G loss: 0.023586]\n",
      "epoch:33 step:26048 [D loss: 0.002016, acc.: 100.00%] [G loss: 0.074291]\n",
      "epoch:33 step:26049 [D loss: 0.000994, acc.: 100.00%] [G loss: 0.037148]\n",
      "epoch:33 step:26050 [D loss: 0.026472, acc.: 99.22%] [G loss: 0.184311]\n",
      "epoch:33 step:26051 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.021476]\n",
      "epoch:33 step:26052 [D loss: 0.006275, acc.: 100.00%] [G loss: 0.008290]\n",
      "epoch:33 step:26053 [D loss: 0.005859, acc.: 100.00%] [G loss: 0.007375]\n",
      "epoch:33 step:26054 [D loss: 0.003247, acc.: 100.00%] [G loss: 0.011306]\n",
      "epoch:33 step:26055 [D loss: 0.010080, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:33 step:26056 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.018721]\n",
      "epoch:33 step:26057 [D loss: 0.001914, acc.: 100.00%] [G loss: 0.005546]\n",
      "epoch:33 step:26058 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.012903]\n",
      "epoch:33 step:26059 [D loss: 0.001701, acc.: 100.00%] [G loss: 0.067009]\n",
      "epoch:33 step:26060 [D loss: 0.007276, acc.: 100.00%] [G loss: 0.006468]\n",
      "epoch:33 step:26061 [D loss: 0.003675, acc.: 100.00%] [G loss: 0.094850]\n",
      "epoch:33 step:26062 [D loss: 0.008268, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:33 step:26063 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.013921]\n",
      "epoch:33 step:26064 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:33 step:26065 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.010418]\n",
      "epoch:33 step:26066 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.013444]\n",
      "epoch:33 step:26067 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000485]\n",
      "epoch:33 step:26068 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.415467]\n",
      "epoch:33 step:26069 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.004488]\n",
      "epoch:33 step:26070 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.188572]\n",
      "epoch:33 step:26071 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.016009]\n",
      "epoch:33 step:26072 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.211688]\n",
      "epoch:33 step:26073 [D loss: 0.002226, acc.: 100.00%] [G loss: 0.011020]\n",
      "epoch:33 step:26074 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.005486]\n",
      "epoch:33 step:26075 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.012278]\n",
      "epoch:33 step:26076 [D loss: 0.005521, acc.: 100.00%] [G loss: 0.020207]\n",
      "epoch:33 step:26077 [D loss: 0.022446, acc.: 100.00%] [G loss: 0.183285]\n",
      "epoch:33 step:26078 [D loss: 0.005337, acc.: 100.00%] [G loss: 0.224091]\n",
      "epoch:33 step:26079 [D loss: 0.034203, acc.: 99.22%] [G loss: 0.172511]\n",
      "epoch:33 step:26080 [D loss: 0.001884, acc.: 100.00%] [G loss: 4.681242]\n",
      "epoch:33 step:26081 [D loss: 0.006526, acc.: 100.00%] [G loss: 0.987951]\n",
      "epoch:33 step:26082 [D loss: 0.292079, acc.: 87.50%] [G loss: 0.855734]\n",
      "epoch:33 step:26083 [D loss: 0.456589, acc.: 78.12%] [G loss: 0.000810]\n",
      "epoch:33 step:26084 [D loss: 0.004924, acc.: 100.00%] [G loss: 8.315681]\n",
      "epoch:33 step:26085 [D loss: 0.001985, acc.: 100.00%] [G loss: 7.074286]\n",
      "epoch:33 step:26086 [D loss: 0.004238, acc.: 100.00%] [G loss: 6.538474]\n",
      "epoch:33 step:26087 [D loss: 0.160647, acc.: 92.19%] [G loss: 6.440594]\n",
      "epoch:33 step:26088 [D loss: 0.806603, acc.: 67.97%] [G loss: 1.723639]\n",
      "epoch:33 step:26089 [D loss: 0.136481, acc.: 94.53%] [G loss: 4.155519]\n",
      "epoch:33 step:26090 [D loss: 0.002398, acc.: 100.00%] [G loss: 3.503291]\n",
      "epoch:33 step:26091 [D loss: 0.015555, acc.: 99.22%] [G loss: 1.866302]\n",
      "epoch:33 step:26092 [D loss: 0.010226, acc.: 100.00%] [G loss: 2.744984]\n",
      "epoch:33 step:26093 [D loss: 0.049466, acc.: 99.22%] [G loss: 0.608113]\n",
      "epoch:33 step:26094 [D loss: 0.147744, acc.: 93.75%] [G loss: 0.125633]\n",
      "epoch:33 step:26095 [D loss: 0.108966, acc.: 96.88%] [G loss: 5.700532]\n",
      "epoch:33 step:26096 [D loss: 0.013119, acc.: 100.00%] [G loss: 3.041785]\n",
      "epoch:33 step:26097 [D loss: 0.055854, acc.: 97.66%] [G loss: 4.124430]\n",
      "epoch:33 step:26098 [D loss: 0.627635, acc.: 71.88%] [G loss: 3.405416]\n",
      "epoch:33 step:26099 [D loss: 1.423445, acc.: 53.91%] [G loss: 2.964484]\n",
      "epoch:33 step:26100 [D loss: 0.040617, acc.: 97.66%] [G loss: 2.532052]\n",
      "epoch:33 step:26101 [D loss: 0.128735, acc.: 96.88%] [G loss: 0.036801]\n",
      "epoch:33 step:26102 [D loss: 0.018969, acc.: 99.22%] [G loss: 2.027792]\n",
      "epoch:33 step:26103 [D loss: 0.005694, acc.: 100.00%] [G loss: 1.017175]\n",
      "epoch:33 step:26104 [D loss: 0.009175, acc.: 100.00%] [G loss: 0.187560]\n",
      "epoch:33 step:26105 [D loss: 0.002313, acc.: 100.00%] [G loss: 0.109449]\n",
      "epoch:33 step:26106 [D loss: 0.002850, acc.: 100.00%] [G loss: 0.026075]\n",
      "epoch:33 step:26107 [D loss: 0.004563, acc.: 100.00%] [G loss: 0.048835]\n",
      "epoch:33 step:26108 [D loss: 0.007666, acc.: 100.00%] [G loss: 0.016166]\n",
      "epoch:33 step:26109 [D loss: 0.011276, acc.: 100.00%] [G loss: 0.025034]\n",
      "epoch:33 step:26110 [D loss: 0.001309, acc.: 100.00%] [G loss: 0.010497]\n",
      "epoch:33 step:26111 [D loss: 0.007977, acc.: 100.00%] [G loss: 0.001487]\n",
      "epoch:33 step:26112 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.002855]\n",
      "epoch:33 step:26113 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.001722]\n",
      "epoch:33 step:26114 [D loss: 0.000840, acc.: 100.00%] [G loss: 0.001163]\n",
      "epoch:33 step:26115 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:33 step:26116 [D loss: 0.007872, acc.: 100.00%] [G loss: 0.034119]\n",
      "epoch:33 step:26117 [D loss: 0.000770, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:33 step:26118 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.002623]\n",
      "epoch:33 step:26119 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.003372]\n",
      "epoch:33 step:26120 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:33 step:26121 [D loss: 0.002941, acc.: 100.00%] [G loss: 0.001092]\n",
      "epoch:33 step:26122 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:33 step:26123 [D loss: 0.006592, acc.: 100.00%] [G loss: 0.001021]\n",
      "epoch:33 step:26124 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.002188]\n",
      "epoch:33 step:26125 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:33 step:26126 [D loss: 0.002182, acc.: 100.00%] [G loss: 0.001283]\n",
      "epoch:33 step:26127 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000713]\n",
      "epoch:33 step:26128 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:33 step:26129 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:33 step:26130 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:33 step:26131 [D loss: 0.000705, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:33 step:26132 [D loss: 0.001608, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:33 step:26133 [D loss: 0.003895, acc.: 100.00%] [G loss: 0.004154]\n",
      "epoch:33 step:26134 [D loss: 0.002714, acc.: 100.00%] [G loss: 0.002239]\n",
      "epoch:33 step:26135 [D loss: 0.006639, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:33 step:26136 [D loss: 0.005897, acc.: 100.00%] [G loss: 0.001982]\n",
      "epoch:33 step:26137 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.002096]\n",
      "epoch:33 step:26138 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.001767]\n",
      "epoch:33 step:26139 [D loss: 0.001186, acc.: 100.00%] [G loss: 0.001087]\n",
      "epoch:33 step:26140 [D loss: 0.001384, acc.: 100.00%] [G loss: 0.001662]\n",
      "epoch:33 step:26141 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.002668]\n",
      "epoch:33 step:26142 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.003776]\n",
      "epoch:33 step:26143 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.002985]\n",
      "epoch:33 step:26144 [D loss: 0.001110, acc.: 100.00%] [G loss: 0.000731]\n",
      "epoch:33 step:26145 [D loss: 0.002020, acc.: 100.00%] [G loss: 0.003044]\n",
      "epoch:33 step:26146 [D loss: 0.003758, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:33 step:26147 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.003405]\n",
      "epoch:33 step:26148 [D loss: 0.011975, acc.: 100.00%] [G loss: 0.001066]\n",
      "epoch:33 step:26149 [D loss: 0.005820, acc.: 100.00%] [G loss: 0.001819]\n",
      "epoch:33 step:26150 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:33 step:26151 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:33 step:26152 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:33 step:26153 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:33 step:26154 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26155 [D loss: 0.002072, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:33 step:26156 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.002278]\n",
      "epoch:33 step:26157 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000934]\n",
      "epoch:33 step:26158 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000723]\n",
      "epoch:33 step:26159 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:33 step:26160 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:33 step:26161 [D loss: 0.009349, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:33 step:26162 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.003164]\n",
      "epoch:33 step:26163 [D loss: 0.005647, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:33 step:26164 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.001798]\n",
      "epoch:33 step:26165 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.001341]\n",
      "epoch:33 step:26166 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.001597]\n",
      "epoch:33 step:26167 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:33 step:26168 [D loss: 0.003021, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:33 step:26169 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000331]\n",
      "epoch:33 step:26170 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000711]\n",
      "epoch:33 step:26171 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:33 step:26172 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:33 step:26173 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:33 step:26174 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000829]\n",
      "epoch:33 step:26175 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:33 step:26176 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:33 step:26177 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:33 step:26178 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:33 step:26179 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:33 step:26180 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001152]\n",
      "epoch:33 step:26181 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:33 step:26182 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:33 step:26183 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:33 step:26184 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.002237]\n",
      "epoch:33 step:26185 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:33 step:26186 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:33 step:26187 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:33 step:26188 [D loss: 0.000601, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:33 step:26189 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:33 step:26190 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:33 step:26191 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:33 step:26192 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.004767]\n",
      "epoch:33 step:26193 [D loss: 0.001636, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:33 step:26194 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:33 step:26195 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:33 step:26196 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.010250]\n",
      "epoch:33 step:26197 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:33 step:26198 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:33 step:26199 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:33 step:26200 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000985]\n",
      "epoch:33 step:26201 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.008968]\n",
      "epoch:33 step:26202 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:33 step:26203 [D loss: 0.004986, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:33 step:26204 [D loss: 0.013181, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26205 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:33 step:26206 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:33 step:26207 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:33 step:26208 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:33 step:26209 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26210 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:33 step:26211 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:33 step:26212 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:33 step:26213 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:33 step:26214 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:33 step:26215 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:33 step:26216 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:33 step:26217 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26218 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:33 step:26219 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:26220 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:33 step:26221 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:26222 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:33 step:26223 [D loss: 0.001742, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:33 step:26224 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.001181]\n",
      "epoch:33 step:26225 [D loss: 0.002630, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:33 step:26226 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:33 step:26227 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:33 step:26228 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:33 step:26229 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:33 step:26230 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:33 step:26231 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:33 step:26232 [D loss: 0.003072, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:33 step:26233 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:33 step:26234 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:33 step:26235 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:33 step:26236 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:33 step:26237 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:33 step:26238 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:33 step:26239 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:33 step:26240 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:33 step:26241 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:33 step:26242 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:33 step:26243 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:33 step:26244 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:33 step:26245 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000606]\n",
      "epoch:33 step:26246 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:33 step:26247 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:33 step:26248 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:33 step:26249 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:33 step:26250 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:26251 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:33 step:26252 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:33 step:26253 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:33 step:26254 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.002317]\n",
      "epoch:33 step:26255 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:33 step:26256 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:33 step:26257 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:33 step:26258 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:33 step:26259 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:33 step:26260 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:33 step:26261 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:33 step:26262 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.002206]\n",
      "epoch:33 step:26263 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:33 step:26264 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:33 step:26265 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:33 step:26266 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:33 step:26267 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:33 step:26268 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26269 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:33 step:26270 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:33 step:26271 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:33 step:26272 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:26273 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:33 step:26274 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:33 step:26275 [D loss: 0.001439, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:33 step:26276 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:33 step:26277 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000823]\n",
      "epoch:33 step:26278 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:33 step:26279 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001709]\n",
      "epoch:33 step:26280 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:33 step:26281 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000925]\n",
      "epoch:33 step:26282 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000955]\n",
      "epoch:33 step:26283 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:33 step:26284 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:33 step:26285 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:33 step:26286 [D loss: 0.001290, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:33 step:26287 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:33 step:26288 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:33 step:26289 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:33 step:26290 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:33 step:26291 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001390]\n",
      "epoch:33 step:26292 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:33 step:26293 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:33 step:26294 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:33 step:26295 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:33 step:26296 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:33 step:26297 [D loss: 0.002824, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:33 step:26298 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:33 step:26299 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26300 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:33 step:26301 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:33 step:26302 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:33 step:26303 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:33 step:26304 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:33 step:26305 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:33 step:26306 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:33 step:26307 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:33 step:26308 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:33 step:26309 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26310 [D loss: 0.003108, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:33 step:26311 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:33 step:26312 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26313 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26314 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:33 step:26315 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:33 step:26316 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:33 step:26317 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:33 step:26318 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:33 step:26319 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:33 step:26320 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:33 step:26321 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26322 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:33 step:26323 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:26324 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:33 step:26325 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26326 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:33 step:26327 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:33 step:26328 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:33 step:26329 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:33 step:26330 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:26331 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:33 step:26332 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:33 step:26333 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26334 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:33 step:26335 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:33 step:26336 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:33 step:26337 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:33 step:26338 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26339 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:33 step:26340 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:33 step:26341 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:33 step:26342 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:33 step:26343 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:33 step:26344 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:33 step:26345 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:33 step:26346 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:33 step:26347 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:33 step:26348 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:33 step:26349 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:33 step:26350 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:33 step:26351 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:33 step:26352 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:33 step:26353 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:33 step:26354 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:26355 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:33 step:26356 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:33 step:26357 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:26358 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:33 step:26359 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:33 step:26360 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:33 step:26361 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:33 step:26362 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26363 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:33 step:26364 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26365 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:33 step:26366 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:33 step:26367 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:33 step:26368 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26369 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:33 step:26370 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:33 step:26371 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:33 step:26372 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:33 step:26373 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:33 step:26374 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:33 step:26375 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:33 step:26376 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:33 step:26377 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:33 step:26378 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26379 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:33 step:26380 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:33 step:26381 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:33 step:26382 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:33 step:26383 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:33 step:26384 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26385 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:33 step:26386 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:33 step:26387 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:33 step:26388 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:33 step:26389 [D loss: 0.001265, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:33 step:26390 [D loss: 0.002043, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:33 step:26391 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:33 step:26392 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:33 step:26393 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:33 step:26394 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:33 step:26395 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:33 step:26396 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:33 step:26397 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:33 step:26398 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:33 step:26399 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26400 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:33 step:26401 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000509]\n",
      "epoch:33 step:26402 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:33 step:26403 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:33 step:26404 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:33 step:26405 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.002089]\n",
      "epoch:33 step:26406 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:33 step:26407 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:33 step:26408 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:33 step:26409 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:33 step:26410 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:33 step:26411 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001095]\n",
      "epoch:33 step:26412 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:33 step:26413 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:33 step:26414 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:33 step:26415 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26416 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:33 step:26417 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:33 step:26418 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:33 step:26419 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:33 step:26420 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:33 step:26421 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:33 step:26422 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:33 step:26423 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:33 step:26424 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:33 step:26425 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:26426 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:26427 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:33 step:26428 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:33 step:26429 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26430 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:33 step:26431 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:26432 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:33 step:26433 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:33 step:26434 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:33 step:26435 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:33 step:26436 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:33 step:26437 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:33 step:26438 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:33 step:26439 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:33 step:26440 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:33 step:26441 [D loss: 0.008864, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:33 step:26442 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000673]\n",
      "epoch:33 step:26443 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:33 step:26444 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:33 step:26445 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:33 step:26446 [D loss: 0.000949, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:33 step:26447 [D loss: 0.005214, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:33 step:26448 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.012424]\n",
      "epoch:33 step:26449 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.001168]\n",
      "epoch:33 step:26450 [D loss: 0.002662, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26451 [D loss: 0.002798, acc.: 100.00%] [G loss: 0.000598]\n",
      "epoch:33 step:26452 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:33 step:26453 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.002107]\n",
      "epoch:33 step:26454 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.004397]\n",
      "epoch:33 step:26455 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:33 step:26456 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:33 step:26457 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:33 step:26458 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:33 step:26459 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:33 step:26460 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26461 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:33 step:26462 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:33 step:26463 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:33 step:26464 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:33 step:26465 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:33 step:26466 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002289]\n",
      "epoch:33 step:26467 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:33 step:26468 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:33 step:26469 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:33 step:26470 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:33 step:26471 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:33 step:26472 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:33 step:26473 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:33 step:26474 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:33 step:26475 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:33 step:26476 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.001044]\n",
      "epoch:33 step:26477 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:33 step:26478 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.002722]\n",
      "epoch:33 step:26479 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:33 step:26480 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:33 step:26481 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:33 step:26482 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:33 step:26483 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:33 step:26484 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000608]\n",
      "epoch:33 step:26485 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:33 step:26486 [D loss: 0.002967, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:33 step:26487 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:33 step:26488 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.000804]\n",
      "epoch:33 step:26489 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:33 step:26490 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:33 step:26491 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:33 step:26492 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:33 step:26493 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:33 step:26494 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:33 step:26495 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:33 step:26496 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:33 step:26497 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:33 step:26498 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:33 step:26499 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26500 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:33 step:26501 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:33 step:26502 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:33 step:26503 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:33 step:26504 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:33 step:26505 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:26506 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:33 step:26507 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:33 step:26508 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000573]\n",
      "epoch:33 step:26509 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:33 step:26510 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:33 step:26511 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:33 step:26512 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.001031]\n",
      "epoch:33 step:26513 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:33 step:26514 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:33 step:26515 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:33 step:26516 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:26517 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:33 step:26518 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:33 step:26519 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:33 step:26520 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:33 step:26521 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:33 step:26522 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.001204]\n",
      "epoch:33 step:26523 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:33 step:26524 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:33 step:26525 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:33 step:26526 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:33 step:26527 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:33 step:26528 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:33 step:26529 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:33 step:26530 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:33 step:26531 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:33 step:26532 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:33 step:26533 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:33 step:26534 [D loss: 0.002393, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:33 step:26535 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:33 step:26536 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:33 step:26537 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:33 step:26538 [D loss: 0.239176, acc.: 85.94%] [G loss: 0.897304]\n",
      "epoch:33 step:26539 [D loss: 0.260828, acc.: 89.84%] [G loss: 0.216277]\n",
      "epoch:33 step:26540 [D loss: 0.029389, acc.: 99.22%] [G loss: 4.265008]\n",
      "epoch:33 step:26541 [D loss: 0.010471, acc.: 100.00%] [G loss: 0.740950]\n",
      "epoch:33 step:26542 [D loss: 0.007871, acc.: 100.00%] [G loss: 0.000802]\n",
      "epoch:33 step:26543 [D loss: 0.001526, acc.: 100.00%] [G loss: 0.002830]\n",
      "epoch:33 step:26544 [D loss: 0.072888, acc.: 97.66%] [G loss: 0.002098]\n",
      "epoch:33 step:26545 [D loss: 0.741664, acc.: 72.66%] [G loss: 5.330282]\n",
      "epoch:33 step:26546 [D loss: 1.705323, acc.: 52.34%] [G loss: 1.411870]\n",
      "epoch:33 step:26547 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.375373]\n",
      "epoch:33 step:26548 [D loss: 0.042112, acc.: 97.66%] [G loss: 0.975695]\n",
      "epoch:33 step:26549 [D loss: 0.002646, acc.: 100.00%] [G loss: 0.979153]\n",
      "epoch:33 step:26550 [D loss: 0.001082, acc.: 100.00%] [G loss: 1.003165]\n",
      "epoch:33 step:26551 [D loss: 0.261054, acc.: 85.16%] [G loss: 2.230306]\n",
      "epoch:33 step:26552 [D loss: 0.010028, acc.: 100.00%] [G loss: 1.626495]\n",
      "epoch:33 step:26553 [D loss: 0.008344, acc.: 100.00%] [G loss: 1.795882]\n",
      "epoch:33 step:26554 [D loss: 0.000674, acc.: 100.00%] [G loss: 1.490668]\n",
      "epoch:34 step:26555 [D loss: 0.099953, acc.: 97.66%] [G loss: 0.018923]\n",
      "epoch:34 step:26556 [D loss: 0.003490, acc.: 100.00%] [G loss: 0.006555]\n",
      "epoch:34 step:26557 [D loss: 0.003993, acc.: 100.00%] [G loss: 0.009046]\n",
      "epoch:34 step:26558 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.064772]\n",
      "epoch:34 step:26559 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.052573]\n",
      "epoch:34 step:26560 [D loss: 0.048095, acc.: 100.00%] [G loss: 0.039112]\n",
      "epoch:34 step:26561 [D loss: 0.012579, acc.: 99.22%] [G loss: 0.039275]\n",
      "epoch:34 step:26562 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.011909]\n",
      "epoch:34 step:26563 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.011546]\n",
      "epoch:34 step:26564 [D loss: 0.002636, acc.: 100.00%] [G loss: 0.012566]\n",
      "epoch:34 step:26565 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.045131]\n",
      "epoch:34 step:26566 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.002224]\n",
      "epoch:34 step:26567 [D loss: 0.007468, acc.: 100.00%] [G loss: 0.024206]\n",
      "epoch:34 step:26568 [D loss: 0.001437, acc.: 100.00%] [G loss: 0.013636]\n",
      "epoch:34 step:26569 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.006002]\n",
      "epoch:34 step:26570 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.037432]\n",
      "epoch:34 step:26571 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.007519]\n",
      "epoch:34 step:26572 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.010445]\n",
      "epoch:34 step:26573 [D loss: 0.002340, acc.: 100.00%] [G loss: 0.006389]\n",
      "epoch:34 step:26574 [D loss: 0.000825, acc.: 100.00%] [G loss: 0.003331]\n",
      "epoch:34 step:26575 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.004392]\n",
      "epoch:34 step:26576 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.003046]\n",
      "epoch:34 step:26577 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.013542]\n",
      "epoch:34 step:26578 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.041050]\n",
      "epoch:34 step:26579 [D loss: 0.002513, acc.: 100.00%] [G loss: 0.002016]\n",
      "epoch:34 step:26580 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.032186]\n",
      "epoch:34 step:26581 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.002608]\n",
      "epoch:34 step:26582 [D loss: 0.002056, acc.: 100.00%] [G loss: 0.001165]\n",
      "epoch:34 step:26583 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.053872]\n",
      "epoch:34 step:26584 [D loss: 0.021280, acc.: 100.00%] [G loss: 0.002545]\n",
      "epoch:34 step:26585 [D loss: 0.007274, acc.: 100.00%] [G loss: 0.064238]\n",
      "epoch:34 step:26586 [D loss: 0.001751, acc.: 100.00%] [G loss: 0.012528]\n",
      "epoch:34 step:26587 [D loss: 0.002513, acc.: 100.00%] [G loss: 0.014662]\n",
      "epoch:34 step:26588 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.013230]\n",
      "epoch:34 step:26589 [D loss: 0.004716, acc.: 100.00%] [G loss: 0.026494]\n",
      "epoch:34 step:26590 [D loss: 0.010220, acc.: 100.00%] [G loss: 0.002126]\n",
      "epoch:34 step:26591 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.007039]\n",
      "epoch:34 step:26592 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.004929]\n",
      "epoch:34 step:26593 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.005649]\n",
      "epoch:34 step:26594 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.010546]\n",
      "epoch:34 step:26595 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000855]\n",
      "epoch:34 step:26596 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000799]\n",
      "epoch:34 step:26597 [D loss: 0.001118, acc.: 100.00%] [G loss: 0.003700]\n",
      "epoch:34 step:26598 [D loss: 0.009602, acc.: 100.00%] [G loss: 0.001148]\n",
      "epoch:34 step:26599 [D loss: 0.002326, acc.: 100.00%] [G loss: 0.002471]\n",
      "epoch:34 step:26600 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.005857]\n",
      "epoch:34 step:26601 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.001938]\n",
      "epoch:34 step:26602 [D loss: 0.000986, acc.: 100.00%] [G loss: 0.003708]\n",
      "epoch:34 step:26603 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.004308]\n",
      "epoch:34 step:26604 [D loss: 0.055642, acc.: 99.22%] [G loss: 0.026974]\n",
      "epoch:34 step:26605 [D loss: 0.001449, acc.: 100.00%] [G loss: 0.232468]\n",
      "epoch:34 step:26606 [D loss: 0.016203, acc.: 99.22%] [G loss: 0.008813]\n",
      "epoch:34 step:26607 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.006152]\n",
      "epoch:34 step:26608 [D loss: 0.004078, acc.: 100.00%] [G loss: 0.017691]\n",
      "epoch:34 step:26609 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.003943]\n",
      "epoch:34 step:26610 [D loss: 0.006361, acc.: 100.00%] [G loss: 0.038668]\n",
      "epoch:34 step:26611 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.053488]\n",
      "epoch:34 step:26612 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.009590]\n",
      "epoch:34 step:26613 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.010706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26614 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.004165]\n",
      "epoch:34 step:26615 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.059848]\n",
      "epoch:34 step:26616 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.010453]\n",
      "epoch:34 step:26617 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:34 step:26618 [D loss: 0.000959, acc.: 100.00%] [G loss: 0.004868]\n",
      "epoch:34 step:26619 [D loss: 0.016689, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:34 step:26620 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:34 step:26621 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001676]\n",
      "epoch:34 step:26622 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:34 step:26623 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.001395]\n",
      "epoch:34 step:26624 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:34 step:26625 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:34 step:26626 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:34 step:26627 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:34 step:26628 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:34 step:26629 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:34 step:26630 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000957]\n",
      "epoch:34 step:26631 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:34 step:26632 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:34 step:26633 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:34 step:26634 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:34 step:26635 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.041358]\n",
      "epoch:34 step:26636 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:34 step:26637 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:34 step:26638 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:34 step:26639 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:34 step:26640 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.008810]\n",
      "epoch:34 step:26641 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:34 step:26642 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:34 step:26643 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:34 step:26644 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:34 step:26645 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:34 step:26646 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001739]\n",
      "epoch:34 step:26647 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.002800]\n",
      "epoch:34 step:26648 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.008982]\n",
      "epoch:34 step:26649 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:34 step:26650 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:34 step:26651 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:34 step:26652 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.001334]\n",
      "epoch:34 step:26653 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:34 step:26654 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:34 step:26655 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001017]\n",
      "epoch:34 step:26656 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:34 step:26657 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000698]\n",
      "epoch:34 step:26658 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:34 step:26659 [D loss: 0.001286, acc.: 100.00%] [G loss: 0.017396]\n",
      "epoch:34 step:26660 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:34 step:26661 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:34 step:26662 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:34 step:26663 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:34 step:26664 [D loss: 0.028631, acc.: 100.00%] [G loss: 0.002247]\n",
      "epoch:34 step:26665 [D loss: 0.001106, acc.: 100.00%] [G loss: 0.010201]\n",
      "epoch:34 step:26666 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.025201]\n",
      "epoch:34 step:26667 [D loss: 0.001057, acc.: 100.00%] [G loss: 0.018650]\n",
      "epoch:34 step:26668 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.044569]\n",
      "epoch:34 step:26669 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.241436]\n",
      "epoch:34 step:26670 [D loss: 0.002885, acc.: 100.00%] [G loss: 0.018529]\n",
      "epoch:34 step:26671 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.010451]\n",
      "epoch:34 step:26672 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001161]\n",
      "epoch:34 step:26673 [D loss: 0.009462, acc.: 100.00%] [G loss: 0.009359]\n",
      "epoch:34 step:26674 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.019560]\n",
      "epoch:34 step:26675 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.009309]\n",
      "epoch:34 step:26676 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.004668]\n",
      "epoch:34 step:26677 [D loss: 0.004720, acc.: 100.00%] [G loss: 0.017983]\n",
      "epoch:34 step:26678 [D loss: 0.004478, acc.: 100.00%] [G loss: 0.001901]\n",
      "epoch:34 step:26679 [D loss: 0.001420, acc.: 100.00%] [G loss: 0.006560]\n",
      "epoch:34 step:26680 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.007684]\n",
      "epoch:34 step:26681 [D loss: 0.139942, acc.: 96.88%] [G loss: 4.096142]\n",
      "epoch:34 step:26682 [D loss: 0.018373, acc.: 100.00%] [G loss: 5.622347]\n",
      "epoch:34 step:26683 [D loss: 0.157184, acc.: 98.44%] [G loss: 1.190916]\n",
      "epoch:34 step:26684 [D loss: 0.055046, acc.: 98.44%] [G loss: 0.262137]\n",
      "epoch:34 step:26685 [D loss: 0.045748, acc.: 98.44%] [G loss: 2.192388]\n",
      "epoch:34 step:26686 [D loss: 0.651240, acc.: 71.09%] [G loss: 6.102230]\n",
      "epoch:34 step:26687 [D loss: 0.194062, acc.: 96.09%] [G loss: 5.610477]\n",
      "epoch:34 step:26688 [D loss: 0.111875, acc.: 97.66%] [G loss: 2.090663]\n",
      "epoch:34 step:26689 [D loss: 0.888139, acc.: 67.19%] [G loss: 6.294856]\n",
      "epoch:34 step:26690 [D loss: 1.471034, acc.: 56.25%] [G loss: 5.622571]\n",
      "epoch:34 step:26691 [D loss: 0.062641, acc.: 96.88%] [G loss: 0.135668]\n",
      "epoch:34 step:26692 [D loss: 0.082030, acc.: 98.44%] [G loss: 2.976316]\n",
      "epoch:34 step:26693 [D loss: 0.096171, acc.: 96.88%] [G loss: 2.809818]\n",
      "epoch:34 step:26694 [D loss: 0.030543, acc.: 98.44%] [G loss: 1.369190]\n",
      "epoch:34 step:26695 [D loss: 0.004792, acc.: 100.00%] [G loss: 0.659124]\n",
      "epoch:34 step:26696 [D loss: 0.017740, acc.: 99.22%] [G loss: 0.074902]\n",
      "epoch:34 step:26697 [D loss: 0.021748, acc.: 99.22%] [G loss: 0.095600]\n",
      "epoch:34 step:26698 [D loss: 0.006774, acc.: 100.00%] [G loss: 0.002147]\n",
      "epoch:34 step:26699 [D loss: 0.113920, acc.: 97.66%] [G loss: 0.977161]\n",
      "epoch:34 step:26700 [D loss: 0.032349, acc.: 98.44%] [G loss: 1.260132]\n",
      "epoch:34 step:26701 [D loss: 0.034350, acc.: 99.22%] [G loss: 0.672739]\n",
      "epoch:34 step:26702 [D loss: 1.043075, acc.: 59.38%] [G loss: 6.689197]\n",
      "epoch:34 step:26703 [D loss: 1.079611, acc.: 63.28%] [G loss: 6.235739]\n",
      "epoch:34 step:26704 [D loss: 0.079002, acc.: 96.88%] [G loss: 4.568254]\n",
      "epoch:34 step:26705 [D loss: 0.078760, acc.: 98.44%] [G loss: 4.482358]\n",
      "epoch:34 step:26706 [D loss: 0.019036, acc.: 100.00%] [G loss: 4.069850]\n",
      "epoch:34 step:26707 [D loss: 0.169252, acc.: 92.97%] [G loss: 0.036845]\n",
      "epoch:34 step:26708 [D loss: 0.002908, acc.: 100.00%] [G loss: 3.160951]\n",
      "epoch:34 step:26709 [D loss: 0.003528, acc.: 100.00%] [G loss: 2.581998]\n",
      "epoch:34 step:26710 [D loss: 0.023906, acc.: 100.00%] [G loss: 0.029859]\n",
      "epoch:34 step:26711 [D loss: 0.022465, acc.: 99.22%] [G loss: 0.007534]\n",
      "epoch:34 step:26712 [D loss: 0.032112, acc.: 100.00%] [G loss: 0.556292]\n",
      "epoch:34 step:26713 [D loss: 0.003793, acc.: 100.00%] [G loss: 0.752276]\n",
      "epoch:34 step:26714 [D loss: 0.030802, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:34 step:26715 [D loss: 0.007262, acc.: 100.00%] [G loss: 0.000641]\n",
      "epoch:34 step:26716 [D loss: 0.048949, acc.: 98.44%] [G loss: 0.412774]\n",
      "epoch:34 step:26717 [D loss: 0.003689, acc.: 100.00%] [G loss: 0.104150]\n",
      "epoch:34 step:26718 [D loss: 0.029096, acc.: 100.00%] [G loss: 0.877529]\n",
      "epoch:34 step:26719 [D loss: 0.054257, acc.: 97.66%] [G loss: 0.411378]\n",
      "epoch:34 step:26720 [D loss: 0.024158, acc.: 98.44%] [G loss: 0.240691]\n",
      "epoch:34 step:26721 [D loss: 0.003351, acc.: 100.00%] [G loss: 0.226006]\n",
      "epoch:34 step:26722 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.148825]\n",
      "epoch:34 step:26723 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.037092]\n",
      "epoch:34 step:26724 [D loss: 0.011042, acc.: 100.00%] [G loss: 0.000489]\n",
      "epoch:34 step:26725 [D loss: 0.025342, acc.: 99.22%] [G loss: 0.001643]\n",
      "epoch:34 step:26726 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.002064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26727 [D loss: 0.003737, acc.: 100.00%] [G loss: 0.020508]\n",
      "epoch:34 step:26728 [D loss: 0.002300, acc.: 100.00%] [G loss: 0.003648]\n",
      "epoch:34 step:26729 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.002264]\n",
      "epoch:34 step:26730 [D loss: 0.011509, acc.: 100.00%] [G loss: 0.003478]\n",
      "epoch:34 step:26731 [D loss: 0.007761, acc.: 100.00%] [G loss: 0.003926]\n",
      "epoch:34 step:26732 [D loss: 0.019014, acc.: 99.22%] [G loss: 0.000636]\n",
      "epoch:34 step:26733 [D loss: 0.151672, acc.: 94.53%] [G loss: 0.008745]\n",
      "epoch:34 step:26734 [D loss: 0.086036, acc.: 96.88%] [G loss: 0.001832]\n",
      "epoch:34 step:26735 [D loss: 0.011577, acc.: 100.00%] [G loss: 0.015679]\n",
      "epoch:34 step:26736 [D loss: 0.000902, acc.: 100.00%] [G loss: 0.050677]\n",
      "epoch:34 step:26737 [D loss: 0.028166, acc.: 99.22%] [G loss: 0.235781]\n",
      "epoch:34 step:26738 [D loss: 0.019727, acc.: 100.00%] [G loss: 0.049638]\n",
      "epoch:34 step:26739 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.007212]\n",
      "epoch:34 step:26740 [D loss: 0.011543, acc.: 100.00%] [G loss: 0.012271]\n",
      "epoch:34 step:26741 [D loss: 0.019949, acc.: 99.22%] [G loss: 0.013219]\n",
      "epoch:34 step:26742 [D loss: 0.008698, acc.: 100.00%] [G loss: 0.000901]\n",
      "epoch:34 step:26743 [D loss: 0.138695, acc.: 96.09%] [G loss: 1.621977]\n",
      "epoch:34 step:26744 [D loss: 1.957720, acc.: 32.03%] [G loss: 5.944741]\n",
      "epoch:34 step:26745 [D loss: 0.510569, acc.: 76.56%] [G loss: 5.108991]\n",
      "epoch:34 step:26746 [D loss: 0.066593, acc.: 98.44%] [G loss: 2.516864]\n",
      "epoch:34 step:26747 [D loss: 0.118375, acc.: 95.31%] [G loss: 0.565677]\n",
      "epoch:34 step:26748 [D loss: 0.095650, acc.: 97.66%] [G loss: 1.085777]\n",
      "epoch:34 step:26749 [D loss: 0.014165, acc.: 100.00%] [G loss: 0.616917]\n",
      "epoch:34 step:26750 [D loss: 0.018472, acc.: 100.00%] [G loss: 0.513526]\n",
      "epoch:34 step:26751 [D loss: 0.036822, acc.: 98.44%] [G loss: 0.080224]\n",
      "epoch:34 step:26752 [D loss: 0.056567, acc.: 98.44%] [G loss: 0.056381]\n",
      "epoch:34 step:26753 [D loss: 0.017306, acc.: 100.00%] [G loss: 0.255025]\n",
      "epoch:34 step:26754 [D loss: 0.075837, acc.: 97.66%] [G loss: 0.355330]\n",
      "epoch:34 step:26755 [D loss: 0.005918, acc.: 100.00%] [G loss: 0.232599]\n",
      "epoch:34 step:26756 [D loss: 0.017206, acc.: 100.00%] [G loss: 0.064565]\n",
      "epoch:34 step:26757 [D loss: 0.008666, acc.: 100.00%] [G loss: 0.031351]\n",
      "epoch:34 step:26758 [D loss: 0.035717, acc.: 99.22%] [G loss: 0.049734]\n",
      "epoch:34 step:26759 [D loss: 0.015239, acc.: 100.00%] [G loss: 0.094961]\n",
      "epoch:34 step:26760 [D loss: 0.067511, acc.: 98.44%] [G loss: 0.006641]\n",
      "epoch:34 step:26761 [D loss: 0.003801, acc.: 100.00%] [G loss: 0.005486]\n",
      "epoch:34 step:26762 [D loss: 0.013961, acc.: 100.00%] [G loss: 0.036020]\n",
      "epoch:34 step:26763 [D loss: 0.053123, acc.: 98.44%] [G loss: 0.066676]\n",
      "epoch:34 step:26764 [D loss: 0.024006, acc.: 99.22%] [G loss: 0.006058]\n",
      "epoch:34 step:26765 [D loss: 0.068674, acc.: 97.66%] [G loss: 0.009320]\n",
      "epoch:34 step:26766 [D loss: 0.002576, acc.: 100.00%] [G loss: 0.002947]\n",
      "epoch:34 step:26767 [D loss: 0.003669, acc.: 100.00%] [G loss: 3.319206]\n",
      "epoch:34 step:26768 [D loss: 0.025410, acc.: 100.00%] [G loss: 0.048194]\n",
      "epoch:34 step:26769 [D loss: 0.013928, acc.: 100.00%] [G loss: 0.013081]\n",
      "epoch:34 step:26770 [D loss: 0.012094, acc.: 100.00%] [G loss: 0.028392]\n",
      "epoch:34 step:26771 [D loss: 0.017419, acc.: 100.00%] [G loss: 0.010539]\n",
      "epoch:34 step:26772 [D loss: 0.029278, acc.: 100.00%] [G loss: 0.015230]\n",
      "epoch:34 step:26773 [D loss: 0.005774, acc.: 100.00%] [G loss: 0.011562]\n",
      "epoch:34 step:26774 [D loss: 0.028280, acc.: 98.44%] [G loss: 0.373410]\n",
      "epoch:34 step:26775 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.029614]\n",
      "epoch:34 step:26776 [D loss: 0.061205, acc.: 100.00%] [G loss: 0.024150]\n",
      "epoch:34 step:26777 [D loss: 0.003427, acc.: 100.00%] [G loss: 0.271132]\n",
      "epoch:34 step:26778 [D loss: 0.004741, acc.: 100.00%] [G loss: 0.192660]\n",
      "epoch:34 step:26779 [D loss: 0.016107, acc.: 100.00%] [G loss: 0.011371]\n",
      "epoch:34 step:26780 [D loss: 0.042216, acc.: 99.22%] [G loss: 0.103381]\n",
      "epoch:34 step:26781 [D loss: 0.036266, acc.: 100.00%] [G loss: 0.094257]\n",
      "epoch:34 step:26782 [D loss: 0.017803, acc.: 100.00%] [G loss: 0.071371]\n",
      "epoch:34 step:26783 [D loss: 0.077977, acc.: 96.88%] [G loss: 0.005754]\n",
      "epoch:34 step:26784 [D loss: 0.015531, acc.: 100.00%] [G loss: 0.009930]\n",
      "epoch:34 step:26785 [D loss: 0.000502, acc.: 100.00%] [G loss: 1.255781]\n",
      "epoch:34 step:26786 [D loss: 0.044742, acc.: 100.00%] [G loss: 0.378470]\n",
      "epoch:34 step:26787 [D loss: 0.008592, acc.: 100.00%] [G loss: 0.634943]\n",
      "epoch:34 step:26788 [D loss: 0.053081, acc.: 98.44%] [G loss: 0.054835]\n",
      "epoch:34 step:26789 [D loss: 0.013120, acc.: 100.00%] [G loss: 0.169785]\n",
      "epoch:34 step:26790 [D loss: 0.512216, acc.: 80.47%] [G loss: 5.672265]\n",
      "epoch:34 step:26791 [D loss: 0.016489, acc.: 100.00%] [G loss: 3.132555]\n",
      "epoch:34 step:26792 [D loss: 0.901077, acc.: 68.75%] [G loss: 0.000146]\n",
      "epoch:34 step:26793 [D loss: 0.170730, acc.: 92.19%] [G loss: 0.141780]\n",
      "epoch:34 step:26794 [D loss: 0.001087, acc.: 100.00%] [G loss: 0.944683]\n",
      "epoch:34 step:26795 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.011761]\n",
      "epoch:34 step:26796 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.005482]\n",
      "epoch:34 step:26797 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.002803]\n",
      "epoch:34 step:26798 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.011704]\n",
      "epoch:34 step:26799 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.004437]\n",
      "epoch:34 step:26800 [D loss: 0.002223, acc.: 100.00%] [G loss: 0.185117]\n",
      "epoch:34 step:26801 [D loss: 0.013096, acc.: 99.22%] [G loss: 0.000743]\n",
      "epoch:34 step:26802 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.082741]\n",
      "epoch:34 step:26803 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.004049]\n",
      "epoch:34 step:26804 [D loss: 0.001406, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:34 step:26805 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.025925]\n",
      "epoch:34 step:26806 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.002939]\n",
      "epoch:34 step:26807 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.010291]\n",
      "epoch:34 step:26808 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.011371]\n",
      "epoch:34 step:26809 [D loss: 0.003071, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:34 step:26810 [D loss: 0.001219, acc.: 100.00%] [G loss: 0.002552]\n",
      "epoch:34 step:26811 [D loss: 0.000915, acc.: 100.00%] [G loss: 0.719972]\n",
      "epoch:34 step:26812 [D loss: 0.005386, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:34 step:26813 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.032350]\n",
      "epoch:34 step:26814 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.019858]\n",
      "epoch:34 step:26815 [D loss: 0.003845, acc.: 100.00%] [G loss: 0.004459]\n",
      "epoch:34 step:26816 [D loss: 0.006865, acc.: 100.00%] [G loss: 0.008300]\n",
      "epoch:34 step:26817 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.000824]\n",
      "epoch:34 step:26818 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:34 step:26819 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.059792]\n",
      "epoch:34 step:26820 [D loss: 0.001918, acc.: 100.00%] [G loss: 0.000809]\n",
      "epoch:34 step:26821 [D loss: 0.031308, acc.: 100.00%] [G loss: 0.012416]\n",
      "epoch:34 step:26822 [D loss: 0.000773, acc.: 100.00%] [G loss: 0.019200]\n",
      "epoch:34 step:26823 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.012566]\n",
      "epoch:34 step:26824 [D loss: 0.001063, acc.: 100.00%] [G loss: 0.003341]\n",
      "epoch:34 step:26825 [D loss: 0.002808, acc.: 100.00%] [G loss: 0.012966]\n",
      "epoch:34 step:26826 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.044915]\n",
      "epoch:34 step:26827 [D loss: 0.001065, acc.: 100.00%] [G loss: 0.023584]\n",
      "epoch:34 step:26828 [D loss: 0.024125, acc.: 100.00%] [G loss: 0.004328]\n",
      "epoch:34 step:26829 [D loss: 0.042952, acc.: 96.88%] [G loss: 0.004054]\n",
      "epoch:34 step:26830 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.001567]\n",
      "epoch:34 step:26831 [D loss: 0.019452, acc.: 100.00%] [G loss: 0.017922]\n",
      "epoch:34 step:26832 [D loss: 0.003831, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:34 step:26833 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:34 step:26834 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.002127]\n",
      "epoch:34 step:26835 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.001656]\n",
      "epoch:34 step:26836 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.004114]\n",
      "epoch:34 step:26837 [D loss: 0.011465, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:34 step:26838 [D loss: 0.016610, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:34 step:26839 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.007551]\n",
      "epoch:34 step:26840 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.001806]\n",
      "epoch:34 step:26841 [D loss: 0.009591, acc.: 99.22%] [G loss: 0.000653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26842 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.039034]\n",
      "epoch:34 step:26843 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.002070]\n",
      "epoch:34 step:26844 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.005515]\n",
      "epoch:34 step:26845 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:34 step:26846 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.001485]\n",
      "epoch:34 step:26847 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.129205]\n",
      "epoch:34 step:26848 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.012301]\n",
      "epoch:34 step:26849 [D loss: 0.002314, acc.: 100.00%] [G loss: 0.000936]\n",
      "epoch:34 step:26850 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.001276]\n",
      "epoch:34 step:26851 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.028904]\n",
      "epoch:34 step:26852 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:34 step:26853 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000986]\n",
      "epoch:34 step:26854 [D loss: 0.002269, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:34 step:26855 [D loss: 0.001213, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:34 step:26856 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001142]\n",
      "epoch:34 step:26857 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.003095]\n",
      "epoch:34 step:26858 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:34 step:26859 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:34 step:26860 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:34 step:26861 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.005199]\n",
      "epoch:34 step:26862 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000830]\n",
      "epoch:34 step:26863 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:34 step:26864 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000743]\n",
      "epoch:34 step:26865 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.034843]\n",
      "epoch:34 step:26866 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:34 step:26867 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:34 step:26868 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000984]\n",
      "epoch:34 step:26869 [D loss: 0.049412, acc.: 98.44%] [G loss: 0.000006]\n",
      "epoch:34 step:26870 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:34 step:26871 [D loss: 0.008624, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:34 step:26872 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:34 step:26873 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:34 step:26874 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:34 step:26875 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.007345]\n",
      "epoch:34 step:26876 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:34 step:26877 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:34 step:26878 [D loss: 0.000943, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:34 step:26879 [D loss: 0.013184, acc.: 99.22%] [G loss: 0.000078]\n",
      "epoch:34 step:26880 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:34 step:26881 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:34 step:26882 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:34 step:26883 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:34 step:26884 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:34 step:26885 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:34 step:26886 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:34 step:26887 [D loss: 0.001024, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:34 step:26888 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000960]\n",
      "epoch:34 step:26889 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:34 step:26890 [D loss: 0.000868, acc.: 100.00%] [G loss: 0.001295]\n",
      "epoch:34 step:26891 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:34 step:26892 [D loss: 0.001295, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:34 step:26893 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:34 step:26894 [D loss: 0.004651, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:34 step:26895 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:34 step:26896 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:34 step:26897 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:34 step:26898 [D loss: 0.004463, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:34 step:26899 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:34 step:26900 [D loss: 0.004287, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:34 step:26901 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:34 step:26902 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:34 step:26903 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:34 step:26904 [D loss: 0.000906, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:34 step:26905 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:34 step:26906 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:34 step:26907 [D loss: 0.003844, acc.: 100.00%] [G loss: 0.001515]\n",
      "epoch:34 step:26908 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:34 step:26909 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:34 step:26910 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:34 step:26911 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.048985]\n",
      "epoch:34 step:26912 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.001885]\n",
      "epoch:34 step:26913 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:34 step:26914 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.001590]\n",
      "epoch:34 step:26915 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.003999]\n",
      "epoch:34 step:26916 [D loss: 0.079063, acc.: 98.44%] [G loss: 0.260442]\n",
      "epoch:34 step:26917 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.017720]\n",
      "epoch:34 step:26918 [D loss: 0.006656, acc.: 100.00%] [G loss: 0.405555]\n",
      "epoch:34 step:26919 [D loss: 0.009229, acc.: 99.22%] [G loss: 0.019176]\n",
      "epoch:34 step:26920 [D loss: 0.002843, acc.: 100.00%] [G loss: 0.089831]\n",
      "epoch:34 step:26921 [D loss: 0.002096, acc.: 100.00%] [G loss: 0.005835]\n",
      "epoch:34 step:26922 [D loss: 0.013307, acc.: 100.00%] [G loss: 0.002595]\n",
      "epoch:34 step:26923 [D loss: 0.006391, acc.: 100.00%] [G loss: 0.000694]\n",
      "epoch:34 step:26924 [D loss: 0.004595, acc.: 100.00%] [G loss: 0.017080]\n",
      "epoch:34 step:26925 [D loss: 0.002631, acc.: 100.00%] [G loss: 0.185138]\n",
      "epoch:34 step:26926 [D loss: 0.029920, acc.: 100.00%] [G loss: 0.189882]\n",
      "epoch:34 step:26927 [D loss: 0.003122, acc.: 100.00%] [G loss: 0.009510]\n",
      "epoch:34 step:26928 [D loss: 0.010059, acc.: 100.00%] [G loss: 0.002955]\n",
      "epoch:34 step:26929 [D loss: 0.012915, acc.: 100.00%] [G loss: 0.001105]\n",
      "epoch:34 step:26930 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.123875]\n",
      "epoch:34 step:26931 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.020830]\n",
      "epoch:34 step:26932 [D loss: 0.024842, acc.: 100.00%] [G loss: 0.055334]\n",
      "epoch:34 step:26933 [D loss: 0.002046, acc.: 100.00%] [G loss: 0.010651]\n",
      "epoch:34 step:26934 [D loss: 0.004231, acc.: 100.00%] [G loss: 0.004700]\n",
      "epoch:34 step:26935 [D loss: 0.004719, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:34 step:26936 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.141682]\n",
      "epoch:34 step:26937 [D loss: 0.001780, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:34 step:26938 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.011865]\n",
      "epoch:34 step:26939 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.002127]\n",
      "epoch:34 step:26940 [D loss: 0.019786, acc.: 99.22%] [G loss: 0.131699]\n",
      "epoch:34 step:26941 [D loss: 0.016624, acc.: 100.00%] [G loss: 0.000829]\n",
      "epoch:34 step:26942 [D loss: 0.302604, acc.: 85.94%] [G loss: 0.004719]\n",
      "epoch:34 step:26943 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:34 step:26944 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.001399]\n",
      "epoch:34 step:26945 [D loss: 0.026575, acc.: 99.22%] [G loss: 0.028273]\n",
      "epoch:34 step:26946 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.005057]\n",
      "epoch:34 step:26947 [D loss: 0.003227, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:34 step:26948 [D loss: 0.000741, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:34 step:26949 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.058707]\n",
      "epoch:34 step:26950 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:34 step:26951 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.006382]\n",
      "epoch:34 step:26952 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001564]\n",
      "epoch:34 step:26953 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:34 step:26954 [D loss: 0.001249, acc.: 100.00%] [G loss: 0.081881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26955 [D loss: 0.024419, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:34 step:26956 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.002801]\n",
      "epoch:34 step:26957 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000423]\n",
      "epoch:34 step:26958 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:34 step:26959 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.008144]\n",
      "epoch:34 step:26960 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.005627]\n",
      "epoch:34 step:26961 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.001139]\n",
      "epoch:34 step:26962 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:34 step:26963 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.004871]\n",
      "epoch:34 step:26964 [D loss: 0.012230, acc.: 99.22%] [G loss: 0.003022]\n",
      "epoch:34 step:26965 [D loss: 0.006993, acc.: 100.00%] [G loss: 0.956767]\n",
      "epoch:34 step:26966 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.167824]\n",
      "epoch:34 step:26967 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.005103]\n",
      "epoch:34 step:26968 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000696]\n",
      "epoch:34 step:26969 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:34 step:26970 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:34 step:26971 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.004595]\n",
      "epoch:34 step:26972 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:34 step:26973 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:34 step:26974 [D loss: 0.001242, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:34 step:26975 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.002933]\n",
      "epoch:34 step:26976 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000595]\n",
      "epoch:34 step:26977 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.004341]\n",
      "epoch:34 step:26978 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:34 step:26979 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001229]\n",
      "epoch:34 step:26980 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:34 step:26981 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:34 step:26982 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:34 step:26983 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000718]\n",
      "epoch:34 step:26984 [D loss: 0.016018, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:34 step:26985 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.001682]\n",
      "epoch:34 step:26986 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:34 step:26987 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:34 step:26988 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:34 step:26989 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:34 step:26990 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:34 step:26991 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:34 step:26992 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:34 step:26993 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:34 step:26994 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:34 step:26995 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002237]\n",
      "epoch:34 step:26996 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:34 step:26997 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000586]\n",
      "epoch:34 step:26998 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:34 step:26999 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:34 step:27000 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:34 step:27001 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:34 step:27002 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:34 step:27003 [D loss: 0.001020, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:34 step:27004 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:34 step:27005 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:34 step:27006 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:34 step:27007 [D loss: 0.008907, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:34 step:27008 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:34 step:27009 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:34 step:27010 [D loss: 0.001918, acc.: 100.00%] [G loss: 0.000375]\n",
      "epoch:34 step:27011 [D loss: 0.001078, acc.: 100.00%] [G loss: 0.021792]\n",
      "epoch:34 step:27012 [D loss: 0.007822, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:34 step:27013 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:34 step:27014 [D loss: 0.003812, acc.: 100.00%] [G loss: 0.000819]\n",
      "epoch:34 step:27015 [D loss: 0.006478, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:34 step:27016 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.002752]\n",
      "epoch:34 step:27017 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.950778]\n",
      "epoch:34 step:27018 [D loss: 0.003330, acc.: 100.00%] [G loss: 0.002274]\n",
      "epoch:34 step:27019 [D loss: 0.003007, acc.: 100.00%] [G loss: 0.000509]\n",
      "epoch:34 step:27020 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.056210]\n",
      "epoch:34 step:27021 [D loss: 0.008728, acc.: 100.00%] [G loss: 0.039549]\n",
      "epoch:34 step:27022 [D loss: 0.002526, acc.: 100.00%] [G loss: 0.013875]\n",
      "epoch:34 step:27023 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.005607]\n",
      "epoch:34 step:27024 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.027409]\n",
      "epoch:34 step:27025 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.092706]\n",
      "epoch:34 step:27026 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.001264]\n",
      "epoch:34 step:27027 [D loss: 0.004114, acc.: 100.00%] [G loss: 0.011173]\n",
      "epoch:34 step:27028 [D loss: 0.078847, acc.: 97.66%] [G loss: 0.432862]\n",
      "epoch:34 step:27029 [D loss: 0.047041, acc.: 97.66%] [G loss: 2.076828]\n",
      "epoch:34 step:27030 [D loss: 0.042483, acc.: 98.44%] [G loss: 1.301525]\n",
      "epoch:34 step:27031 [D loss: 0.005988, acc.: 100.00%] [G loss: 0.004108]\n",
      "epoch:34 step:27032 [D loss: 0.047204, acc.: 100.00%] [G loss: 0.239873]\n",
      "epoch:34 step:27033 [D loss: 0.014931, acc.: 100.00%] [G loss: 1.131155]\n",
      "epoch:34 step:27034 [D loss: 0.038917, acc.: 97.66%] [G loss: 0.022022]\n",
      "epoch:34 step:27035 [D loss: 0.042463, acc.: 98.44%] [G loss: 1.967549]\n",
      "epoch:34 step:27036 [D loss: 0.109550, acc.: 95.31%] [G loss: 0.300430]\n",
      "epoch:34 step:27037 [D loss: 0.420781, acc.: 81.25%] [G loss: 6.344376]\n",
      "epoch:34 step:27038 [D loss: 1.976512, acc.: 53.91%] [G loss: 0.001478]\n",
      "epoch:34 step:27039 [D loss: 0.396149, acc.: 85.94%] [G loss: 0.118655]\n",
      "epoch:34 step:27040 [D loss: 0.000516, acc.: 100.00%] [G loss: 2.025282]\n",
      "epoch:34 step:27041 [D loss: 0.077808, acc.: 97.66%] [G loss: 0.215940]\n",
      "epoch:34 step:27042 [D loss: 0.009829, acc.: 100.00%] [G loss: 0.027593]\n",
      "epoch:34 step:27043 [D loss: 0.007502, acc.: 100.00%] [G loss: 8.068035]\n",
      "epoch:34 step:27044 [D loss: 0.001623, acc.: 100.00%] [G loss: 7.814703]\n",
      "epoch:34 step:27045 [D loss: 0.058823, acc.: 96.88%] [G loss: 5.786202]\n",
      "epoch:34 step:27046 [D loss: 0.016107, acc.: 100.00%] [G loss: 3.285083]\n",
      "epoch:34 step:27047 [D loss: 0.228766, acc.: 91.41%] [G loss: 0.520430]\n",
      "epoch:34 step:27048 [D loss: 0.102603, acc.: 95.31%] [G loss: 6.475917]\n",
      "epoch:34 step:27049 [D loss: 0.059881, acc.: 98.44%] [G loss: 0.012626]\n",
      "epoch:34 step:27050 [D loss: 0.010126, acc.: 100.00%] [G loss: 1.812045]\n",
      "epoch:34 step:27051 [D loss: 0.084300, acc.: 99.22%] [G loss: 0.002208]\n",
      "epoch:34 step:27052 [D loss: 0.047889, acc.: 98.44%] [G loss: 0.487643]\n",
      "epoch:34 step:27053 [D loss: 0.699699, acc.: 71.09%] [G loss: 5.533016]\n",
      "epoch:34 step:27054 [D loss: 2.246820, acc.: 50.78%] [G loss: 4.058829]\n",
      "epoch:34 step:27055 [D loss: 0.130011, acc.: 93.75%] [G loss: 2.056583]\n",
      "epoch:34 step:27056 [D loss: 0.415740, acc.: 83.59%] [G loss: 0.645823]\n",
      "epoch:34 step:27057 [D loss: 0.026121, acc.: 100.00%] [G loss: 5.914515]\n",
      "epoch:34 step:27058 [D loss: 0.193946, acc.: 92.97%] [G loss: 0.426989]\n",
      "epoch:34 step:27059 [D loss: 0.047040, acc.: 98.44%] [G loss: 0.020851]\n",
      "epoch:34 step:27060 [D loss: 0.078896, acc.: 96.88%] [G loss: 0.054239]\n",
      "epoch:34 step:27061 [D loss: 0.029492, acc.: 98.44%] [G loss: 0.006424]\n",
      "epoch:34 step:27062 [D loss: 0.011074, acc.: 100.00%] [G loss: 3.045592]\n",
      "epoch:34 step:27063 [D loss: 0.090875, acc.: 96.88%] [G loss: 2.865563]\n",
      "epoch:34 step:27064 [D loss: 0.013499, acc.: 100.00%] [G loss: 0.272386]\n",
      "epoch:34 step:27065 [D loss: 0.019950, acc.: 99.22%] [G loss: 2.517207]\n",
      "epoch:34 step:27066 [D loss: 0.073112, acc.: 96.88%] [G loss: 2.107261]\n",
      "epoch:34 step:27067 [D loss: 0.189384, acc.: 93.75%] [G loss: 0.023060]\n",
      "epoch:34 step:27068 [D loss: 0.091512, acc.: 98.44%] [G loss: 2.737884]\n",
      "epoch:34 step:27069 [D loss: 0.029171, acc.: 99.22%] [G loss: 0.006528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27070 [D loss: 0.012405, acc.: 100.00%] [G loss: 0.019987]\n",
      "epoch:34 step:27071 [D loss: 0.028299, acc.: 100.00%] [G loss: 0.008526]\n",
      "epoch:34 step:27072 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.008373]\n",
      "epoch:34 step:27073 [D loss: 0.001080, acc.: 100.00%] [G loss: 0.002813]\n",
      "epoch:34 step:27074 [D loss: 0.001623, acc.: 100.00%] [G loss: 0.194421]\n",
      "epoch:34 step:27075 [D loss: 0.003499, acc.: 100.00%] [G loss: 0.093179]\n",
      "epoch:34 step:27076 [D loss: 0.002874, acc.: 100.00%] [G loss: 0.045329]\n",
      "epoch:34 step:27077 [D loss: 0.017476, acc.: 100.00%] [G loss: 0.043799]\n",
      "epoch:34 step:27078 [D loss: 0.039580, acc.: 100.00%] [G loss: 0.004906]\n",
      "epoch:34 step:27079 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.002563]\n",
      "epoch:34 step:27080 [D loss: 0.039030, acc.: 99.22%] [G loss: 0.126694]\n",
      "epoch:34 step:27081 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:34 step:27082 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.013000]\n",
      "epoch:34 step:27083 [D loss: 0.000824, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:34 step:27084 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:34 step:27085 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.000706]\n",
      "epoch:34 step:27086 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:34 step:27087 [D loss: 0.001571, acc.: 100.00%] [G loss: 0.029515]\n",
      "epoch:34 step:27088 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.018172]\n",
      "epoch:34 step:27089 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.000750]\n",
      "epoch:34 step:27090 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:34 step:27091 [D loss: 0.014234, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:34 step:27092 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:34 step:27093 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:34 step:27094 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:34 step:27095 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:34 step:27096 [D loss: 0.001663, acc.: 100.00%] [G loss: 0.000392]\n",
      "epoch:34 step:27097 [D loss: 0.001614, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:34 step:27098 [D loss: 0.009303, acc.: 100.00%] [G loss: 0.000973]\n",
      "epoch:34 step:27099 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:34 step:27100 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:34 step:27101 [D loss: 0.000771, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:34 step:27102 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:34 step:27103 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.001290]\n",
      "epoch:34 step:27104 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.003093]\n",
      "epoch:34 step:27105 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:34 step:27106 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002198]\n",
      "epoch:34 step:27107 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000863]\n",
      "epoch:34 step:27108 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000546]\n",
      "epoch:34 step:27109 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:34 step:27110 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:34 step:27111 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:34 step:27112 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:34 step:27113 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:34 step:27114 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:34 step:27115 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.002130]\n",
      "epoch:34 step:27116 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:34 step:27117 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001259]\n",
      "epoch:34 step:27118 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:34 step:27119 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.005178]\n",
      "epoch:34 step:27120 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.003077]\n",
      "epoch:34 step:27121 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:34 step:27122 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:34 step:27123 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:34 step:27124 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:34 step:27125 [D loss: 0.007946, acc.: 100.00%] [G loss: 0.001015]\n",
      "epoch:34 step:27126 [D loss: 0.036298, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:34 step:27127 [D loss: 0.005058, acc.: 100.00%] [G loss: 0.027654]\n",
      "epoch:34 step:27128 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.001224]\n",
      "epoch:34 step:27129 [D loss: 0.003605, acc.: 100.00%] [G loss: 0.002245]\n",
      "epoch:34 step:27130 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.064385]\n",
      "epoch:34 step:27131 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.001545]\n",
      "epoch:34 step:27132 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.026196]\n",
      "epoch:34 step:27133 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001315]\n",
      "epoch:34 step:27134 [D loss: 0.000778, acc.: 100.00%] [G loss: 0.005772]\n",
      "epoch:34 step:27135 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.012929]\n",
      "epoch:34 step:27136 [D loss: 0.001339, acc.: 100.00%] [G loss: 0.009447]\n",
      "epoch:34 step:27137 [D loss: 0.003236, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:34 step:27138 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.003559]\n",
      "epoch:34 step:27139 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:34 step:27140 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:34 step:27141 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001454]\n",
      "epoch:34 step:27142 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.002053]\n",
      "epoch:34 step:27143 [D loss: 0.001047, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:34 step:27144 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002349]\n",
      "epoch:34 step:27145 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.002118]\n",
      "epoch:34 step:27146 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.006572]\n",
      "epoch:34 step:27147 [D loss: 0.002938, acc.: 100.00%] [G loss: 0.001566]\n",
      "epoch:34 step:27148 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.004763]\n",
      "epoch:34 step:27149 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:34 step:27150 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.002116]\n",
      "epoch:34 step:27151 [D loss: 0.001647, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:34 step:27152 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000577]\n",
      "epoch:34 step:27153 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001124]\n",
      "epoch:34 step:27154 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.001185]\n",
      "epoch:34 step:27155 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:34 step:27156 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:34 step:27157 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:34 step:27158 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.000411]\n",
      "epoch:34 step:27159 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:34 step:27160 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.001297]\n",
      "epoch:34 step:27161 [D loss: 0.001062, acc.: 100.00%] [G loss: 0.001794]\n",
      "epoch:34 step:27162 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:34 step:27163 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:34 step:27164 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:34 step:27165 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:34 step:27166 [D loss: 0.001059, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:34 step:27167 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:34 step:27168 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001726]\n",
      "epoch:34 step:27169 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:34 step:27170 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:34 step:27171 [D loss: 0.002235, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:34 step:27172 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.001821]\n",
      "epoch:34 step:27173 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:34 step:27174 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:34 step:27175 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:34 step:27176 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003370]\n",
      "epoch:34 step:27177 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:34 step:27178 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:34 step:27179 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:34 step:27180 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.002806]\n",
      "epoch:34 step:27181 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:34 step:27182 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:34 step:27183 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27184 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.004627]\n",
      "epoch:34 step:27185 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:34 step:27186 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:34 step:27187 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:34 step:27188 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:34 step:27189 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:34 step:27190 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:34 step:27191 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:34 step:27192 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:34 step:27193 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:34 step:27194 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:34 step:27195 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:34 step:27196 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000331]\n",
      "epoch:34 step:27197 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001168]\n",
      "epoch:34 step:27198 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:34 step:27199 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:34 step:27200 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.001240]\n",
      "epoch:34 step:27201 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:34 step:27202 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:34 step:27203 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:34 step:27204 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:34 step:27205 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:34 step:27206 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:34 step:27207 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:34 step:27208 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:34 step:27209 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:34 step:27210 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:34 step:27211 [D loss: 0.001580, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:34 step:27212 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:34 step:27213 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:34 step:27214 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:34 step:27215 [D loss: 0.020300, acc.: 100.00%] [G loss: 0.001246]\n",
      "epoch:34 step:27216 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:34 step:27217 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.022817]\n",
      "epoch:34 step:27218 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.001489]\n",
      "epoch:34 step:27219 [D loss: 0.006887, acc.: 100.00%] [G loss: 0.008486]\n",
      "epoch:34 step:27220 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.001878]\n",
      "epoch:34 step:27221 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002530]\n",
      "epoch:34 step:27222 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000622]\n",
      "epoch:34 step:27223 [D loss: 0.000885, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:34 step:27224 [D loss: 0.004287, acc.: 100.00%] [G loss: 0.001141]\n",
      "epoch:34 step:27225 [D loss: 0.005337, acc.: 100.00%] [G loss: 0.004562]\n",
      "epoch:34 step:27226 [D loss: 0.001480, acc.: 100.00%] [G loss: 0.010967]\n",
      "epoch:34 step:27227 [D loss: 0.071973, acc.: 98.44%] [G loss: 0.252169]\n",
      "epoch:34 step:27228 [D loss: 0.011133, acc.: 100.00%] [G loss: 0.390431]\n",
      "epoch:34 step:27229 [D loss: 0.029438, acc.: 98.44%] [G loss: 0.026721]\n",
      "epoch:34 step:27230 [D loss: 0.050792, acc.: 97.66%] [G loss: 0.406430]\n",
      "epoch:34 step:27231 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.043268]\n",
      "epoch:34 step:27232 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.031905]\n",
      "epoch:34 step:27233 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.002593]\n",
      "epoch:34 step:27234 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:34 step:27235 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.003079]\n",
      "epoch:34 step:27236 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001469]\n",
      "epoch:34 step:27237 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:34 step:27238 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000994]\n",
      "epoch:34 step:27239 [D loss: 0.002004, acc.: 100.00%] [G loss: 0.002247]\n",
      "epoch:34 step:27240 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001526]\n",
      "epoch:34 step:27241 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.002044]\n",
      "epoch:34 step:27242 [D loss: 0.007122, acc.: 100.00%] [G loss: 0.003125]\n",
      "epoch:34 step:27243 [D loss: 0.000803, acc.: 100.00%] [G loss: 0.002147]\n",
      "epoch:34 step:27244 [D loss: 0.006957, acc.: 100.00%] [G loss: 0.007774]\n",
      "epoch:34 step:27245 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000973]\n",
      "epoch:34 step:27246 [D loss: 0.000791, acc.: 100.00%] [G loss: 0.002440]\n",
      "epoch:34 step:27247 [D loss: 0.005391, acc.: 100.00%] [G loss: 0.019773]\n",
      "epoch:34 step:27248 [D loss: 0.035901, acc.: 97.66%] [G loss: 0.022055]\n",
      "epoch:34 step:27249 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.782509]\n",
      "epoch:34 step:27250 [D loss: 0.004885, acc.: 100.00%] [G loss: 0.004607]\n",
      "epoch:34 step:27251 [D loss: 0.003821, acc.: 100.00%] [G loss: 0.016045]\n",
      "epoch:34 step:27252 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.158852]\n",
      "epoch:34 step:27253 [D loss: 0.000878, acc.: 100.00%] [G loss: 0.006798]\n",
      "epoch:34 step:27254 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.016242]\n",
      "epoch:34 step:27255 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.012680]\n",
      "epoch:34 step:27256 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.007089]\n",
      "epoch:34 step:27257 [D loss: 0.032121, acc.: 99.22%] [G loss: 0.017113]\n",
      "epoch:34 step:27258 [D loss: 0.008662, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:34 step:27259 [D loss: 0.002183, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:34 step:27260 [D loss: 0.008467, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:34 step:27261 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.000965]\n",
      "epoch:34 step:27262 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001866]\n",
      "epoch:34 step:27263 [D loss: 0.019533, acc.: 99.22%] [G loss: 0.008742]\n",
      "epoch:34 step:27264 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.013913]\n",
      "epoch:34 step:27265 [D loss: 0.004864, acc.: 100.00%] [G loss: 0.610639]\n",
      "epoch:34 step:27266 [D loss: 0.026521, acc.: 100.00%] [G loss: 0.231377]\n",
      "epoch:34 step:27267 [D loss: 6.752260, acc.: 26.56%] [G loss: 7.631730]\n",
      "epoch:34 step:27268 [D loss: 0.576327, acc.: 78.91%] [G loss: 4.032235]\n",
      "epoch:34 step:27269 [D loss: 1.157515, acc.: 57.03%] [G loss: 1.811957]\n",
      "epoch:34 step:27270 [D loss: 0.255746, acc.: 90.62%] [G loss: 0.365826]\n",
      "epoch:34 step:27271 [D loss: 0.071268, acc.: 99.22%] [G loss: 3.834325]\n",
      "epoch:34 step:27272 [D loss: 0.158094, acc.: 95.31%] [G loss: 3.883058]\n",
      "epoch:34 step:27273 [D loss: 0.065578, acc.: 98.44%] [G loss: 3.847762]\n",
      "epoch:34 step:27274 [D loss: 0.181243, acc.: 93.75%] [G loss: 0.237712]\n",
      "epoch:34 step:27275 [D loss: 0.246494, acc.: 90.62%] [G loss: 2.718004]\n",
      "epoch:34 step:27276 [D loss: 0.050186, acc.: 100.00%] [G loss: 3.038568]\n",
      "epoch:34 step:27277 [D loss: 0.095803, acc.: 97.66%] [G loss: 0.680917]\n",
      "epoch:34 step:27278 [D loss: 0.049347, acc.: 100.00%] [G loss: 1.928783]\n",
      "epoch:34 step:27279 [D loss: 0.039423, acc.: 100.00%] [G loss: 1.811612]\n",
      "epoch:34 step:27280 [D loss: 0.005877, acc.: 100.00%] [G loss: 1.306612]\n",
      "epoch:34 step:27281 [D loss: 0.069853, acc.: 99.22%] [G loss: 1.267469]\n",
      "epoch:34 step:27282 [D loss: 0.024301, acc.: 100.00%] [G loss: 0.150269]\n",
      "epoch:34 step:27283 [D loss: 0.009529, acc.: 100.00%] [G loss: 1.002146]\n",
      "epoch:34 step:27284 [D loss: 0.016945, acc.: 100.00%] [G loss: 0.620351]\n",
      "epoch:34 step:27285 [D loss: 0.014854, acc.: 100.00%] [G loss: 0.026577]\n",
      "epoch:34 step:27286 [D loss: 0.011370, acc.: 100.00%] [G loss: 0.457946]\n",
      "epoch:34 step:27287 [D loss: 0.012069, acc.: 100.00%] [G loss: 0.009194]\n",
      "epoch:34 step:27288 [D loss: 0.005033, acc.: 100.00%] [G loss: 0.272451]\n",
      "epoch:34 step:27289 [D loss: 0.010641, acc.: 100.00%] [G loss: 0.010762]\n",
      "epoch:34 step:27290 [D loss: 0.024187, acc.: 100.00%] [G loss: 0.186828]\n",
      "epoch:34 step:27291 [D loss: 0.003520, acc.: 100.00%] [G loss: 0.139597]\n",
      "epoch:34 step:27292 [D loss: 0.003588, acc.: 100.00%] [G loss: 0.110255]\n",
      "epoch:34 step:27293 [D loss: 0.001995, acc.: 100.00%] [G loss: 0.067138]\n",
      "epoch:34 step:27294 [D loss: 0.005309, acc.: 100.00%] [G loss: 0.154624]\n",
      "epoch:34 step:27295 [D loss: 0.009181, acc.: 100.00%] [G loss: 0.092378]\n",
      "epoch:34 step:27296 [D loss: 0.003003, acc.: 100.00%] [G loss: 0.065375]\n",
      "epoch:34 step:27297 [D loss: 0.005860, acc.: 100.00%] [G loss: 0.021310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27298 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.095239]\n",
      "epoch:34 step:27299 [D loss: 0.005318, acc.: 100.00%] [G loss: 0.020732]\n",
      "epoch:34 step:27300 [D loss: 0.004623, acc.: 100.00%] [G loss: 0.031372]\n",
      "epoch:34 step:27301 [D loss: 0.025415, acc.: 99.22%] [G loss: 0.032018]\n",
      "epoch:34 step:27302 [D loss: 0.001924, acc.: 100.00%] [G loss: 0.023792]\n",
      "epoch:34 step:27303 [D loss: 0.010241, acc.: 100.00%] [G loss: 0.094014]\n",
      "epoch:34 step:27304 [D loss: 0.004154, acc.: 100.00%] [G loss: 0.032712]\n",
      "epoch:34 step:27305 [D loss: 0.037038, acc.: 99.22%] [G loss: 0.011409]\n",
      "epoch:34 step:27306 [D loss: 0.008984, acc.: 100.00%] [G loss: 0.014596]\n",
      "epoch:34 step:27307 [D loss: 0.001640, acc.: 100.00%] [G loss: 0.005798]\n",
      "epoch:34 step:27308 [D loss: 0.004538, acc.: 100.00%] [G loss: 0.005361]\n",
      "epoch:34 step:27309 [D loss: 0.006313, acc.: 100.00%] [G loss: 0.013352]\n",
      "epoch:34 step:27310 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.001402]\n",
      "epoch:34 step:27311 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.003075]\n",
      "epoch:34 step:27312 [D loss: 0.001816, acc.: 100.00%] [G loss: 0.004485]\n",
      "epoch:34 step:27313 [D loss: 0.002678, acc.: 100.00%] [G loss: 0.007896]\n",
      "epoch:34 step:27314 [D loss: 0.001876, acc.: 100.00%] [G loss: 0.014695]\n",
      "epoch:34 step:27315 [D loss: 0.001877, acc.: 100.00%] [G loss: 0.023952]\n",
      "epoch:34 step:27316 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.005912]\n",
      "epoch:34 step:27317 [D loss: 0.001164, acc.: 100.00%] [G loss: 0.009559]\n",
      "epoch:34 step:27318 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.015285]\n",
      "epoch:34 step:27319 [D loss: 0.001495, acc.: 100.00%] [G loss: 0.008786]\n",
      "epoch:34 step:27320 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.004777]\n",
      "epoch:34 step:27321 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:34 step:27322 [D loss: 0.001731, acc.: 100.00%] [G loss: 0.002241]\n",
      "epoch:34 step:27323 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.000842]\n",
      "epoch:34 step:27324 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.004244]\n",
      "epoch:34 step:27325 [D loss: 0.001526, acc.: 100.00%] [G loss: 0.005656]\n",
      "epoch:34 step:27326 [D loss: 0.001375, acc.: 100.00%] [G loss: 0.001824]\n",
      "epoch:34 step:27327 [D loss: 0.001956, acc.: 100.00%] [G loss: 0.003040]\n",
      "epoch:34 step:27328 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.004549]\n",
      "epoch:34 step:27329 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.002313]\n",
      "epoch:34 step:27330 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.005796]\n",
      "epoch:34 step:27331 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.005943]\n",
      "epoch:34 step:27332 [D loss: 0.003540, acc.: 100.00%] [G loss: 0.002484]\n",
      "epoch:34 step:27333 [D loss: 0.001696, acc.: 100.00%] [G loss: 0.001971]\n",
      "epoch:34 step:27334 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.005050]\n",
      "epoch:34 step:27335 [D loss: 0.002124, acc.: 100.00%] [G loss: 0.001000]\n",
      "epoch:35 step:27336 [D loss: 0.002086, acc.: 100.00%] [G loss: 0.003845]\n",
      "epoch:35 step:27337 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.002309]\n",
      "epoch:35 step:27338 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.001090]\n",
      "epoch:35 step:27339 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.011854]\n",
      "epoch:35 step:27340 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.001703]\n",
      "epoch:35 step:27341 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:35 step:27342 [D loss: 0.001744, acc.: 100.00%] [G loss: 0.000908]\n",
      "epoch:35 step:27343 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:35 step:27344 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.002302]\n",
      "epoch:35 step:27345 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.001399]\n",
      "epoch:35 step:27346 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.002528]\n",
      "epoch:35 step:27347 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.005646]\n",
      "epoch:35 step:27348 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.001310]\n",
      "epoch:35 step:27349 [D loss: 0.000809, acc.: 100.00%] [G loss: 0.002065]\n",
      "epoch:35 step:27350 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:35 step:27351 [D loss: 0.001405, acc.: 100.00%] [G loss: 0.002366]\n",
      "epoch:35 step:27352 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:35 step:27353 [D loss: 0.003333, acc.: 100.00%] [G loss: 0.003463]\n",
      "epoch:35 step:27354 [D loss: 0.001871, acc.: 100.00%] [G loss: 0.004734]\n",
      "epoch:35 step:27355 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.001029]\n",
      "epoch:35 step:27356 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.003297]\n",
      "epoch:35 step:27357 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.000789]\n",
      "epoch:35 step:27358 [D loss: 0.001102, acc.: 100.00%] [G loss: 0.000671]\n",
      "epoch:35 step:27359 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.002015]\n",
      "epoch:35 step:27360 [D loss: 0.001188, acc.: 100.00%] [G loss: 0.000834]\n",
      "epoch:35 step:27361 [D loss: 0.000819, acc.: 100.00%] [G loss: 0.000835]\n",
      "epoch:35 step:27362 [D loss: 0.002394, acc.: 100.00%] [G loss: 0.012348]\n",
      "epoch:35 step:27363 [D loss: 0.005331, acc.: 100.00%] [G loss: 0.002574]\n",
      "epoch:35 step:27364 [D loss: 0.052460, acc.: 98.44%] [G loss: 0.004462]\n",
      "epoch:35 step:27365 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.001970]\n",
      "epoch:35 step:27366 [D loss: 0.010195, acc.: 100.00%] [G loss: 0.013340]\n",
      "epoch:35 step:27367 [D loss: 0.007142, acc.: 100.00%] [G loss: 0.003518]\n",
      "epoch:35 step:27368 [D loss: 0.002967, acc.: 100.00%] [G loss: 0.007315]\n",
      "epoch:35 step:27369 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.001683]\n",
      "epoch:35 step:27370 [D loss: 0.001821, acc.: 100.00%] [G loss: 0.009177]\n",
      "epoch:35 step:27371 [D loss: 0.001077, acc.: 100.00%] [G loss: 0.007652]\n",
      "epoch:35 step:27372 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.006800]\n",
      "epoch:35 step:27373 [D loss: 0.001211, acc.: 100.00%] [G loss: 0.010616]\n",
      "epoch:35 step:27374 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.004880]\n",
      "epoch:35 step:27375 [D loss: 0.004036, acc.: 100.00%] [G loss: 0.001272]\n",
      "epoch:35 step:27376 [D loss: 0.001682, acc.: 100.00%] [G loss: 0.002701]\n",
      "epoch:35 step:27377 [D loss: 0.001068, acc.: 100.00%] [G loss: 0.001826]\n",
      "epoch:35 step:27378 [D loss: 0.007397, acc.: 100.00%] [G loss: 0.002400]\n",
      "epoch:35 step:27379 [D loss: 0.001493, acc.: 100.00%] [G loss: 0.002920]\n",
      "epoch:35 step:27380 [D loss: 0.002829, acc.: 100.00%] [G loss: 0.001459]\n",
      "epoch:35 step:27381 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.006259]\n",
      "epoch:35 step:27382 [D loss: 0.018549, acc.: 100.00%] [G loss: 0.006265]\n",
      "epoch:35 step:27383 [D loss: 0.004266, acc.: 100.00%] [G loss: 0.004564]\n",
      "epoch:35 step:27384 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.005809]\n",
      "epoch:35 step:27385 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.005826]\n",
      "epoch:35 step:27386 [D loss: 0.005038, acc.: 100.00%] [G loss: 0.003041]\n",
      "epoch:35 step:27387 [D loss: 0.001046, acc.: 100.00%] [G loss: 0.007116]\n",
      "epoch:35 step:27388 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.005401]\n",
      "epoch:35 step:27389 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.040149]\n",
      "epoch:35 step:27390 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.001239]\n",
      "epoch:35 step:27391 [D loss: 0.046147, acc.: 99.22%] [G loss: 0.001739]\n",
      "epoch:35 step:27392 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:35 step:27393 [D loss: 0.001752, acc.: 100.00%] [G loss: 0.000384]\n",
      "epoch:35 step:27394 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.002744]\n",
      "epoch:35 step:27395 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.002401]\n",
      "epoch:35 step:27396 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.001403]\n",
      "epoch:35 step:27397 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001119]\n",
      "epoch:35 step:27398 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.000824]\n",
      "epoch:35 step:27399 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.000918]\n",
      "epoch:35 step:27400 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:35 step:27401 [D loss: 0.000925, acc.: 100.00%] [G loss: 0.001291]\n",
      "epoch:35 step:27402 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.001731]\n",
      "epoch:35 step:27403 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:35 step:27404 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000675]\n",
      "epoch:35 step:27405 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:35 step:27406 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:35 step:27407 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:35 step:27408 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:35 step:27409 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.000870]\n",
      "epoch:35 step:27410 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27411 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:35 step:27412 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:35 step:27413 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:35 step:27414 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:35 step:27415 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:35 step:27416 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:35 step:27417 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000709]\n",
      "epoch:35 step:27418 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000811]\n",
      "epoch:35 step:27419 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.001841]\n",
      "epoch:35 step:27420 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:35 step:27421 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000696]\n",
      "epoch:35 step:27422 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:35 step:27423 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000526]\n",
      "epoch:35 step:27424 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:35 step:27425 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.001427]\n",
      "epoch:35 step:27426 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.001626]\n",
      "epoch:35 step:27427 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:35 step:27428 [D loss: 0.001709, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:35 step:27429 [D loss: 0.001901, acc.: 100.00%] [G loss: 0.000938]\n",
      "epoch:35 step:27430 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001714]\n",
      "epoch:35 step:27431 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.001936]\n",
      "epoch:35 step:27432 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:35 step:27433 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.000615]\n",
      "epoch:35 step:27434 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:35 step:27435 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:35 step:27436 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000641]\n",
      "epoch:35 step:27437 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:35 step:27438 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000791]\n",
      "epoch:35 step:27439 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000876]\n",
      "epoch:35 step:27440 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:35 step:27441 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:35 step:27442 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:35 step:27443 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001115]\n",
      "epoch:35 step:27444 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000654]\n",
      "epoch:35 step:27445 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:35 step:27446 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:35 step:27447 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:35 step:27448 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:35 step:27449 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:35 step:27450 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.001885]\n",
      "epoch:35 step:27451 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:35 step:27452 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:35 step:27453 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:35 step:27454 [D loss: 0.002534, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:35 step:27455 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:35 step:27456 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:35 step:27457 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:35 step:27458 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000852]\n",
      "epoch:35 step:27459 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:35 step:27460 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:35 step:27461 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:35 step:27462 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000435]\n",
      "epoch:35 step:27463 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000612]\n",
      "epoch:35 step:27464 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000435]\n",
      "epoch:35 step:27465 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000460]\n",
      "epoch:35 step:27466 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:35 step:27467 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000891]\n",
      "epoch:35 step:27468 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:35 step:27469 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000758]\n",
      "epoch:35 step:27470 [D loss: 0.002821, acc.: 100.00%] [G loss: 0.001323]\n",
      "epoch:35 step:27471 [D loss: 0.001045, acc.: 100.00%] [G loss: 0.002341]\n",
      "epoch:35 step:27472 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000869]\n",
      "epoch:35 step:27473 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000671]\n",
      "epoch:35 step:27474 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000829]\n",
      "epoch:35 step:27475 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.001202]\n",
      "epoch:35 step:27476 [D loss: 0.001065, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:35 step:27477 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:35 step:27478 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000577]\n",
      "epoch:35 step:27479 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:35 step:27480 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:35 step:27481 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:35 step:27482 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:35 step:27483 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000424]\n",
      "epoch:35 step:27484 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000765]\n",
      "epoch:35 step:27485 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000330]\n",
      "epoch:35 step:27486 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.001085]\n",
      "epoch:35 step:27487 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:35 step:27488 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000932]\n",
      "epoch:35 step:27489 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:35 step:27490 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:35 step:27491 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:35 step:27492 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.003659]\n",
      "epoch:35 step:27493 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:35 step:27494 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:35 step:27495 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:35 step:27496 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001647]\n",
      "epoch:35 step:27497 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000686]\n",
      "epoch:35 step:27498 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:35 step:27499 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001603]\n",
      "epoch:35 step:27500 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:35 step:27501 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:35 step:27502 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:35 step:27503 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:35 step:27504 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:35 step:27505 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:35 step:27506 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:35 step:27507 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:35 step:27508 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:35 step:27509 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.000416]\n",
      "epoch:35 step:27510 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000523]\n",
      "epoch:35 step:27511 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.004359]\n",
      "epoch:35 step:27512 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:35 step:27513 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000561]\n",
      "epoch:35 step:27514 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:35 step:27515 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:35 step:27516 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:35 step:27517 [D loss: 0.001480, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:35 step:27518 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:35 step:27519 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:35 step:27520 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:35 step:27521 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:35 step:27522 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:35 step:27523 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27524 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:35 step:27525 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:35 step:27526 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:35 step:27527 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:35 step:27528 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:35 step:27529 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:35 step:27530 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:35 step:27531 [D loss: 0.011342, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:35 step:27532 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:35 step:27533 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.001865]\n",
      "epoch:35 step:27534 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:35 step:27535 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:35 step:27536 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:35 step:27537 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000863]\n",
      "epoch:35 step:27538 [D loss: 0.002197, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:35 step:27539 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:35 step:27540 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000852]\n",
      "epoch:35 step:27541 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.000699]\n",
      "epoch:35 step:27542 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.001088]\n",
      "epoch:35 step:27543 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.001276]\n",
      "epoch:35 step:27544 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.001326]\n",
      "epoch:35 step:27545 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.001030]\n",
      "epoch:35 step:27546 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:35 step:27547 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002026]\n",
      "epoch:35 step:27548 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:35 step:27549 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000810]\n",
      "epoch:35 step:27550 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:35 step:27551 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:35 step:27552 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:35 step:27553 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:35 step:27554 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:35 step:27555 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:35 step:27556 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000606]\n",
      "epoch:35 step:27557 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000468]\n",
      "epoch:35 step:27558 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:35 step:27559 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:35 step:27560 [D loss: 0.009841, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:35 step:27561 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:35 step:27562 [D loss: 0.020582, acc.: 100.00%] [G loss: 0.029552]\n",
      "epoch:35 step:27563 [D loss: 0.001485, acc.: 100.00%] [G loss: 0.027838]\n",
      "epoch:35 step:27564 [D loss: 0.018142, acc.: 99.22%] [G loss: 0.000547]\n",
      "epoch:35 step:27565 [D loss: 0.001476, acc.: 100.00%] [G loss: 0.010831]\n",
      "epoch:35 step:27566 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.001523]\n",
      "epoch:35 step:27567 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:35 step:27568 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:35 step:27569 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001787]\n",
      "epoch:35 step:27570 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.009723]\n",
      "epoch:35 step:27571 [D loss: 0.004259, acc.: 100.00%] [G loss: 0.000587]\n",
      "epoch:35 step:27572 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.001793]\n",
      "epoch:35 step:27573 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002532]\n",
      "epoch:35 step:27574 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:35 step:27575 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:35 step:27576 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:35 step:27577 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:35 step:27578 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000384]\n",
      "epoch:35 step:27579 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:35 step:27580 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:35 step:27581 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:35 step:27582 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:35 step:27583 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:35 step:27584 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.001475]\n",
      "epoch:35 step:27585 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:35 step:27586 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.001300]\n",
      "epoch:35 step:27587 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:35 step:27588 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.001220]\n",
      "epoch:35 step:27589 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000731]\n",
      "epoch:35 step:27590 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.001985]\n",
      "epoch:35 step:27591 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:35 step:27592 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001975]\n",
      "epoch:35 step:27593 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:35 step:27594 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:35 step:27595 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:35 step:27596 [D loss: 0.000593, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:35 step:27597 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:35 step:27598 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.000507]\n",
      "epoch:35 step:27599 [D loss: 0.001849, acc.: 100.00%] [G loss: 0.001368]\n",
      "epoch:35 step:27600 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:35 step:27601 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000955]\n",
      "epoch:35 step:27602 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:35 step:27603 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:35 step:27604 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:35 step:27605 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:35 step:27606 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:35 step:27607 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:35 step:27608 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:35 step:27609 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:35 step:27610 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000988]\n",
      "epoch:35 step:27611 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:35 step:27612 [D loss: 0.002867, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:35 step:27613 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:35 step:27614 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:35 step:27615 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:35 step:27616 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:35 step:27617 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:35 step:27618 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:35 step:27619 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:35 step:27620 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:35 step:27621 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:35 step:27622 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:35 step:27623 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:35 step:27624 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:35 step:27625 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:35 step:27626 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:35 step:27627 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:35 step:27628 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:35 step:27629 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:35 step:27630 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:35 step:27631 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:35 step:27632 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:35 step:27633 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:35 step:27634 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:35 step:27635 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:35 step:27636 [D loss: 0.002314, acc.: 100.00%] [G loss: 0.000181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27637 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:35 step:27638 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:35 step:27639 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:35 step:27640 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:35 step:27641 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:35 step:27642 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:35 step:27643 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:35 step:27644 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:35 step:27645 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:35 step:27646 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:35 step:27647 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:35 step:27648 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:35 step:27649 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:35 step:27650 [D loss: 0.004660, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:35 step:27651 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:35 step:27652 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:35 step:27653 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:35 step:27654 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:35 step:27655 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:35 step:27656 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:35 step:27657 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:35 step:27658 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:35 step:27659 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:35 step:27660 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:35 step:27661 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:35 step:27662 [D loss: 0.001704, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:35 step:27663 [D loss: 0.235846, acc.: 87.50%] [G loss: 1.928916]\n",
      "epoch:35 step:27664 [D loss: 0.073675, acc.: 97.66%] [G loss: 1.121595]\n",
      "epoch:35 step:27665 [D loss: 0.596315, acc.: 69.53%] [G loss: 2.595929]\n",
      "epoch:35 step:27666 [D loss: 0.021451, acc.: 99.22%] [G loss: 4.473092]\n",
      "epoch:35 step:27667 [D loss: 0.003983, acc.: 100.00%] [G loss: 2.686274]\n",
      "epoch:35 step:27668 [D loss: 0.013043, acc.: 100.00%] [G loss: 1.497073]\n",
      "epoch:35 step:27669 [D loss: 0.094184, acc.: 95.31%] [G loss: 4.990239]\n",
      "epoch:35 step:27670 [D loss: 0.084929, acc.: 98.44%] [G loss: 0.215847]\n",
      "epoch:35 step:27671 [D loss: 0.052650, acc.: 98.44%] [G loss: 3.332785]\n",
      "epoch:35 step:27672 [D loss: 0.025976, acc.: 100.00%] [G loss: 0.014112]\n",
      "epoch:35 step:27673 [D loss: 0.036441, acc.: 100.00%] [G loss: 1.062256]\n",
      "epoch:35 step:27674 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.800382]\n",
      "epoch:35 step:27675 [D loss: 0.328549, acc.: 78.12%] [G loss: 0.966133]\n",
      "epoch:35 step:27676 [D loss: 0.105773, acc.: 96.09%] [G loss: 6.257377]\n",
      "epoch:35 step:27677 [D loss: 1.184212, acc.: 59.38%] [G loss: 0.779839]\n",
      "epoch:35 step:27678 [D loss: 2.504898, acc.: 53.91%] [G loss: 5.806258]\n",
      "epoch:35 step:27679 [D loss: 0.174415, acc.: 90.62%] [G loss: 6.365943]\n",
      "epoch:35 step:27680 [D loss: 0.790860, acc.: 65.62%] [G loss: 3.246323]\n",
      "epoch:35 step:27681 [D loss: 0.027787, acc.: 99.22%] [G loss: 2.487138]\n",
      "epoch:35 step:27682 [D loss: 0.026011, acc.: 100.00%] [G loss: 4.214863]\n",
      "epoch:35 step:27683 [D loss: 0.039173, acc.: 100.00%] [G loss: 1.996524]\n",
      "epoch:35 step:27684 [D loss: 0.096754, acc.: 96.88%] [G loss: 2.604839]\n",
      "epoch:35 step:27685 [D loss: 0.031583, acc.: 100.00%] [G loss: 2.749741]\n",
      "epoch:35 step:27686 [D loss: 0.040496, acc.: 100.00%] [G loss: 3.016514]\n",
      "epoch:35 step:27687 [D loss: 0.041538, acc.: 100.00%] [G loss: 3.420971]\n",
      "epoch:35 step:27688 [D loss: 0.023254, acc.: 100.00%] [G loss: 3.146585]\n",
      "epoch:35 step:27689 [D loss: 0.029513, acc.: 100.00%] [G loss: 3.426562]\n",
      "epoch:35 step:27690 [D loss: 0.059276, acc.: 100.00%] [G loss: 0.220374]\n",
      "epoch:35 step:27691 [D loss: 0.082286, acc.: 95.31%] [G loss: 0.380687]\n",
      "epoch:35 step:27692 [D loss: 0.140749, acc.: 95.31%] [G loss: 0.300499]\n",
      "epoch:35 step:27693 [D loss: 0.074481, acc.: 96.88%] [G loss: 4.418660]\n",
      "epoch:35 step:27694 [D loss: 0.055462, acc.: 100.00%] [G loss: 3.643095]\n",
      "epoch:35 step:27695 [D loss: 0.067547, acc.: 98.44%] [G loss: 3.350132]\n",
      "epoch:35 step:27696 [D loss: 0.025135, acc.: 100.00%] [G loss: 0.668534]\n",
      "epoch:35 step:27697 [D loss: 0.094081, acc.: 99.22%] [G loss: 3.867863]\n",
      "epoch:35 step:27698 [D loss: 0.073594, acc.: 100.00%] [G loss: 0.041933]\n",
      "epoch:35 step:27699 [D loss: 0.776841, acc.: 57.81%] [G loss: 3.537014]\n",
      "epoch:35 step:27700 [D loss: 0.293578, acc.: 82.81%] [G loss: 6.172155]\n",
      "epoch:35 step:27701 [D loss: 0.070782, acc.: 97.66%] [G loss: 0.080847]\n",
      "epoch:35 step:27702 [D loss: 0.137718, acc.: 95.31%] [G loss: 2.246168]\n",
      "epoch:35 step:27703 [D loss: 0.022988, acc.: 100.00%] [G loss: 2.045321]\n",
      "epoch:35 step:27704 [D loss: 0.099604, acc.: 96.88%] [G loss: 1.617253]\n",
      "epoch:35 step:27705 [D loss: 0.077140, acc.: 98.44%] [G loss: 0.827446]\n",
      "epoch:35 step:27706 [D loss: 0.020764, acc.: 100.00%] [G loss: 0.075849]\n",
      "epoch:35 step:27707 [D loss: 0.347600, acc.: 83.59%] [G loss: 0.738708]\n",
      "epoch:35 step:27708 [D loss: 0.045777, acc.: 98.44%] [G loss: 3.431807]\n",
      "epoch:35 step:27709 [D loss: 0.415963, acc.: 85.94%] [G loss: 0.749795]\n",
      "epoch:35 step:27710 [D loss: 0.042190, acc.: 100.00%] [G loss: 0.092254]\n",
      "epoch:35 step:27711 [D loss: 0.094765, acc.: 96.09%] [G loss: 0.902978]\n",
      "epoch:35 step:27712 [D loss: 0.030732, acc.: 99.22%] [G loss: 1.185403]\n",
      "epoch:35 step:27713 [D loss: 0.054764, acc.: 100.00%] [G loss: 0.722837]\n",
      "epoch:35 step:27714 [D loss: 0.021232, acc.: 99.22%] [G loss: 0.601317]\n",
      "epoch:35 step:27715 [D loss: 0.057253, acc.: 98.44%] [G loss: 0.175876]\n",
      "epoch:35 step:27716 [D loss: 0.127628, acc.: 96.09%] [G loss: 4.358912]\n",
      "epoch:35 step:27717 [D loss: 0.084443, acc.: 96.88%] [G loss: 0.142083]\n",
      "epoch:35 step:27718 [D loss: 0.029349, acc.: 100.00%] [G loss: 0.096469]\n",
      "epoch:35 step:27719 [D loss: 0.027121, acc.: 100.00%] [G loss: 0.518391]\n",
      "epoch:35 step:27720 [D loss: 0.019400, acc.: 100.00%] [G loss: 0.269220]\n",
      "epoch:35 step:27721 [D loss: 0.204463, acc.: 92.19%] [G loss: 2.651125]\n",
      "epoch:35 step:27722 [D loss: 0.023546, acc.: 100.00%] [G loss: 3.076716]\n",
      "epoch:35 step:27723 [D loss: 1.363811, acc.: 41.41%] [G loss: 5.043345]\n",
      "epoch:35 step:27724 [D loss: 0.040665, acc.: 98.44%] [G loss: 6.357404]\n",
      "epoch:35 step:27725 [D loss: 0.239515, acc.: 88.28%] [G loss: 0.536125]\n",
      "epoch:35 step:27726 [D loss: 0.311177, acc.: 87.50%] [G loss: 3.746184]\n",
      "epoch:35 step:27727 [D loss: 0.026293, acc.: 99.22%] [G loss: 0.673229]\n",
      "epoch:35 step:27728 [D loss: 0.159037, acc.: 93.75%] [G loss: 0.055584]\n",
      "epoch:35 step:27729 [D loss: 0.034806, acc.: 100.00%] [G loss: 0.134396]\n",
      "epoch:35 step:27730 [D loss: 0.177321, acc.: 95.31%] [G loss: 0.428107]\n",
      "epoch:35 step:27731 [D loss: 0.434757, acc.: 79.69%] [G loss: 1.092017]\n",
      "epoch:35 step:27732 [D loss: 0.608323, acc.: 72.66%] [G loss: 0.832705]\n",
      "epoch:35 step:27733 [D loss: 0.097442, acc.: 97.66%] [G loss: 3.043544]\n",
      "epoch:35 step:27734 [D loss: 0.179071, acc.: 94.53%] [G loss: 0.335615]\n",
      "epoch:35 step:27735 [D loss: 0.020579, acc.: 100.00%] [G loss: 0.706179]\n",
      "epoch:35 step:27736 [D loss: 0.026914, acc.: 100.00%] [G loss: 4.516708]\n",
      "epoch:35 step:27737 [D loss: 0.049454, acc.: 99.22%] [G loss: 1.955711]\n",
      "epoch:35 step:27738 [D loss: 0.493163, acc.: 75.00%] [G loss: 4.394578]\n",
      "epoch:35 step:27739 [D loss: 0.211235, acc.: 90.62%] [G loss: 3.487812]\n",
      "epoch:35 step:27740 [D loss: 0.231061, acc.: 91.41%] [G loss: 0.905071]\n",
      "epoch:35 step:27741 [D loss: 0.063066, acc.: 97.66%] [G loss: 3.387966]\n",
      "epoch:35 step:27742 [D loss: 0.035726, acc.: 99.22%] [G loss: 0.899558]\n",
      "epoch:35 step:27743 [D loss: 0.031179, acc.: 99.22%] [G loss: 0.687860]\n",
      "epoch:35 step:27744 [D loss: 0.071320, acc.: 98.44%] [G loss: 0.402767]\n",
      "epoch:35 step:27745 [D loss: 0.012844, acc.: 100.00%] [G loss: 0.312948]\n",
      "epoch:35 step:27746 [D loss: 0.425939, acc.: 75.00%] [G loss: 1.419396]\n",
      "epoch:35 step:27747 [D loss: 0.034401, acc.: 100.00%] [G loss: 1.782749]\n",
      "epoch:35 step:27748 [D loss: 0.237831, acc.: 89.06%] [G loss: 0.073480]\n",
      "epoch:35 step:27749 [D loss: 0.044125, acc.: 99.22%] [G loss: 0.109048]\n",
      "epoch:35 step:27750 [D loss: 0.122476, acc.: 95.31%] [G loss: 1.601603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27751 [D loss: 0.018318, acc.: 100.00%] [G loss: 5.912242]\n",
      "epoch:35 step:27752 [D loss: 0.029244, acc.: 100.00%] [G loss: 6.447320]\n",
      "epoch:35 step:27753 [D loss: 0.020026, acc.: 100.00%] [G loss: 1.496980]\n",
      "epoch:35 step:27754 [D loss: 0.035079, acc.: 100.00%] [G loss: 1.353793]\n",
      "epoch:35 step:27755 [D loss: 0.211220, acc.: 92.19%] [G loss: 2.349102]\n",
      "epoch:35 step:27756 [D loss: 0.040352, acc.: 99.22%] [G loss: 2.183253]\n",
      "epoch:35 step:27757 [D loss: 0.059237, acc.: 99.22%] [G loss: 1.930870]\n",
      "epoch:35 step:27758 [D loss: 0.020829, acc.: 100.00%] [G loss: 2.747559]\n",
      "epoch:35 step:27759 [D loss: 0.026756, acc.: 100.00%] [G loss: 2.411749]\n",
      "epoch:35 step:27760 [D loss: 0.014745, acc.: 100.00%] [G loss: 3.227326]\n",
      "epoch:35 step:27761 [D loss: 0.032554, acc.: 99.22%] [G loss: 3.000951]\n",
      "epoch:35 step:27762 [D loss: 0.005712, acc.: 100.00%] [G loss: 2.367642]\n",
      "epoch:35 step:27763 [D loss: 0.003915, acc.: 100.00%] [G loss: 1.914083]\n",
      "epoch:35 step:27764 [D loss: 0.004853, acc.: 100.00%] [G loss: 1.305554]\n",
      "epoch:35 step:27765 [D loss: 0.040458, acc.: 98.44%] [G loss: 0.318283]\n",
      "epoch:35 step:27766 [D loss: 0.047294, acc.: 98.44%] [G loss: 0.230378]\n",
      "epoch:35 step:27767 [D loss: 0.025849, acc.: 100.00%] [G loss: 0.260749]\n",
      "epoch:35 step:27768 [D loss: 0.073813, acc.: 100.00%] [G loss: 0.225382]\n",
      "epoch:35 step:27769 [D loss: 0.003435, acc.: 100.00%] [G loss: 2.830146]\n",
      "epoch:35 step:27770 [D loss: 0.001622, acc.: 100.00%] [G loss: 1.715136]\n",
      "epoch:35 step:27771 [D loss: 0.036025, acc.: 98.44%] [G loss: 1.933581]\n",
      "epoch:35 step:27772 [D loss: 0.000864, acc.: 100.00%] [G loss: 1.244198]\n",
      "epoch:35 step:27773 [D loss: 0.004692, acc.: 100.00%] [G loss: 0.870609]\n",
      "epoch:35 step:27774 [D loss: 0.005765, acc.: 100.00%] [G loss: 0.604500]\n",
      "epoch:35 step:27775 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.267942]\n",
      "epoch:35 step:27776 [D loss: 0.003589, acc.: 100.00%] [G loss: 0.147901]\n",
      "epoch:35 step:27777 [D loss: 0.003942, acc.: 100.00%] [G loss: 0.073079]\n",
      "epoch:35 step:27778 [D loss: 0.011537, acc.: 100.00%] [G loss: 0.114130]\n",
      "epoch:35 step:27779 [D loss: 0.002777, acc.: 100.00%] [G loss: 0.157724]\n",
      "epoch:35 step:27780 [D loss: 0.045720, acc.: 100.00%] [G loss: 0.369238]\n",
      "epoch:35 step:27781 [D loss: 0.011345, acc.: 100.00%] [G loss: 0.033513]\n",
      "epoch:35 step:27782 [D loss: 0.009050, acc.: 100.00%] [G loss: 1.077042]\n",
      "epoch:35 step:27783 [D loss: 0.030785, acc.: 99.22%] [G loss: 0.038834]\n",
      "epoch:35 step:27784 [D loss: 0.017177, acc.: 100.00%] [G loss: 0.858863]\n",
      "epoch:35 step:27785 [D loss: 0.003203, acc.: 100.00%] [G loss: 0.011699]\n",
      "epoch:35 step:27786 [D loss: 0.002046, acc.: 100.00%] [G loss: 1.218465]\n",
      "epoch:35 step:27787 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.283362]\n",
      "epoch:35 step:27788 [D loss: 0.031987, acc.: 98.44%] [G loss: 0.349613]\n",
      "epoch:35 step:27789 [D loss: 0.001133, acc.: 100.00%] [G loss: 1.691936]\n",
      "epoch:35 step:27790 [D loss: 0.009707, acc.: 100.00%] [G loss: 0.331170]\n",
      "epoch:35 step:27791 [D loss: 0.003570, acc.: 100.00%] [G loss: 0.295167]\n",
      "epoch:35 step:27792 [D loss: 0.097511, acc.: 98.44%] [G loss: 2.870396]\n",
      "epoch:35 step:27793 [D loss: 0.063102, acc.: 97.66%] [G loss: 4.048490]\n",
      "epoch:35 step:27794 [D loss: 0.219314, acc.: 89.06%] [G loss: 0.195343]\n",
      "epoch:35 step:27795 [D loss: 0.341230, acc.: 84.38%] [G loss: 10.192514]\n",
      "epoch:35 step:27796 [D loss: 0.005291, acc.: 100.00%] [G loss: 8.234693]\n",
      "epoch:35 step:27797 [D loss: 0.538052, acc.: 76.56%] [G loss: 7.498713]\n",
      "epoch:35 step:27798 [D loss: 0.824030, acc.: 75.00%] [G loss: 9.468349]\n",
      "epoch:35 step:27799 [D loss: 0.032737, acc.: 98.44%] [G loss: 0.711053]\n",
      "epoch:35 step:27800 [D loss: 0.167045, acc.: 93.75%] [G loss: 9.303179]\n",
      "epoch:35 step:27801 [D loss: 0.053350, acc.: 97.66%] [G loss: 7.279313]\n",
      "epoch:35 step:27802 [D loss: 0.042355, acc.: 97.66%] [G loss: 6.352983]\n",
      "epoch:35 step:27803 [D loss: 0.069558, acc.: 100.00%] [G loss: 2.149616]\n",
      "epoch:35 step:27804 [D loss: 0.021230, acc.: 100.00%] [G loss: 6.428677]\n",
      "epoch:35 step:27805 [D loss: 0.011777, acc.: 100.00%] [G loss: 5.340472]\n",
      "epoch:35 step:27806 [D loss: 0.014262, acc.: 100.00%] [G loss: 0.006724]\n",
      "epoch:35 step:27807 [D loss: 0.001646, acc.: 100.00%] [G loss: 0.017141]\n",
      "epoch:35 step:27808 [D loss: 0.008095, acc.: 100.00%] [G loss: 0.172653]\n",
      "epoch:35 step:27809 [D loss: 0.026211, acc.: 100.00%] [G loss: 0.003191]\n",
      "epoch:35 step:27810 [D loss: 0.532998, acc.: 72.66%] [G loss: 8.125971]\n",
      "epoch:35 step:27811 [D loss: 0.724156, acc.: 63.28%] [G loss: 3.091744]\n",
      "epoch:35 step:27812 [D loss: 0.007638, acc.: 100.00%] [G loss: 3.809769]\n",
      "epoch:35 step:27813 [D loss: 0.006549, acc.: 100.00%] [G loss: 0.172809]\n",
      "epoch:35 step:27814 [D loss: 0.028118, acc.: 99.22%] [G loss: 2.039137]\n",
      "epoch:35 step:27815 [D loss: 0.008151, acc.: 100.00%] [G loss: 1.957764]\n",
      "epoch:35 step:27816 [D loss: 0.011332, acc.: 100.00%] [G loss: 0.634752]\n",
      "epoch:35 step:27817 [D loss: 0.010451, acc.: 100.00%] [G loss: 0.377870]\n",
      "epoch:35 step:27818 [D loss: 0.012667, acc.: 100.00%] [G loss: 0.205949]\n",
      "epoch:35 step:27819 [D loss: 0.022916, acc.: 100.00%] [G loss: 0.158873]\n",
      "epoch:35 step:27820 [D loss: 0.012424, acc.: 100.00%] [G loss: 0.455845]\n",
      "epoch:35 step:27821 [D loss: 0.048716, acc.: 100.00%] [G loss: 0.804558]\n",
      "epoch:35 step:27822 [D loss: 0.022283, acc.: 100.00%] [G loss: 0.301466]\n",
      "epoch:35 step:27823 [D loss: 0.019245, acc.: 100.00%] [G loss: 0.474263]\n",
      "epoch:35 step:27824 [D loss: 0.028775, acc.: 99.22%] [G loss: 0.307171]\n",
      "epoch:35 step:27825 [D loss: 0.026668, acc.: 99.22%] [G loss: 0.269591]\n",
      "epoch:35 step:27826 [D loss: 0.734260, acc.: 67.19%] [G loss: 7.277409]\n",
      "epoch:35 step:27827 [D loss: 0.849761, acc.: 66.41%] [G loss: 2.966428]\n",
      "epoch:35 step:27828 [D loss: 0.011232, acc.: 100.00%] [G loss: 1.396219]\n",
      "epoch:35 step:27829 [D loss: 0.006516, acc.: 100.00%] [G loss: 0.916429]\n",
      "epoch:35 step:27830 [D loss: 0.009497, acc.: 100.00%] [G loss: 0.680817]\n",
      "epoch:35 step:27831 [D loss: 0.018198, acc.: 100.00%] [G loss: 0.924031]\n",
      "epoch:35 step:27832 [D loss: 0.042358, acc.: 99.22%] [G loss: 0.232206]\n",
      "epoch:35 step:27833 [D loss: 0.028937, acc.: 100.00%] [G loss: 0.154989]\n",
      "epoch:35 step:27834 [D loss: 0.016140, acc.: 100.00%] [G loss: 0.085009]\n",
      "epoch:35 step:27835 [D loss: 0.005854, acc.: 100.00%] [G loss: 0.146357]\n",
      "epoch:35 step:27836 [D loss: 0.018955, acc.: 100.00%] [G loss: 2.601189]\n",
      "epoch:35 step:27837 [D loss: 0.172700, acc.: 91.41%] [G loss: 0.554767]\n",
      "epoch:35 step:27838 [D loss: 0.021054, acc.: 99.22%] [G loss: 1.105947]\n",
      "epoch:35 step:27839 [D loss: 0.121377, acc.: 95.31%] [G loss: 0.277147]\n",
      "epoch:35 step:27840 [D loss: 0.217915, acc.: 92.97%] [G loss: 0.768080]\n",
      "epoch:35 step:27841 [D loss: 0.040411, acc.: 98.44%] [G loss: 0.340454]\n",
      "epoch:35 step:27842 [D loss: 0.037206, acc.: 99.22%] [G loss: 1.070989]\n",
      "epoch:35 step:27843 [D loss: 0.031186, acc.: 99.22%] [G loss: 0.018335]\n",
      "epoch:35 step:27844 [D loss: 0.134871, acc.: 96.88%] [G loss: 0.132583]\n",
      "epoch:35 step:27845 [D loss: 0.002480, acc.: 100.00%] [G loss: 0.184039]\n",
      "epoch:35 step:27846 [D loss: 0.004760, acc.: 100.00%] [G loss: 0.179130]\n",
      "epoch:35 step:27847 [D loss: 0.001884, acc.: 100.00%] [G loss: 0.225503]\n",
      "epoch:35 step:27848 [D loss: 0.020962, acc.: 100.00%] [G loss: 0.025717]\n",
      "epoch:35 step:27849 [D loss: 0.007164, acc.: 100.00%] [G loss: 0.054429]\n",
      "epoch:35 step:27850 [D loss: 0.007345, acc.: 100.00%] [G loss: 0.053103]\n",
      "epoch:35 step:27851 [D loss: 0.004019, acc.: 100.00%] [G loss: 0.044057]\n",
      "epoch:35 step:27852 [D loss: 0.000894, acc.: 100.00%] [G loss: 0.019077]\n",
      "epoch:35 step:27853 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.019103]\n",
      "epoch:35 step:27854 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.042131]\n",
      "epoch:35 step:27855 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.008851]\n",
      "epoch:35 step:27856 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.009295]\n",
      "epoch:35 step:27857 [D loss: 0.002770, acc.: 100.00%] [G loss: 0.004163]\n",
      "epoch:35 step:27858 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.008967]\n",
      "epoch:35 step:27859 [D loss: 0.006681, acc.: 100.00%] [G loss: 0.013834]\n",
      "epoch:35 step:27860 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.015889]\n",
      "epoch:35 step:27861 [D loss: 0.014630, acc.: 100.00%] [G loss: 0.006883]\n",
      "epoch:35 step:27862 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.007906]\n",
      "epoch:35 step:27863 [D loss: 0.001704, acc.: 100.00%] [G loss: 0.000369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27864 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.004197]\n",
      "epoch:35 step:27865 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.004437]\n",
      "epoch:35 step:27866 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.005594]\n",
      "epoch:35 step:27867 [D loss: 0.001215, acc.: 100.00%] [G loss: 1.812051]\n",
      "epoch:35 step:27868 [D loss: 0.002886, acc.: 100.00%] [G loss: 0.007558]\n",
      "epoch:35 step:27869 [D loss: 0.065217, acc.: 100.00%] [G loss: 0.020560]\n",
      "epoch:35 step:27870 [D loss: 0.004142, acc.: 100.00%] [G loss: 0.037030]\n",
      "epoch:35 step:27871 [D loss: 0.003179, acc.: 100.00%] [G loss: 0.057894]\n",
      "epoch:35 step:27872 [D loss: 0.043471, acc.: 99.22%] [G loss: 0.007553]\n",
      "epoch:35 step:27873 [D loss: 0.000930, acc.: 100.00%] [G loss: 0.005543]\n",
      "epoch:35 step:27874 [D loss: 0.000942, acc.: 100.00%] [G loss: 0.002825]\n",
      "epoch:35 step:27875 [D loss: 0.009459, acc.: 100.00%] [G loss: 0.004258]\n",
      "epoch:35 step:27876 [D loss: 0.012065, acc.: 100.00%] [G loss: 0.005122]\n",
      "epoch:35 step:27877 [D loss: 0.046076, acc.: 98.44%] [G loss: 0.063817]\n",
      "epoch:35 step:27878 [D loss: 0.007537, acc.: 100.00%] [G loss: 0.069892]\n",
      "epoch:35 step:27879 [D loss: 0.003848, acc.: 100.00%] [G loss: 0.039313]\n",
      "epoch:35 step:27880 [D loss: 0.001304, acc.: 100.00%] [G loss: 0.035550]\n",
      "epoch:35 step:27881 [D loss: 0.005029, acc.: 100.00%] [G loss: 0.039979]\n",
      "epoch:35 step:27882 [D loss: 0.019699, acc.: 99.22%] [G loss: 0.009943]\n",
      "epoch:35 step:27883 [D loss: 0.018101, acc.: 100.00%] [G loss: 0.040504]\n",
      "epoch:35 step:27884 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.024138]\n",
      "epoch:35 step:27885 [D loss: 0.005154, acc.: 100.00%] [G loss: 0.028892]\n",
      "epoch:35 step:27886 [D loss: 0.009372, acc.: 100.00%] [G loss: 0.026380]\n",
      "epoch:35 step:27887 [D loss: 0.003071, acc.: 100.00%] [G loss: 0.008596]\n",
      "epoch:35 step:27888 [D loss: 0.001823, acc.: 100.00%] [G loss: 0.009910]\n",
      "epoch:35 step:27889 [D loss: 0.002023, acc.: 100.00%] [G loss: 0.094262]\n",
      "epoch:35 step:27890 [D loss: 0.000925, acc.: 100.00%] [G loss: 0.033896]\n",
      "epoch:35 step:27891 [D loss: 0.006265, acc.: 100.00%] [G loss: 0.011457]\n",
      "epoch:35 step:27892 [D loss: 0.003293, acc.: 100.00%] [G loss: 0.009768]\n",
      "epoch:35 step:27893 [D loss: 0.002454, acc.: 100.00%] [G loss: 0.016326]\n",
      "epoch:35 step:27894 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.012916]\n",
      "epoch:35 step:27895 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.002991]\n",
      "epoch:35 step:27896 [D loss: 0.002614, acc.: 100.00%] [G loss: 0.024354]\n",
      "epoch:35 step:27897 [D loss: 0.001928, acc.: 100.00%] [G loss: 0.006662]\n",
      "epoch:35 step:27898 [D loss: 0.010124, acc.: 100.00%] [G loss: 0.033222]\n",
      "epoch:35 step:27899 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.033471]\n",
      "epoch:35 step:27900 [D loss: 0.013272, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:35 step:27901 [D loss: 0.009937, acc.: 100.00%] [G loss: 0.011373]\n",
      "epoch:35 step:27902 [D loss: 0.013823, acc.: 100.00%] [G loss: 0.043707]\n",
      "epoch:35 step:27903 [D loss: 0.003897, acc.: 100.00%] [G loss: 0.004743]\n",
      "epoch:35 step:27904 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.026378]\n",
      "epoch:35 step:27905 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.042181]\n",
      "epoch:35 step:27906 [D loss: 0.004687, acc.: 100.00%] [G loss: 0.010428]\n",
      "epoch:35 step:27907 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.040723]\n",
      "epoch:35 step:27908 [D loss: 0.028538, acc.: 99.22%] [G loss: 0.030967]\n",
      "epoch:35 step:27909 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.867793]\n",
      "epoch:35 step:27910 [D loss: 0.007047, acc.: 100.00%] [G loss: 0.028677]\n",
      "epoch:35 step:27911 [D loss: 0.002307, acc.: 100.00%] [G loss: 0.009297]\n",
      "epoch:35 step:27912 [D loss: 0.082923, acc.: 98.44%] [G loss: 0.031323]\n",
      "epoch:35 step:27913 [D loss: 0.038792, acc.: 99.22%] [G loss: 0.210332]\n",
      "epoch:35 step:27914 [D loss: 0.008439, acc.: 100.00%] [G loss: 0.033428]\n",
      "epoch:35 step:27915 [D loss: 0.030522, acc.: 99.22%] [G loss: 0.481463]\n",
      "epoch:35 step:27916 [D loss: 0.003479, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:35 step:27917 [D loss: 0.066422, acc.: 97.66%] [G loss: 0.579201]\n",
      "epoch:35 step:27918 [D loss: 0.009990, acc.: 100.00%] [G loss: 0.046641]\n",
      "epoch:35 step:27919 [D loss: 0.005290, acc.: 100.00%] [G loss: 0.036845]\n",
      "epoch:35 step:27920 [D loss: 0.106598, acc.: 94.53%] [G loss: 0.938660]\n",
      "epoch:35 step:27921 [D loss: 0.006860, acc.: 100.00%] [G loss: 2.591216]\n",
      "epoch:35 step:27922 [D loss: 0.287763, acc.: 83.59%] [G loss: 0.000229]\n",
      "epoch:35 step:27923 [D loss: 0.980338, acc.: 65.62%] [G loss: 8.288924]\n",
      "epoch:35 step:27924 [D loss: 0.184856, acc.: 96.09%] [G loss: 9.985387]\n",
      "epoch:35 step:27925 [D loss: 1.036807, acc.: 64.84%] [G loss: 5.161052]\n",
      "epoch:35 step:27926 [D loss: 0.360642, acc.: 81.25%] [G loss: 8.069246]\n",
      "epoch:35 step:27927 [D loss: 0.004804, acc.: 100.00%] [G loss: 8.470695]\n",
      "epoch:35 step:27928 [D loss: 0.210327, acc.: 89.84%] [G loss: 6.820280]\n",
      "epoch:35 step:27929 [D loss: 0.024441, acc.: 99.22%] [G loss: 0.260958]\n",
      "epoch:35 step:27930 [D loss: 0.013230, acc.: 100.00%] [G loss: 5.060134]\n",
      "epoch:35 step:27931 [D loss: 0.034828, acc.: 100.00%] [G loss: 4.774374]\n",
      "epoch:35 step:27932 [D loss: 0.015944, acc.: 100.00%] [G loss: 5.247771]\n",
      "epoch:35 step:27933 [D loss: 0.036459, acc.: 100.00%] [G loss: 6.369359]\n",
      "epoch:35 step:27934 [D loss: 0.031080, acc.: 100.00%] [G loss: 5.152714]\n",
      "epoch:35 step:27935 [D loss: 0.019549, acc.: 99.22%] [G loss: 5.718942]\n",
      "epoch:35 step:27936 [D loss: 0.044337, acc.: 99.22%] [G loss: 4.676581]\n",
      "epoch:35 step:27937 [D loss: 0.011877, acc.: 100.00%] [G loss: 0.888674]\n",
      "epoch:35 step:27938 [D loss: 0.120519, acc.: 94.53%] [G loss: 3.846383]\n",
      "epoch:35 step:27939 [D loss: 0.210980, acc.: 93.75%] [G loss: 3.382534]\n",
      "epoch:35 step:27940 [D loss: 0.001994, acc.: 100.00%] [G loss: 4.798394]\n",
      "epoch:35 step:27941 [D loss: 0.014940, acc.: 100.00%] [G loss: 3.889036]\n",
      "epoch:35 step:27942 [D loss: 0.288338, acc.: 86.72%] [G loss: 7.013834]\n",
      "epoch:35 step:27943 [D loss: 0.022274, acc.: 99.22%] [G loss: 8.112860]\n",
      "epoch:35 step:27944 [D loss: 1.108155, acc.: 60.16%] [G loss: 2.209296]\n",
      "epoch:35 step:27945 [D loss: 1.033059, acc.: 62.50%] [G loss: 7.607898]\n",
      "epoch:35 step:27946 [D loss: 0.012365, acc.: 100.00%] [G loss: 8.586777]\n",
      "epoch:35 step:27947 [D loss: 0.365724, acc.: 85.94%] [G loss: 7.264369]\n",
      "epoch:35 step:27948 [D loss: 0.055570, acc.: 97.66%] [G loss: 0.076759]\n",
      "epoch:35 step:27949 [D loss: 0.008677, acc.: 100.00%] [G loss: 0.014462]\n",
      "epoch:35 step:27950 [D loss: 0.037299, acc.: 100.00%] [G loss: 0.038891]\n",
      "epoch:35 step:27951 [D loss: 0.103432, acc.: 96.09%] [G loss: 6.327229]\n",
      "epoch:35 step:27952 [D loss: 0.032620, acc.: 98.44%] [G loss: 6.336732]\n",
      "epoch:35 step:27953 [D loss: 0.037027, acc.: 98.44%] [G loss: 5.806607]\n",
      "epoch:35 step:27954 [D loss: 0.019564, acc.: 99.22%] [G loss: 4.783259]\n",
      "epoch:35 step:27955 [D loss: 0.040833, acc.: 98.44%] [G loss: 4.058127]\n",
      "epoch:35 step:27956 [D loss: 0.003855, acc.: 100.00%] [G loss: 3.580617]\n",
      "epoch:35 step:27957 [D loss: 0.010983, acc.: 100.00%] [G loss: 3.302532]\n",
      "epoch:35 step:27958 [D loss: 0.033403, acc.: 99.22%] [G loss: 3.282951]\n",
      "epoch:35 step:27959 [D loss: 0.010855, acc.: 100.00%] [G loss: 1.489793]\n",
      "epoch:35 step:27960 [D loss: 0.011884, acc.: 100.00%] [G loss: 3.209208]\n",
      "epoch:35 step:27961 [D loss: 0.165643, acc.: 92.97%] [G loss: 3.483234]\n",
      "epoch:35 step:27962 [D loss: 0.029348, acc.: 100.00%] [G loss: 4.391503]\n",
      "epoch:35 step:27963 [D loss: 0.149428, acc.: 96.09%] [G loss: 2.012661]\n",
      "epoch:35 step:27964 [D loss: 0.048562, acc.: 99.22%] [G loss: 1.696455]\n",
      "epoch:35 step:27965 [D loss: 0.028711, acc.: 98.44%] [G loss: 3.407511]\n",
      "epoch:35 step:27966 [D loss: 0.122991, acc.: 96.88%] [G loss: 2.442223]\n",
      "epoch:35 step:27967 [D loss: 0.010036, acc.: 100.00%] [G loss: 2.729454]\n",
      "epoch:35 step:27968 [D loss: 0.007067, acc.: 100.00%] [G loss: 1.860777]\n",
      "epoch:35 step:27969 [D loss: 0.011668, acc.: 100.00%] [G loss: 2.844173]\n",
      "epoch:35 step:27970 [D loss: 0.049883, acc.: 100.00%] [G loss: 4.806159]\n",
      "epoch:35 step:27971 [D loss: 0.022902, acc.: 100.00%] [G loss: 0.103807]\n",
      "epoch:35 step:27972 [D loss: 0.015049, acc.: 100.00%] [G loss: 5.999229]\n",
      "epoch:35 step:27973 [D loss: 0.067639, acc.: 98.44%] [G loss: 0.366170]\n",
      "epoch:35 step:27974 [D loss: 0.098344, acc.: 97.66%] [G loss: 6.954149]\n",
      "epoch:35 step:27975 [D loss: 0.013478, acc.: 100.00%] [G loss: 7.118225]\n",
      "epoch:35 step:27976 [D loss: 0.031038, acc.: 100.00%] [G loss: 5.755146]\n",
      "epoch:35 step:27977 [D loss: 0.064398, acc.: 97.66%] [G loss: 0.495266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27978 [D loss: 0.014247, acc.: 100.00%] [G loss: 4.381750]\n",
      "epoch:35 step:27979 [D loss: 0.005624, acc.: 100.00%] [G loss: 0.006782]\n",
      "epoch:35 step:27980 [D loss: 0.039419, acc.: 100.00%] [G loss: 4.968600]\n",
      "epoch:35 step:27981 [D loss: 0.047408, acc.: 99.22%] [G loss: 4.161124]\n",
      "epoch:35 step:27982 [D loss: 0.009383, acc.: 100.00%] [G loss: 0.001864]\n",
      "epoch:35 step:27983 [D loss: 0.027924, acc.: 100.00%] [G loss: 3.486616]\n",
      "epoch:35 step:27984 [D loss: 0.009661, acc.: 100.00%] [G loss: 1.620992]\n",
      "epoch:35 step:27985 [D loss: 0.011616, acc.: 100.00%] [G loss: 1.172149]\n",
      "epoch:35 step:27986 [D loss: 0.004888, acc.: 100.00%] [G loss: 1.109401]\n",
      "epoch:35 step:27987 [D loss: 0.016906, acc.: 99.22%] [G loss: 1.127256]\n",
      "epoch:35 step:27988 [D loss: 0.002392, acc.: 100.00%] [G loss: 0.873995]\n",
      "epoch:35 step:27989 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.021039]\n",
      "epoch:35 step:27990 [D loss: 0.000921, acc.: 100.00%] [G loss: 0.459417]\n",
      "epoch:35 step:27991 [D loss: 0.001859, acc.: 100.00%] [G loss: 0.345602]\n",
      "epoch:35 step:27992 [D loss: 0.002173, acc.: 100.00%] [G loss: 0.078691]\n",
      "epoch:35 step:27993 [D loss: 0.000945, acc.: 100.00%] [G loss: 0.163741]\n",
      "epoch:35 step:27994 [D loss: 0.002238, acc.: 100.00%] [G loss: 2.319757]\n",
      "epoch:35 step:27995 [D loss: 0.033315, acc.: 100.00%] [G loss: 0.127627]\n",
      "epoch:35 step:27996 [D loss: 0.004566, acc.: 100.00%] [G loss: 0.948658]\n",
      "epoch:35 step:27997 [D loss: 0.014857, acc.: 99.22%] [G loss: 0.298601]\n",
      "epoch:35 step:27998 [D loss: 0.003407, acc.: 100.00%] [G loss: 0.397672]\n",
      "epoch:35 step:27999 [D loss: 0.007658, acc.: 100.00%] [G loss: 0.251965]\n",
      "epoch:35 step:28000 [D loss: 0.081856, acc.: 97.66%] [G loss: 0.017984]\n",
      "epoch:35 step:28001 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.069827]\n",
      "epoch:35 step:28002 [D loss: 0.000162, acc.: 100.00%] [G loss: 1.720905]\n",
      "epoch:35 step:28003 [D loss: 0.002025, acc.: 100.00%] [G loss: 0.243799]\n",
      "epoch:35 step:28004 [D loss: 0.010378, acc.: 100.00%] [G loss: 0.028249]\n",
      "epoch:35 step:28005 [D loss: 0.011887, acc.: 99.22%] [G loss: 0.478801]\n",
      "epoch:35 step:28006 [D loss: 0.004277, acc.: 100.00%] [G loss: 0.005029]\n",
      "epoch:35 step:28007 [D loss: 0.002149, acc.: 100.00%] [G loss: 0.029438]\n",
      "epoch:35 step:28008 [D loss: 0.004130, acc.: 100.00%] [G loss: 0.111035]\n",
      "epoch:35 step:28009 [D loss: 0.002435, acc.: 100.00%] [G loss: 0.010030]\n",
      "epoch:35 step:28010 [D loss: 0.001104, acc.: 100.00%] [G loss: 0.168765]\n",
      "epoch:35 step:28011 [D loss: 0.002648, acc.: 100.00%] [G loss: 0.063081]\n",
      "epoch:35 step:28012 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.079642]\n",
      "epoch:35 step:28013 [D loss: 0.002056, acc.: 100.00%] [G loss: 0.055570]\n",
      "epoch:35 step:28014 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.133365]\n",
      "epoch:35 step:28015 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.030389]\n",
      "epoch:35 step:28016 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.014417]\n",
      "epoch:35 step:28017 [D loss: 0.002088, acc.: 100.00%] [G loss: 0.010046]\n",
      "epoch:35 step:28018 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.212629]\n",
      "epoch:35 step:28019 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.028630]\n",
      "epoch:35 step:28020 [D loss: 0.005584, acc.: 100.00%] [G loss: 0.035823]\n",
      "epoch:35 step:28021 [D loss: 0.001601, acc.: 100.00%] [G loss: 0.018263]\n",
      "epoch:35 step:28022 [D loss: 0.000994, acc.: 100.00%] [G loss: 0.002699]\n",
      "epoch:35 step:28023 [D loss: 0.004331, acc.: 100.00%] [G loss: 0.006636]\n",
      "epoch:35 step:28024 [D loss: 0.003217, acc.: 100.00%] [G loss: 0.019882]\n",
      "epoch:35 step:28025 [D loss: 0.003442, acc.: 100.00%] [G loss: 0.013053]\n",
      "epoch:35 step:28026 [D loss: 0.003372, acc.: 100.00%] [G loss: 0.024881]\n",
      "epoch:35 step:28027 [D loss: 0.018857, acc.: 100.00%] [G loss: 0.023245]\n",
      "epoch:35 step:28028 [D loss: 0.001757, acc.: 100.00%] [G loss: 0.025606]\n",
      "epoch:35 step:28029 [D loss: 0.002457, acc.: 100.00%] [G loss: 0.016729]\n",
      "epoch:35 step:28030 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.003220]\n",
      "epoch:35 step:28031 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.029639]\n",
      "epoch:35 step:28032 [D loss: 0.025179, acc.: 100.00%] [G loss: 0.012166]\n",
      "epoch:35 step:28033 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.073275]\n",
      "epoch:35 step:28034 [D loss: 0.001295, acc.: 100.00%] [G loss: 0.029234]\n",
      "epoch:35 step:28035 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.071594]\n",
      "epoch:35 step:28036 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.055546]\n",
      "epoch:35 step:28037 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.053763]\n",
      "epoch:35 step:28038 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.010190]\n",
      "epoch:35 step:28039 [D loss: 0.080098, acc.: 96.88%] [G loss: 0.006124]\n",
      "epoch:35 step:28040 [D loss: 0.005068, acc.: 100.00%] [G loss: 0.000987]\n",
      "epoch:35 step:28041 [D loss: 0.002158, acc.: 100.00%] [G loss: 0.003655]\n",
      "epoch:35 step:28042 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001098]\n",
      "epoch:35 step:28043 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.001945]\n",
      "epoch:35 step:28044 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.033448]\n",
      "epoch:35 step:28045 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:35 step:28046 [D loss: 0.001952, acc.: 100.00%] [G loss: 0.002189]\n",
      "epoch:35 step:28047 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.006652]\n",
      "epoch:35 step:28048 [D loss: 0.003163, acc.: 100.00%] [G loss: 0.001524]\n",
      "epoch:35 step:28049 [D loss: 0.001779, acc.: 100.00%] [G loss: 0.002643]\n",
      "epoch:35 step:28050 [D loss: 0.003661, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:35 step:28051 [D loss: 0.001612, acc.: 100.00%] [G loss: 0.001001]\n",
      "epoch:35 step:28052 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.001682]\n",
      "epoch:35 step:28053 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.001238]\n",
      "epoch:35 step:28054 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.000693]\n",
      "epoch:35 step:28055 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001954]\n",
      "epoch:35 step:28056 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:35 step:28057 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.002731]\n",
      "epoch:35 step:28058 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.031279]\n",
      "epoch:35 step:28059 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.001087]\n",
      "epoch:35 step:28060 [D loss: 0.006032, acc.: 100.00%] [G loss: 0.001777]\n",
      "epoch:35 step:28061 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000972]\n",
      "epoch:35 step:28062 [D loss: 0.000595, acc.: 100.00%] [G loss: 0.003413]\n",
      "epoch:35 step:28063 [D loss: 0.001919, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:35 step:28064 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.009604]\n",
      "epoch:35 step:28065 [D loss: 0.044316, acc.: 99.22%] [G loss: 0.004008]\n",
      "epoch:35 step:28066 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.011767]\n",
      "epoch:35 step:28067 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.164237]\n",
      "epoch:35 step:28068 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.011451]\n",
      "epoch:35 step:28069 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.010920]\n",
      "epoch:35 step:28070 [D loss: 0.001907, acc.: 100.00%] [G loss: 0.008349]\n",
      "epoch:35 step:28071 [D loss: 0.004549, acc.: 100.00%] [G loss: 0.032231]\n",
      "epoch:35 step:28072 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.007008]\n",
      "epoch:35 step:28073 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.136482]\n",
      "epoch:35 step:28074 [D loss: 0.003542, acc.: 100.00%] [G loss: 0.010234]\n",
      "epoch:35 step:28075 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.020250]\n",
      "epoch:35 step:28076 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:35 step:28077 [D loss: 0.002368, acc.: 100.00%] [G loss: 0.007891]\n",
      "epoch:35 step:28078 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.013929]\n",
      "epoch:35 step:28079 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.001025]\n",
      "epoch:35 step:28080 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.007508]\n",
      "epoch:35 step:28081 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.008566]\n",
      "epoch:35 step:28082 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.010347]\n",
      "epoch:35 step:28083 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.015984]\n",
      "epoch:35 step:28084 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.128148]\n",
      "epoch:35 step:28085 [D loss: 0.008783, acc.: 100.00%] [G loss: 0.007957]\n",
      "epoch:35 step:28086 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:35 step:28087 [D loss: 0.001093, acc.: 100.00%] [G loss: 0.039630]\n",
      "epoch:35 step:28088 [D loss: 0.001375, acc.: 100.00%] [G loss: 0.006902]\n",
      "epoch:35 step:28089 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.017093]\n",
      "epoch:35 step:28090 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.002693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:28091 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.004479]\n",
      "epoch:35 step:28092 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.046434]\n",
      "epoch:35 step:28093 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.011594]\n",
      "epoch:35 step:28094 [D loss: 0.002146, acc.: 100.00%] [G loss: 0.018467]\n",
      "epoch:35 step:28095 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.012041]\n",
      "epoch:35 step:28096 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.013098]\n",
      "epoch:35 step:28097 [D loss: 0.004172, acc.: 100.00%] [G loss: 0.011403]\n",
      "epoch:35 step:28098 [D loss: 0.002376, acc.: 100.00%] [G loss: 0.008774]\n",
      "epoch:35 step:28099 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.016513]\n",
      "epoch:35 step:28100 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.002234]\n",
      "epoch:35 step:28101 [D loss: 0.034249, acc.: 100.00%] [G loss: 0.054356]\n",
      "epoch:35 step:28102 [D loss: 0.000790, acc.: 100.00%] [G loss: 0.011149]\n",
      "epoch:35 step:28103 [D loss: 0.008599, acc.: 100.00%] [G loss: 0.851812]\n",
      "epoch:35 step:28104 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.001983]\n",
      "epoch:35 step:28105 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.670837]\n",
      "epoch:35 step:28106 [D loss: 0.019496, acc.: 99.22%] [G loss: 0.219601]\n",
      "epoch:35 step:28107 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:35 step:28108 [D loss: 0.002475, acc.: 100.00%] [G loss: 0.242473]\n",
      "epoch:35 step:28109 [D loss: 0.001295, acc.: 100.00%] [G loss: 0.652507]\n",
      "epoch:35 step:28110 [D loss: 0.000639, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:35 step:28111 [D loss: 0.007926, acc.: 100.00%] [G loss: 0.120973]\n",
      "epoch:35 step:28112 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.184001]\n",
      "epoch:35 step:28113 [D loss: 0.001941, acc.: 100.00%] [G loss: 0.062393]\n",
      "epoch:35 step:28114 [D loss: 0.002695, acc.: 100.00%] [G loss: 0.092882]\n",
      "epoch:35 step:28115 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.058998]\n",
      "epoch:35 step:28116 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.024430]\n",
      "epoch:36 step:28117 [D loss: 0.001636, acc.: 100.00%] [G loss: 0.172745]\n",
      "epoch:36 step:28118 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.028225]\n",
      "epoch:36 step:28119 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.014482]\n",
      "epoch:36 step:28120 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.021760]\n",
      "epoch:36 step:28121 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.016027]\n",
      "epoch:36 step:28122 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.193996]\n",
      "epoch:36 step:28123 [D loss: 0.007826, acc.: 100.00%] [G loss: 0.136350]\n",
      "epoch:36 step:28124 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.063001]\n",
      "epoch:36 step:28125 [D loss: 0.001305, acc.: 100.00%] [G loss: 0.001121]\n",
      "epoch:36 step:28126 [D loss: 0.002288, acc.: 100.00%] [G loss: 0.010597]\n",
      "epoch:36 step:28127 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.736606]\n",
      "epoch:36 step:28128 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.026771]\n",
      "epoch:36 step:28129 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.090961]\n",
      "epoch:36 step:28130 [D loss: 0.001953, acc.: 100.00%] [G loss: 0.231908]\n",
      "epoch:36 step:28131 [D loss: 0.002614, acc.: 100.00%] [G loss: 0.011031]\n",
      "epoch:36 step:28132 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.025460]\n",
      "epoch:36 step:28133 [D loss: 0.429306, acc.: 73.44%] [G loss: 7.533710]\n",
      "epoch:36 step:28134 [D loss: 2.371911, acc.: 51.56%] [G loss: 1.917678]\n",
      "epoch:36 step:28135 [D loss: 0.147617, acc.: 94.53%] [G loss: 0.272322]\n",
      "epoch:36 step:28136 [D loss: 0.038018, acc.: 99.22%] [G loss: 2.507397]\n",
      "epoch:36 step:28137 [D loss: 0.274437, acc.: 90.62%] [G loss: 2.524550]\n",
      "epoch:36 step:28138 [D loss: 0.005881, acc.: 100.00%] [G loss: 3.289467]\n",
      "epoch:36 step:28139 [D loss: 0.496903, acc.: 84.38%] [G loss: 0.403556]\n",
      "epoch:36 step:28140 [D loss: 0.001805, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:36 step:28141 [D loss: 0.081692, acc.: 96.88%] [G loss: 0.171641]\n",
      "epoch:36 step:28142 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.058241]\n",
      "epoch:36 step:28143 [D loss: 0.003565, acc.: 100.00%] [G loss: 0.028603]\n",
      "epoch:36 step:28144 [D loss: 0.230407, acc.: 85.94%] [G loss: 2.177610]\n",
      "epoch:36 step:28145 [D loss: 0.183031, acc.: 92.19%] [G loss: 1.106124]\n",
      "epoch:36 step:28146 [D loss: 0.014206, acc.: 100.00%] [G loss: 0.038018]\n",
      "epoch:36 step:28147 [D loss: 0.037310, acc.: 98.44%] [G loss: 0.032190]\n",
      "epoch:36 step:28148 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.004953]\n",
      "epoch:36 step:28149 [D loss: 0.012839, acc.: 99.22%] [G loss: 0.004123]\n",
      "epoch:36 step:28150 [D loss: 0.003164, acc.: 100.00%] [G loss: 0.146365]\n",
      "epoch:36 step:28151 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.178000]\n",
      "epoch:36 step:28152 [D loss: 0.004267, acc.: 100.00%] [G loss: 3.214075]\n",
      "epoch:36 step:28153 [D loss: 0.005469, acc.: 100.00%] [G loss: 0.021068]\n",
      "epoch:36 step:28154 [D loss: 0.095135, acc.: 96.09%] [G loss: 0.141056]\n",
      "epoch:36 step:28155 [D loss: 0.001580, acc.: 100.00%] [G loss: 1.388402]\n",
      "epoch:36 step:28156 [D loss: 0.053423, acc.: 97.66%] [G loss: 0.061990]\n",
      "epoch:36 step:28157 [D loss: 0.023008, acc.: 98.44%] [G loss: 0.634012]\n",
      "epoch:36 step:28158 [D loss: 0.176396, acc.: 92.97%] [G loss: 0.971746]\n",
      "epoch:36 step:28159 [D loss: 0.127355, acc.: 96.09%] [G loss: 3.234551]\n",
      "epoch:36 step:28160 [D loss: 0.559418, acc.: 76.56%] [G loss: 1.154230]\n",
      "epoch:36 step:28161 [D loss: 0.012316, acc.: 100.00%] [G loss: 2.584864]\n",
      "epoch:36 step:28162 [D loss: 0.093463, acc.: 96.88%] [G loss: 2.369764]\n",
      "epoch:36 step:28163 [D loss: 0.051047, acc.: 98.44%] [G loss: 0.667582]\n",
      "epoch:36 step:28164 [D loss: 0.091905, acc.: 97.66%] [G loss: 1.643748]\n",
      "epoch:36 step:28165 [D loss: 0.029700, acc.: 98.44%] [G loss: 0.762456]\n",
      "epoch:36 step:28166 [D loss: 0.013174, acc.: 100.00%] [G loss: 0.390041]\n",
      "epoch:36 step:28167 [D loss: 0.007948, acc.: 100.00%] [G loss: 0.348865]\n",
      "epoch:36 step:28168 [D loss: 0.032643, acc.: 99.22%] [G loss: 0.040993]\n",
      "epoch:36 step:28169 [D loss: 0.007624, acc.: 100.00%] [G loss: 0.011844]\n",
      "epoch:36 step:28170 [D loss: 0.028113, acc.: 98.44%] [G loss: 0.014047]\n",
      "epoch:36 step:28171 [D loss: 0.004549, acc.: 100.00%] [G loss: 0.013713]\n",
      "epoch:36 step:28172 [D loss: 0.010896, acc.: 100.00%] [G loss: 0.004737]\n",
      "epoch:36 step:28173 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.015266]\n",
      "epoch:36 step:28174 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.002156]\n",
      "epoch:36 step:28175 [D loss: 0.000307, acc.: 100.00%] [G loss: 1.184759]\n",
      "epoch:36 step:28176 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.014708]\n",
      "epoch:36 step:28177 [D loss: 0.002007, acc.: 100.00%] [G loss: 0.280033]\n",
      "epoch:36 step:28178 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.003717]\n",
      "epoch:36 step:28179 [D loss: 0.000775, acc.: 100.00%] [G loss: 0.002140]\n",
      "epoch:36 step:28180 [D loss: 0.004902, acc.: 100.00%] [G loss: 0.002828]\n",
      "epoch:36 step:28181 [D loss: 0.043142, acc.: 98.44%] [G loss: 0.000724]\n",
      "epoch:36 step:28182 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000731]\n",
      "epoch:36 step:28183 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:36 step:28184 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:36 step:28185 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001024]\n",
      "epoch:36 step:28186 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.009074]\n",
      "epoch:36 step:28187 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000922]\n",
      "epoch:36 step:28188 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:36 step:28189 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002730]\n",
      "epoch:36 step:28190 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:36 step:28191 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.032803]\n",
      "epoch:36 step:28192 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:36 step:28193 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:36 step:28194 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:36 step:28195 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:36 step:28196 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.055459]\n",
      "epoch:36 step:28197 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:36 step:28198 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:36 step:28199 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002145]\n",
      "epoch:36 step:28200 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:36 step:28201 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:36 step:28202 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:36 step:28203 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28204 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:36 step:28205 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:36 step:28206 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:36 step:28207 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:36 step:28208 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.018874]\n",
      "epoch:36 step:28209 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:36 step:28210 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:36 step:28211 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000776]\n",
      "epoch:36 step:28212 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.001905]\n",
      "epoch:36 step:28213 [D loss: 0.001324, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:36 step:28214 [D loss: 0.001296, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:36 step:28215 [D loss: 0.001530, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:36 step:28216 [D loss: 0.274905, acc.: 85.94%] [G loss: 0.225400]\n",
      "epoch:36 step:28217 [D loss: 0.003639, acc.: 100.00%] [G loss: 7.060300]\n",
      "epoch:36 step:28218 [D loss: 0.222197, acc.: 89.84%] [G loss: 3.191431]\n",
      "epoch:36 step:28219 [D loss: 0.025757, acc.: 98.44%] [G loss: 0.000013]\n",
      "epoch:36 step:28220 [D loss: 0.000195, acc.: 100.00%] [G loss: 1.121894]\n",
      "epoch:36 step:28221 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.773512]\n",
      "epoch:36 step:28222 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:36 step:28223 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.477139]\n",
      "epoch:36 step:28224 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.287030]\n",
      "epoch:36 step:28225 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.203793]\n",
      "epoch:36 step:28226 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.055597]\n",
      "epoch:36 step:28227 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.126417]\n",
      "epoch:36 step:28228 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.049707]\n",
      "epoch:36 step:28229 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.079864]\n",
      "epoch:36 step:28230 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.014340]\n",
      "epoch:36 step:28231 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.007191]\n",
      "epoch:36 step:28232 [D loss: 0.004625, acc.: 100.00%] [G loss: 0.035067]\n",
      "epoch:36 step:28233 [D loss: 0.041209, acc.: 98.44%] [G loss: 0.041199]\n",
      "epoch:36 step:28234 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:36 step:28235 [D loss: 0.002050, acc.: 100.00%] [G loss: 0.503089]\n",
      "epoch:36 step:28236 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.816686]\n",
      "epoch:36 step:28237 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.459465]\n",
      "epoch:36 step:28238 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.005433]\n",
      "epoch:36 step:28239 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.388483]\n",
      "epoch:36 step:28240 [D loss: 0.008099, acc.: 100.00%] [G loss: 0.075104]\n",
      "epoch:36 step:28241 [D loss: 0.001027, acc.: 100.00%] [G loss: 0.006929]\n",
      "epoch:36 step:28242 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.100909]\n",
      "epoch:36 step:28243 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.053389]\n",
      "epoch:36 step:28244 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.024213]\n",
      "epoch:36 step:28245 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:36 step:28246 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.030993]\n",
      "epoch:36 step:28247 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.002299]\n",
      "epoch:36 step:28248 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.014984]\n",
      "epoch:36 step:28249 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:36 step:28250 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.060394]\n",
      "epoch:36 step:28251 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:36 step:28252 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.022145]\n",
      "epoch:36 step:28253 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.023153]\n",
      "epoch:36 step:28254 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.008104]\n",
      "epoch:36 step:28255 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.012967]\n",
      "epoch:36 step:28256 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:36 step:28257 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:36 step:28258 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.006384]\n",
      "epoch:36 step:28259 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.029546]\n",
      "epoch:36 step:28260 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.024627]\n",
      "epoch:36 step:28261 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.006387]\n",
      "epoch:36 step:28262 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001682]\n",
      "epoch:36 step:28263 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:36 step:28264 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:36 step:28265 [D loss: 0.001791, acc.: 100.00%] [G loss: 0.024268]\n",
      "epoch:36 step:28266 [D loss: 0.000715, acc.: 100.00%] [G loss: 0.002775]\n",
      "epoch:36 step:28267 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:36 step:28268 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.008604]\n",
      "epoch:36 step:28269 [D loss: 0.002122, acc.: 100.00%] [G loss: 0.366423]\n",
      "epoch:36 step:28270 [D loss: 0.001135, acc.: 100.00%] [G loss: 0.014504]\n",
      "epoch:36 step:28271 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.004836]\n",
      "epoch:36 step:28272 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:36 step:28273 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.001297]\n",
      "epoch:36 step:28274 [D loss: 0.000746, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:36 step:28275 [D loss: 0.054915, acc.: 98.44%] [G loss: 0.000049]\n",
      "epoch:36 step:28276 [D loss: 0.009223, acc.: 100.00%] [G loss: 0.070217]\n",
      "epoch:36 step:28277 [D loss: 0.018293, acc.: 99.22%] [G loss: 0.019975]\n",
      "epoch:36 step:28278 [D loss: 0.001216, acc.: 100.00%] [G loss: 0.014220]\n",
      "epoch:36 step:28279 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.004305]\n",
      "epoch:36 step:28280 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.137014]\n",
      "epoch:36 step:28281 [D loss: 0.001072, acc.: 100.00%] [G loss: 0.010356]\n",
      "epoch:36 step:28282 [D loss: 0.004334, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:36 step:28283 [D loss: 0.003523, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:36 step:28284 [D loss: 0.193570, acc.: 91.41%] [G loss: 2.036780]\n",
      "epoch:36 step:28285 [D loss: 0.039314, acc.: 98.44%] [G loss: 2.429712]\n",
      "epoch:36 step:28286 [D loss: 0.804649, acc.: 70.31%] [G loss: 0.000204]\n",
      "epoch:36 step:28287 [D loss: 0.002460, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:36 step:28288 [D loss: 0.002997, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:36 step:28289 [D loss: 0.003908, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:36 step:28290 [D loss: 0.007180, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:36 step:28291 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:36 step:28292 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:36 step:28293 [D loss: 0.831959, acc.: 63.28%] [G loss: 7.997772]\n",
      "epoch:36 step:28294 [D loss: 0.975336, acc.: 63.28%] [G loss: 1.320293]\n",
      "epoch:36 step:28295 [D loss: 0.019238, acc.: 99.22%] [G loss: 0.312233]\n",
      "epoch:36 step:28296 [D loss: 0.031233, acc.: 99.22%] [G loss: 0.040189]\n",
      "epoch:36 step:28297 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.012833]\n",
      "epoch:36 step:28298 [D loss: 0.001186, acc.: 100.00%] [G loss: 0.005961]\n",
      "epoch:36 step:28299 [D loss: 0.001269, acc.: 100.00%] [G loss: 1.785713]\n",
      "epoch:36 step:28300 [D loss: 0.001269, acc.: 100.00%] [G loss: 0.011215]\n",
      "epoch:36 step:28301 [D loss: 0.002021, acc.: 100.00%] [G loss: 0.007102]\n",
      "epoch:36 step:28302 [D loss: 0.004790, acc.: 100.00%] [G loss: 0.037436]\n",
      "epoch:36 step:28303 [D loss: 0.011636, acc.: 100.00%] [G loss: 0.011312]\n",
      "epoch:36 step:28304 [D loss: 0.283603, acc.: 88.28%] [G loss: 2.406318]\n",
      "epoch:36 step:28305 [D loss: 0.004561, acc.: 100.00%] [G loss: 3.073675]\n",
      "epoch:36 step:28306 [D loss: 0.323827, acc.: 83.59%] [G loss: 0.039772]\n",
      "epoch:36 step:28307 [D loss: 0.263573, acc.: 85.94%] [G loss: 4.111317]\n",
      "epoch:36 step:28308 [D loss: 0.014780, acc.: 100.00%] [G loss: 5.220883]\n",
      "epoch:36 step:28309 [D loss: 0.361749, acc.: 85.16%] [G loss: 0.192626]\n",
      "epoch:36 step:28310 [D loss: 0.006998, acc.: 100.00%] [G loss: 0.258239]\n",
      "epoch:36 step:28311 [D loss: 0.058830, acc.: 99.22%] [G loss: 0.317494]\n",
      "epoch:36 step:28312 [D loss: 0.018537, acc.: 100.00%] [G loss: 0.389406]\n",
      "epoch:36 step:28313 [D loss: 0.030477, acc.: 100.00%] [G loss: 0.132613]\n",
      "epoch:36 step:28314 [D loss: 0.009001, acc.: 100.00%] [G loss: 0.132439]\n",
      "epoch:36 step:28315 [D loss: 0.013945, acc.: 100.00%] [G loss: 0.192990]\n",
      "epoch:36 step:28316 [D loss: 0.006725, acc.: 100.00%] [G loss: 0.044121]\n",
      "epoch:36 step:28317 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.013606]\n",
      "epoch:36 step:28318 [D loss: 0.005079, acc.: 100.00%] [G loss: 0.001937]\n",
      "epoch:36 step:28319 [D loss: 0.006636, acc.: 100.00%] [G loss: 0.003014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28320 [D loss: 0.011342, acc.: 99.22%] [G loss: 0.001348]\n",
      "epoch:36 step:28321 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.002831]\n",
      "epoch:36 step:28322 [D loss: 0.006671, acc.: 100.00%] [G loss: 0.002056]\n",
      "epoch:36 step:28323 [D loss: 0.006614, acc.: 100.00%] [G loss: 0.003487]\n",
      "epoch:36 step:28324 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.017739]\n",
      "epoch:36 step:28325 [D loss: 0.004732, acc.: 100.00%] [G loss: 0.001881]\n",
      "epoch:36 step:28326 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.000866]\n",
      "epoch:36 step:28327 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:36 step:28328 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:36 step:28329 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:36 step:28330 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:36 step:28331 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:36 step:28332 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.000945]\n",
      "epoch:36 step:28333 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.003106]\n",
      "epoch:36 step:28334 [D loss: 0.002569, acc.: 100.00%] [G loss: 0.006985]\n",
      "epoch:36 step:28335 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.000654]\n",
      "epoch:36 step:28336 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.001282]\n",
      "epoch:36 step:28337 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.005435]\n",
      "epoch:36 step:28338 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.023931]\n",
      "epoch:36 step:28339 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:36 step:28340 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:36 step:28341 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:36 step:28342 [D loss: 0.001338, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:36 step:28343 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:36 step:28344 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:36 step:28345 [D loss: 0.000614, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:36 step:28346 [D loss: 0.000778, acc.: 100.00%] [G loss: 0.000895]\n",
      "epoch:36 step:28347 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002058]\n",
      "epoch:36 step:28348 [D loss: 0.001033, acc.: 100.00%] [G loss: 0.017494]\n",
      "epoch:36 step:28349 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:36 step:28350 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:36 step:28351 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:36 step:28352 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:36 step:28353 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.001052]\n",
      "epoch:36 step:28354 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:36 step:28355 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:36 step:28356 [D loss: 0.001685, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:36 step:28357 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.056285]\n",
      "epoch:36 step:28358 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:36 step:28359 [D loss: 0.006669, acc.: 100.00%] [G loss: 0.003483]\n",
      "epoch:36 step:28360 [D loss: 0.007308, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:36 step:28361 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:36 step:28362 [D loss: 0.015509, acc.: 100.00%] [G loss: 0.008525]\n",
      "epoch:36 step:28363 [D loss: 0.007040, acc.: 100.00%] [G loss: 0.031714]\n",
      "epoch:36 step:28364 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004600]\n",
      "epoch:36 step:28365 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:36 step:28366 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000477]\n",
      "epoch:36 step:28367 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000655]\n",
      "epoch:36 step:28368 [D loss: 0.001348, acc.: 100.00%] [G loss: 0.002757]\n",
      "epoch:36 step:28369 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.004383]\n",
      "epoch:36 step:28370 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.007978]\n",
      "epoch:36 step:28371 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:36 step:28372 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002146]\n",
      "epoch:36 step:28373 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:36 step:28374 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:36 step:28375 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:36 step:28376 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.005549]\n",
      "epoch:36 step:28377 [D loss: 0.023913, acc.: 99.22%] [G loss: 0.000560]\n",
      "epoch:36 step:28378 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:36 step:28379 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:36 step:28380 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.200772]\n",
      "epoch:36 step:28381 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.001976]\n",
      "epoch:36 step:28382 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:36 step:28383 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:36 step:28384 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:36 step:28385 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:36 step:28386 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.001776]\n",
      "epoch:36 step:28387 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:36 step:28388 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:36 step:28389 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:36 step:28390 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.003914]\n",
      "epoch:36 step:28391 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:36 step:28392 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001280]\n",
      "epoch:36 step:28393 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:36 step:28394 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.002109]\n",
      "epoch:36 step:28395 [D loss: 0.004691, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:36 step:28396 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:36 step:28397 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:36 step:28398 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:36 step:28399 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.002279]\n",
      "epoch:36 step:28400 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001242]\n",
      "epoch:36 step:28401 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.001078]\n",
      "epoch:36 step:28402 [D loss: 0.007723, acc.: 100.00%] [G loss: 0.001011]\n",
      "epoch:36 step:28403 [D loss: 0.018272, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:36 step:28404 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.022194]\n",
      "epoch:36 step:28405 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:36 step:28406 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000717]\n",
      "epoch:36 step:28407 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.005376]\n",
      "epoch:36 step:28408 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:36 step:28409 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.007125]\n",
      "epoch:36 step:28410 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:36 step:28411 [D loss: 0.002131, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:36 step:28412 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:36 step:28413 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:36 step:28414 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001639]\n",
      "epoch:36 step:28415 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:36 step:28416 [D loss: 0.005798, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:36 step:28417 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:36 step:28418 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.186073]\n",
      "epoch:36 step:28419 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:36 step:28420 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:36 step:28421 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:36 step:28422 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.037531]\n",
      "epoch:36 step:28423 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.024977]\n",
      "epoch:36 step:28424 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.012148]\n",
      "epoch:36 step:28425 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:36 step:28426 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:36 step:28427 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:36 step:28428 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.003561]\n",
      "epoch:36 step:28429 [D loss: 0.024985, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:36 step:28430 [D loss: 0.000612, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:36 step:28431 [D loss: 0.390662, acc.: 82.03%] [G loss: 0.000008]\n",
      "epoch:36 step:28432 [D loss: 0.557136, acc.: 74.22%] [G loss: 0.006142]\n",
      "epoch:36 step:28433 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.674613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28434 [D loss: 0.017319, acc.: 99.22%] [G loss: 4.677140]\n",
      "epoch:36 step:28435 [D loss: 0.056096, acc.: 96.88%] [G loss: 0.047815]\n",
      "epoch:36 step:28436 [D loss: 0.143009, acc.: 93.75%] [G loss: 0.095386]\n",
      "epoch:36 step:28437 [D loss: 0.002801, acc.: 100.00%] [G loss: 0.003635]\n",
      "epoch:36 step:28438 [D loss: 0.049557, acc.: 97.66%] [G loss: 0.004578]\n",
      "epoch:36 step:28439 [D loss: 0.002224, acc.: 100.00%] [G loss: 0.001310]\n",
      "epoch:36 step:28440 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.001231]\n",
      "epoch:36 step:28441 [D loss: 0.012707, acc.: 99.22%] [G loss: 1.284186]\n",
      "epoch:36 step:28442 [D loss: 0.004853, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:36 step:28443 [D loss: 0.008458, acc.: 100.00%] [G loss: 0.000880]\n",
      "epoch:36 step:28444 [D loss: 0.001217, acc.: 100.00%] [G loss: 0.006630]\n",
      "epoch:36 step:28445 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:36 step:28446 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.002332]\n",
      "epoch:36 step:28447 [D loss: 0.001301, acc.: 100.00%] [G loss: 0.000658]\n",
      "epoch:36 step:28448 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:36 step:28449 [D loss: 0.001763, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:36 step:28450 [D loss: 0.000943, acc.: 100.00%] [G loss: 0.001136]\n",
      "epoch:36 step:28451 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.013547]\n",
      "epoch:36 step:28452 [D loss: 0.009848, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:36 step:28453 [D loss: 0.002565, acc.: 100.00%] [G loss: 0.013072]\n",
      "epoch:36 step:28454 [D loss: 0.054562, acc.: 96.88%] [G loss: 0.002435]\n",
      "epoch:36 step:28455 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000385]\n",
      "epoch:36 step:28456 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:36 step:28457 [D loss: 0.002326, acc.: 100.00%] [G loss: 0.006834]\n",
      "epoch:36 step:28458 [D loss: 1.601552, acc.: 54.69%] [G loss: 8.078068]\n",
      "epoch:36 step:28459 [D loss: 1.490699, acc.: 57.03%] [G loss: 2.392707]\n",
      "epoch:36 step:28460 [D loss: 0.465871, acc.: 81.25%] [G loss: 0.277709]\n",
      "epoch:36 step:28461 [D loss: 0.709016, acc.: 70.31%] [G loss: 1.396706]\n",
      "epoch:36 step:28462 [D loss: 0.029676, acc.: 99.22%] [G loss: 1.323086]\n",
      "epoch:36 step:28463 [D loss: 0.367092, acc.: 82.03%] [G loss: 2.880810]\n",
      "epoch:36 step:28464 [D loss: 0.039458, acc.: 97.66%] [G loss: 0.012628]\n",
      "epoch:36 step:28465 [D loss: 0.001691, acc.: 100.00%] [G loss: 0.058221]\n",
      "epoch:36 step:28466 [D loss: 0.005112, acc.: 100.00%] [G loss: 0.004144]\n",
      "epoch:36 step:28467 [D loss: 0.002746, acc.: 100.00%] [G loss: 1.590576]\n",
      "epoch:36 step:28468 [D loss: 0.011267, acc.: 100.00%] [G loss: 0.006066]\n",
      "epoch:36 step:28469 [D loss: 0.011371, acc.: 100.00%] [G loss: 0.013534]\n",
      "epoch:36 step:28470 [D loss: 0.035910, acc.: 100.00%] [G loss: 0.008775]\n",
      "epoch:36 step:28471 [D loss: 0.028708, acc.: 100.00%] [G loss: 0.002337]\n",
      "epoch:36 step:28472 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.002128]\n",
      "epoch:36 step:28473 [D loss: 0.006966, acc.: 100.00%] [G loss: 0.004727]\n",
      "epoch:36 step:28474 [D loss: 0.006588, acc.: 100.00%] [G loss: 0.009419]\n",
      "epoch:36 step:28475 [D loss: 0.003666, acc.: 100.00%] [G loss: 0.008348]\n",
      "epoch:36 step:28476 [D loss: 0.020017, acc.: 100.00%] [G loss: 0.012884]\n",
      "epoch:36 step:28477 [D loss: 0.016772, acc.: 100.00%] [G loss: 0.924582]\n",
      "epoch:36 step:28478 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.003629]\n",
      "epoch:36 step:28479 [D loss: 0.003438, acc.: 100.00%] [G loss: 0.014328]\n",
      "epoch:36 step:28480 [D loss: 0.005367, acc.: 100.00%] [G loss: 0.037502]\n",
      "epoch:36 step:28481 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.244534]\n",
      "epoch:36 step:28482 [D loss: 0.001995, acc.: 100.00%] [G loss: 0.613889]\n",
      "epoch:36 step:28483 [D loss: 0.003234, acc.: 100.00%] [G loss: 0.014016]\n",
      "epoch:36 step:28484 [D loss: 0.001933, acc.: 100.00%] [G loss: 0.003073]\n",
      "epoch:36 step:28485 [D loss: 0.003674, acc.: 100.00%] [G loss: 0.016757]\n",
      "epoch:36 step:28486 [D loss: 0.002041, acc.: 100.00%] [G loss: 0.005777]\n",
      "epoch:36 step:28487 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.111476]\n",
      "epoch:36 step:28488 [D loss: 0.416715, acc.: 77.34%] [G loss: 0.314429]\n",
      "epoch:36 step:28489 [D loss: 0.067229, acc.: 97.66%] [G loss: 0.487423]\n",
      "epoch:36 step:28490 [D loss: 0.459249, acc.: 80.47%] [G loss: 1.697863]\n",
      "epoch:36 step:28491 [D loss: 0.011104, acc.: 100.00%] [G loss: 0.012934]\n",
      "epoch:36 step:28492 [D loss: 0.001560, acc.: 100.00%] [G loss: 0.005219]\n",
      "epoch:36 step:28493 [D loss: 0.001060, acc.: 100.00%] [G loss: 0.215889]\n",
      "epoch:36 step:28494 [D loss: 0.012594, acc.: 100.00%] [G loss: 0.004248]\n",
      "epoch:36 step:28495 [D loss: 0.000691, acc.: 100.00%] [G loss: 0.001151]\n",
      "epoch:36 step:28496 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.159851]\n",
      "epoch:36 step:28497 [D loss: 0.001763, acc.: 100.00%] [G loss: 0.001250]\n",
      "epoch:36 step:28498 [D loss: 0.003092, acc.: 100.00%] [G loss: 0.476043]\n",
      "epoch:36 step:28499 [D loss: 0.005234, acc.: 100.00%] [G loss: 0.158418]\n",
      "epoch:36 step:28500 [D loss: 0.025372, acc.: 100.00%] [G loss: 0.019315]\n",
      "epoch:36 step:28501 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.001629]\n",
      "epoch:36 step:28502 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.004083]\n",
      "epoch:36 step:28503 [D loss: 0.002284, acc.: 100.00%] [G loss: 0.014176]\n",
      "epoch:36 step:28504 [D loss: 0.029943, acc.: 99.22%] [G loss: 0.002564]\n",
      "epoch:36 step:28505 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.003828]\n",
      "epoch:36 step:28506 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.010397]\n",
      "epoch:36 step:28507 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.003603]\n",
      "epoch:36 step:28508 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.008521]\n",
      "epoch:36 step:28509 [D loss: 0.001451, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:36 step:28510 [D loss: 0.004826, acc.: 100.00%] [G loss: 0.001104]\n",
      "epoch:36 step:28511 [D loss: 0.014229, acc.: 100.00%] [G loss: 0.031648]\n",
      "epoch:36 step:28512 [D loss: 0.000869, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:36 step:28513 [D loss: 0.000542, acc.: 100.00%] [G loss: 0.070004]\n",
      "epoch:36 step:28514 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:36 step:28515 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.005742]\n",
      "epoch:36 step:28516 [D loss: 0.006345, acc.: 100.00%] [G loss: 0.034972]\n",
      "epoch:36 step:28517 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.000823]\n",
      "epoch:36 step:28518 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.000968]\n",
      "epoch:36 step:28519 [D loss: 0.004270, acc.: 100.00%] [G loss: 0.001050]\n",
      "epoch:36 step:28520 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.005837]\n",
      "epoch:36 step:28521 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.013425]\n",
      "epoch:36 step:28522 [D loss: 0.000912, acc.: 100.00%] [G loss: 0.001002]\n",
      "epoch:36 step:28523 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.040403]\n",
      "epoch:36 step:28524 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000692]\n",
      "epoch:36 step:28525 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.001827]\n",
      "epoch:36 step:28526 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:36 step:28527 [D loss: 0.002475, acc.: 100.00%] [G loss: 0.019501]\n",
      "epoch:36 step:28528 [D loss: 0.001122, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:36 step:28529 [D loss: 0.002112, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:36 step:28530 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:36 step:28531 [D loss: 0.000772, acc.: 100.00%] [G loss: 0.000586]\n",
      "epoch:36 step:28532 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.004328]\n",
      "epoch:36 step:28533 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:36 step:28534 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.003467]\n",
      "epoch:36 step:28535 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.008070]\n",
      "epoch:36 step:28536 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:36 step:28537 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:36 step:28538 [D loss: 0.001158, acc.: 100.00%] [G loss: 0.062128]\n",
      "epoch:36 step:28539 [D loss: 0.001053, acc.: 100.00%] [G loss: 0.023670]\n",
      "epoch:36 step:28540 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:36 step:28541 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000677]\n",
      "epoch:36 step:28542 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:36 step:28543 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.001985]\n",
      "epoch:36 step:28544 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.108170]\n",
      "epoch:36 step:28545 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.044342]\n",
      "epoch:36 step:28546 [D loss: 0.009009, acc.: 100.00%] [G loss: 0.002396]\n",
      "epoch:36 step:28547 [D loss: 0.027829, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:36 step:28548 [D loss: 0.004828, acc.: 100.00%] [G loss: 0.000261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28549 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.002317]\n",
      "epoch:36 step:28550 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:36 step:28551 [D loss: 0.000831, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:36 step:28552 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.000997]\n",
      "epoch:36 step:28553 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.008273]\n",
      "epoch:36 step:28554 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:36 step:28555 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:36 step:28556 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:36 step:28557 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.004654]\n",
      "epoch:36 step:28558 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.017966]\n",
      "epoch:36 step:28559 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:36 step:28560 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.000415]\n",
      "epoch:36 step:28561 [D loss: 0.002911, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:36 step:28562 [D loss: 0.001072, acc.: 100.00%] [G loss: 0.004616]\n",
      "epoch:36 step:28563 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:36 step:28564 [D loss: 0.003886, acc.: 100.00%] [G loss: 0.004034]\n",
      "epoch:36 step:28565 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:36 step:28566 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:36 step:28567 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:36 step:28568 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:36 step:28569 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:36 step:28570 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:36 step:28571 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:36 step:28572 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:36 step:28573 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:36 step:28574 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:36 step:28575 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.002691]\n",
      "epoch:36 step:28576 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:36 step:28577 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:36 step:28578 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.005686]\n",
      "epoch:36 step:28579 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.007964]\n",
      "epoch:36 step:28580 [D loss: 0.001208, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:36 step:28581 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:36 step:28582 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.004527]\n",
      "epoch:36 step:28583 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:36 step:28584 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:36 step:28585 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:36 step:28586 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:36 step:28587 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.002466]\n",
      "epoch:36 step:28588 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.002791]\n",
      "epoch:36 step:28589 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.019323]\n",
      "epoch:36 step:28590 [D loss: 0.008735, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:36 step:28591 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:36 step:28592 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:36 step:28593 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:36 step:28594 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:36 step:28595 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:36 step:28596 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:36 step:28597 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.026103]\n",
      "epoch:36 step:28598 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:36 step:28599 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:36 step:28600 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:36 step:28601 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:36 step:28602 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:36 step:28603 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000507]\n",
      "epoch:36 step:28604 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:36 step:28605 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001353]\n",
      "epoch:36 step:28606 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:36 step:28607 [D loss: 0.001983, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:36 step:28608 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:36 step:28609 [D loss: 0.006107, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:36 step:28610 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.001535]\n",
      "epoch:36 step:28611 [D loss: 0.139335, acc.: 95.31%] [G loss: 0.651040]\n",
      "epoch:36 step:28612 [D loss: 0.009077, acc.: 100.00%] [G loss: 0.539423]\n",
      "epoch:36 step:28613 [D loss: 0.289082, acc.: 87.50%] [G loss: 0.007219]\n",
      "epoch:36 step:28614 [D loss: 0.001007, acc.: 100.00%] [G loss: 0.001597]\n",
      "epoch:36 step:28615 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:36 step:28616 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:36 step:28617 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:36 step:28618 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:36 step:28619 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:36 step:28620 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.002208]\n",
      "epoch:36 step:28621 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:36 step:28622 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:36 step:28623 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:36 step:28624 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:36 step:28625 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:36 step:28626 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:36 step:28627 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:36 step:28628 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:36 step:28629 [D loss: 0.002021, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:36 step:28630 [D loss: 0.000803, acc.: 100.00%] [G loss: 0.002671]\n",
      "epoch:36 step:28631 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:36 step:28632 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:36 step:28633 [D loss: 0.002143, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:36 step:28634 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000639]\n",
      "epoch:36 step:28635 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.001265]\n",
      "epoch:36 step:28636 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:36 step:28637 [D loss: 0.110065, acc.: 94.53%] [G loss: 0.001163]\n",
      "epoch:36 step:28638 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.003920]\n",
      "epoch:36 step:28639 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.004349]\n",
      "epoch:36 step:28640 [D loss: 0.005029, acc.: 100.00%] [G loss: 0.005017]\n",
      "epoch:36 step:28641 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.001488]\n",
      "epoch:36 step:28642 [D loss: 0.239796, acc.: 88.28%] [G loss: 0.000007]\n",
      "epoch:36 step:28643 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:36 step:28644 [D loss: 0.015376, acc.: 99.22%] [G loss: 0.000006]\n",
      "epoch:36 step:28645 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:36 step:28646 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:36 step:28647 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.001581]\n",
      "epoch:36 step:28648 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:36 step:28649 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:36 step:28650 [D loss: 0.004525, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:36 step:28651 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:36 step:28652 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:36 step:28653 [D loss: 0.002875, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:36 step:28654 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:36 step:28655 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:36 step:28656 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:36 step:28657 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:36 step:28658 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.006311]\n",
      "epoch:36 step:28659 [D loss: 0.011272, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:36 step:28660 [D loss: 0.578790, acc.: 70.31%] [G loss: 0.037161]\n",
      "epoch:36 step:28661 [D loss: 0.072993, acc.: 97.66%] [G loss: 0.333208]\n",
      "epoch:36 step:28662 [D loss: 0.585369, acc.: 77.34%] [G loss: 0.004434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28663 [D loss: 0.011283, acc.: 99.22%] [G loss: 0.000078]\n",
      "epoch:36 step:28664 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:36 step:28665 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:36 step:28666 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:36 step:28667 [D loss: 0.002794, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:36 step:28668 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:36 step:28669 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.958895]\n",
      "epoch:36 step:28670 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:36 step:28671 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.029879]\n",
      "epoch:36 step:28672 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000656]\n",
      "epoch:36 step:28673 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:36 step:28674 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.696652]\n",
      "epoch:36 step:28675 [D loss: 0.000705, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:36 step:28676 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:36 step:28677 [D loss: 0.000844, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:36 step:28678 [D loss: 0.046552, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:36 step:28679 [D loss: 0.007176, acc.: 100.00%] [G loss: 0.611266]\n",
      "epoch:36 step:28680 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.350928]\n",
      "epoch:36 step:28681 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.365531]\n",
      "epoch:36 step:28682 [D loss: 0.001548, acc.: 100.00%] [G loss: 0.050717]\n",
      "epoch:36 step:28683 [D loss: 0.004025, acc.: 100.00%] [G loss: 0.044302]\n",
      "epoch:36 step:28684 [D loss: 0.088200, acc.: 98.44%] [G loss: 0.003231]\n",
      "epoch:36 step:28685 [D loss: 0.001640, acc.: 100.00%] [G loss: 1.160702]\n",
      "epoch:36 step:28686 [D loss: 0.035989, acc.: 97.66%] [G loss: 0.007865]\n",
      "epoch:36 step:28687 [D loss: 0.022639, acc.: 100.00%] [G loss: 0.005172]\n",
      "epoch:36 step:28688 [D loss: 0.004410, acc.: 100.00%] [G loss: 0.001346]\n",
      "epoch:36 step:28689 [D loss: 0.022087, acc.: 99.22%] [G loss: 0.338574]\n",
      "epoch:36 step:28690 [D loss: 0.004108, acc.: 100.00%] [G loss: 0.123360]\n",
      "epoch:36 step:28691 [D loss: 0.004354, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:36 step:28692 [D loss: 0.003242, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:36 step:28693 [D loss: 0.001362, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:36 step:28694 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.001741]\n",
      "epoch:36 step:28695 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.002304]\n",
      "epoch:36 step:28696 [D loss: 0.001773, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:36 step:28697 [D loss: 0.002794, acc.: 100.00%] [G loss: 0.086160]\n",
      "epoch:36 step:28698 [D loss: 0.004333, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:36 step:28699 [D loss: 0.001254, acc.: 100.00%] [G loss: 0.000755]\n",
      "epoch:36 step:28700 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:36 step:28701 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.036044]\n",
      "epoch:36 step:28702 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.078750]\n",
      "epoch:36 step:28703 [D loss: 0.006292, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:36 step:28704 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.037543]\n",
      "epoch:36 step:28705 [D loss: 0.026454, acc.: 100.00%] [G loss: 0.001176]\n",
      "epoch:36 step:28706 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:36 step:28707 [D loss: 0.031533, acc.: 99.22%] [G loss: 0.198659]\n",
      "epoch:36 step:28708 [D loss: 0.001125, acc.: 100.00%] [G loss: 0.001339]\n",
      "epoch:36 step:28709 [D loss: 0.005105, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:36 step:28710 [D loss: 0.002290, acc.: 100.00%] [G loss: 0.000724]\n",
      "epoch:36 step:28711 [D loss: 0.002232, acc.: 100.00%] [G loss: 0.040633]\n",
      "epoch:36 step:28712 [D loss: 0.002078, acc.: 100.00%] [G loss: 0.000498]\n",
      "epoch:36 step:28713 [D loss: 0.036080, acc.: 99.22%] [G loss: 0.002460]\n",
      "epoch:36 step:28714 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.000984]\n",
      "epoch:36 step:28715 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.002368]\n",
      "epoch:36 step:28716 [D loss: 0.014160, acc.: 100.00%] [G loss: 0.000960]\n",
      "epoch:36 step:28717 [D loss: 0.062304, acc.: 98.44%] [G loss: 0.008926]\n",
      "epoch:36 step:28718 [D loss: 0.048887, acc.: 98.44%] [G loss: 0.079096]\n",
      "epoch:36 step:28719 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.001186]\n",
      "epoch:36 step:28720 [D loss: 0.043197, acc.: 99.22%] [G loss: 0.004392]\n",
      "epoch:36 step:28721 [D loss: 0.001529, acc.: 100.00%] [G loss: 0.028876]\n",
      "epoch:36 step:28722 [D loss: 0.020417, acc.: 99.22%] [G loss: 0.020024]\n",
      "epoch:36 step:28723 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.005944]\n",
      "epoch:36 step:28724 [D loss: 0.001043, acc.: 100.00%] [G loss: 0.018785]\n",
      "epoch:36 step:28725 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.012981]\n",
      "epoch:36 step:28726 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.831267]\n",
      "epoch:36 step:28727 [D loss: 0.001400, acc.: 100.00%] [G loss: 0.004277]\n",
      "epoch:36 step:28728 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.002767]\n",
      "epoch:36 step:28729 [D loss: 0.042442, acc.: 99.22%] [G loss: 0.002345]\n",
      "epoch:36 step:28730 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.002110]\n",
      "epoch:36 step:28731 [D loss: 0.064831, acc.: 99.22%] [G loss: 0.026813]\n",
      "epoch:36 step:28732 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.078676]\n",
      "epoch:36 step:28733 [D loss: 0.003561, acc.: 100.00%] [G loss: 0.037402]\n",
      "epoch:36 step:28734 [D loss: 0.022840, acc.: 100.00%] [G loss: 0.008956]\n",
      "epoch:36 step:28735 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.003562]\n",
      "epoch:36 step:28736 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.007376]\n",
      "epoch:36 step:28737 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.002756]\n",
      "epoch:36 step:28738 [D loss: 0.001246, acc.: 100.00%] [G loss: 0.003284]\n",
      "epoch:36 step:28739 [D loss: 0.108985, acc.: 94.53%] [G loss: 0.008271]\n",
      "epoch:36 step:28740 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000801]\n",
      "epoch:36 step:28741 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:36 step:28742 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:36 step:28743 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000539]\n",
      "epoch:36 step:28744 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:36 step:28745 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:36 step:28746 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:36 step:28747 [D loss: 0.015715, acc.: 100.00%] [G loss: 0.002403]\n",
      "epoch:36 step:28748 [D loss: 0.001090, acc.: 100.00%] [G loss: 0.002763]\n",
      "epoch:36 step:28749 [D loss: 0.008664, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:36 step:28750 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.007216]\n",
      "epoch:36 step:28751 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.040579]\n",
      "epoch:36 step:28752 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.013456]\n",
      "epoch:36 step:28753 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000850]\n",
      "epoch:36 step:28754 [D loss: 0.002961, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:36 step:28755 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.003061]\n",
      "epoch:36 step:28756 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001261]\n",
      "epoch:36 step:28757 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:36 step:28758 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.010831]\n",
      "epoch:36 step:28759 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.003289]\n",
      "epoch:36 step:28760 [D loss: 0.001619, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:36 step:28761 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:36 step:28762 [D loss: 0.006922, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:36 step:28763 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:36 step:28764 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:36 step:28765 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:36 step:28766 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.014656]\n",
      "epoch:36 step:28767 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.003269]\n",
      "epoch:36 step:28768 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.007004]\n",
      "epoch:36 step:28769 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.013449]\n",
      "epoch:36 step:28770 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.007732]\n",
      "epoch:36 step:28771 [D loss: 0.128125, acc.: 96.88%] [G loss: 0.738130]\n",
      "epoch:36 step:28772 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.206145]\n",
      "epoch:36 step:28773 [D loss: 0.030592, acc.: 100.00%] [G loss: 0.021696]\n",
      "epoch:36 step:28774 [D loss: 0.023029, acc.: 99.22%] [G loss: 0.021861]\n",
      "epoch:36 step:28775 [D loss: 0.002044, acc.: 100.00%] [G loss: 0.011341]\n",
      "epoch:36 step:28776 [D loss: 0.001073, acc.: 100.00%] [G loss: 0.296005]\n",
      "epoch:36 step:28777 [D loss: 0.007275, acc.: 100.00%] [G loss: 0.008257]\n",
      "epoch:36 step:28778 [D loss: 0.016264, acc.: 99.22%] [G loss: 0.000709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28779 [D loss: 0.002089, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:36 step:28780 [D loss: 0.002695, acc.: 100.00%] [G loss: 0.022902]\n",
      "epoch:36 step:28781 [D loss: 0.003421, acc.: 100.00%] [G loss: 0.005135]\n",
      "epoch:36 step:28782 [D loss: 0.009631, acc.: 100.00%] [G loss: 0.014167]\n",
      "epoch:36 step:28783 [D loss: 0.006268, acc.: 100.00%] [G loss: 0.054276]\n",
      "epoch:36 step:28784 [D loss: 0.023439, acc.: 100.00%] [G loss: 0.000947]\n",
      "epoch:36 step:28785 [D loss: 0.003367, acc.: 100.00%] [G loss: 0.439078]\n",
      "epoch:36 step:28786 [D loss: 0.008240, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:36 step:28787 [D loss: 0.002061, acc.: 100.00%] [G loss: 0.001176]\n",
      "epoch:36 step:28788 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.016082]\n",
      "epoch:36 step:28789 [D loss: 0.001779, acc.: 100.00%] [G loss: 0.221590]\n",
      "epoch:36 step:28790 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.087475]\n",
      "epoch:36 step:28791 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.001951]\n",
      "epoch:36 step:28792 [D loss: 0.174052, acc.: 93.75%] [G loss: 0.832400]\n",
      "epoch:36 step:28793 [D loss: 0.023558, acc.: 99.22%] [G loss: 0.231518]\n",
      "epoch:36 step:28794 [D loss: 0.185411, acc.: 91.41%] [G loss: 0.003899]\n",
      "epoch:36 step:28795 [D loss: 0.002834, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:36 step:28796 [D loss: 0.001954, acc.: 100.00%] [G loss: 0.003496]\n",
      "epoch:36 step:28797 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.003381]\n",
      "epoch:36 step:28798 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.692587]\n",
      "epoch:36 step:28799 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:36 step:28800 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:36 step:28801 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:36 step:28802 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.000729]\n",
      "epoch:36 step:28803 [D loss: 0.004964, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:36 step:28804 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.464682]\n",
      "epoch:36 step:28805 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.042092]\n",
      "epoch:36 step:28806 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.031614]\n",
      "epoch:36 step:28807 [D loss: 0.052724, acc.: 97.66%] [G loss: 0.001252]\n",
      "epoch:36 step:28808 [D loss: 0.014141, acc.: 99.22%] [G loss: 0.163555]\n",
      "epoch:36 step:28809 [D loss: 0.006422, acc.: 100.00%] [G loss: 0.001168]\n",
      "epoch:36 step:28810 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001228]\n",
      "epoch:36 step:28811 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.497490]\n",
      "epoch:36 step:28812 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.001359]\n",
      "epoch:36 step:28813 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.229120]\n",
      "epoch:36 step:28814 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.001039]\n",
      "epoch:36 step:28815 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.003773]\n",
      "epoch:36 step:28816 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.010527]\n",
      "epoch:36 step:28817 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.026763]\n",
      "epoch:36 step:28818 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.001297]\n",
      "epoch:36 step:28819 [D loss: 0.017743, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:36 step:28820 [D loss: 0.004771, acc.: 100.00%] [G loss: 0.032211]\n",
      "epoch:36 step:28821 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:36 step:28822 [D loss: 0.003355, acc.: 100.00%] [G loss: 0.041262]\n",
      "epoch:36 step:28823 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.057382]\n",
      "epoch:36 step:28824 [D loss: 0.006226, acc.: 100.00%] [G loss: 0.045578]\n",
      "epoch:36 step:28825 [D loss: 0.029769, acc.: 100.00%] [G loss: 0.323529]\n",
      "epoch:36 step:28826 [D loss: 0.002987, acc.: 100.00%] [G loss: 0.290830]\n",
      "epoch:36 step:28827 [D loss: 0.005337, acc.: 100.00%] [G loss: 0.198753]\n",
      "epoch:36 step:28828 [D loss: 0.077552, acc.: 98.44%] [G loss: 0.102859]\n",
      "epoch:36 step:28829 [D loss: 0.433804, acc.: 82.81%] [G loss: 0.000385]\n",
      "epoch:36 step:28830 [D loss: 0.165225, acc.: 92.97%] [G loss: 0.008974]\n",
      "epoch:36 step:28831 [D loss: 0.005872, acc.: 100.00%] [G loss: 5.232295]\n",
      "epoch:36 step:28832 [D loss: 0.034082, acc.: 98.44%] [G loss: 0.061162]\n",
      "epoch:36 step:28833 [D loss: 0.011119, acc.: 100.00%] [G loss: 0.033565]\n",
      "epoch:36 step:28834 [D loss: 0.003618, acc.: 100.00%] [G loss: 0.009318]\n",
      "epoch:36 step:28835 [D loss: 0.007064, acc.: 99.22%] [G loss: 0.002668]\n",
      "epoch:36 step:28836 [D loss: 0.015175, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:36 step:28837 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:36 step:28838 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:36 step:28839 [D loss: 0.001103, acc.: 100.00%] [G loss: 1.642849]\n",
      "epoch:36 step:28840 [D loss: 0.003703, acc.: 100.00%] [G loss: 0.264904]\n",
      "epoch:36 step:28841 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:36 step:28842 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.061880]\n",
      "epoch:36 step:28843 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:36 step:28844 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.012803]\n",
      "epoch:36 step:28845 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:36 step:28846 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:36 step:28847 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:36 step:28848 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.001661]\n",
      "epoch:36 step:28849 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:36 step:28850 [D loss: 0.006730, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:36 step:28851 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:36 step:28852 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:36 step:28853 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:36 step:28854 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:36 step:28855 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.003197]\n",
      "epoch:36 step:28856 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:36 step:28857 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.010480]\n",
      "epoch:36 step:28858 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:36 step:28859 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:36 step:28860 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:36 step:28861 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.009195]\n",
      "epoch:36 step:28862 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:36 step:28863 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.006910]\n",
      "epoch:36 step:28864 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.002886]\n",
      "epoch:36 step:28865 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:36 step:28866 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:36 step:28867 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:36 step:28868 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:36 step:28869 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:36 step:28870 [D loss: 0.003099, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:36 step:28871 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:36 step:28872 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:36 step:28873 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002824]\n",
      "epoch:36 step:28874 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.006159]\n",
      "epoch:36 step:28875 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:36 step:28876 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:36 step:28877 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:36 step:28878 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:36 step:28879 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:36 step:28880 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:36 step:28881 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:36 step:28882 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:36 step:28883 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:36 step:28884 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:36 step:28885 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:36 step:28886 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:36 step:28887 [D loss: 0.055222, acc.: 98.44%] [G loss: 0.000328]\n",
      "epoch:36 step:28888 [D loss: 0.008276, acc.: 99.22%] [G loss: 0.158462]\n",
      "epoch:36 step:28889 [D loss: 0.002215, acc.: 100.00%] [G loss: 0.107778]\n",
      "epoch:36 step:28890 [D loss: 0.011844, acc.: 100.00%] [G loss: 0.001757]\n",
      "epoch:36 step:28891 [D loss: 0.003713, acc.: 100.00%] [G loss: 0.003437]\n",
      "epoch:36 step:28892 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.137554]\n",
      "epoch:36 step:28893 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.011147]\n",
      "epoch:36 step:28894 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.002018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28895 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.008589]\n",
      "epoch:36 step:28896 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.002863]\n",
      "epoch:36 step:28897 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.001250]\n",
      "epoch:37 step:28898 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.006658]\n",
      "epoch:37 step:28899 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:37 step:28900 [D loss: 0.005131, acc.: 100.00%] [G loss: 0.001228]\n",
      "epoch:37 step:28901 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002851]\n",
      "epoch:37 step:28902 [D loss: 0.001199, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:37 step:28903 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000961]\n",
      "epoch:37 step:28904 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:37 step:28905 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000363]\n",
      "epoch:37 step:28906 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:37 step:28907 [D loss: 0.004028, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:37 step:28908 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:37 step:28909 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.001583]\n",
      "epoch:37 step:28910 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:37 step:28911 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:37 step:28912 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.002804]\n",
      "epoch:37 step:28913 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:37 step:28914 [D loss: 0.028041, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:37 step:28915 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001559]\n",
      "epoch:37 step:28916 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.003326]\n",
      "epoch:37 step:28917 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.004086]\n",
      "epoch:37 step:28918 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.018180]\n",
      "epoch:37 step:28919 [D loss: 0.004828, acc.: 100.00%] [G loss: 0.001705]\n",
      "epoch:37 step:28920 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.003220]\n",
      "epoch:37 step:28921 [D loss: 0.001097, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:37 step:28922 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.003475]\n",
      "epoch:37 step:28923 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:37 step:28924 [D loss: 0.129007, acc.: 96.88%] [G loss: 0.418026]\n",
      "epoch:37 step:28925 [D loss: 0.161595, acc.: 94.53%] [G loss: 0.755987]\n",
      "epoch:37 step:28926 [D loss: 0.007156, acc.: 100.00%] [G loss: 0.563990]\n",
      "epoch:37 step:28927 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.007419]\n",
      "epoch:37 step:28928 [D loss: 0.003779, acc.: 100.00%] [G loss: 0.004684]\n",
      "epoch:37 step:28929 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.002653]\n",
      "epoch:37 step:28930 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.004811]\n",
      "epoch:37 step:28931 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.002312]\n",
      "epoch:37 step:28932 [D loss: 0.001323, acc.: 100.00%] [G loss: 0.001790]\n",
      "epoch:37 step:28933 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.004976]\n",
      "epoch:37 step:28934 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.006881]\n",
      "epoch:37 step:28935 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:37 step:28936 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001389]\n",
      "epoch:37 step:28937 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003517]\n",
      "epoch:37 step:28938 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.003581]\n",
      "epoch:37 step:28939 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.002425]\n",
      "epoch:37 step:28940 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.003412]\n",
      "epoch:37 step:28941 [D loss: 0.006613, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:37 step:28942 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:37 step:28943 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:37 step:28944 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.001588]\n",
      "epoch:37 step:28945 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:37 step:28946 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:37 step:28947 [D loss: 0.001749, acc.: 100.00%] [G loss: 0.001321]\n",
      "epoch:37 step:28948 [D loss: 0.085638, acc.: 93.75%] [G loss: 0.033301]\n",
      "epoch:37 step:28949 [D loss: 0.001804, acc.: 100.00%] [G loss: 0.817372]\n",
      "epoch:37 step:28950 [D loss: 0.064420, acc.: 98.44%] [G loss: 0.102263]\n",
      "epoch:37 step:28951 [D loss: 0.049436, acc.: 98.44%] [G loss: 0.012584]\n",
      "epoch:37 step:28952 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.004804]\n",
      "epoch:37 step:28953 [D loss: 0.068891, acc.: 96.88%] [G loss: 0.007649]\n",
      "epoch:37 step:28954 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:37 step:28955 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:37 step:28956 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:37 step:28957 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:37 step:28958 [D loss: 0.003435, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:37 step:28959 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:37 step:28960 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:37 step:28961 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:37 step:28962 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000452]\n",
      "epoch:37 step:28963 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:37 step:28964 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:37 step:28965 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:37 step:28966 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:37 step:28967 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:37 step:28968 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:37 step:28969 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:37 step:28970 [D loss: 0.001149, acc.: 100.00%] [G loss: 0.000526]\n",
      "epoch:37 step:28971 [D loss: 0.098414, acc.: 96.88%] [G loss: 0.005317]\n",
      "epoch:37 step:28972 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.030394]\n",
      "epoch:37 step:28973 [D loss: 0.046006, acc.: 97.66%] [G loss: 1.097807]\n",
      "epoch:37 step:28974 [D loss: 0.057340, acc.: 97.66%] [G loss: 0.000868]\n",
      "epoch:37 step:28975 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.002004]\n",
      "epoch:37 step:28976 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.629869]\n",
      "epoch:37 step:28977 [D loss: 0.016801, acc.: 99.22%] [G loss: 0.001074]\n",
      "epoch:37 step:28978 [D loss: 0.055550, acc.: 100.00%] [G loss: 0.019175]\n",
      "epoch:37 step:28979 [D loss: 0.005922, acc.: 99.22%] [G loss: 0.104112]\n",
      "epoch:37 step:28980 [D loss: 0.003374, acc.: 100.00%] [G loss: 0.032592]\n",
      "epoch:37 step:28981 [D loss: 0.000951, acc.: 100.00%] [G loss: 0.020780]\n",
      "epoch:37 step:28982 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.009331]\n",
      "epoch:37 step:28983 [D loss: 0.023988, acc.: 100.00%] [G loss: 0.001138]\n",
      "epoch:37 step:28984 [D loss: 0.006292, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:37 step:28985 [D loss: 0.020532, acc.: 100.00%] [G loss: 0.004287]\n",
      "epoch:37 step:28986 [D loss: 0.019539, acc.: 100.00%] [G loss: 0.113057]\n",
      "epoch:37 step:28987 [D loss: 2.660586, acc.: 33.59%] [G loss: 8.637851]\n",
      "epoch:37 step:28988 [D loss: 0.087779, acc.: 96.88%] [G loss: 3.766800]\n",
      "epoch:37 step:28989 [D loss: 0.805276, acc.: 69.53%] [G loss: 8.352096]\n",
      "epoch:37 step:28990 [D loss: 0.012458, acc.: 100.00%] [G loss: 5.060900]\n",
      "epoch:37 step:28991 [D loss: 0.128125, acc.: 95.31%] [G loss: 4.350692]\n",
      "epoch:37 step:28992 [D loss: 0.002486, acc.: 100.00%] [G loss: 4.603130]\n",
      "epoch:37 step:28993 [D loss: 0.010308, acc.: 100.00%] [G loss: 2.991453]\n",
      "epoch:37 step:28994 [D loss: 0.088306, acc.: 98.44%] [G loss: 3.240330]\n",
      "epoch:37 step:28995 [D loss: 0.271123, acc.: 85.16%] [G loss: 3.977198]\n",
      "epoch:37 step:28996 [D loss: 0.296955, acc.: 83.59%] [G loss: 0.144011]\n",
      "epoch:37 step:28997 [D loss: 0.026951, acc.: 100.00%] [G loss: 2.430744]\n",
      "epoch:37 step:28998 [D loss: 0.020323, acc.: 100.00%] [G loss: 2.400126]\n",
      "epoch:37 step:28999 [D loss: 0.070844, acc.: 100.00%] [G loss: 2.555766]\n",
      "epoch:37 step:29000 [D loss: 0.088117, acc.: 97.66%] [G loss: 2.878175]\n",
      "epoch:37 step:29001 [D loss: 0.018949, acc.: 100.00%] [G loss: 2.969656]\n",
      "epoch:37 step:29002 [D loss: 0.082513, acc.: 98.44%] [G loss: 3.629972]\n",
      "epoch:37 step:29003 [D loss: 0.009904, acc.: 100.00%] [G loss: 4.308541]\n",
      "epoch:37 step:29004 [D loss: 0.071121, acc.: 96.09%] [G loss: 1.253716]\n",
      "epoch:37 step:29005 [D loss: 0.053627, acc.: 100.00%] [G loss: 5.385584]\n",
      "epoch:37 step:29006 [D loss: 0.007973, acc.: 100.00%] [G loss: 5.917355]\n",
      "epoch:37 step:29007 [D loss: 0.023631, acc.: 100.00%] [G loss: 6.692358]\n",
      "epoch:37 step:29008 [D loss: 0.018168, acc.: 100.00%] [G loss: 6.104666]\n",
      "epoch:37 step:29009 [D loss: 0.005762, acc.: 100.00%] [G loss: 3.907815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29010 [D loss: 0.004614, acc.: 100.00%] [G loss: 4.652639]\n",
      "epoch:37 step:29011 [D loss: 0.014203, acc.: 100.00%] [G loss: 0.258329]\n",
      "epoch:37 step:29012 [D loss: 0.541431, acc.: 77.34%] [G loss: 10.624828]\n",
      "epoch:37 step:29013 [D loss: 1.488670, acc.: 53.91%] [G loss: 7.149917]\n",
      "epoch:37 step:29014 [D loss: 0.026619, acc.: 100.00%] [G loss: 0.477022]\n",
      "epoch:37 step:29015 [D loss: 0.030022, acc.: 100.00%] [G loss: 6.594851]\n",
      "epoch:37 step:29016 [D loss: 0.025839, acc.: 100.00%] [G loss: 7.195508]\n",
      "epoch:37 step:29017 [D loss: 0.017057, acc.: 100.00%] [G loss: 6.601855]\n",
      "epoch:37 step:29018 [D loss: 0.009979, acc.: 100.00%] [G loss: 0.119966]\n",
      "epoch:37 step:29019 [D loss: 0.040378, acc.: 100.00%] [G loss: 0.144354]\n",
      "epoch:37 step:29020 [D loss: 0.035387, acc.: 100.00%] [G loss: 0.273922]\n",
      "epoch:37 step:29021 [D loss: 0.057244, acc.: 99.22%] [G loss: 0.226711]\n",
      "epoch:37 step:29022 [D loss: 0.088964, acc.: 98.44%] [G loss: 7.723203]\n",
      "epoch:37 step:29023 [D loss: 0.062201, acc.: 98.44%] [G loss: 7.171144]\n",
      "epoch:37 step:29024 [D loss: 0.008413, acc.: 100.00%] [G loss: 6.760883]\n",
      "epoch:37 step:29025 [D loss: 0.016056, acc.: 99.22%] [G loss: 6.342467]\n",
      "epoch:37 step:29026 [D loss: 0.004165, acc.: 100.00%] [G loss: 0.056776]\n",
      "epoch:37 step:29027 [D loss: 0.015994, acc.: 100.00%] [G loss: 0.119826]\n",
      "epoch:37 step:29028 [D loss: 0.005499, acc.: 100.00%] [G loss: 5.835992]\n",
      "epoch:37 step:29029 [D loss: 0.006162, acc.: 100.00%] [G loss: 6.419712]\n",
      "epoch:37 step:29030 [D loss: 0.004095, acc.: 100.00%] [G loss: 5.560674]\n",
      "epoch:37 step:29031 [D loss: 0.005794, acc.: 100.00%] [G loss: 5.488243]\n",
      "epoch:37 step:29032 [D loss: 0.006986, acc.: 100.00%] [G loss: 0.104006]\n",
      "epoch:37 step:29033 [D loss: 0.005981, acc.: 100.00%] [G loss: 4.690689]\n",
      "epoch:37 step:29034 [D loss: 0.007620, acc.: 100.00%] [G loss: 4.547743]\n",
      "epoch:37 step:29035 [D loss: 0.014965, acc.: 100.00%] [G loss: 3.665603]\n",
      "epoch:37 step:29036 [D loss: 0.009159, acc.: 100.00%] [G loss: 4.011342]\n",
      "epoch:37 step:29037 [D loss: 0.010982, acc.: 100.00%] [G loss: 3.554053]\n",
      "epoch:37 step:29038 [D loss: 0.013811, acc.: 100.00%] [G loss: 2.828531]\n",
      "epoch:37 step:29039 [D loss: 0.009282, acc.: 100.00%] [G loss: 2.914551]\n",
      "epoch:37 step:29040 [D loss: 0.071409, acc.: 99.22%] [G loss: 3.454860]\n",
      "epoch:37 step:29041 [D loss: 0.008453, acc.: 100.00%] [G loss: 3.800205]\n",
      "epoch:37 step:29042 [D loss: 0.009555, acc.: 100.00%] [G loss: 3.438830]\n",
      "epoch:37 step:29043 [D loss: 0.013323, acc.: 100.00%] [G loss: 1.798956]\n",
      "epoch:37 step:29044 [D loss: 0.006565, acc.: 100.00%] [G loss: 1.840205]\n",
      "epoch:37 step:29045 [D loss: 0.053978, acc.: 98.44%] [G loss: 2.111923]\n",
      "epoch:37 step:29046 [D loss: 0.032991, acc.: 99.22%] [G loss: 2.923665]\n",
      "epoch:37 step:29047 [D loss: 0.074937, acc.: 96.88%] [G loss: 5.308616]\n",
      "epoch:37 step:29048 [D loss: 0.029251, acc.: 99.22%] [G loss: 0.504092]\n",
      "epoch:37 step:29049 [D loss: 0.006573, acc.: 100.00%] [G loss: 5.624553]\n",
      "epoch:37 step:29050 [D loss: 0.081519, acc.: 96.88%] [G loss: 3.969505]\n",
      "epoch:37 step:29051 [D loss: 0.035143, acc.: 100.00%] [G loss: 1.330504]\n",
      "epoch:37 step:29052 [D loss: 0.009346, acc.: 100.00%] [G loss: 1.104789]\n",
      "epoch:37 step:29053 [D loss: 0.335890, acc.: 79.69%] [G loss: 6.978322]\n",
      "epoch:37 step:29054 [D loss: 0.845882, acc.: 66.41%] [G loss: 6.372532]\n",
      "epoch:37 step:29055 [D loss: 0.024717, acc.: 99.22%] [G loss: 3.619198]\n",
      "epoch:37 step:29056 [D loss: 0.325865, acc.: 86.72%] [G loss: 4.724761]\n",
      "epoch:37 step:29057 [D loss: 0.060135, acc.: 97.66%] [G loss: 7.830006]\n",
      "epoch:37 step:29058 [D loss: 0.228547, acc.: 93.75%] [G loss: 4.666375]\n",
      "epoch:37 step:29059 [D loss: 0.005316, acc.: 100.00%] [G loss: 1.370886]\n",
      "epoch:37 step:29060 [D loss: 0.307446, acc.: 88.28%] [G loss: 5.486838]\n",
      "epoch:37 step:29061 [D loss: 0.037009, acc.: 99.22%] [G loss: 6.362759]\n",
      "epoch:37 step:29062 [D loss: 0.075600, acc.: 96.09%] [G loss: 1.075898]\n",
      "epoch:37 step:29063 [D loss: 0.009213, acc.: 100.00%] [G loss: 3.067320]\n",
      "epoch:37 step:29064 [D loss: 0.009971, acc.: 100.00%] [G loss: 1.183801]\n",
      "epoch:37 step:29065 [D loss: 0.004474, acc.: 100.00%] [G loss: 0.194065]\n",
      "epoch:37 step:29066 [D loss: 0.496415, acc.: 75.78%] [G loss: 5.930326]\n",
      "epoch:37 step:29067 [D loss: 0.526483, acc.: 76.56%] [G loss: 1.081026]\n",
      "epoch:37 step:29068 [D loss: 0.110846, acc.: 98.44%] [G loss: 1.906857]\n",
      "epoch:37 step:29069 [D loss: 0.002122, acc.: 100.00%] [G loss: 1.900395]\n",
      "epoch:37 step:29070 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.557899]\n",
      "epoch:37 step:29071 [D loss: 0.021106, acc.: 98.44%] [G loss: 0.380423]\n",
      "epoch:37 step:29072 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.091470]\n",
      "epoch:37 step:29073 [D loss: 0.005095, acc.: 100.00%] [G loss: 0.187241]\n",
      "epoch:37 step:29074 [D loss: 0.004989, acc.: 100.00%] [G loss: 0.018304]\n",
      "epoch:37 step:29075 [D loss: 0.007817, acc.: 100.00%] [G loss: 0.531804]\n",
      "epoch:37 step:29076 [D loss: 0.002319, acc.: 100.00%] [G loss: 0.441399]\n",
      "epoch:37 step:29077 [D loss: 0.003932, acc.: 100.00%] [G loss: 0.000990]\n",
      "epoch:37 step:29078 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.004479]\n",
      "epoch:37 step:29079 [D loss: 0.003718, acc.: 100.00%] [G loss: 0.002826]\n",
      "epoch:37 step:29080 [D loss: 0.008852, acc.: 100.00%] [G loss: 0.680841]\n",
      "epoch:37 step:29081 [D loss: 0.003190, acc.: 100.00%] [G loss: 0.425723]\n",
      "epoch:37 step:29082 [D loss: 0.000692, acc.: 100.00%] [G loss: 1.050016]\n",
      "epoch:37 step:29083 [D loss: 0.002067, acc.: 100.00%] [G loss: 0.291265]\n",
      "epoch:37 step:29084 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.003968]\n",
      "epoch:37 step:29085 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.072037]\n",
      "epoch:37 step:29086 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.002308]\n",
      "epoch:37 step:29087 [D loss: 0.008300, acc.: 100.00%] [G loss: 0.033918]\n",
      "epoch:37 step:29088 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.005814]\n",
      "epoch:37 step:29089 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.045192]\n",
      "epoch:37 step:29090 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.001928]\n",
      "epoch:37 step:29091 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.003274]\n",
      "epoch:37 step:29092 [D loss: 0.001014, acc.: 100.00%] [G loss: 0.015904]\n",
      "epoch:37 step:29093 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.004166]\n",
      "epoch:37 step:29094 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.000676]\n",
      "epoch:37 step:29095 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.000804]\n",
      "epoch:37 step:29096 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.001162]\n",
      "epoch:37 step:29097 [D loss: 0.003634, acc.: 100.00%] [G loss: 0.003746]\n",
      "epoch:37 step:29098 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.020850]\n",
      "epoch:37 step:29099 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.013025]\n",
      "epoch:37 step:29100 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000600]\n",
      "epoch:37 step:29101 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.002785]\n",
      "epoch:37 step:29102 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.003994]\n",
      "epoch:37 step:29103 [D loss: 0.001580, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:37 step:29104 [D loss: 0.001213, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:37 step:29105 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000881]\n",
      "epoch:37 step:29106 [D loss: 0.001132, acc.: 100.00%] [G loss: 0.004704]\n",
      "epoch:37 step:29107 [D loss: 0.001339, acc.: 100.00%] [G loss: 0.003270]\n",
      "epoch:37 step:29108 [D loss: 0.009344, acc.: 99.22%] [G loss: 0.006019]\n",
      "epoch:37 step:29109 [D loss: 0.002872, acc.: 100.00%] [G loss: 0.002443]\n",
      "epoch:37 step:29110 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.034254]\n",
      "epoch:37 step:29111 [D loss: 0.002862, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:37 step:29112 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.000724]\n",
      "epoch:37 step:29113 [D loss: 0.004299, acc.: 100.00%] [G loss: 0.001291]\n",
      "epoch:37 step:29114 [D loss: 0.043631, acc.: 99.22%] [G loss: 0.017127]\n",
      "epoch:37 step:29115 [D loss: 0.000885, acc.: 100.00%] [G loss: 0.010150]\n",
      "epoch:37 step:29116 [D loss: 0.002136, acc.: 100.00%] [G loss: 0.023741]\n",
      "epoch:37 step:29117 [D loss: 0.024241, acc.: 100.00%] [G loss: 0.037876]\n",
      "epoch:37 step:29118 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.006370]\n",
      "epoch:37 step:29119 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.002470]\n",
      "epoch:37 step:29120 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.127647]\n",
      "epoch:37 step:29121 [D loss: 0.001284, acc.: 100.00%] [G loss: 0.008568]\n",
      "epoch:37 step:29122 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29123 [D loss: 0.003175, acc.: 100.00%] [G loss: 0.034625]\n",
      "epoch:37 step:29124 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.001495]\n",
      "epoch:37 step:29125 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:37 step:29126 [D loss: 0.002025, acc.: 100.00%] [G loss: 0.020000]\n",
      "epoch:37 step:29127 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.002083]\n",
      "epoch:37 step:29128 [D loss: 0.000987, acc.: 100.00%] [G loss: 0.041988]\n",
      "epoch:37 step:29129 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.001155]\n",
      "epoch:37 step:29130 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:37 step:29131 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.004739]\n",
      "epoch:37 step:29132 [D loss: 0.017093, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:37 step:29133 [D loss: 0.059694, acc.: 98.44%] [G loss: 0.000997]\n",
      "epoch:37 step:29134 [D loss: 0.009980, acc.: 100.00%] [G loss: 0.003720]\n",
      "epoch:37 step:29135 [D loss: 0.003822, acc.: 100.00%] [G loss: 0.003194]\n",
      "epoch:37 step:29136 [D loss: 0.002295, acc.: 100.00%] [G loss: 0.056621]\n",
      "epoch:37 step:29137 [D loss: 0.088435, acc.: 96.09%] [G loss: 0.000047]\n",
      "epoch:37 step:29138 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:37 step:29139 [D loss: 0.002178, acc.: 100.00%] [G loss: 0.001991]\n",
      "epoch:37 step:29140 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:37 step:29141 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.005723]\n",
      "epoch:37 step:29142 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.002472]\n",
      "epoch:37 step:29143 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:37 step:29144 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:37 step:29145 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.011845]\n",
      "epoch:37 step:29146 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:37 step:29147 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:37 step:29148 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:37 step:29149 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:37 step:29150 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:37 step:29151 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001294]\n",
      "epoch:37 step:29152 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:37 step:29153 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002436]\n",
      "epoch:37 step:29154 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:37 step:29155 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:37 step:29156 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:37 step:29157 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:37 step:29158 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.003852]\n",
      "epoch:37 step:29159 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:37 step:29160 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:37 step:29161 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.005682]\n",
      "epoch:37 step:29162 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:37 step:29163 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.002547]\n",
      "epoch:37 step:29164 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:37 step:29165 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002289]\n",
      "epoch:37 step:29166 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001146]\n",
      "epoch:37 step:29167 [D loss: 0.002720, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:37 step:29168 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:37 step:29169 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:37 step:29170 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:37 step:29171 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002858]\n",
      "epoch:37 step:29172 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:37 step:29173 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:37 step:29174 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:37 step:29175 [D loss: 0.018586, acc.: 99.22%] [G loss: 0.003271]\n",
      "epoch:37 step:29176 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.003103]\n",
      "epoch:37 step:29177 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.035803]\n",
      "epoch:37 step:29178 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:37 step:29179 [D loss: 0.001057, acc.: 100.00%] [G loss: 0.001070]\n",
      "epoch:37 step:29180 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000604]\n",
      "epoch:37 step:29181 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.002775]\n",
      "epoch:37 step:29182 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:37 step:29183 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.000481]\n",
      "epoch:37 step:29184 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.011218]\n",
      "epoch:37 step:29185 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:37 step:29186 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:37 step:29187 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.106744]\n",
      "epoch:37 step:29188 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.004952]\n",
      "epoch:37 step:29189 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000594]\n",
      "epoch:37 step:29190 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.001592]\n",
      "epoch:37 step:29191 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:37 step:29192 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:37 step:29193 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:37 step:29194 [D loss: 0.001063, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:37 step:29195 [D loss: 0.016354, acc.: 99.22%] [G loss: 0.000074]\n",
      "epoch:37 step:29196 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:37 step:29197 [D loss: 0.009597, acc.: 100.00%] [G loss: 0.007275]\n",
      "epoch:37 step:29198 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:37 step:29199 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.010093]\n",
      "epoch:37 step:29200 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000659]\n",
      "epoch:37 step:29201 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:37 step:29202 [D loss: 0.004388, acc.: 100.00%] [G loss: 0.007174]\n",
      "epoch:37 step:29203 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:37 step:29204 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.002986]\n",
      "epoch:37 step:29205 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:37 step:29206 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.007781]\n",
      "epoch:37 step:29207 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:37 step:29208 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.092074]\n",
      "epoch:37 step:29209 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.009027]\n",
      "epoch:37 step:29210 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:37 step:29211 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.003075]\n",
      "epoch:37 step:29212 [D loss: 0.007414, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:37 step:29213 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:37 step:29214 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.024874]\n",
      "epoch:37 step:29215 [D loss: 0.007566, acc.: 100.00%] [G loss: 0.000468]\n",
      "epoch:37 step:29216 [D loss: 0.002262, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:37 step:29217 [D loss: 0.019766, acc.: 99.22%] [G loss: 0.000096]\n",
      "epoch:37 step:29218 [D loss: 0.012487, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:37 step:29219 [D loss: 0.005781, acc.: 100.00%] [G loss: 0.043962]\n",
      "epoch:37 step:29220 [D loss: 0.001433, acc.: 100.00%] [G loss: 0.006872]\n",
      "epoch:37 step:29221 [D loss: 0.042648, acc.: 100.00%] [G loss: 0.387490]\n",
      "epoch:37 step:29222 [D loss: 0.076034, acc.: 97.66%] [G loss: 0.084531]\n",
      "epoch:37 step:29223 [D loss: 0.004493, acc.: 100.00%] [G loss: 0.026425]\n",
      "epoch:37 step:29224 [D loss: 0.002462, acc.: 100.00%] [G loss: 1.106094]\n",
      "epoch:37 step:29225 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.078666]\n",
      "epoch:37 step:29226 [D loss: 0.002529, acc.: 100.00%] [G loss: 1.434024]\n",
      "epoch:37 step:29227 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.023830]\n",
      "epoch:37 step:29228 [D loss: 0.001941, acc.: 100.00%] [G loss: 0.041104]\n",
      "epoch:37 step:29229 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.354935]\n",
      "epoch:37 step:29230 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.050024]\n",
      "epoch:37 step:29231 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.072836]\n",
      "epoch:37 step:29232 [D loss: 0.004235, acc.: 100.00%] [G loss: 0.000799]\n",
      "epoch:37 step:29233 [D loss: 0.006424, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:37 step:29234 [D loss: 0.002458, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:37 step:29235 [D loss: 0.435646, acc.: 82.03%] [G loss: 5.599725]\n",
      "epoch:37 step:29236 [D loss: 0.360419, acc.: 85.94%] [G loss: 9.716700]\n",
      "epoch:37 step:29237 [D loss: 1.185478, acc.: 68.75%] [G loss: 0.425989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29238 [D loss: 0.221394, acc.: 90.62%] [G loss: 0.240492]\n",
      "epoch:37 step:29239 [D loss: 0.253187, acc.: 92.19%] [G loss: 2.766996]\n",
      "epoch:37 step:29240 [D loss: 0.001293, acc.: 100.00%] [G loss: 4.006199]\n",
      "epoch:37 step:29241 [D loss: 0.017659, acc.: 98.44%] [G loss: 0.063963]\n",
      "epoch:37 step:29242 [D loss: 0.012127, acc.: 100.00%] [G loss: 0.682547]\n",
      "epoch:37 step:29243 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.611809]\n",
      "epoch:37 step:29244 [D loss: 0.003850, acc.: 100.00%] [G loss: 0.002146]\n",
      "epoch:37 step:29245 [D loss: 0.010221, acc.: 99.22%] [G loss: 0.190726]\n",
      "epoch:37 step:29246 [D loss: 0.069191, acc.: 97.66%] [G loss: 0.046794]\n",
      "epoch:37 step:29247 [D loss: 0.001663, acc.: 100.00%] [G loss: 0.002465]\n",
      "epoch:37 step:29248 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.006805]\n",
      "epoch:37 step:29249 [D loss: 0.004988, acc.: 100.00%] [G loss: 0.002345]\n",
      "epoch:37 step:29250 [D loss: 0.010218, acc.: 100.00%] [G loss: 0.008821]\n",
      "epoch:37 step:29251 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.003594]\n",
      "epoch:37 step:29252 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.007833]\n",
      "epoch:37 step:29253 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.093454]\n",
      "epoch:37 step:29254 [D loss: 0.013295, acc.: 100.00%] [G loss: 0.002077]\n",
      "epoch:37 step:29255 [D loss: 0.002271, acc.: 100.00%] [G loss: 0.018800]\n",
      "epoch:37 step:29256 [D loss: 0.006978, acc.: 100.00%] [G loss: 0.002019]\n",
      "epoch:37 step:29257 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.001505]\n",
      "epoch:37 step:29258 [D loss: 0.003830, acc.: 100.00%] [G loss: 0.032699]\n",
      "epoch:37 step:29259 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.001939]\n",
      "epoch:37 step:29260 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.009990]\n",
      "epoch:37 step:29261 [D loss: 0.000986, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:37 step:29262 [D loss: 0.002156, acc.: 100.00%] [G loss: 0.001285]\n",
      "epoch:37 step:29263 [D loss: 0.011100, acc.: 100.00%] [G loss: 0.002329]\n",
      "epoch:37 step:29264 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.004739]\n",
      "epoch:37 step:29265 [D loss: 0.001096, acc.: 100.00%] [G loss: 0.007380]\n",
      "epoch:37 step:29266 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.015372]\n",
      "epoch:37 step:29267 [D loss: 0.002710, acc.: 100.00%] [G loss: 0.000783]\n",
      "epoch:37 step:29268 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.016397]\n",
      "epoch:37 step:29269 [D loss: 0.002648, acc.: 100.00%] [G loss: 0.001335]\n",
      "epoch:37 step:29270 [D loss: 0.000904, acc.: 100.00%] [G loss: 0.001429]\n",
      "epoch:37 step:29271 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.002050]\n",
      "epoch:37 step:29272 [D loss: 0.001301, acc.: 100.00%] [G loss: 0.004568]\n",
      "epoch:37 step:29273 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.003587]\n",
      "epoch:37 step:29274 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:37 step:29275 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.001909]\n",
      "epoch:37 step:29276 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:37 step:29277 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.007647]\n",
      "epoch:37 step:29278 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:37 step:29279 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:37 step:29280 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.007551]\n",
      "epoch:37 step:29281 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001534]\n",
      "epoch:37 step:29282 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.013312]\n",
      "epoch:37 step:29283 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.008254]\n",
      "epoch:37 step:29284 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:37 step:29285 [D loss: 0.011730, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:37 step:29286 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:37 step:29287 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.012295]\n",
      "epoch:37 step:29288 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.001191]\n",
      "epoch:37 step:29289 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:37 step:29290 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003012]\n",
      "epoch:37 step:29291 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.001802]\n",
      "epoch:37 step:29292 [D loss: 0.006244, acc.: 100.00%] [G loss: 0.001262]\n",
      "epoch:37 step:29293 [D loss: 0.003677, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:37 step:29294 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.007285]\n",
      "epoch:37 step:29295 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:37 step:29296 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:37 step:29297 [D loss: 0.000665, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:37 step:29298 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000840]\n",
      "epoch:37 step:29299 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:37 step:29300 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:37 step:29301 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.004197]\n",
      "epoch:37 step:29302 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:37 step:29303 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.002537]\n",
      "epoch:37 step:29304 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.100983]\n",
      "epoch:37 step:29305 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:37 step:29306 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.001634]\n",
      "epoch:37 step:29307 [D loss: 0.002727, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:37 step:29308 [D loss: 0.002636, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:37 step:29309 [D loss: 0.058633, acc.: 97.66%] [G loss: 0.022195]\n",
      "epoch:37 step:29310 [D loss: 0.004552, acc.: 100.00%] [G loss: 0.183479]\n",
      "epoch:37 step:29311 [D loss: 0.007996, acc.: 100.00%] [G loss: 0.077532]\n",
      "epoch:37 step:29312 [D loss: 0.296691, acc.: 84.38%] [G loss: 1.397296]\n",
      "epoch:37 step:29313 [D loss: 0.476438, acc.: 78.12%] [G loss: 0.004045]\n",
      "epoch:37 step:29314 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:37 step:29315 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:37 step:29316 [D loss: 0.000194, acc.: 100.00%] [G loss: 3.709636]\n",
      "epoch:37 step:29317 [D loss: 0.003611, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:37 step:29318 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.422081]\n",
      "epoch:37 step:29319 [D loss: 0.063899, acc.: 97.66%] [G loss: 0.002569]\n",
      "epoch:37 step:29320 [D loss: 0.000785, acc.: 100.00%] [G loss: 3.067667]\n",
      "epoch:37 step:29321 [D loss: 0.222009, acc.: 92.97%] [G loss: 0.046590]\n",
      "epoch:37 step:29322 [D loss: 0.009431, acc.: 99.22%] [G loss: 0.157918]\n",
      "epoch:37 step:29323 [D loss: 0.044666, acc.: 98.44%] [G loss: 0.273191]\n",
      "epoch:37 step:29324 [D loss: 0.002640, acc.: 100.00%] [G loss: 0.081366]\n",
      "epoch:37 step:29325 [D loss: 0.036375, acc.: 97.66%] [G loss: 0.000510]\n",
      "epoch:37 step:29326 [D loss: 0.006019, acc.: 100.00%] [G loss: 0.000943]\n",
      "epoch:37 step:29327 [D loss: 0.104724, acc.: 95.31%] [G loss: 0.002963]\n",
      "epoch:37 step:29328 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.001818]\n",
      "epoch:37 step:29329 [D loss: 0.052258, acc.: 97.66%] [G loss: 0.000103]\n",
      "epoch:37 step:29330 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:37 step:29331 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.039400]\n",
      "epoch:37 step:29332 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:37 step:29333 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.182120]\n",
      "epoch:37 step:29334 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.025291]\n",
      "epoch:37 step:29335 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:37 step:29336 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.003779]\n",
      "epoch:37 step:29337 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:37 step:29338 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.003218]\n",
      "epoch:37 step:29339 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001596]\n",
      "epoch:37 step:29340 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.015393]\n",
      "epoch:37 step:29341 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.002541]\n",
      "epoch:37 step:29342 [D loss: 0.007417, acc.: 100.00%] [G loss: 0.024057]\n",
      "epoch:37 step:29343 [D loss: 0.023143, acc.: 100.00%] [G loss: 0.016042]\n",
      "epoch:37 step:29344 [D loss: 0.010178, acc.: 100.00%] [G loss: 0.218226]\n",
      "epoch:37 step:29345 [D loss: 0.014027, acc.: 100.00%] [G loss: 0.675984]\n",
      "epoch:37 step:29346 [D loss: 0.001031, acc.: 100.00%] [G loss: 0.001466]\n",
      "epoch:37 step:29347 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.006312]\n",
      "epoch:37 step:29348 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.009610]\n",
      "epoch:37 step:29349 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.008197]\n",
      "epoch:37 step:29350 [D loss: 0.002600, acc.: 100.00%] [G loss: 0.531300]\n",
      "epoch:37 step:29351 [D loss: 0.000951, acc.: 100.00%] [G loss: 0.000884]\n",
      "epoch:37 step:29352 [D loss: 0.018151, acc.: 99.22%] [G loss: 0.028796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29353 [D loss: 0.001581, acc.: 100.00%] [G loss: 0.017714]\n",
      "epoch:37 step:29354 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.183987]\n",
      "epoch:37 step:29355 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.217228]\n",
      "epoch:37 step:29356 [D loss: 0.007279, acc.: 100.00%] [G loss: 0.073356]\n",
      "epoch:37 step:29357 [D loss: 0.025038, acc.: 100.00%] [G loss: 0.012549]\n",
      "epoch:37 step:29358 [D loss: 0.000889, acc.: 100.00%] [G loss: 0.195991]\n",
      "epoch:37 step:29359 [D loss: 0.005432, acc.: 100.00%] [G loss: 0.317831]\n",
      "epoch:37 step:29360 [D loss: 0.018853, acc.: 99.22%] [G loss: 0.000402]\n",
      "epoch:37 step:29361 [D loss: 0.005855, acc.: 100.00%] [G loss: 0.000520]\n",
      "epoch:37 step:29362 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.001740]\n",
      "epoch:37 step:29363 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000715]\n",
      "epoch:37 step:29364 [D loss: 0.003057, acc.: 100.00%] [G loss: 0.000526]\n",
      "epoch:37 step:29365 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:37 step:29366 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:37 step:29367 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001784]\n",
      "epoch:37 step:29368 [D loss: 0.001724, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:37 step:29369 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:37 step:29370 [D loss: 0.004883, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:37 step:29371 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.006925]\n",
      "epoch:37 step:29372 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.001810]\n",
      "epoch:37 step:29373 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:37 step:29374 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:37 step:29375 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:37 step:29376 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.020415]\n",
      "epoch:37 step:29377 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.005006]\n",
      "epoch:37 step:29378 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000536]\n",
      "epoch:37 step:29379 [D loss: 0.002347, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:37 step:29380 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:37 step:29381 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.011824]\n",
      "epoch:37 step:29382 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:37 step:29383 [D loss: 0.001513, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:37 step:29384 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000718]\n",
      "epoch:37 step:29385 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:37 step:29386 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:37 step:29387 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.002663]\n",
      "epoch:37 step:29388 [D loss: 0.005975, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:37 step:29389 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001762]\n",
      "epoch:37 step:29390 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:37 step:29391 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:37 step:29392 [D loss: 0.002526, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:37 step:29393 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:37 step:29394 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.001070]\n",
      "epoch:37 step:29395 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:37 step:29396 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:37 step:29397 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:37 step:29398 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:37 step:29399 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:37 step:29400 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:37 step:29401 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.002942]\n",
      "epoch:37 step:29402 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:37 step:29403 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:37 step:29404 [D loss: 0.001576, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:37 step:29405 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:37 step:29406 [D loss: 0.001849, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:37 step:29407 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:37 step:29408 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:37 step:29409 [D loss: 0.001932, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:37 step:29410 [D loss: 0.002260, acc.: 100.00%] [G loss: 0.003413]\n",
      "epoch:37 step:29411 [D loss: 0.002567, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:37 step:29412 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:37 step:29413 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:37 step:29414 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:37 step:29415 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:37 step:29416 [D loss: 0.013345, acc.: 100.00%] [G loss: 0.008226]\n",
      "epoch:37 step:29417 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:37 step:29418 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.050471]\n",
      "epoch:37 step:29419 [D loss: 0.008070, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:37 step:29420 [D loss: 0.003991, acc.: 100.00%] [G loss: 0.014527]\n",
      "epoch:37 step:29421 [D loss: 0.001316, acc.: 100.00%] [G loss: 0.013714]\n",
      "epoch:37 step:29422 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.001526]\n",
      "epoch:37 step:29423 [D loss: 0.129005, acc.: 96.09%] [G loss: 0.000082]\n",
      "epoch:37 step:29424 [D loss: 0.001414, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:37 step:29425 [D loss: 2.145401, acc.: 50.78%] [G loss: 14.024366]\n",
      "epoch:37 step:29426 [D loss: 2.833890, acc.: 50.78%] [G loss: 3.304150]\n",
      "epoch:37 step:29427 [D loss: 0.558061, acc.: 75.78%] [G loss: 2.986213]\n",
      "epoch:37 step:29428 [D loss: 0.038225, acc.: 99.22%] [G loss: 2.540766]\n",
      "epoch:37 step:29429 [D loss: 1.212517, acc.: 57.03%] [G loss: 0.681470]\n",
      "epoch:37 step:29430 [D loss: 0.206213, acc.: 93.75%] [G loss: 0.819168]\n",
      "epoch:37 step:29431 [D loss: 0.394876, acc.: 79.69%] [G loss: 0.080470]\n",
      "epoch:37 step:29432 [D loss: 0.228369, acc.: 92.19%] [G loss: 0.015203]\n",
      "epoch:37 step:29433 [D loss: 0.015656, acc.: 100.00%] [G loss: 0.010572]\n",
      "epoch:37 step:29434 [D loss: 0.041692, acc.: 100.00%] [G loss: 0.002919]\n",
      "epoch:37 step:29435 [D loss: 0.006875, acc.: 100.00%] [G loss: 0.005220]\n",
      "epoch:37 step:29436 [D loss: 0.008143, acc.: 100.00%] [G loss: 0.001750]\n",
      "epoch:37 step:29437 [D loss: 0.016254, acc.: 100.00%] [G loss: 0.035884]\n",
      "epoch:37 step:29438 [D loss: 0.079549, acc.: 99.22%] [G loss: 2.859369]\n",
      "epoch:37 step:29439 [D loss: 0.040574, acc.: 99.22%] [G loss: 0.013206]\n",
      "epoch:37 step:29440 [D loss: 0.035074, acc.: 99.22%] [G loss: 0.004155]\n",
      "epoch:37 step:29441 [D loss: 0.042731, acc.: 98.44%] [G loss: 1.773811]\n",
      "epoch:37 step:29442 [D loss: 0.003485, acc.: 100.00%] [G loss: 0.960292]\n",
      "epoch:37 step:29443 [D loss: 0.033511, acc.: 98.44%] [G loss: 0.000838]\n",
      "epoch:37 step:29444 [D loss: 0.018691, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:37 step:29445 [D loss: 0.002297, acc.: 100.00%] [G loss: 0.001532]\n",
      "epoch:37 step:29446 [D loss: 0.046277, acc.: 98.44%] [G loss: 0.007780]\n",
      "epoch:37 step:29447 [D loss: 0.003345, acc.: 100.00%] [G loss: 0.604562]\n",
      "epoch:37 step:29448 [D loss: 0.009520, acc.: 100.00%] [G loss: 0.002059]\n",
      "epoch:37 step:29449 [D loss: 0.013159, acc.: 100.00%] [G loss: 0.284946]\n",
      "epoch:37 step:29450 [D loss: 0.006585, acc.: 100.00%] [G loss: 0.001579]\n",
      "epoch:37 step:29451 [D loss: 0.001784, acc.: 100.00%] [G loss: 0.001756]\n",
      "epoch:37 step:29452 [D loss: 0.038074, acc.: 99.22%] [G loss: 0.000665]\n",
      "epoch:37 step:29453 [D loss: 0.003855, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:37 step:29454 [D loss: 0.005576, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:37 step:29455 [D loss: 0.010403, acc.: 100.00%] [G loss: 0.020379]\n",
      "epoch:37 step:29456 [D loss: 0.040039, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:37 step:29457 [D loss: 0.002299, acc.: 100.00%] [G loss: 0.004728]\n",
      "epoch:37 step:29458 [D loss: 0.011009, acc.: 100.00%] [G loss: 0.004916]\n",
      "epoch:37 step:29459 [D loss: 0.005991, acc.: 100.00%] [G loss: 0.000670]\n",
      "epoch:37 step:29460 [D loss: 0.009001, acc.: 100.00%] [G loss: 0.001398]\n",
      "epoch:37 step:29461 [D loss: 0.046619, acc.: 100.00%] [G loss: 0.003213]\n",
      "epoch:37 step:29462 [D loss: 0.009415, acc.: 100.00%] [G loss: 0.006572]\n",
      "epoch:37 step:29463 [D loss: 0.016213, acc.: 100.00%] [G loss: 0.006784]\n",
      "epoch:37 step:29464 [D loss: 0.053857, acc.: 98.44%] [G loss: 0.003306]\n",
      "epoch:37 step:29465 [D loss: 0.002461, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:37 step:29466 [D loss: 0.445043, acc.: 74.22%] [G loss: 1.562205]\n",
      "epoch:37 step:29467 [D loss: 0.172822, acc.: 93.75%] [G loss: 6.565699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29468 [D loss: 0.522623, acc.: 72.66%] [G loss: 0.229939]\n",
      "epoch:37 step:29469 [D loss: 0.014413, acc.: 100.00%] [G loss: 0.072015]\n",
      "epoch:37 step:29470 [D loss: 0.072565, acc.: 99.22%] [G loss: 2.142555]\n",
      "epoch:37 step:29471 [D loss: 0.047782, acc.: 98.44%] [G loss: 0.621189]\n",
      "epoch:37 step:29472 [D loss: 0.020876, acc.: 100.00%] [G loss: 0.239686]\n",
      "epoch:37 step:29473 [D loss: 0.024018, acc.: 100.00%] [G loss: 0.311359]\n",
      "epoch:37 step:29474 [D loss: 0.015252, acc.: 100.00%] [G loss: 0.132350]\n",
      "epoch:37 step:29475 [D loss: 0.003706, acc.: 100.00%] [G loss: 0.064597]\n",
      "epoch:37 step:29476 [D loss: 0.012205, acc.: 100.00%] [G loss: 0.078539]\n",
      "epoch:37 step:29477 [D loss: 0.015606, acc.: 100.00%] [G loss: 0.019279]\n",
      "epoch:37 step:29478 [D loss: 0.038552, acc.: 98.44%] [G loss: 0.029728]\n",
      "epoch:37 step:29479 [D loss: 0.009327, acc.: 100.00%] [G loss: 0.078949]\n",
      "epoch:37 step:29480 [D loss: 0.030873, acc.: 99.22%] [G loss: 0.837506]\n",
      "epoch:37 step:29481 [D loss: 0.028404, acc.: 100.00%] [G loss: 0.266577]\n",
      "epoch:37 step:29482 [D loss: 0.003119, acc.: 100.00%] [G loss: 0.051930]\n",
      "epoch:37 step:29483 [D loss: 0.070856, acc.: 100.00%] [G loss: 0.050845]\n",
      "epoch:37 step:29484 [D loss: 0.006933, acc.: 100.00%] [G loss: 0.616086]\n",
      "epoch:37 step:29485 [D loss: 0.008251, acc.: 100.00%] [G loss: 0.140581]\n",
      "epoch:37 step:29486 [D loss: 0.033218, acc.: 100.00%] [G loss: 0.120039]\n",
      "epoch:37 step:29487 [D loss: 0.002998, acc.: 100.00%] [G loss: 0.053788]\n",
      "epoch:37 step:29488 [D loss: 0.022580, acc.: 99.22%] [G loss: 0.118989]\n",
      "epoch:37 step:29489 [D loss: 0.033993, acc.: 100.00%] [G loss: 0.022894]\n",
      "epoch:37 step:29490 [D loss: 0.034917, acc.: 99.22%] [G loss: 0.026345]\n",
      "epoch:37 step:29491 [D loss: 0.020314, acc.: 99.22%] [G loss: 0.006780]\n",
      "epoch:37 step:29492 [D loss: 0.021290, acc.: 100.00%] [G loss: 0.001150]\n",
      "epoch:37 step:29493 [D loss: 0.030087, acc.: 99.22%] [G loss: 0.001869]\n",
      "epoch:37 step:29494 [D loss: 0.013426, acc.: 100.00%] [G loss: 0.005359]\n",
      "epoch:37 step:29495 [D loss: 0.037820, acc.: 99.22%] [G loss: 0.007792]\n",
      "epoch:37 step:29496 [D loss: 0.004705, acc.: 100.00%] [G loss: 0.233453]\n",
      "epoch:37 step:29497 [D loss: 0.007961, acc.: 100.00%] [G loss: 0.035241]\n",
      "epoch:37 step:29498 [D loss: 0.035920, acc.: 100.00%] [G loss: 0.001954]\n",
      "epoch:37 step:29499 [D loss: 0.360660, acc.: 82.81%] [G loss: 3.262708]\n",
      "epoch:37 step:29500 [D loss: 0.071344, acc.: 98.44%] [G loss: 5.509497]\n",
      "epoch:37 step:29501 [D loss: 1.246576, acc.: 57.81%] [G loss: 1.643963]\n",
      "epoch:37 step:29502 [D loss: 0.043787, acc.: 98.44%] [G loss: 0.332897]\n",
      "epoch:37 step:29503 [D loss: 0.060406, acc.: 98.44%] [G loss: 3.354949]\n",
      "epoch:37 step:29504 [D loss: 0.025348, acc.: 100.00%] [G loss: 3.584685]\n",
      "epoch:37 step:29505 [D loss: 0.155209, acc.: 94.53%] [G loss: 1.200866]\n",
      "epoch:37 step:29506 [D loss: 0.020886, acc.: 99.22%] [G loss: 0.637151]\n",
      "epoch:37 step:29507 [D loss: 0.079087, acc.: 97.66%] [G loss: 0.056745]\n",
      "epoch:37 step:29508 [D loss: 0.038940, acc.: 100.00%] [G loss: 0.042043]\n",
      "epoch:37 step:29509 [D loss: 0.003382, acc.: 100.00%] [G loss: 0.015245]\n",
      "epoch:37 step:29510 [D loss: 0.211566, acc.: 88.28%] [G loss: 0.299950]\n",
      "epoch:37 step:29511 [D loss: 0.018660, acc.: 100.00%] [G loss: 1.172130]\n",
      "epoch:37 step:29512 [D loss: 0.151921, acc.: 94.53%] [G loss: 0.299492]\n",
      "epoch:37 step:29513 [D loss: 0.049641, acc.: 98.44%] [G loss: 0.009509]\n",
      "epoch:37 step:29514 [D loss: 0.010384, acc.: 100.00%] [G loss: 0.038179]\n",
      "epoch:37 step:29515 [D loss: 0.006868, acc.: 100.00%] [G loss: 0.037512]\n",
      "epoch:37 step:29516 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.014033]\n",
      "epoch:37 step:29517 [D loss: 0.001855, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:37 step:29518 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.036473]\n",
      "epoch:37 step:29519 [D loss: 0.002809, acc.: 100.00%] [G loss: 0.003739]\n",
      "epoch:37 step:29520 [D loss: 0.033944, acc.: 100.00%] [G loss: 0.001472]\n",
      "epoch:37 step:29521 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.005478]\n",
      "epoch:37 step:29522 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.001227]\n",
      "epoch:37 step:29523 [D loss: 0.002136, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:37 step:29524 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.001166]\n",
      "epoch:37 step:29525 [D loss: 0.005143, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:37 step:29526 [D loss: 0.001294, acc.: 100.00%] [G loss: 0.002221]\n",
      "epoch:37 step:29527 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.001178]\n",
      "epoch:37 step:29528 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.001549]\n",
      "epoch:37 step:29529 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.001117]\n",
      "epoch:37 step:29530 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:37 step:29531 [D loss: 0.002485, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:37 step:29532 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.000304]\n",
      "epoch:37 step:29533 [D loss: 0.012056, acc.: 99.22%] [G loss: 0.000103]\n",
      "epoch:37 step:29534 [D loss: 0.099911, acc.: 95.31%] [G loss: 0.001440]\n",
      "epoch:37 step:29535 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.006419]\n",
      "epoch:37 step:29536 [D loss: 0.002644, acc.: 100.00%] [G loss: 0.016730]\n",
      "epoch:37 step:29537 [D loss: 0.003357, acc.: 100.00%] [G loss: 0.022922]\n",
      "epoch:37 step:29538 [D loss: 0.007578, acc.: 100.00%] [G loss: 0.003272]\n",
      "epoch:37 step:29539 [D loss: 0.001491, acc.: 100.00%] [G loss: 0.048621]\n",
      "epoch:37 step:29540 [D loss: 0.003458, acc.: 100.00%] [G loss: 0.097490]\n",
      "epoch:37 step:29541 [D loss: 0.020500, acc.: 100.00%] [G loss: 0.001205]\n",
      "epoch:37 step:29542 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:37 step:29543 [D loss: 0.008097, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:37 step:29544 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:37 step:29545 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000827]\n",
      "epoch:37 step:29546 [D loss: 0.005449, acc.: 100.00%] [G loss: 0.001102]\n",
      "epoch:37 step:29547 [D loss: 0.000980, acc.: 100.00%] [G loss: 0.004378]\n",
      "epoch:37 step:29548 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:37 step:29549 [D loss: 0.001479, acc.: 100.00%] [G loss: 0.019627]\n",
      "epoch:37 step:29550 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:37 step:29551 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:37 step:29552 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:37 step:29553 [D loss: 0.001501, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:37 step:29554 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.346377]\n",
      "epoch:37 step:29555 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.001044]\n",
      "epoch:37 step:29556 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.016296]\n",
      "epoch:37 step:29557 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.004109]\n",
      "epoch:37 step:29558 [D loss: 0.001128, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:37 step:29559 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:37 step:29560 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:37 step:29561 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.001115]\n",
      "epoch:37 step:29562 [D loss: 0.011600, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:37 step:29563 [D loss: 0.013555, acc.: 100.00%] [G loss: 0.026030]\n",
      "epoch:37 step:29564 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:37 step:29565 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:37 step:29566 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:37 step:29567 [D loss: 0.001274, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:37 step:29568 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.004959]\n",
      "epoch:37 step:29569 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:37 step:29570 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:37 step:29571 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.004873]\n",
      "epoch:37 step:29572 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:37 step:29573 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.003982]\n",
      "epoch:37 step:29574 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:37 step:29575 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.008442]\n",
      "epoch:37 step:29576 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:37 step:29577 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000847]\n",
      "epoch:37 step:29578 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.001555]\n",
      "epoch:37 step:29579 [D loss: 0.001493, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:37 step:29580 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:37 step:29581 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.005223]\n",
      "epoch:37 step:29582 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.001392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29583 [D loss: 0.000617, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:37 step:29584 [D loss: 0.006369, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:37 step:29585 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:37 step:29586 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.012138]\n",
      "epoch:37 step:29587 [D loss: 0.001267, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:37 step:29588 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:37 step:29589 [D loss: 0.001590, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:37 step:29590 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:37 step:29591 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:37 step:29592 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:37 step:29593 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:37 step:29594 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:37 step:29595 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:37 step:29596 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:37 step:29597 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:37 step:29598 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:37 step:29599 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:37 step:29600 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:37 step:29601 [D loss: 0.010490, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:37 step:29602 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:37 step:29603 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:37 step:29604 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:37 step:29605 [D loss: 0.002354, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:37 step:29606 [D loss: 0.003147, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:37 step:29607 [D loss: 0.052725, acc.: 97.66%] [G loss: 0.001804]\n",
      "epoch:37 step:29608 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.030784]\n",
      "epoch:37 step:29609 [D loss: 0.002704, acc.: 100.00%] [G loss: 0.026970]\n",
      "epoch:37 step:29610 [D loss: 0.110921, acc.: 95.31%] [G loss: 0.004888]\n",
      "epoch:37 step:29611 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.002459]\n",
      "epoch:37 step:29612 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:37 step:29613 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:37 step:29614 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:37 step:29615 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000958]\n",
      "epoch:37 step:29616 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.001119]\n",
      "epoch:37 step:29617 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:37 step:29618 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.047390]\n",
      "epoch:37 step:29619 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:37 step:29620 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.001801]\n",
      "epoch:37 step:29621 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.004884]\n",
      "epoch:37 step:29622 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:37 step:29623 [D loss: 0.001320, acc.: 100.00%] [G loss: 0.010518]\n",
      "epoch:37 step:29624 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:37 step:29625 [D loss: 0.383982, acc.: 77.34%] [G loss: 0.268102]\n",
      "epoch:37 step:29626 [D loss: 0.011427, acc.: 100.00%] [G loss: 0.600952]\n",
      "epoch:37 step:29627 [D loss: 1.037355, acc.: 63.28%] [G loss: 0.009640]\n",
      "epoch:37 step:29628 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.000481]\n",
      "epoch:37 step:29629 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.001396]\n",
      "epoch:37 step:29630 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:37 step:29631 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:37 step:29632 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:37 step:29633 [D loss: 0.001220, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:37 step:29634 [D loss: 0.001694, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:37 step:29635 [D loss: 0.011610, acc.: 99.22%] [G loss: 0.000148]\n",
      "epoch:37 step:29636 [D loss: 0.000733, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:37 step:29637 [D loss: 0.033868, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:37 step:29638 [D loss: 0.021125, acc.: 100.00%] [G loss: 0.001033]\n",
      "epoch:37 step:29639 [D loss: 0.025194, acc.: 99.22%] [G loss: 0.003492]\n",
      "epoch:37 step:29640 [D loss: 0.003604, acc.: 100.00%] [G loss: 0.052150]\n",
      "epoch:37 step:29641 [D loss: 0.147783, acc.: 92.97%] [G loss: 6.187785]\n",
      "epoch:37 step:29642 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.284544]\n",
      "epoch:37 step:29643 [D loss: 0.002651, acc.: 100.00%] [G loss: 0.394555]\n",
      "epoch:37 step:29644 [D loss: 0.047567, acc.: 98.44%] [G loss: 0.061025]\n",
      "epoch:37 step:29645 [D loss: 0.018056, acc.: 100.00%] [G loss: 0.016646]\n",
      "epoch:37 step:29646 [D loss: 0.002957, acc.: 100.00%] [G loss: 2.935772]\n",
      "epoch:37 step:29647 [D loss: 0.002317, acc.: 100.00%] [G loss: 0.007280]\n",
      "epoch:37 step:29648 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.005560]\n",
      "epoch:37 step:29649 [D loss: 0.003715, acc.: 100.00%] [G loss: 0.006841]\n",
      "epoch:37 step:29650 [D loss: 0.002938, acc.: 100.00%] [G loss: 0.000999]\n",
      "epoch:37 step:29651 [D loss: 0.018531, acc.: 100.00%] [G loss: 1.862108]\n",
      "epoch:37 step:29652 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.022458]\n",
      "epoch:37 step:29653 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.876934]\n",
      "epoch:37 step:29654 [D loss: 0.037439, acc.: 98.44%] [G loss: 0.006341]\n",
      "epoch:37 step:29655 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.019511]\n",
      "epoch:37 step:29656 [D loss: 0.008941, acc.: 99.22%] [G loss: 0.003771]\n",
      "epoch:37 step:29657 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.081347]\n",
      "epoch:37 step:29658 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.085181]\n",
      "epoch:37 step:29659 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000980]\n",
      "epoch:37 step:29660 [D loss: 0.001561, acc.: 100.00%] [G loss: 0.001506]\n",
      "epoch:37 step:29661 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:37 step:29662 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:37 step:29663 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.056505]\n",
      "epoch:37 step:29664 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:37 step:29665 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001721]\n",
      "epoch:37 step:29666 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:37 step:29667 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:37 step:29668 [D loss: 0.005818, acc.: 100.00%] [G loss: 0.050456]\n",
      "epoch:37 step:29669 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.030431]\n",
      "epoch:37 step:29670 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.006082]\n",
      "epoch:37 step:29671 [D loss: 0.002014, acc.: 100.00%] [G loss: 0.019016]\n",
      "epoch:37 step:29672 [D loss: 0.003451, acc.: 100.00%] [G loss: 0.001329]\n",
      "epoch:37 step:29673 [D loss: 0.001568, acc.: 100.00%] [G loss: 0.017856]\n",
      "epoch:37 step:29674 [D loss: 0.002013, acc.: 100.00%] [G loss: 0.017299]\n",
      "epoch:37 step:29675 [D loss: 0.036436, acc.: 99.22%] [G loss: 0.002678]\n",
      "epoch:37 step:29676 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.098893]\n",
      "epoch:37 step:29677 [D loss: 0.002631, acc.: 100.00%] [G loss: 0.000568]\n",
      "epoch:37 step:29678 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.001418]\n",
      "epoch:38 step:29679 [D loss: 0.003093, acc.: 100.00%] [G loss: 0.126882]\n",
      "epoch:38 step:29680 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.001686]\n",
      "epoch:38 step:29681 [D loss: 0.004455, acc.: 100.00%] [G loss: 0.002647]\n",
      "epoch:38 step:29682 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:38 step:29683 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.003153]\n",
      "epoch:38 step:29684 [D loss: 0.001714, acc.: 100.00%] [G loss: 0.000468]\n",
      "epoch:38 step:29685 [D loss: 0.000889, acc.: 100.00%] [G loss: 0.002149]\n",
      "epoch:38 step:29686 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:38 step:29687 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:38 step:29688 [D loss: 0.005347, acc.: 100.00%] [G loss: 0.000964]\n",
      "epoch:38 step:29689 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.027043]\n",
      "epoch:38 step:29690 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.014432]\n",
      "epoch:38 step:29691 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.002400]\n",
      "epoch:38 step:29692 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:38 step:29693 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.018442]\n",
      "epoch:38 step:29694 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:38 step:29695 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:38 step:29696 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.014295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29697 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:38 step:29698 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.001734]\n",
      "epoch:38 step:29699 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001413]\n",
      "epoch:38 step:29700 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:38 step:29701 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:38 step:29702 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:38 step:29703 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000424]\n",
      "epoch:38 step:29704 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:38 step:29705 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.011205]\n",
      "epoch:38 step:29706 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:38 step:29707 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:38 step:29708 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:38 step:29709 [D loss: 0.000982, acc.: 100.00%] [G loss: 0.000668]\n",
      "epoch:38 step:29710 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:38 step:29711 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.033730]\n",
      "epoch:38 step:29712 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:38 step:29713 [D loss: 0.003806, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:38 step:29714 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:38 step:29715 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:38 step:29716 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.011021]\n",
      "epoch:38 step:29717 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:38 step:29718 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.014036]\n",
      "epoch:38 step:29719 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:38 step:29720 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:38 step:29721 [D loss: 0.000993, acc.: 100.00%] [G loss: 0.000923]\n",
      "epoch:38 step:29722 [D loss: 0.002761, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:38 step:29723 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:38 step:29724 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:38 step:29725 [D loss: 0.001766, acc.: 100.00%] [G loss: 0.001025]\n",
      "epoch:38 step:29726 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:38 step:29727 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:38 step:29728 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.002982]\n",
      "epoch:38 step:29729 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:38 step:29730 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:38 step:29731 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.007801]\n",
      "epoch:38 step:29732 [D loss: 0.001702, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:38 step:29733 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:38 step:29734 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:38 step:29735 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:38 step:29736 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:38 step:29737 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:38 step:29738 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:38 step:29739 [D loss: 0.002270, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:38 step:29740 [D loss: 0.005443, acc.: 100.00%] [G loss: 0.009698]\n",
      "epoch:38 step:29741 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:38 step:29742 [D loss: 0.002372, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:38 step:29743 [D loss: 0.005386, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:38 step:29744 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:38 step:29745 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000498]\n",
      "epoch:38 step:29746 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.004641]\n",
      "epoch:38 step:29747 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:38 step:29748 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.002805]\n",
      "epoch:38 step:29749 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.004683]\n",
      "epoch:38 step:29750 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:38 step:29751 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.012038]\n",
      "epoch:38 step:29752 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:38 step:29753 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.007610]\n",
      "epoch:38 step:29754 [D loss: 0.024555, acc.: 100.00%] [G loss: 0.002707]\n",
      "epoch:38 step:29755 [D loss: 0.002409, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:38 step:29756 [D loss: 0.002831, acc.: 100.00%] [G loss: 0.043671]\n",
      "epoch:38 step:29757 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.001591]\n",
      "epoch:38 step:29758 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.000652]\n",
      "epoch:38 step:29759 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:38 step:29760 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.001270]\n",
      "epoch:38 step:29761 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.039048]\n",
      "epoch:38 step:29762 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:38 step:29763 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:38 step:29764 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:38 step:29765 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:38 step:29766 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:38 step:29767 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:38 step:29768 [D loss: 0.006102, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:38 step:29769 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:38 step:29770 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:38 step:29771 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.009246]\n",
      "epoch:38 step:29772 [D loss: 0.001854, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:38 step:29773 [D loss: 0.021660, acc.: 100.00%] [G loss: 0.001151]\n",
      "epoch:38 step:29774 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.001608]\n",
      "epoch:38 step:29775 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.067298]\n",
      "epoch:38 step:29776 [D loss: 0.000970, acc.: 100.00%] [G loss: 0.001003]\n",
      "epoch:38 step:29777 [D loss: 0.241748, acc.: 88.28%] [G loss: 1.128098]\n",
      "epoch:38 step:29778 [D loss: 0.362710, acc.: 80.47%] [G loss: 0.853577]\n",
      "epoch:38 step:29779 [D loss: 0.002246, acc.: 100.00%] [G loss: 0.441734]\n",
      "epoch:38 step:29780 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.059452]\n",
      "epoch:38 step:29781 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.040198]\n",
      "epoch:38 step:29782 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.077799]\n",
      "epoch:38 step:29783 [D loss: 0.003721, acc.: 100.00%] [G loss: 0.012073]\n",
      "epoch:38 step:29784 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.020186]\n",
      "epoch:38 step:29785 [D loss: 0.002961, acc.: 100.00%] [G loss: 0.018946]\n",
      "epoch:38 step:29786 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.005038]\n",
      "epoch:38 step:29787 [D loss: 0.064210, acc.: 99.22%] [G loss: 0.261860]\n",
      "epoch:38 step:29788 [D loss: 0.001794, acc.: 100.00%] [G loss: 0.269673]\n",
      "epoch:38 step:29789 [D loss: 0.070336, acc.: 98.44%] [G loss: 0.238310]\n",
      "epoch:38 step:29790 [D loss: 0.008981, acc.: 100.00%] [G loss: 0.028595]\n",
      "epoch:38 step:29791 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.017774]\n",
      "epoch:38 step:29792 [D loss: 0.001035, acc.: 100.00%] [G loss: 0.034388]\n",
      "epoch:38 step:29793 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.023281]\n",
      "epoch:38 step:29794 [D loss: 0.001941, acc.: 100.00%] [G loss: 0.019785]\n",
      "epoch:38 step:29795 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.058786]\n",
      "epoch:38 step:29796 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.018690]\n",
      "epoch:38 step:29797 [D loss: 0.009769, acc.: 99.22%] [G loss: 0.007381]\n",
      "epoch:38 step:29798 [D loss: 0.001326, acc.: 100.00%] [G loss: 0.008248]\n",
      "epoch:38 step:29799 [D loss: 0.040547, acc.: 98.44%] [G loss: 0.008802]\n",
      "epoch:38 step:29800 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.008937]\n",
      "epoch:38 step:29801 [D loss: 0.004938, acc.: 100.00%] [G loss: 0.008138]\n",
      "epoch:38 step:29802 [D loss: 0.032607, acc.: 99.22%] [G loss: 1.046603]\n",
      "epoch:38 step:29803 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.032214]\n",
      "epoch:38 step:29804 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.027880]\n",
      "epoch:38 step:29805 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.009264]\n",
      "epoch:38 step:29806 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.086247]\n",
      "epoch:38 step:29807 [D loss: 0.003106, acc.: 100.00%] [G loss: 0.047055]\n",
      "epoch:38 step:29808 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.009855]\n",
      "epoch:38 step:29809 [D loss: 0.044089, acc.: 99.22%] [G loss: 0.076802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29810 [D loss: 0.003844, acc.: 100.00%] [G loss: 0.022409]\n",
      "epoch:38 step:29811 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.106605]\n",
      "epoch:38 step:29812 [D loss: 0.008259, acc.: 100.00%] [G loss: 0.010400]\n",
      "epoch:38 step:29813 [D loss: 0.008900, acc.: 100.00%] [G loss: 0.172131]\n",
      "epoch:38 step:29814 [D loss: 0.008731, acc.: 100.00%] [G loss: 0.029278]\n",
      "epoch:38 step:29815 [D loss: 0.006054, acc.: 100.00%] [G loss: 0.015230]\n",
      "epoch:38 step:29816 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.014851]\n",
      "epoch:38 step:29817 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.007813]\n",
      "epoch:38 step:29818 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.146796]\n",
      "epoch:38 step:29819 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.027659]\n",
      "epoch:38 step:29820 [D loss: 0.009123, acc.: 100.00%] [G loss: 0.003802]\n",
      "epoch:38 step:29821 [D loss: 0.001915, acc.: 100.00%] [G loss: 0.003623]\n",
      "epoch:38 step:29822 [D loss: 0.004680, acc.: 100.00%] [G loss: 0.001814]\n",
      "epoch:38 step:29823 [D loss: 0.001773, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:38 step:29824 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.001672]\n",
      "epoch:38 step:29825 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.005135]\n",
      "epoch:38 step:29826 [D loss: 0.000742, acc.: 100.00%] [G loss: 0.003886]\n",
      "epoch:38 step:29827 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.001798]\n",
      "epoch:38 step:29828 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.002007]\n",
      "epoch:38 step:29829 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:38 step:29830 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.020851]\n",
      "epoch:38 step:29831 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:38 step:29832 [D loss: 0.017015, acc.: 100.00%] [G loss: 0.011551]\n",
      "epoch:38 step:29833 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.003963]\n",
      "epoch:38 step:29834 [D loss: 0.001181, acc.: 100.00%] [G loss: 0.005436]\n",
      "epoch:38 step:29835 [D loss: 0.001446, acc.: 100.00%] [G loss: 0.006489]\n",
      "epoch:38 step:29836 [D loss: 0.021179, acc.: 99.22%] [G loss: 0.012061]\n",
      "epoch:38 step:29837 [D loss: 0.002975, acc.: 100.00%] [G loss: 0.004748]\n",
      "epoch:38 step:29838 [D loss: 0.027183, acc.: 100.00%] [G loss: 0.014499]\n",
      "epoch:38 step:29839 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.000435]\n",
      "epoch:38 step:29840 [D loss: 0.164798, acc.: 89.84%] [G loss: 0.083375]\n",
      "epoch:38 step:29841 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.803628]\n",
      "epoch:38 step:29842 [D loss: 0.051242, acc.: 98.44%] [G loss: 0.107573]\n",
      "epoch:38 step:29843 [D loss: 0.032242, acc.: 99.22%] [G loss: 0.024572]\n",
      "epoch:38 step:29844 [D loss: 0.059059, acc.: 98.44%] [G loss: 0.891751]\n",
      "epoch:38 step:29845 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.001554]\n",
      "epoch:38 step:29846 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:38 step:29847 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.031535]\n",
      "epoch:38 step:29848 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.002517]\n",
      "epoch:38 step:29849 [D loss: 0.007091, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:38 step:29850 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:38 step:29851 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:38 step:29852 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.061585]\n",
      "epoch:38 step:29853 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.141633]\n",
      "epoch:38 step:29854 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:38 step:29855 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.038919]\n",
      "epoch:38 step:29856 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.033786]\n",
      "epoch:38 step:29857 [D loss: 0.012521, acc.: 100.00%] [G loss: 0.007606]\n",
      "epoch:38 step:29858 [D loss: 0.014812, acc.: 99.22%] [G loss: 0.000331]\n",
      "epoch:38 step:29859 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002289]\n",
      "epoch:38 step:29860 [D loss: 0.001546, acc.: 100.00%] [G loss: 0.181595]\n",
      "epoch:38 step:29861 [D loss: 0.004897, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:38 step:29862 [D loss: 0.003914, acc.: 100.00%] [G loss: 0.000766]\n",
      "epoch:38 step:29863 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:38 step:29864 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.014658]\n",
      "epoch:38 step:29865 [D loss: 0.002416, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:38 step:29866 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.001371]\n",
      "epoch:38 step:29867 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.108029]\n",
      "epoch:38 step:29868 [D loss: 0.001663, acc.: 100.00%] [G loss: 1.241542]\n",
      "epoch:38 step:29869 [D loss: 0.000939, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:38 step:29870 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.036464]\n",
      "epoch:38 step:29871 [D loss: 0.011684, acc.: 100.00%] [G loss: 0.012068]\n",
      "epoch:38 step:29872 [D loss: 0.627965, acc.: 65.62%] [G loss: 1.431745]\n",
      "epoch:38 step:29873 [D loss: 0.842309, acc.: 64.84%] [G loss: 0.075309]\n",
      "epoch:38 step:29874 [D loss: 0.023088, acc.: 100.00%] [G loss: 0.103913]\n",
      "epoch:38 step:29875 [D loss: 0.010999, acc.: 100.00%] [G loss: 3.330145]\n",
      "epoch:38 step:29876 [D loss: 0.008458, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:38 step:29877 [D loss: 0.014840, acc.: 100.00%] [G loss: 2.282871]\n",
      "epoch:38 step:29878 [D loss: 0.425370, acc.: 79.69%] [G loss: 0.208642]\n",
      "epoch:38 step:29879 [D loss: 0.060313, acc.: 98.44%] [G loss: 0.155392]\n",
      "epoch:38 step:29880 [D loss: 0.248157, acc.: 86.72%] [G loss: 0.005438]\n",
      "epoch:38 step:29881 [D loss: 0.009178, acc.: 100.00%] [G loss: 4.643403]\n",
      "epoch:38 step:29882 [D loss: 0.044157, acc.: 98.44%] [G loss: 0.000283]\n",
      "epoch:38 step:29883 [D loss: 0.008186, acc.: 100.00%] [G loss: 3.072815]\n",
      "epoch:38 step:29884 [D loss: 0.233607, acc.: 93.75%] [G loss: 0.049763]\n",
      "epoch:38 step:29885 [D loss: 0.227050, acc.: 91.41%] [G loss: 0.001077]\n",
      "epoch:38 step:29886 [D loss: 0.153049, acc.: 93.75%] [G loss: 0.003638]\n",
      "epoch:38 step:29887 [D loss: 0.026278, acc.: 100.00%] [G loss: 0.000956]\n",
      "epoch:38 step:29888 [D loss: 0.151791, acc.: 94.53%] [G loss: 0.001590]\n",
      "epoch:38 step:29889 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.003196]\n",
      "epoch:38 step:29890 [D loss: 0.004603, acc.: 100.00%] [G loss: 0.002353]\n",
      "epoch:38 step:29891 [D loss: 0.159660, acc.: 92.97%] [G loss: 1.848338]\n",
      "epoch:38 step:29892 [D loss: 0.071868, acc.: 98.44%] [G loss: 0.598351]\n",
      "epoch:38 step:29893 [D loss: 0.031978, acc.: 100.00%] [G loss: 0.245989]\n",
      "epoch:38 step:29894 [D loss: 0.108833, acc.: 96.09%] [G loss: 3.238471]\n",
      "epoch:38 step:29895 [D loss: 1.054042, acc.: 61.72%] [G loss: 5.565403]\n",
      "epoch:38 step:29896 [D loss: 1.400223, acc.: 55.47%] [G loss: 2.811714]\n",
      "epoch:38 step:29897 [D loss: 0.086197, acc.: 96.88%] [G loss: 1.124797]\n",
      "epoch:38 step:29898 [D loss: 0.023156, acc.: 100.00%] [G loss: 1.417373]\n",
      "epoch:38 step:29899 [D loss: 0.020648, acc.: 100.00%] [G loss: 1.313942]\n",
      "epoch:38 step:29900 [D loss: 0.068348, acc.: 98.44%] [G loss: 0.279391]\n",
      "epoch:38 step:29901 [D loss: 0.055138, acc.: 98.44%] [G loss: 0.802400]\n",
      "epoch:38 step:29902 [D loss: 0.060932, acc.: 96.88%] [G loss: 0.344483]\n",
      "epoch:38 step:29903 [D loss: 0.124001, acc.: 96.88%] [G loss: 0.548240]\n",
      "epoch:38 step:29904 [D loss: 0.134287, acc.: 96.88%] [G loss: 0.213865]\n",
      "epoch:38 step:29905 [D loss: 0.003768, acc.: 100.00%] [G loss: 0.101924]\n",
      "epoch:38 step:29906 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.030207]\n",
      "epoch:38 step:29907 [D loss: 0.005794, acc.: 100.00%] [G loss: 0.013059]\n",
      "epoch:38 step:29908 [D loss: 0.044479, acc.: 99.22%] [G loss: 0.005415]\n",
      "epoch:38 step:29909 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.004579]\n",
      "epoch:38 step:29910 [D loss: 0.002335, acc.: 100.00%] [G loss: 0.011536]\n",
      "epoch:38 step:29911 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.009094]\n",
      "epoch:38 step:29912 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.021932]\n",
      "epoch:38 step:29913 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.005702]\n",
      "epoch:38 step:29914 [D loss: 0.002054, acc.: 100.00%] [G loss: 0.003385]\n",
      "epoch:38 step:29915 [D loss: 0.002743, acc.: 100.00%] [G loss: 0.002613]\n",
      "epoch:38 step:29916 [D loss: 0.001340, acc.: 100.00%] [G loss: 0.003377]\n",
      "epoch:38 step:29917 [D loss: 0.003054, acc.: 100.00%] [G loss: 0.000498]\n",
      "epoch:38 step:29918 [D loss: 0.067467, acc.: 98.44%] [G loss: 0.008452]\n",
      "epoch:38 step:29919 [D loss: 0.001188, acc.: 100.00%] [G loss: 0.003669]\n",
      "epoch:38 step:29920 [D loss: 0.001947, acc.: 100.00%] [G loss: 0.004608]\n",
      "epoch:38 step:29921 [D loss: 0.000731, acc.: 100.00%] [G loss: 0.004582]\n",
      "epoch:38 step:29922 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.001348]\n",
      "epoch:38 step:29923 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.003906]\n",
      "epoch:38 step:29924 [D loss: 0.001149, acc.: 100.00%] [G loss: 0.002838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29925 [D loss: 0.008808, acc.: 100.00%] [G loss: 0.006404]\n",
      "epoch:38 step:29926 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.005265]\n",
      "epoch:38 step:29927 [D loss: 0.001702, acc.: 100.00%] [G loss: 0.002056]\n",
      "epoch:38 step:29928 [D loss: 0.006636, acc.: 100.00%] [G loss: 0.073850]\n",
      "epoch:38 step:29929 [D loss: 0.001029, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:38 step:29930 [D loss: 0.001331, acc.: 100.00%] [G loss: 0.004587]\n",
      "epoch:38 step:29931 [D loss: 0.002968, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:38 step:29932 [D loss: 0.005112, acc.: 100.00%] [G loss: 0.004369]\n",
      "epoch:38 step:29933 [D loss: 0.019373, acc.: 99.22%] [G loss: 0.001173]\n",
      "epoch:38 step:29934 [D loss: 0.017538, acc.: 100.00%] [G loss: 0.004588]\n",
      "epoch:38 step:29935 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.004405]\n",
      "epoch:38 step:29936 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.001939]\n",
      "epoch:38 step:29937 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.014357]\n",
      "epoch:38 step:29938 [D loss: 0.002558, acc.: 100.00%] [G loss: 0.006202]\n",
      "epoch:38 step:29939 [D loss: 0.033943, acc.: 98.44%] [G loss: 0.003397]\n",
      "epoch:38 step:29940 [D loss: 0.001277, acc.: 100.00%] [G loss: 0.004656]\n",
      "epoch:38 step:29941 [D loss: 0.084821, acc.: 96.88%] [G loss: 0.008410]\n",
      "epoch:38 step:29942 [D loss: 0.001090, acc.: 100.00%] [G loss: 0.014212]\n",
      "epoch:38 step:29943 [D loss: 0.040920, acc.: 98.44%] [G loss: 0.011678]\n",
      "epoch:38 step:29944 [D loss: 0.001367, acc.: 100.00%] [G loss: 0.004128]\n",
      "epoch:38 step:29945 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.240966]\n",
      "epoch:38 step:29946 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.048722]\n",
      "epoch:38 step:29947 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.001150]\n",
      "epoch:38 step:29948 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.004331]\n",
      "epoch:38 step:29949 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.010864]\n",
      "epoch:38 step:29950 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.001497]\n",
      "epoch:38 step:29951 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.001374]\n",
      "epoch:38 step:29952 [D loss: 0.000883, acc.: 100.00%] [G loss: 0.002193]\n",
      "epoch:38 step:29953 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.001175]\n",
      "epoch:38 step:29954 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.002775]\n",
      "epoch:38 step:29955 [D loss: 0.030772, acc.: 98.44%] [G loss: 0.073010]\n",
      "epoch:38 step:29956 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.001412]\n",
      "epoch:38 step:29957 [D loss: 0.001966, acc.: 100.00%] [G loss: 0.001792]\n",
      "epoch:38 step:29958 [D loss: 0.009832, acc.: 100.00%] [G loss: 0.003171]\n",
      "epoch:38 step:29959 [D loss: 0.062743, acc.: 98.44%] [G loss: 0.001979]\n",
      "epoch:38 step:29960 [D loss: 0.006213, acc.: 100.00%] [G loss: 0.017122]\n",
      "epoch:38 step:29961 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.015195]\n",
      "epoch:38 step:29962 [D loss: 0.001842, acc.: 100.00%] [G loss: 0.508409]\n",
      "epoch:38 step:29963 [D loss: 0.001559, acc.: 100.00%] [G loss: 0.019925]\n",
      "epoch:38 step:29964 [D loss: 0.017764, acc.: 100.00%] [G loss: 0.050470]\n",
      "epoch:38 step:29965 [D loss: 0.003710, acc.: 100.00%] [G loss: 0.039466]\n",
      "epoch:38 step:29966 [D loss: 0.006970, acc.: 100.00%] [G loss: 0.004066]\n",
      "epoch:38 step:29967 [D loss: 0.010235, acc.: 99.22%] [G loss: 0.008593]\n",
      "epoch:38 step:29968 [D loss: 0.004475, acc.: 100.00%] [G loss: 0.006667]\n",
      "epoch:38 step:29969 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.004204]\n",
      "epoch:38 step:29970 [D loss: 0.002345, acc.: 100.00%] [G loss: 0.002557]\n",
      "epoch:38 step:29971 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.008073]\n",
      "epoch:38 step:29972 [D loss: 0.001317, acc.: 100.00%] [G loss: 0.003796]\n",
      "epoch:38 step:29973 [D loss: 0.001689, acc.: 100.00%] [G loss: 0.003151]\n",
      "epoch:38 step:29974 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.006124]\n",
      "epoch:38 step:29975 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.003449]\n",
      "epoch:38 step:29976 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.042595]\n",
      "epoch:38 step:29977 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.003048]\n",
      "epoch:38 step:29978 [D loss: 0.006723, acc.: 100.00%] [G loss: 0.001900]\n",
      "epoch:38 step:29979 [D loss: 0.001374, acc.: 100.00%] [G loss: 0.001136]\n",
      "epoch:38 step:29980 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.001111]\n",
      "epoch:38 step:29981 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001381]\n",
      "epoch:38 step:29982 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.009663]\n",
      "epoch:38 step:29983 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.003205]\n",
      "epoch:38 step:29984 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.001148]\n",
      "epoch:38 step:29985 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000701]\n",
      "epoch:38 step:29986 [D loss: 0.006356, acc.: 100.00%] [G loss: 0.006580]\n",
      "epoch:38 step:29987 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.004200]\n",
      "epoch:38 step:29988 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:38 step:29989 [D loss: 0.001157, acc.: 100.00%] [G loss: 0.015491]\n",
      "epoch:38 step:29990 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.009781]\n",
      "epoch:38 step:29991 [D loss: 0.004408, acc.: 100.00%] [G loss: 0.022233]\n",
      "epoch:38 step:29992 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000991]\n",
      "epoch:38 step:29993 [D loss: 0.062699, acc.: 98.44%] [G loss: 0.000086]\n",
      "epoch:38 step:29994 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:38 step:29995 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:38 step:29996 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:38 step:29997 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:38 step:29998 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:38 step:29999 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:38 step:30000 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:38 step:30001 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:38 step:30002 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:38 step:30003 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:38 step:30004 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:38 step:30005 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:38 step:30006 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:38 step:30007 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:38 step:30008 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:38 step:30009 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:38 step:30010 [D loss: 0.004890, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:38 step:30011 [D loss: 0.001942, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:38 step:30012 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:38 step:30013 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000982]\n",
      "epoch:38 step:30014 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:38 step:30015 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:38 step:30016 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.001675]\n",
      "epoch:38 step:30017 [D loss: 0.001274, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:38 step:30018 [D loss: 0.070150, acc.: 96.09%] [G loss: 0.000722]\n",
      "epoch:38 step:30019 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.005159]\n",
      "epoch:38 step:30020 [D loss: 0.001573, acc.: 100.00%] [G loss: 0.014026]\n",
      "epoch:38 step:30021 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.016270]\n",
      "epoch:38 step:30022 [D loss: 0.002839, acc.: 100.00%] [G loss: 0.063808]\n",
      "epoch:38 step:30023 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.033755]\n",
      "epoch:38 step:30024 [D loss: 0.002825, acc.: 100.00%] [G loss: 0.004339]\n",
      "epoch:38 step:30025 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.008472]\n",
      "epoch:38 step:30026 [D loss: 0.006486, acc.: 100.00%] [G loss: 0.006932]\n",
      "epoch:38 step:30027 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.005159]\n",
      "epoch:38 step:30028 [D loss: 0.000790, acc.: 100.00%] [G loss: 0.000751]\n",
      "epoch:38 step:30029 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.015617]\n",
      "epoch:38 step:30030 [D loss: 0.006325, acc.: 100.00%] [G loss: 0.001139]\n",
      "epoch:38 step:30031 [D loss: 0.038747, acc.: 99.22%] [G loss: 0.000226]\n",
      "epoch:38 step:30032 [D loss: 0.116205, acc.: 96.88%] [G loss: 0.147621]\n",
      "epoch:38 step:30033 [D loss: 0.007946, acc.: 100.00%] [G loss: 1.396321]\n",
      "epoch:38 step:30034 [D loss: 0.004625, acc.: 100.00%] [G loss: 0.423400]\n",
      "epoch:38 step:30035 [D loss: 0.011156, acc.: 100.00%] [G loss: 0.168826]\n",
      "epoch:38 step:30036 [D loss: 0.608681, acc.: 70.31%] [G loss: 9.153346]\n",
      "epoch:38 step:30037 [D loss: 0.920814, acc.: 61.72%] [G loss: 5.833915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30038 [D loss: 0.080459, acc.: 96.09%] [G loss: 4.341276]\n",
      "epoch:38 step:30039 [D loss: 0.165702, acc.: 95.31%] [G loss: 0.782005]\n",
      "epoch:38 step:30040 [D loss: 0.008509, acc.: 100.00%] [G loss: 4.825591]\n",
      "epoch:38 step:30041 [D loss: 0.081923, acc.: 96.88%] [G loss: 4.786783]\n",
      "epoch:38 step:30042 [D loss: 0.062293, acc.: 99.22%] [G loss: 4.059312]\n",
      "epoch:38 step:30043 [D loss: 0.045090, acc.: 98.44%] [G loss: 4.274889]\n",
      "epoch:38 step:30044 [D loss: 0.076249, acc.: 97.66%] [G loss: 4.010876]\n",
      "epoch:38 step:30045 [D loss: 0.020900, acc.: 100.00%] [G loss: 4.676989]\n",
      "epoch:38 step:30046 [D loss: 0.167746, acc.: 93.75%] [G loss: 1.569413]\n",
      "epoch:38 step:30047 [D loss: 0.030318, acc.: 100.00%] [G loss: 4.796673]\n",
      "epoch:38 step:30048 [D loss: 0.068650, acc.: 96.09%] [G loss: 0.025822]\n",
      "epoch:38 step:30049 [D loss: 0.004823, acc.: 100.00%] [G loss: 2.485641]\n",
      "epoch:38 step:30050 [D loss: 0.014939, acc.: 100.00%] [G loss: 1.082783]\n",
      "epoch:38 step:30051 [D loss: 0.005889, acc.: 100.00%] [G loss: 0.257526]\n",
      "epoch:38 step:30052 [D loss: 0.003998, acc.: 100.00%] [G loss: 0.187077]\n",
      "epoch:38 step:30053 [D loss: 0.021094, acc.: 100.00%] [G loss: 0.034233]\n",
      "epoch:38 step:30054 [D loss: 0.002571, acc.: 100.00%] [G loss: 0.140561]\n",
      "epoch:38 step:30055 [D loss: 0.003719, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:38 step:30056 [D loss: 0.001984, acc.: 100.00%] [G loss: 0.020418]\n",
      "epoch:38 step:30057 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.012563]\n",
      "epoch:38 step:30058 [D loss: 0.021419, acc.: 100.00%] [G loss: 0.029173]\n",
      "epoch:38 step:30059 [D loss: 0.132993, acc.: 96.88%] [G loss: 0.256402]\n",
      "epoch:38 step:30060 [D loss: 0.040281, acc.: 99.22%] [G loss: 1.213280]\n",
      "epoch:38 step:30061 [D loss: 0.004620, acc.: 100.00%] [G loss: 0.560599]\n",
      "epoch:38 step:30062 [D loss: 0.043115, acc.: 99.22%] [G loss: 3.898398]\n",
      "epoch:38 step:30063 [D loss: 0.502537, acc.: 76.56%] [G loss: 4.262642]\n",
      "epoch:38 step:30064 [D loss: 0.970258, acc.: 63.28%] [G loss: 0.231341]\n",
      "epoch:38 step:30065 [D loss: 0.333278, acc.: 81.25%] [G loss: 3.181828]\n",
      "epoch:38 step:30066 [D loss: 0.431270, acc.: 82.81%] [G loss: 1.477309]\n",
      "epoch:38 step:30067 [D loss: 0.028007, acc.: 100.00%] [G loss: 3.609014]\n",
      "epoch:38 step:30068 [D loss: 0.017318, acc.: 100.00%] [G loss: 2.097349]\n",
      "epoch:38 step:30069 [D loss: 0.033486, acc.: 99.22%] [G loss: 1.070379]\n",
      "epoch:38 step:30070 [D loss: 0.050153, acc.: 98.44%] [G loss: 0.638822]\n",
      "epoch:38 step:30071 [D loss: 0.003066, acc.: 100.00%] [G loss: 0.267252]\n",
      "epoch:38 step:30072 [D loss: 0.002271, acc.: 100.00%] [G loss: 0.468672]\n",
      "epoch:38 step:30073 [D loss: 0.036750, acc.: 99.22%] [G loss: 0.387953]\n",
      "epoch:38 step:30074 [D loss: 0.002353, acc.: 100.00%] [G loss: 0.044714]\n",
      "epoch:38 step:30075 [D loss: 0.002490, acc.: 100.00%] [G loss: 0.017621]\n",
      "epoch:38 step:30076 [D loss: 0.001351, acc.: 100.00%] [G loss: 0.061689]\n",
      "epoch:38 step:30077 [D loss: 0.001492, acc.: 100.00%] [G loss: 0.069166]\n",
      "epoch:38 step:30078 [D loss: 0.001033, acc.: 100.00%] [G loss: 0.213967]\n",
      "epoch:38 step:30079 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.004076]\n",
      "epoch:38 step:30080 [D loss: 0.001684, acc.: 100.00%] [G loss: 0.019083]\n",
      "epoch:38 step:30081 [D loss: 0.003630, acc.: 100.00%] [G loss: 0.018032]\n",
      "epoch:38 step:30082 [D loss: 0.002601, acc.: 100.00%] [G loss: 0.020475]\n",
      "epoch:38 step:30083 [D loss: 0.001027, acc.: 100.00%] [G loss: 0.005838]\n",
      "epoch:38 step:30084 [D loss: 0.000396, acc.: 100.00%] [G loss: 1.740075]\n",
      "epoch:38 step:30085 [D loss: 0.007815, acc.: 100.00%] [G loss: 0.017771]\n",
      "epoch:38 step:30086 [D loss: 0.056795, acc.: 98.44%] [G loss: 0.033262]\n",
      "epoch:38 step:30087 [D loss: 0.005100, acc.: 100.00%] [G loss: 0.035546]\n",
      "epoch:38 step:30088 [D loss: 0.018459, acc.: 100.00%] [G loss: 0.076997]\n",
      "epoch:38 step:30089 [D loss: 0.092901, acc.: 97.66%] [G loss: 0.034678]\n",
      "epoch:38 step:30090 [D loss: 0.034851, acc.: 99.22%] [G loss: 0.040483]\n",
      "epoch:38 step:30091 [D loss: 0.022627, acc.: 100.00%] [G loss: 0.038396]\n",
      "epoch:38 step:30092 [D loss: 0.016590, acc.: 100.00%] [G loss: 0.102580]\n",
      "epoch:38 step:30093 [D loss: 0.060580, acc.: 100.00%] [G loss: 1.790838]\n",
      "epoch:38 step:30094 [D loss: 0.001845, acc.: 100.00%] [G loss: 3.568795]\n",
      "epoch:38 step:30095 [D loss: 0.019068, acc.: 100.00%] [G loss: 1.061840]\n",
      "epoch:38 step:30096 [D loss: 0.002985, acc.: 100.00%] [G loss: 2.957854]\n",
      "epoch:38 step:30097 [D loss: 0.019621, acc.: 100.00%] [G loss: 2.238982]\n",
      "epoch:38 step:30098 [D loss: 0.170758, acc.: 93.75%] [G loss: 0.000367]\n",
      "epoch:38 step:30099 [D loss: 0.002183, acc.: 100.00%] [G loss: 0.178126]\n",
      "epoch:38 step:30100 [D loss: 0.318568, acc.: 81.25%] [G loss: 1.998524]\n",
      "epoch:38 step:30101 [D loss: 0.016908, acc.: 100.00%] [G loss: 1.474326]\n",
      "epoch:38 step:30102 [D loss: 0.514411, acc.: 80.47%] [G loss: 0.000759]\n",
      "epoch:38 step:30103 [D loss: 0.001303, acc.: 100.00%] [G loss: 0.017435]\n",
      "epoch:38 step:30104 [D loss: 0.007563, acc.: 100.00%] [G loss: 0.118547]\n",
      "epoch:38 step:30105 [D loss: 0.001268, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:38 step:30106 [D loss: 0.001363, acc.: 100.00%] [G loss: 0.050077]\n",
      "epoch:38 step:30107 [D loss: 0.008146, acc.: 100.00%] [G loss: 0.044017]\n",
      "epoch:38 step:30108 [D loss: 0.013799, acc.: 100.00%] [G loss: 0.003867]\n",
      "epoch:38 step:30109 [D loss: 0.028314, acc.: 99.22%] [G loss: 0.001264]\n",
      "epoch:38 step:30110 [D loss: 0.006499, acc.: 100.00%] [G loss: 0.001010]\n",
      "epoch:38 step:30111 [D loss: 0.011074, acc.: 100.00%] [G loss: 0.001298]\n",
      "epoch:38 step:30112 [D loss: 0.017189, acc.: 100.00%] [G loss: 0.004760]\n",
      "epoch:38 step:30113 [D loss: 0.001382, acc.: 100.00%] [G loss: 0.005556]\n",
      "epoch:38 step:30114 [D loss: 0.065974, acc.: 98.44%] [G loss: 2.555184]\n",
      "epoch:38 step:30115 [D loss: 0.157059, acc.: 91.41%] [G loss: 4.320675]\n",
      "epoch:38 step:30116 [D loss: 0.124071, acc.: 95.31%] [G loss: 7.047539]\n",
      "epoch:38 step:30117 [D loss: 0.051813, acc.: 98.44%] [G loss: 5.714761]\n",
      "epoch:38 step:30118 [D loss: 0.037841, acc.: 99.22%] [G loss: 4.397893]\n",
      "epoch:38 step:30119 [D loss: 0.028692, acc.: 99.22%] [G loss: 4.372991]\n",
      "epoch:38 step:30120 [D loss: 0.031926, acc.: 98.44%] [G loss: 4.523124]\n",
      "epoch:38 step:30121 [D loss: 0.006440, acc.: 100.00%] [G loss: 4.332316]\n",
      "epoch:38 step:30122 [D loss: 0.018847, acc.: 100.00%] [G loss: 4.948558]\n",
      "epoch:38 step:30123 [D loss: 0.172931, acc.: 93.75%] [G loss: 6.399426]\n",
      "epoch:38 step:30124 [D loss: 0.006909, acc.: 100.00%] [G loss: 7.897717]\n",
      "epoch:38 step:30125 [D loss: 0.002714, acc.: 100.00%] [G loss: 7.471410]\n",
      "epoch:38 step:30126 [D loss: 0.023154, acc.: 100.00%] [G loss: 0.309808]\n",
      "epoch:38 step:30127 [D loss: 0.002966, acc.: 100.00%] [G loss: 6.180329]\n",
      "epoch:38 step:30128 [D loss: 0.002302, acc.: 100.00%] [G loss: 5.845507]\n",
      "epoch:38 step:30129 [D loss: 0.027348, acc.: 99.22%] [G loss: 4.993475]\n",
      "epoch:38 step:30130 [D loss: 0.001175, acc.: 100.00%] [G loss: 6.431934]\n",
      "epoch:38 step:30131 [D loss: 0.060861, acc.: 98.44%] [G loss: 5.065579]\n",
      "epoch:38 step:30132 [D loss: 0.010205, acc.: 100.00%] [G loss: 4.985520]\n",
      "epoch:38 step:30133 [D loss: 0.034854, acc.: 100.00%] [G loss: 5.207208]\n",
      "epoch:38 step:30134 [D loss: 0.015562, acc.: 100.00%] [G loss: 4.770473]\n",
      "epoch:38 step:30135 [D loss: 0.009074, acc.: 100.00%] [G loss: 4.924191]\n",
      "epoch:38 step:30136 [D loss: 0.068382, acc.: 98.44%] [G loss: 2.526355]\n",
      "epoch:38 step:30137 [D loss: 0.029633, acc.: 100.00%] [G loss: 3.904743]\n",
      "epoch:38 step:30138 [D loss: 0.022016, acc.: 99.22%] [G loss: 2.507339]\n",
      "epoch:38 step:30139 [D loss: 0.012978, acc.: 100.00%] [G loss: 4.377683]\n",
      "epoch:38 step:30140 [D loss: 0.012607, acc.: 100.00%] [G loss: 0.398198]\n",
      "epoch:38 step:30141 [D loss: 0.024497, acc.: 100.00%] [G loss: 6.352281]\n",
      "epoch:38 step:30142 [D loss: 0.020575, acc.: 99.22%] [G loss: 0.303096]\n",
      "epoch:38 step:30143 [D loss: 0.061958, acc.: 97.66%] [G loss: 9.283431]\n",
      "epoch:38 step:30144 [D loss: 0.112378, acc.: 97.66%] [G loss: 7.084562]\n",
      "epoch:38 step:30145 [D loss: 0.071618, acc.: 96.88%] [G loss: 3.963813]\n",
      "epoch:38 step:30146 [D loss: 0.235181, acc.: 89.84%] [G loss: 6.980859]\n",
      "epoch:38 step:30147 [D loss: 0.464953, acc.: 79.69%] [G loss: 10.364471]\n",
      "epoch:38 step:30148 [D loss: 0.159466, acc.: 94.53%] [G loss: 12.047658]\n",
      "epoch:38 step:30149 [D loss: 0.151362, acc.: 96.09%] [G loss: 10.588236]\n",
      "epoch:38 step:30150 [D loss: 0.002312, acc.: 100.00%] [G loss: 1.769139]\n",
      "epoch:38 step:30151 [D loss: 0.003008, acc.: 100.00%] [G loss: 8.634651]\n",
      "epoch:38 step:30152 [D loss: 0.017579, acc.: 100.00%] [G loss: 7.450735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30153 [D loss: 0.005017, acc.: 100.00%] [G loss: 6.002362]\n",
      "epoch:38 step:30154 [D loss: 0.008329, acc.: 100.00%] [G loss: 5.837213]\n",
      "epoch:38 step:30155 [D loss: 0.001938, acc.: 100.00%] [G loss: 5.534685]\n",
      "epoch:38 step:30156 [D loss: 0.030450, acc.: 100.00%] [G loss: 5.006273]\n",
      "epoch:38 step:30157 [D loss: 0.005776, acc.: 100.00%] [G loss: 2.660528]\n",
      "epoch:38 step:30158 [D loss: 0.011514, acc.: 100.00%] [G loss: 4.975700]\n",
      "epoch:38 step:30159 [D loss: 0.190139, acc.: 89.84%] [G loss: 6.493016]\n",
      "epoch:38 step:30160 [D loss: 0.387094, acc.: 81.25%] [G loss: 4.921381]\n",
      "epoch:38 step:30161 [D loss: 0.000288, acc.: 100.00%] [G loss: 3.812099]\n",
      "epoch:38 step:30162 [D loss: 0.044364, acc.: 99.22%] [G loss: 3.916089]\n",
      "epoch:38 step:30163 [D loss: 0.064758, acc.: 95.31%] [G loss: 3.174757]\n",
      "epoch:38 step:30164 [D loss: 0.072297, acc.: 98.44%] [G loss: 0.040739]\n",
      "epoch:38 step:30165 [D loss: 0.248893, acc.: 88.28%] [G loss: 8.633900]\n",
      "epoch:38 step:30166 [D loss: 0.105788, acc.: 95.31%] [G loss: 2.182986]\n",
      "epoch:38 step:30167 [D loss: 0.123136, acc.: 96.09%] [G loss: 1.564384]\n",
      "epoch:38 step:30168 [D loss: 0.122028, acc.: 96.09%] [G loss: 0.335981]\n",
      "epoch:38 step:30169 [D loss: 0.268824, acc.: 89.06%] [G loss: 5.368364]\n",
      "epoch:38 step:30170 [D loss: 0.171277, acc.: 93.75%] [G loss: 6.166844]\n",
      "epoch:38 step:30171 [D loss: 0.005189, acc.: 100.00%] [G loss: 4.325714]\n",
      "epoch:38 step:30172 [D loss: 0.006061, acc.: 100.00%] [G loss: 0.856814]\n",
      "epoch:38 step:30173 [D loss: 0.000379, acc.: 100.00%] [G loss: 3.030005]\n",
      "epoch:38 step:30174 [D loss: 0.002504, acc.: 100.00%] [G loss: 1.967867]\n",
      "epoch:38 step:30175 [D loss: 0.043492, acc.: 97.66%] [G loss: 0.528125]\n",
      "epoch:38 step:30176 [D loss: 0.005759, acc.: 100.00%] [G loss: 0.078192]\n",
      "epoch:38 step:30177 [D loss: 0.022452, acc.: 100.00%] [G loss: 0.154877]\n",
      "epoch:38 step:30178 [D loss: 0.001658, acc.: 100.00%] [G loss: 0.324430]\n",
      "epoch:38 step:30179 [D loss: 0.001199, acc.: 100.00%] [G loss: 0.027943]\n",
      "epoch:38 step:30180 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.053113]\n",
      "epoch:38 step:30181 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.006164]\n",
      "epoch:38 step:30182 [D loss: 0.001203, acc.: 100.00%] [G loss: 0.038908]\n",
      "epoch:38 step:30183 [D loss: 0.007121, acc.: 99.22%] [G loss: 0.000205]\n",
      "epoch:38 step:30184 [D loss: 0.005179, acc.: 100.00%] [G loss: 0.008887]\n",
      "epoch:38 step:30185 [D loss: 0.002974, acc.: 100.00%] [G loss: 1.841729]\n",
      "epoch:38 step:30186 [D loss: 0.023955, acc.: 99.22%] [G loss: 0.018727]\n",
      "epoch:38 step:30187 [D loss: 0.019072, acc.: 100.00%] [G loss: 0.038215]\n",
      "epoch:38 step:30188 [D loss: 0.067020, acc.: 99.22%] [G loss: 0.051877]\n",
      "epoch:38 step:30189 [D loss: 0.006238, acc.: 100.00%] [G loss: 0.920488]\n",
      "epoch:38 step:30190 [D loss: 0.002545, acc.: 100.00%] [G loss: 0.987019]\n",
      "epoch:38 step:30191 [D loss: 0.510025, acc.: 81.25%] [G loss: 0.000023]\n",
      "epoch:38 step:30192 [D loss: 2.639875, acc.: 52.34%] [G loss: 7.079549]\n",
      "epoch:38 step:30193 [D loss: 3.115402, acc.: 50.00%] [G loss: 4.943915]\n",
      "epoch:38 step:30194 [D loss: 0.724735, acc.: 71.09%] [G loss: 1.712153]\n",
      "epoch:38 step:30195 [D loss: 0.013958, acc.: 99.22%] [G loss: 0.861139]\n",
      "epoch:38 step:30196 [D loss: 0.006643, acc.: 100.00%] [G loss: 0.705021]\n",
      "epoch:38 step:30197 [D loss: 0.019235, acc.: 100.00%] [G loss: 0.541609]\n",
      "epoch:38 step:30198 [D loss: 0.005848, acc.: 100.00%] [G loss: 0.261801]\n",
      "epoch:38 step:30199 [D loss: 0.100817, acc.: 96.88%] [G loss: 0.176084]\n",
      "epoch:38 step:30200 [D loss: 0.031000, acc.: 100.00%] [G loss: 0.133424]\n",
      "epoch:38 step:30201 [D loss: 0.005673, acc.: 100.00%] [G loss: 0.110497]\n",
      "epoch:38 step:30202 [D loss: 0.056116, acc.: 99.22%] [G loss: 0.072092]\n",
      "epoch:38 step:30203 [D loss: 0.004774, acc.: 100.00%] [G loss: 0.026205]\n",
      "epoch:38 step:30204 [D loss: 0.056798, acc.: 99.22%] [G loss: 0.107212]\n",
      "epoch:38 step:30205 [D loss: 0.017478, acc.: 100.00%] [G loss: 0.079594]\n",
      "epoch:38 step:30206 [D loss: 0.011621, acc.: 100.00%] [G loss: 0.023437]\n",
      "epoch:38 step:30207 [D loss: 0.006776, acc.: 100.00%] [G loss: 0.083031]\n",
      "epoch:38 step:30208 [D loss: 0.001430, acc.: 100.00%] [G loss: 0.093603]\n",
      "epoch:38 step:30209 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.097929]\n",
      "epoch:38 step:30210 [D loss: 0.007901, acc.: 100.00%] [G loss: 0.056177]\n",
      "epoch:38 step:30211 [D loss: 0.003986, acc.: 100.00%] [G loss: 0.012153]\n",
      "epoch:38 step:30212 [D loss: 0.000925, acc.: 100.00%] [G loss: 0.371397]\n",
      "epoch:38 step:30213 [D loss: 0.011956, acc.: 99.22%] [G loss: 0.030202]\n",
      "epoch:38 step:30214 [D loss: 0.001587, acc.: 100.00%] [G loss: 0.023033]\n",
      "epoch:38 step:30215 [D loss: 0.021087, acc.: 100.00%] [G loss: 0.002874]\n",
      "epoch:38 step:30216 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.017503]\n",
      "epoch:38 step:30217 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.394199]\n",
      "epoch:38 step:30218 [D loss: 0.000791, acc.: 100.00%] [G loss: 0.007964]\n",
      "epoch:38 step:30219 [D loss: 0.003469, acc.: 100.00%] [G loss: 0.658313]\n",
      "epoch:38 step:30220 [D loss: 0.005897, acc.: 100.00%] [G loss: 0.002122]\n",
      "epoch:38 step:30221 [D loss: 0.008913, acc.: 100.00%] [G loss: 0.004808]\n",
      "epoch:38 step:30222 [D loss: 0.341602, acc.: 84.38%] [G loss: 0.309632]\n",
      "epoch:38 step:30223 [D loss: 0.011026, acc.: 100.00%] [G loss: 3.861161]\n",
      "epoch:38 step:30224 [D loss: 0.102034, acc.: 96.88%] [G loss: 0.066490]\n",
      "epoch:38 step:30225 [D loss: 0.634454, acc.: 76.56%] [G loss: 0.054696]\n",
      "epoch:38 step:30226 [D loss: 0.012541, acc.: 100.00%] [G loss: 0.045766]\n",
      "epoch:38 step:30227 [D loss: 0.047181, acc.: 99.22%] [G loss: 0.010121]\n",
      "epoch:38 step:30228 [D loss: 0.010522, acc.: 100.00%] [G loss: 0.024851]\n",
      "epoch:38 step:30229 [D loss: 0.116065, acc.: 96.88%] [G loss: 0.094436]\n",
      "epoch:38 step:30230 [D loss: 0.004106, acc.: 100.00%] [G loss: 0.643370]\n",
      "epoch:38 step:30231 [D loss: 0.010444, acc.: 100.00%] [G loss: 0.006292]\n",
      "epoch:38 step:30232 [D loss: 0.020366, acc.: 100.00%] [G loss: 0.184380]\n",
      "epoch:38 step:30233 [D loss: 0.002955, acc.: 100.00%] [G loss: 0.115629]\n",
      "epoch:38 step:30234 [D loss: 0.002531, acc.: 100.00%] [G loss: 0.184598]\n",
      "epoch:38 step:30235 [D loss: 0.001990, acc.: 100.00%] [G loss: 0.005037]\n",
      "epoch:38 step:30236 [D loss: 0.018469, acc.: 99.22%] [G loss: 0.063322]\n",
      "epoch:38 step:30237 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.020654]\n",
      "epoch:38 step:30238 [D loss: 0.001233, acc.: 100.00%] [G loss: 0.015539]\n",
      "epoch:38 step:30239 [D loss: 0.003676, acc.: 100.00%] [G loss: 0.033498]\n",
      "epoch:38 step:30240 [D loss: 0.005136, acc.: 100.00%] [G loss: 0.012235]\n",
      "epoch:38 step:30241 [D loss: 0.008493, acc.: 100.00%] [G loss: 0.065817]\n",
      "epoch:38 step:30242 [D loss: 0.062966, acc.: 100.00%] [G loss: 0.088413]\n",
      "epoch:38 step:30243 [D loss: 0.006339, acc.: 100.00%] [G loss: 0.014841]\n",
      "epoch:38 step:30244 [D loss: 0.019891, acc.: 100.00%] [G loss: 0.197373]\n",
      "epoch:38 step:30245 [D loss: 0.014017, acc.: 100.00%] [G loss: 0.166247]\n",
      "epoch:38 step:30246 [D loss: 0.001909, acc.: 100.00%] [G loss: 0.164465]\n",
      "epoch:38 step:30247 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.027030]\n",
      "epoch:38 step:30248 [D loss: 0.004449, acc.: 100.00%] [G loss: 0.017684]\n",
      "epoch:38 step:30249 [D loss: 0.063635, acc.: 100.00%] [G loss: 0.060810]\n",
      "epoch:38 step:30250 [D loss: 0.002477, acc.: 100.00%] [G loss: 0.670085]\n",
      "epoch:38 step:30251 [D loss: 0.013048, acc.: 100.00%] [G loss: 0.039272]\n",
      "epoch:38 step:30252 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.322438]\n",
      "epoch:38 step:30253 [D loss: 0.003954, acc.: 100.00%] [G loss: 0.337813]\n",
      "epoch:38 step:30254 [D loss: 0.016110, acc.: 99.22%] [G loss: 0.015282]\n",
      "epoch:38 step:30255 [D loss: 0.005001, acc.: 100.00%] [G loss: 0.005530]\n",
      "epoch:38 step:30256 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.002061]\n",
      "epoch:38 step:30257 [D loss: 0.000538, acc.: 100.00%] [G loss: 0.038517]\n",
      "epoch:38 step:30258 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.004558]\n",
      "epoch:38 step:30259 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.045273]\n",
      "epoch:38 step:30260 [D loss: 0.003833, acc.: 100.00%] [G loss: 0.000988]\n",
      "epoch:38 step:30261 [D loss: 0.004578, acc.: 100.00%] [G loss: 0.001402]\n",
      "epoch:38 step:30262 [D loss: 0.000906, acc.: 100.00%] [G loss: 0.078783]\n",
      "epoch:38 step:30263 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.011353]\n",
      "epoch:38 step:30264 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.010535]\n",
      "epoch:38 step:30265 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.010192]\n",
      "epoch:38 step:30266 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.002016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30267 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000451]\n",
      "epoch:38 step:30268 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.005764]\n",
      "epoch:38 step:30269 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.290099]\n",
      "epoch:38 step:30270 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.015983]\n",
      "epoch:38 step:30271 [D loss: 0.000972, acc.: 100.00%] [G loss: 0.005909]\n",
      "epoch:38 step:30272 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.005117]\n",
      "epoch:38 step:30273 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.003744]\n",
      "epoch:38 step:30274 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.045575]\n",
      "epoch:38 step:30275 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:38 step:30276 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.236535]\n",
      "epoch:38 step:30277 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.008656]\n",
      "epoch:38 step:30278 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.004441]\n",
      "epoch:38 step:30279 [D loss: 0.001889, acc.: 100.00%] [G loss: 0.010606]\n",
      "epoch:38 step:30280 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.001373]\n",
      "epoch:38 step:30281 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.004535]\n",
      "epoch:38 step:30282 [D loss: 0.004173, acc.: 100.00%] [G loss: 0.003264]\n",
      "epoch:38 step:30283 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.002237]\n",
      "epoch:38 step:30284 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.003950]\n",
      "epoch:38 step:30285 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.003214]\n",
      "epoch:38 step:30286 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.004160]\n",
      "epoch:38 step:30287 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.005671]\n",
      "epoch:38 step:30288 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.002971]\n",
      "epoch:38 step:30289 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.008798]\n",
      "epoch:38 step:30290 [D loss: 0.002241, acc.: 100.00%] [G loss: 0.000426]\n",
      "epoch:38 step:30291 [D loss: 0.016451, acc.: 99.22%] [G loss: 0.002343]\n",
      "epoch:38 step:30292 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:38 step:30293 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002615]\n",
      "epoch:38 step:30294 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.025032]\n",
      "epoch:38 step:30295 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.001993]\n",
      "epoch:38 step:30296 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.005241]\n",
      "epoch:38 step:30297 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.004128]\n",
      "epoch:38 step:30298 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:38 step:30299 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.003800]\n",
      "epoch:38 step:30300 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.006240]\n",
      "epoch:38 step:30301 [D loss: 0.014302, acc.: 100.00%] [G loss: 0.001005]\n",
      "epoch:38 step:30302 [D loss: 0.030951, acc.: 100.00%] [G loss: 0.007457]\n",
      "epoch:38 step:30303 [D loss: 0.009990, acc.: 100.00%] [G loss: 0.029773]\n",
      "epoch:38 step:30304 [D loss: 0.004100, acc.: 100.00%] [G loss: 0.054487]\n",
      "epoch:38 step:30305 [D loss: 0.051868, acc.: 99.22%] [G loss: 0.261158]\n",
      "epoch:38 step:30306 [D loss: 0.088537, acc.: 97.66%] [G loss: 0.151673]\n",
      "epoch:38 step:30307 [D loss: 0.053770, acc.: 99.22%] [G loss: 0.960719]\n",
      "epoch:38 step:30308 [D loss: 0.011552, acc.: 100.00%] [G loss: 1.254329]\n",
      "epoch:38 step:30309 [D loss: 0.071599, acc.: 99.22%] [G loss: 0.003280]\n",
      "epoch:38 step:30310 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.233040]\n",
      "epoch:38 step:30311 [D loss: 0.012005, acc.: 100.00%] [G loss: 1.771545]\n",
      "epoch:38 step:30312 [D loss: 0.037310, acc.: 98.44%] [G loss: 0.001091]\n",
      "epoch:38 step:30313 [D loss: 0.000481, acc.: 100.00%] [G loss: 1.379669]\n",
      "epoch:38 step:30314 [D loss: 0.000863, acc.: 100.00%] [G loss: 0.184565]\n",
      "epoch:38 step:30315 [D loss: 0.005932, acc.: 100.00%] [G loss: 0.001374]\n",
      "epoch:38 step:30316 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001245]\n",
      "epoch:38 step:30317 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.204901]\n",
      "epoch:38 step:30318 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.056267]\n",
      "epoch:38 step:30319 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.056272]\n",
      "epoch:38 step:30320 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.084835]\n",
      "epoch:38 step:30321 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.014844]\n",
      "epoch:38 step:30322 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:38 step:30323 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000962]\n",
      "epoch:38 step:30324 [D loss: 0.004989, acc.: 100.00%] [G loss: 0.027078]\n",
      "epoch:38 step:30325 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.021083]\n",
      "epoch:38 step:30326 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.025388]\n",
      "epoch:38 step:30327 [D loss: 0.005219, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:38 step:30328 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.009363]\n",
      "epoch:38 step:30329 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:38 step:30330 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.002384]\n",
      "epoch:38 step:30331 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.006538]\n",
      "epoch:38 step:30332 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.003156]\n",
      "epoch:38 step:30333 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.006816]\n",
      "epoch:38 step:30334 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.022782]\n",
      "epoch:38 step:30335 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.008166]\n",
      "epoch:38 step:30336 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.025215]\n",
      "epoch:38 step:30337 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:38 step:30338 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.021328]\n",
      "epoch:38 step:30339 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.006145]\n",
      "epoch:38 step:30340 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.006008]\n",
      "epoch:38 step:30341 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.008217]\n",
      "epoch:38 step:30342 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.005816]\n",
      "epoch:38 step:30343 [D loss: 0.016604, acc.: 99.22%] [G loss: 0.006510]\n",
      "epoch:38 step:30344 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:38 step:30345 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:38 step:30346 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.021954]\n",
      "epoch:38 step:30347 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.006088]\n",
      "epoch:38 step:30348 [D loss: 0.001958, acc.: 100.00%] [G loss: 0.010677]\n",
      "epoch:38 step:30349 [D loss: 0.002415, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:38 step:30350 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.002183]\n",
      "epoch:38 step:30351 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.007187]\n",
      "epoch:38 step:30352 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:38 step:30353 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.002410]\n",
      "epoch:38 step:30354 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.003539]\n",
      "epoch:38 step:30355 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001668]\n",
      "epoch:38 step:30356 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:38 step:30357 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.000811]\n",
      "epoch:38 step:30358 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.014120]\n",
      "epoch:38 step:30359 [D loss: 0.001379, acc.: 100.00%] [G loss: 0.002629]\n",
      "epoch:38 step:30360 [D loss: 0.309740, acc.: 87.50%] [G loss: 0.978982]\n",
      "epoch:38 step:30361 [D loss: 0.024202, acc.: 99.22%] [G loss: 0.367700]\n",
      "epoch:38 step:30362 [D loss: 0.137005, acc.: 94.53%] [G loss: 1.578955]\n",
      "epoch:38 step:30363 [D loss: 0.019201, acc.: 99.22%] [G loss: 0.370509]\n",
      "epoch:38 step:30364 [D loss: 0.034576, acc.: 98.44%] [G loss: 0.010272]\n",
      "epoch:38 step:30365 [D loss: 0.008186, acc.: 100.00%] [G loss: 0.068488]\n",
      "epoch:38 step:30366 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.037542]\n",
      "epoch:38 step:30367 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.015106]\n",
      "epoch:38 step:30368 [D loss: 0.001742, acc.: 100.00%] [G loss: 0.053745]\n",
      "epoch:38 step:30369 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.019254]\n",
      "epoch:38 step:30370 [D loss: 0.024613, acc.: 100.00%] [G loss: 0.160438]\n",
      "epoch:38 step:30371 [D loss: 0.003829, acc.: 100.00%] [G loss: 0.031948]\n",
      "epoch:38 step:30372 [D loss: 0.001521, acc.: 100.00%] [G loss: 0.070116]\n",
      "epoch:38 step:30373 [D loss: 0.007619, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:38 step:30374 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.029986]\n",
      "epoch:38 step:30375 [D loss: 0.001743, acc.: 100.00%] [G loss: 0.029819]\n",
      "epoch:38 step:30376 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.064477]\n",
      "epoch:38 step:30377 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:38 step:30378 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:38 step:30379 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004094]\n",
      "epoch:38 step:30380 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.019444]\n",
      "epoch:38 step:30381 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.054068]\n",
      "epoch:38 step:30382 [D loss: 0.008704, acc.: 99.22%] [G loss: 0.005937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30383 [D loss: 0.001712, acc.: 100.00%] [G loss: 0.012399]\n",
      "epoch:38 step:30384 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.051910]\n",
      "epoch:38 step:30385 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.002067]\n",
      "epoch:38 step:30386 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.002892]\n",
      "epoch:38 step:30387 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:38 step:30388 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.004531]\n",
      "epoch:38 step:30389 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.003534]\n",
      "epoch:38 step:30390 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.010995]\n",
      "epoch:38 step:30391 [D loss: 0.002484, acc.: 100.00%] [G loss: 0.006066]\n",
      "epoch:38 step:30392 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:38 step:30393 [D loss: 0.013803, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:38 step:30394 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.002130]\n",
      "epoch:38 step:30395 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:38 step:30396 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.002318]\n",
      "epoch:38 step:30397 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.002642]\n",
      "epoch:38 step:30398 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.005255]\n",
      "epoch:38 step:30399 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:38 step:30400 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.003750]\n",
      "epoch:38 step:30401 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.007156]\n",
      "epoch:38 step:30402 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:38 step:30403 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:38 step:30404 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000673]\n",
      "epoch:38 step:30405 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.002406]\n",
      "epoch:38 step:30406 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.299680]\n",
      "epoch:38 step:30407 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001648]\n",
      "epoch:38 step:30408 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001290]\n",
      "epoch:38 step:30409 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000906]\n",
      "epoch:38 step:30410 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000905]\n",
      "epoch:38 step:30411 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.005465]\n",
      "epoch:38 step:30412 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.002663]\n",
      "epoch:38 step:30413 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.003464]\n",
      "epoch:38 step:30414 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.002606]\n",
      "epoch:38 step:30415 [D loss: 0.003005, acc.: 100.00%] [G loss: 0.088880]\n",
      "epoch:38 step:30416 [D loss: 0.072659, acc.: 97.66%] [G loss: 0.019567]\n",
      "epoch:38 step:30417 [D loss: 0.067114, acc.: 97.66%] [G loss: 0.885442]\n",
      "epoch:38 step:30418 [D loss: 0.003945, acc.: 100.00%] [G loss: 1.991005]\n",
      "epoch:38 step:30419 [D loss: 0.040921, acc.: 98.44%] [G loss: 1.030222]\n",
      "epoch:38 step:30420 [D loss: 0.159427, acc.: 95.31%] [G loss: 0.175223]\n",
      "epoch:38 step:30421 [D loss: 0.652024, acc.: 67.97%] [G loss: 7.227344]\n",
      "epoch:38 step:30422 [D loss: 0.688257, acc.: 74.22%] [G loss: 3.916177]\n",
      "epoch:38 step:30423 [D loss: 0.034108, acc.: 99.22%] [G loss: 0.042154]\n",
      "epoch:38 step:30424 [D loss: 0.044265, acc.: 99.22%] [G loss: 6.638848]\n",
      "epoch:38 step:30425 [D loss: 0.028288, acc.: 98.44%] [G loss: 0.000414]\n",
      "epoch:38 step:30426 [D loss: 0.005089, acc.: 100.00%] [G loss: 0.028485]\n",
      "epoch:38 step:30427 [D loss: 0.023991, acc.: 99.22%] [G loss: 4.935183]\n",
      "epoch:38 step:30428 [D loss: 0.001967, acc.: 100.00%] [G loss: 4.259166]\n",
      "epoch:38 step:30429 [D loss: 0.002972, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:38 step:30430 [D loss: 0.059857, acc.: 98.44%] [G loss: 1.963787]\n",
      "epoch:38 step:30431 [D loss: 0.002597, acc.: 100.00%] [G loss: 2.383682]\n",
      "epoch:38 step:30432 [D loss: 0.007632, acc.: 100.00%] [G loss: 0.791685]\n",
      "epoch:38 step:30433 [D loss: 0.009530, acc.: 100.00%] [G loss: 0.751356]\n",
      "epoch:38 step:30434 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.045001]\n",
      "epoch:38 step:30435 [D loss: 0.005521, acc.: 100.00%] [G loss: 0.007543]\n",
      "epoch:38 step:30436 [D loss: 0.001337, acc.: 100.00%] [G loss: 0.005427]\n",
      "epoch:38 step:30437 [D loss: 0.010906, acc.: 100.00%] [G loss: 0.003221]\n",
      "epoch:38 step:30438 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.002590]\n",
      "epoch:38 step:30439 [D loss: 0.000734, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:38 step:30440 [D loss: 0.002669, acc.: 100.00%] [G loss: 0.004323]\n",
      "epoch:38 step:30441 [D loss: 0.001074, acc.: 100.00%] [G loss: 0.002823]\n",
      "epoch:38 step:30442 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.001913]\n",
      "epoch:38 step:30443 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.013345]\n",
      "epoch:38 step:30444 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:38 step:30445 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.000852]\n",
      "epoch:38 step:30446 [D loss: 0.001377, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:38 step:30447 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.002218]\n",
      "epoch:38 step:30448 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:38 step:30449 [D loss: 0.016096, acc.: 100.00%] [G loss: 0.002654]\n",
      "epoch:38 step:30450 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.002151]\n",
      "epoch:38 step:30451 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:38 step:30452 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:38 step:30453 [D loss: 0.063853, acc.: 97.66%] [G loss: 0.000547]\n",
      "epoch:38 step:30454 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.005029]\n",
      "epoch:38 step:30455 [D loss: 0.321572, acc.: 83.59%] [G loss: 1.190325]\n",
      "epoch:38 step:30456 [D loss: 0.399444, acc.: 85.16%] [G loss: 0.004346]\n",
      "epoch:38 step:30457 [D loss: 0.018536, acc.: 99.22%] [G loss: 0.000751]\n",
      "epoch:38 step:30458 [D loss: 0.007148, acc.: 99.22%] [G loss: 0.000098]\n",
      "epoch:38 step:30459 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.001013]\n",
      "epoch:39 step:30460 [D loss: 0.001915, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:39 step:30461 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:39 step:30462 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:39 step:30463 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:39 step:30464 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:39 step:30465 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:39 step:30466 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.005762]\n",
      "epoch:39 step:30467 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:39 step:30468 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:39 step:30469 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:39 step:30470 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:39 step:30471 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:39 step:30472 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:39 step:30473 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:39 step:30474 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:39 step:30475 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:39 step:30476 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.023697]\n",
      "epoch:39 step:30477 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:39 step:30478 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.001533]\n",
      "epoch:39 step:30479 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001470]\n",
      "epoch:39 step:30480 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:39 step:30481 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:39 step:30482 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:39 step:30483 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:39 step:30484 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:39 step:30485 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.005225]\n",
      "epoch:39 step:30486 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:39 step:30487 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:39 step:30488 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:39 step:30489 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:39 step:30490 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:39 step:30491 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.001160]\n",
      "epoch:39 step:30492 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:39 step:30493 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:39 step:30494 [D loss: 0.001617, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:39 step:30495 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:39 step:30496 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30497 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.003273]\n",
      "epoch:39 step:30498 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001432]\n",
      "epoch:39 step:30499 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:39 step:30500 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:39 step:30501 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:39 step:30502 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.004278]\n",
      "epoch:39 step:30503 [D loss: 0.016538, acc.: 99.22%] [G loss: 0.011376]\n",
      "epoch:39 step:30504 [D loss: 0.001902, acc.: 100.00%] [G loss: 0.000442]\n",
      "epoch:39 step:30505 [D loss: 0.265636, acc.: 84.38%] [G loss: 0.158873]\n",
      "epoch:39 step:30506 [D loss: 0.315500, acc.: 84.38%] [G loss: 0.349522]\n",
      "epoch:39 step:30507 [D loss: 0.006073, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:39 step:30508 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.168025]\n",
      "epoch:39 step:30509 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.378576]\n",
      "epoch:39 step:30510 [D loss: 0.002289, acc.: 100.00%] [G loss: 0.098218]\n",
      "epoch:39 step:30511 [D loss: 0.005409, acc.: 100.00%] [G loss: 0.001503]\n",
      "epoch:39 step:30512 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.335727]\n",
      "epoch:39 step:30513 [D loss: 0.007042, acc.: 100.00%] [G loss: 0.037837]\n",
      "epoch:39 step:30514 [D loss: 0.015378, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:39 step:30515 [D loss: 0.048076, acc.: 99.22%] [G loss: 0.337182]\n",
      "epoch:39 step:30516 [D loss: 0.008230, acc.: 100.00%] [G loss: 0.001764]\n",
      "epoch:39 step:30517 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.235059]\n",
      "epoch:39 step:30518 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.002600]\n",
      "epoch:39 step:30519 [D loss: 0.001201, acc.: 100.00%] [G loss: 0.027861]\n",
      "epoch:39 step:30520 [D loss: 0.001149, acc.: 100.00%] [G loss: 0.001635]\n",
      "epoch:39 step:30521 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.002012]\n",
      "epoch:39 step:30522 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:39 step:30523 [D loss: 0.010963, acc.: 100.00%] [G loss: 0.001076]\n",
      "epoch:39 step:30524 [D loss: 0.042564, acc.: 99.22%] [G loss: 0.000038]\n",
      "epoch:39 step:30525 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30526 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:39 step:30527 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:39 step:30528 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.003933]\n",
      "epoch:39 step:30529 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.005772]\n",
      "epoch:39 step:30530 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30531 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:30532 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.016986]\n",
      "epoch:39 step:30533 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:39 step:30534 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:39 step:30535 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:39 step:30536 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:39 step:30537 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.001370]\n",
      "epoch:39 step:30538 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:39 step:30539 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000536]\n",
      "epoch:39 step:30540 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:30541 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000997]\n",
      "epoch:39 step:30542 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:39 step:30543 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.007544]\n",
      "epoch:39 step:30544 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:39 step:30545 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000680]\n",
      "epoch:39 step:30546 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:39 step:30547 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30548 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:39 step:30549 [D loss: 0.003636, acc.: 100.00%] [G loss: 0.005341]\n",
      "epoch:39 step:30550 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:39 step:30551 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002445]\n",
      "epoch:39 step:30552 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:30553 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30554 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30555 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.001993]\n",
      "epoch:39 step:30556 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:39 step:30557 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:30558 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.005581]\n",
      "epoch:39 step:30559 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:39 step:30560 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30561 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:30562 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30563 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.004051]\n",
      "epoch:39 step:30564 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30565 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30566 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:39 step:30567 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:30568 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:39 step:30569 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:39 step:30570 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.000900]\n",
      "epoch:39 step:30571 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30572 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30573 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.002993]\n",
      "epoch:39 step:30574 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:39 step:30575 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30576 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30577 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:39 step:30578 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.002223]\n",
      "epoch:39 step:30579 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:39 step:30580 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:39 step:30581 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30582 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30583 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.027605]\n",
      "epoch:39 step:30584 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:30585 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:30586 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30587 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:30588 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000903]\n",
      "epoch:39 step:30589 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:39 step:30590 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:39 step:30591 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:39 step:30592 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004117]\n",
      "epoch:39 step:30593 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30594 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000692]\n",
      "epoch:39 step:30595 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000993]\n",
      "epoch:39 step:30596 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:39 step:30597 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:39 step:30598 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:30599 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:39 step:30600 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:39 step:30601 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:30602 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.001887]\n",
      "epoch:39 step:30603 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001107]\n",
      "epoch:39 step:30604 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:39 step:30605 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:39 step:30606 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:39 step:30607 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30608 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30609 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:39 step:30610 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30611 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30612 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30613 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:39 step:30614 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:39 step:30615 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:39 step:30616 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:39 step:30617 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:39 step:30618 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30619 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:39 step:30620 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000573]\n",
      "epoch:39 step:30621 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:39 step:30622 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30623 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:39 step:30624 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30625 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30626 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30627 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30628 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:39 step:30629 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001439]\n",
      "epoch:39 step:30630 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:39 step:30631 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30632 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:39 step:30633 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:30634 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:30635 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:39 step:30636 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30637 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30638 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:39 step:30639 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:30640 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:39 step:30641 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30642 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:30643 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:39 step:30644 [D loss: 0.002431, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:30645 [D loss: 0.020867, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:39 step:30646 [D loss: 0.002276, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:39 step:30647 [D loss: 0.711556, acc.: 67.97%] [G loss: 8.897309]\n",
      "epoch:39 step:30648 [D loss: 1.881848, acc.: 53.91%] [G loss: 2.090945]\n",
      "epoch:39 step:30649 [D loss: 0.416516, acc.: 86.72%] [G loss: 3.121275]\n",
      "epoch:39 step:30650 [D loss: 0.099436, acc.: 97.66%] [G loss: 4.075025]\n",
      "epoch:39 step:30651 [D loss: 0.102568, acc.: 96.88%] [G loss: 0.261322]\n",
      "epoch:39 step:30652 [D loss: 0.116918, acc.: 96.09%] [G loss: 1.682917]\n",
      "epoch:39 step:30653 [D loss: 0.021389, acc.: 100.00%] [G loss: 0.744870]\n",
      "epoch:39 step:30654 [D loss: 0.219413, acc.: 92.19%] [G loss: 1.893781]\n",
      "epoch:39 step:30655 [D loss: 0.004134, acc.: 100.00%] [G loss: 1.322807]\n",
      "epoch:39 step:30656 [D loss: 0.343283, acc.: 79.69%] [G loss: 0.198285]\n",
      "epoch:39 step:30657 [D loss: 0.049298, acc.: 98.44%] [G loss: 0.079947]\n",
      "epoch:39 step:30658 [D loss: 0.032278, acc.: 100.00%] [G loss: 0.086759]\n",
      "epoch:39 step:30659 [D loss: 0.068033, acc.: 98.44%] [G loss: 0.336271]\n",
      "epoch:39 step:30660 [D loss: 0.021573, acc.: 100.00%] [G loss: 0.281449]\n",
      "epoch:39 step:30661 [D loss: 0.004817, acc.: 100.00%] [G loss: 0.707605]\n",
      "epoch:39 step:30662 [D loss: 0.050232, acc.: 99.22%] [G loss: 1.197446]\n",
      "epoch:39 step:30663 [D loss: 0.138801, acc.: 95.31%] [G loss: 1.394284]\n",
      "epoch:39 step:30664 [D loss: 0.023923, acc.: 100.00%] [G loss: 2.477457]\n",
      "epoch:39 step:30665 [D loss: 0.780790, acc.: 62.50%] [G loss: 3.815010]\n",
      "epoch:39 step:30666 [D loss: 0.080623, acc.: 98.44%] [G loss: 5.616993]\n",
      "epoch:39 step:30667 [D loss: 0.865797, acc.: 64.06%] [G loss: 6.283679]\n",
      "epoch:39 step:30668 [D loss: 0.190311, acc.: 92.97%] [G loss: 4.201993]\n",
      "epoch:39 step:30669 [D loss: 0.201932, acc.: 92.97%] [G loss: 4.801182]\n",
      "epoch:39 step:30670 [D loss: 0.090400, acc.: 96.88%] [G loss: 5.060662]\n",
      "epoch:39 step:30671 [D loss: 0.012785, acc.: 100.00%] [G loss: 5.233578]\n",
      "epoch:39 step:30672 [D loss: 0.083257, acc.: 97.66%] [G loss: 4.867895]\n",
      "epoch:39 step:30673 [D loss: 0.101444, acc.: 96.09%] [G loss: 4.794640]\n",
      "epoch:39 step:30674 [D loss: 0.007204, acc.: 100.00%] [G loss: 4.777645]\n",
      "epoch:39 step:30675 [D loss: 0.011715, acc.: 100.00%] [G loss: 4.810763]\n",
      "epoch:39 step:30676 [D loss: 0.069885, acc.: 100.00%] [G loss: 5.510902]\n",
      "epoch:39 step:30677 [D loss: 0.051193, acc.: 98.44%] [G loss: 0.214477]\n",
      "epoch:39 step:30678 [D loss: 0.025986, acc.: 100.00%] [G loss: 4.430377]\n",
      "epoch:39 step:30679 [D loss: 0.047331, acc.: 100.00%] [G loss: 5.099579]\n",
      "epoch:39 step:30680 [D loss: 0.025428, acc.: 100.00%] [G loss: 3.409578]\n",
      "epoch:39 step:30681 [D loss: 0.088255, acc.: 99.22%] [G loss: 4.431979]\n",
      "epoch:39 step:30682 [D loss: 0.166753, acc.: 97.66%] [G loss: 0.023807]\n",
      "epoch:39 step:30683 [D loss: 0.077834, acc.: 96.09%] [G loss: 0.062931]\n",
      "epoch:39 step:30684 [D loss: 0.027769, acc.: 99.22%] [G loss: 0.007808]\n",
      "epoch:39 step:30685 [D loss: 0.199010, acc.: 92.97%] [G loss: 0.108438]\n",
      "epoch:39 step:30686 [D loss: 0.011495, acc.: 99.22%] [G loss: 5.469368]\n",
      "epoch:39 step:30687 [D loss: 0.074214, acc.: 95.31%] [G loss: 3.233743]\n",
      "epoch:39 step:30688 [D loss: 0.168952, acc.: 94.53%] [G loss: 0.029935]\n",
      "epoch:39 step:30689 [D loss: 0.701978, acc.: 67.97%] [G loss: 3.081876]\n",
      "epoch:39 step:30690 [D loss: 0.198529, acc.: 89.06%] [G loss: 2.164936]\n",
      "epoch:39 step:30691 [D loss: 0.086641, acc.: 96.09%] [G loss: 1.781286]\n",
      "epoch:39 step:30692 [D loss: 0.040723, acc.: 98.44%] [G loss: 1.972424]\n",
      "epoch:39 step:30693 [D loss: 0.007992, acc.: 100.00%] [G loss: 0.043527]\n",
      "epoch:39 step:30694 [D loss: 0.070165, acc.: 98.44%] [G loss: 0.014776]\n",
      "epoch:39 step:30695 [D loss: 0.183664, acc.: 91.41%] [G loss: 0.003533]\n",
      "epoch:39 step:30696 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.001818]\n",
      "epoch:39 step:30697 [D loss: 0.011351, acc.: 100.00%] [G loss: 0.006100]\n",
      "epoch:39 step:30698 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.001260]\n",
      "epoch:39 step:30699 [D loss: 0.021077, acc.: 100.00%] [G loss: 0.015499]\n",
      "epoch:39 step:30700 [D loss: 0.003403, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:39 step:30701 [D loss: 0.006603, acc.: 100.00%] [G loss: 0.000532]\n",
      "epoch:39 step:30702 [D loss: 0.003470, acc.: 100.00%] [G loss: 0.053845]\n",
      "epoch:39 step:30703 [D loss: 0.001162, acc.: 100.00%] [G loss: 0.097564]\n",
      "epoch:39 step:30704 [D loss: 0.002493, acc.: 100.00%] [G loss: 0.003075]\n",
      "epoch:39 step:30705 [D loss: 0.001255, acc.: 100.00%] [G loss: 0.043167]\n",
      "epoch:39 step:30706 [D loss: 0.424431, acc.: 78.91%] [G loss: 0.488310]\n",
      "epoch:39 step:30707 [D loss: 0.020421, acc.: 99.22%] [G loss: 3.393919]\n",
      "epoch:39 step:30708 [D loss: 1.791748, acc.: 53.12%] [G loss: 0.010192]\n",
      "epoch:39 step:30709 [D loss: 0.007573, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:39 step:30710 [D loss: 0.001335, acc.: 100.00%] [G loss: 0.044329]\n",
      "epoch:39 step:30711 [D loss: 0.133529, acc.: 94.53%] [G loss: 0.039128]\n",
      "epoch:39 step:30712 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.007167]\n",
      "epoch:39 step:30713 [D loss: 0.001843, acc.: 100.00%] [G loss: 2.222576]\n",
      "epoch:39 step:30714 [D loss: 0.005132, acc.: 100.00%] [G loss: 1.093456]\n",
      "epoch:39 step:30715 [D loss: 0.004014, acc.: 100.00%] [G loss: 0.320543]\n",
      "epoch:39 step:30716 [D loss: 0.052122, acc.: 99.22%] [G loss: 0.003341]\n",
      "epoch:39 step:30717 [D loss: 0.004093, acc.: 100.00%] [G loss: 0.008382]\n",
      "epoch:39 step:30718 [D loss: 0.002466, acc.: 100.00%] [G loss: 0.076195]\n",
      "epoch:39 step:30719 [D loss: 0.015768, acc.: 100.00%] [G loss: 0.051885]\n",
      "epoch:39 step:30720 [D loss: 0.088748, acc.: 97.66%] [G loss: 0.080976]\n",
      "epoch:39 step:30721 [D loss: 0.114524, acc.: 95.31%] [G loss: 0.005254]\n",
      "epoch:39 step:30722 [D loss: 0.002180, acc.: 100.00%] [G loss: 0.036048]\n",
      "epoch:39 step:30723 [D loss: 0.006616, acc.: 100.00%] [G loss: 0.052152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30724 [D loss: 0.157492, acc.: 94.53%] [G loss: 0.000781]\n",
      "epoch:39 step:30725 [D loss: 0.000900, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:39 step:30726 [D loss: 0.001957, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:39 step:30727 [D loss: 0.005858, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:39 step:30728 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.191967]\n",
      "epoch:39 step:30729 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:39 step:30730 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:39 step:30731 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:39 step:30732 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.001086]\n",
      "epoch:39 step:30733 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000684]\n",
      "epoch:39 step:30734 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:39 step:30735 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:39 step:30736 [D loss: 0.015126, acc.: 99.22%] [G loss: 0.000183]\n",
      "epoch:39 step:30737 [D loss: 0.003057, acc.: 100.00%] [G loss: 0.093246]\n",
      "epoch:39 step:30738 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.143376]\n",
      "epoch:39 step:30739 [D loss: 0.002418, acc.: 100.00%] [G loss: 0.108181]\n",
      "epoch:39 step:30740 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:39 step:30741 [D loss: 0.004579, acc.: 100.00%] [G loss: 0.305658]\n",
      "epoch:39 step:30742 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:39 step:30743 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:39 step:30744 [D loss: 0.005380, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:39 step:30745 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:39 step:30746 [D loss: 0.000443, acc.: 100.00%] [G loss: 0.000855]\n",
      "epoch:39 step:30747 [D loss: 0.002388, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:39 step:30748 [D loss: 0.004379, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:39 step:30749 [D loss: 0.003389, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:39 step:30750 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.029085]\n",
      "epoch:39 step:30751 [D loss: 0.001205, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:39 step:30752 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:39 step:30753 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.053275]\n",
      "epoch:39 step:30754 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.001337]\n",
      "epoch:39 step:30755 [D loss: 0.001198, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:39 step:30756 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:39 step:30757 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:39 step:30758 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.014097]\n",
      "epoch:39 step:30759 [D loss: 0.000876, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:39 step:30760 [D loss: 0.001664, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:39 step:30761 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:39 step:30762 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:39 step:30763 [D loss: 0.002048, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:39 step:30764 [D loss: 0.013622, acc.: 100.00%] [G loss: 0.000626]\n",
      "epoch:39 step:30765 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:39 step:30766 [D loss: 0.013541, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:39 step:30767 [D loss: 0.001807, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:39 step:30768 [D loss: 0.051229, acc.: 99.22%] [G loss: 0.006868]\n",
      "epoch:39 step:30769 [D loss: 0.003496, acc.: 100.00%] [G loss: 0.399913]\n",
      "epoch:39 step:30770 [D loss: 0.011588, acc.: 99.22%] [G loss: 0.548102]\n",
      "epoch:39 step:30771 [D loss: 0.001695, acc.: 100.00%] [G loss: 0.029913]\n",
      "epoch:39 step:30772 [D loss: 0.004462, acc.: 100.00%] [G loss: 0.024205]\n",
      "epoch:39 step:30773 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.008275]\n",
      "epoch:39 step:30774 [D loss: 0.540114, acc.: 80.47%] [G loss: 0.000262]\n",
      "epoch:39 step:30775 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.000766]\n",
      "epoch:39 step:30776 [D loss: 0.009767, acc.: 100.00%] [G loss: 0.007089]\n",
      "epoch:39 step:30777 [D loss: 0.012741, acc.: 100.00%] [G loss: 0.030927]\n",
      "epoch:39 step:30778 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.030327]\n",
      "epoch:39 step:30779 [D loss: 0.012819, acc.: 99.22%] [G loss: 0.003943]\n",
      "epoch:39 step:30780 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.004935]\n",
      "epoch:39 step:30781 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.004208]\n",
      "epoch:39 step:30782 [D loss: 0.004570, acc.: 100.00%] [G loss: 0.004124]\n",
      "epoch:39 step:30783 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.430786]\n",
      "epoch:39 step:30784 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.092556]\n",
      "epoch:39 step:30785 [D loss: 0.010811, acc.: 100.00%] [G loss: 0.013572]\n",
      "epoch:39 step:30786 [D loss: 0.008888, acc.: 100.00%] [G loss: 0.100711]\n",
      "epoch:39 step:30787 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.005260]\n",
      "epoch:39 step:30788 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.013809]\n",
      "epoch:39 step:30789 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.023804]\n",
      "epoch:39 step:30790 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.003103]\n",
      "epoch:39 step:30791 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.019512]\n",
      "epoch:39 step:30792 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.006820]\n",
      "epoch:39 step:30793 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.027174]\n",
      "epoch:39 step:30794 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.006812]\n",
      "epoch:39 step:30795 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.029876]\n",
      "epoch:39 step:30796 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.015572]\n",
      "epoch:39 step:30797 [D loss: 0.023306, acc.: 100.00%] [G loss: 0.003519]\n",
      "epoch:39 step:30798 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.004301]\n",
      "epoch:39 step:30799 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.002155]\n",
      "epoch:39 step:30800 [D loss: 0.001178, acc.: 100.00%] [G loss: 0.037300]\n",
      "epoch:39 step:30801 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.031018]\n",
      "epoch:39 step:30802 [D loss: 0.001387, acc.: 100.00%] [G loss: 0.019681]\n",
      "epoch:39 step:30803 [D loss: 0.001118, acc.: 100.00%] [G loss: 0.022931]\n",
      "epoch:39 step:30804 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.001847]\n",
      "epoch:39 step:30805 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.012389]\n",
      "epoch:39 step:30806 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.003640]\n",
      "epoch:39 step:30807 [D loss: 0.000446, acc.: 100.00%] [G loss: 0.002327]\n",
      "epoch:39 step:30808 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000979]\n",
      "epoch:39 step:30809 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.005034]\n",
      "epoch:39 step:30810 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:39 step:30811 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.001333]\n",
      "epoch:39 step:30812 [D loss: 0.009569, acc.: 99.22%] [G loss: 0.000290]\n",
      "epoch:39 step:30813 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:39 step:30814 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.009458]\n",
      "epoch:39 step:30815 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.009625]\n",
      "epoch:39 step:30816 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.008306]\n",
      "epoch:39 step:30817 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.015688]\n",
      "epoch:39 step:30818 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.008292]\n",
      "epoch:39 step:30819 [D loss: 0.036340, acc.: 100.00%] [G loss: 0.001181]\n",
      "epoch:39 step:30820 [D loss: 0.002395, acc.: 100.00%] [G loss: 0.495861]\n",
      "epoch:39 step:30821 [D loss: 0.003452, acc.: 100.00%] [G loss: 0.700291]\n",
      "epoch:39 step:30822 [D loss: 0.006228, acc.: 100.00%] [G loss: 0.005223]\n",
      "epoch:39 step:30823 [D loss: 0.093019, acc.: 97.66%] [G loss: 0.501480]\n",
      "epoch:39 step:30824 [D loss: 0.024763, acc.: 99.22%] [G loss: 1.831848]\n",
      "epoch:39 step:30825 [D loss: 0.021582, acc.: 100.00%] [G loss: 2.440015]\n",
      "epoch:39 step:30826 [D loss: 0.097282, acc.: 95.31%] [G loss: 0.391002]\n",
      "epoch:39 step:30827 [D loss: 0.136896, acc.: 92.97%] [G loss: 2.343757]\n",
      "epoch:39 step:30828 [D loss: 0.101029, acc.: 95.31%] [G loss: 1.886777]\n",
      "epoch:39 step:30829 [D loss: 0.024235, acc.: 100.00%] [G loss: 0.138181]\n",
      "epoch:39 step:30830 [D loss: 0.007850, acc.: 100.00%] [G loss: 0.138794]\n",
      "epoch:39 step:30831 [D loss: 0.055773, acc.: 97.66%] [G loss: 0.020747]\n",
      "epoch:39 step:30832 [D loss: 0.006256, acc.: 100.00%] [G loss: 1.211629]\n",
      "epoch:39 step:30833 [D loss: 0.003451, acc.: 100.00%] [G loss: 1.850256]\n",
      "epoch:39 step:30834 [D loss: 0.539443, acc.: 76.56%] [G loss: 2.448642]\n",
      "epoch:39 step:30835 [D loss: 1.365270, acc.: 56.25%] [G loss: 6.102818]\n",
      "epoch:39 step:30836 [D loss: 0.036358, acc.: 100.00%] [G loss: 4.034549]\n",
      "epoch:39 step:30837 [D loss: 0.103541, acc.: 96.09%] [G loss: 4.078797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30838 [D loss: 0.013970, acc.: 100.00%] [G loss: 0.001280]\n",
      "epoch:39 step:30839 [D loss: 0.006510, acc.: 100.00%] [G loss: 0.005856]\n",
      "epoch:39 step:30840 [D loss: 0.022291, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:39 step:30841 [D loss: 0.029541, acc.: 98.44%] [G loss: 4.187354]\n",
      "epoch:39 step:30842 [D loss: 0.015052, acc.: 100.00%] [G loss: 1.275020]\n",
      "epoch:39 step:30843 [D loss: 0.075453, acc.: 98.44%] [G loss: 0.003219]\n",
      "epoch:39 step:30844 [D loss: 0.006059, acc.: 100.00%] [G loss: 2.853685]\n",
      "epoch:39 step:30845 [D loss: 0.074493, acc.: 97.66%] [G loss: 0.009780]\n",
      "epoch:39 step:30846 [D loss: 0.032734, acc.: 99.22%] [G loss: 0.000628]\n",
      "epoch:39 step:30847 [D loss: 0.169789, acc.: 92.97%] [G loss: 0.000044]\n",
      "epoch:39 step:30848 [D loss: 0.001777, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:30849 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:39 step:30850 [D loss: 0.004808, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:39 step:30851 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.587422]\n",
      "epoch:39 step:30852 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:39 step:30853 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.179004]\n",
      "epoch:39 step:30854 [D loss: 0.013493, acc.: 100.00%] [G loss: 0.034339]\n",
      "epoch:39 step:30855 [D loss: 0.014100, acc.: 100.00%] [G loss: 0.094510]\n",
      "epoch:39 step:30856 [D loss: 0.146838, acc.: 95.31%] [G loss: 0.005494]\n",
      "epoch:39 step:30857 [D loss: 0.014183, acc.: 100.00%] [G loss: 0.892230]\n",
      "epoch:39 step:30858 [D loss: 0.080334, acc.: 95.31%] [G loss: 5.471254]\n",
      "epoch:39 step:30859 [D loss: 0.011980, acc.: 100.00%] [G loss: 2.055417]\n",
      "epoch:39 step:30860 [D loss: 0.102506, acc.: 97.66%] [G loss: 2.769298]\n",
      "epoch:39 step:30861 [D loss: 0.054899, acc.: 99.22%] [G loss: 0.044010]\n",
      "epoch:39 step:30862 [D loss: 0.029397, acc.: 99.22%] [G loss: 0.019647]\n",
      "epoch:39 step:30863 [D loss: 0.149641, acc.: 97.66%] [G loss: 0.018288]\n",
      "epoch:39 step:30864 [D loss: 0.009959, acc.: 100.00%] [G loss: 0.001096]\n",
      "epoch:39 step:30865 [D loss: 0.006702, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:39 step:30866 [D loss: 0.002651, acc.: 100.00%] [G loss: 0.024357]\n",
      "epoch:39 step:30867 [D loss: 0.008594, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:39 step:30868 [D loss: 0.011278, acc.: 100.00%] [G loss: 0.298467]\n",
      "epoch:39 step:30869 [D loss: 0.020307, acc.: 100.00%] [G loss: 0.000594]\n",
      "epoch:39 step:30870 [D loss: 0.442524, acc.: 85.94%] [G loss: 0.000734]\n",
      "epoch:39 step:30871 [D loss: 0.009482, acc.: 100.00%] [G loss: 1.500614]\n",
      "epoch:39 step:30872 [D loss: 0.192630, acc.: 92.97%] [G loss: 0.033973]\n",
      "epoch:39 step:30873 [D loss: 0.012068, acc.: 100.00%] [G loss: 1.335791]\n",
      "epoch:39 step:30874 [D loss: 0.051311, acc.: 97.66%] [G loss: 3.523260]\n",
      "epoch:39 step:30875 [D loss: 0.003462, acc.: 100.00%] [G loss: 0.009388]\n",
      "epoch:39 step:30876 [D loss: 0.004913, acc.: 100.00%] [G loss: 0.003683]\n",
      "epoch:39 step:30877 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.001695]\n",
      "epoch:39 step:30878 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.003117]\n",
      "epoch:39 step:30879 [D loss: 0.009377, acc.: 100.00%] [G loss: 0.001141]\n",
      "epoch:39 step:30880 [D loss: 0.001594, acc.: 100.00%] [G loss: 0.004520]\n",
      "epoch:39 step:30881 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:39 step:30882 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:39 step:30883 [D loss: 0.003320, acc.: 100.00%] [G loss: 0.002081]\n",
      "epoch:39 step:30884 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.944644]\n",
      "epoch:39 step:30885 [D loss: 0.001339, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:39 step:30886 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.012237]\n",
      "epoch:39 step:30887 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:39 step:30888 [D loss: 0.002759, acc.: 100.00%] [G loss: 0.002543]\n",
      "epoch:39 step:30889 [D loss: 0.042014, acc.: 97.66%] [G loss: 0.000004]\n",
      "epoch:39 step:30890 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.593244]\n",
      "epoch:39 step:30891 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30892 [D loss: 0.015407, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:30893 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:30894 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30895 [D loss: 0.031824, acc.: 100.00%] [G loss: 0.008716]\n",
      "epoch:39 step:30896 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.001410]\n",
      "epoch:39 step:30897 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.021350]\n",
      "epoch:39 step:30898 [D loss: 0.003719, acc.: 100.00%] [G loss: 0.000778]\n",
      "epoch:39 step:30899 [D loss: 0.004205, acc.: 100.00%] [G loss: 0.002639]\n",
      "epoch:39 step:30900 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:39 step:30901 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:39 step:30902 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.005826]\n",
      "epoch:39 step:30903 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:39 step:30904 [D loss: 0.001932, acc.: 100.00%] [G loss: 0.005055]\n",
      "epoch:39 step:30905 [D loss: 0.005660, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:39 step:30906 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000604]\n",
      "epoch:39 step:30907 [D loss: 0.005782, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:39 step:30908 [D loss: 0.002663, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:39 step:30909 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:39 step:30910 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.003015]\n",
      "epoch:39 step:30911 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.006559]\n",
      "epoch:39 step:30912 [D loss: 0.001760, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:39 step:30913 [D loss: 0.001399, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:39 step:30914 [D loss: 0.023892, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:39 step:30915 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.523392]\n",
      "epoch:39 step:30916 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.417525]\n",
      "epoch:39 step:30917 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.750977]\n",
      "epoch:39 step:30918 [D loss: 0.003648, acc.: 100.00%] [G loss: 0.008877]\n",
      "epoch:39 step:30919 [D loss: 0.265373, acc.: 85.94%] [G loss: 6.121071]\n",
      "epoch:39 step:30920 [D loss: 0.259897, acc.: 85.94%] [G loss: 0.605550]\n",
      "epoch:39 step:30921 [D loss: 0.098557, acc.: 94.53%] [G loss: 0.000505]\n",
      "epoch:39 step:30922 [D loss: 0.014748, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:39 step:30923 [D loss: 0.006899, acc.: 100.00%] [G loss: 0.475960]\n",
      "epoch:39 step:30924 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30925 [D loss: 0.011228, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:39 step:30926 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:39 step:30927 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:39 step:30928 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:39 step:30929 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.319938]\n",
      "epoch:39 step:30930 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:39 step:30931 [D loss: 0.004953, acc.: 100.00%] [G loss: 0.501228]\n",
      "epoch:39 step:30932 [D loss: 0.001457, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30933 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:39 step:30934 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:39 step:30935 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30936 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.147884]\n",
      "epoch:39 step:30937 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:30938 [D loss: 0.001835, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:39 step:30939 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30940 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:39 step:30941 [D loss: 0.001115, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:39 step:30942 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.052824]\n",
      "epoch:39 step:30943 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.001449]\n",
      "epoch:39 step:30944 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:39 step:30945 [D loss: 0.011996, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:39 step:30946 [D loss: 0.055818, acc.: 98.44%] [G loss: 0.635528]\n",
      "epoch:39 step:30947 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.041461]\n",
      "epoch:39 step:30948 [D loss: 0.019226, acc.: 99.22%] [G loss: 0.020033]\n",
      "epoch:39 step:30949 [D loss: 0.029299, acc.: 98.44%] [G loss: 0.001580]\n",
      "epoch:39 step:30950 [D loss: 0.361603, acc.: 83.59%] [G loss: 0.134999]\n",
      "epoch:39 step:30951 [D loss: 0.000975, acc.: 100.00%] [G loss: 5.436542]\n",
      "epoch:39 step:30952 [D loss: 0.024506, acc.: 99.22%] [G loss: 1.341105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30953 [D loss: 0.005378, acc.: 100.00%] [G loss: 0.172923]\n",
      "epoch:39 step:30954 [D loss: 0.011007, acc.: 99.22%] [G loss: 0.651119]\n",
      "epoch:39 step:30955 [D loss: 0.006317, acc.: 100.00%] [G loss: 0.019130]\n",
      "epoch:39 step:30956 [D loss: 0.041368, acc.: 98.44%] [G loss: 0.002274]\n",
      "epoch:39 step:30957 [D loss: 0.000747, acc.: 100.00%] [G loss: 0.001021]\n",
      "epoch:39 step:30958 [D loss: 0.024773, acc.: 100.00%] [G loss: 0.001332]\n",
      "epoch:39 step:30959 [D loss: 0.001945, acc.: 100.00%] [G loss: 0.023587]\n",
      "epoch:39 step:30960 [D loss: 0.003815, acc.: 100.00%] [G loss: 0.004956]\n",
      "epoch:39 step:30961 [D loss: 0.010042, acc.: 100.00%] [G loss: 0.001526]\n",
      "epoch:39 step:30962 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:39 step:30963 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.015048]\n",
      "epoch:39 step:30964 [D loss: 0.020496, acc.: 99.22%] [G loss: 0.122537]\n",
      "epoch:39 step:30965 [D loss: 0.122340, acc.: 93.75%] [G loss: 0.014530]\n",
      "epoch:39 step:30966 [D loss: 0.139140, acc.: 95.31%] [G loss: 0.538640]\n",
      "epoch:39 step:30967 [D loss: 0.014028, acc.: 99.22%] [G loss: 0.868897]\n",
      "epoch:39 step:30968 [D loss: 2.078236, acc.: 50.78%] [G loss: 4.342667]\n",
      "epoch:39 step:30969 [D loss: 1.366245, acc.: 56.25%] [G loss: 0.739248]\n",
      "epoch:39 step:30970 [D loss: 0.290922, acc.: 87.50%] [G loss: 0.086540]\n",
      "epoch:39 step:30971 [D loss: 0.199888, acc.: 94.53%] [G loss: 0.290193]\n",
      "epoch:39 step:30972 [D loss: 0.371214, acc.: 79.69%] [G loss: 0.048584]\n",
      "epoch:39 step:30973 [D loss: 0.271746, acc.: 87.50%] [G loss: 4.048670]\n",
      "epoch:39 step:30974 [D loss: 0.027558, acc.: 99.22%] [G loss: 0.169047]\n",
      "epoch:39 step:30975 [D loss: 0.025589, acc.: 100.00%] [G loss: 0.033693]\n",
      "epoch:39 step:30976 [D loss: 0.043949, acc.: 99.22%] [G loss: 3.581465]\n",
      "epoch:39 step:30977 [D loss: 0.143212, acc.: 92.97%] [G loss: 3.179239]\n",
      "epoch:39 step:30978 [D loss: 0.104123, acc.: 96.88%] [G loss: 0.157171]\n",
      "epoch:39 step:30979 [D loss: 0.115893, acc.: 95.31%] [G loss: 2.322642]\n",
      "epoch:39 step:30980 [D loss: 0.046566, acc.: 98.44%] [G loss: 0.005213]\n",
      "epoch:39 step:30981 [D loss: 0.211120, acc.: 90.62%] [G loss: 1.110480]\n",
      "epoch:39 step:30982 [D loss: 0.093314, acc.: 97.66%] [G loss: 0.277173]\n",
      "epoch:39 step:30983 [D loss: 0.304895, acc.: 89.84%] [G loss: 3.394602]\n",
      "epoch:39 step:30984 [D loss: 0.070115, acc.: 99.22%] [G loss: 0.149551]\n",
      "epoch:39 step:30985 [D loss: 1.518203, acc.: 31.25%] [G loss: 2.386965]\n",
      "epoch:39 step:30986 [D loss: 0.139550, acc.: 92.97%] [G loss: 1.178915]\n",
      "epoch:39 step:30987 [D loss: 0.274777, acc.: 87.50%] [G loss: 3.214495]\n",
      "epoch:39 step:30988 [D loss: 0.057857, acc.: 97.66%] [G loss: 0.005047]\n",
      "epoch:39 step:30989 [D loss: 0.032537, acc.: 99.22%] [G loss: 0.001714]\n",
      "epoch:39 step:30990 [D loss: 0.013291, acc.: 100.00%] [G loss: 0.002398]\n",
      "epoch:39 step:30991 [D loss: 0.042967, acc.: 99.22%] [G loss: 0.000962]\n",
      "epoch:39 step:30992 [D loss: 0.086279, acc.: 97.66%] [G loss: 0.002938]\n",
      "epoch:39 step:30993 [D loss: 0.003335, acc.: 100.00%] [G loss: 0.007777]\n",
      "epoch:39 step:30994 [D loss: 0.100632, acc.: 96.88%] [G loss: 0.017253]\n",
      "epoch:39 step:30995 [D loss: 0.025151, acc.: 99.22%] [G loss: 0.009777]\n",
      "epoch:39 step:30996 [D loss: 0.111938, acc.: 96.88%] [G loss: 0.915819]\n",
      "epoch:39 step:30997 [D loss: 0.016932, acc.: 100.00%] [G loss: 0.352448]\n",
      "epoch:39 step:30998 [D loss: 0.037914, acc.: 99.22%] [G loss: 0.018463]\n",
      "epoch:39 step:30999 [D loss: 0.034608, acc.: 99.22%] [G loss: 0.252031]\n",
      "epoch:39 step:31000 [D loss: 0.002170, acc.: 100.00%] [G loss: 0.821468]\n",
      "epoch:39 step:31001 [D loss: 0.032775, acc.: 99.22%] [G loss: 0.303500]\n",
      "epoch:39 step:31002 [D loss: 0.010453, acc.: 100.00%] [G loss: 0.000759]\n",
      "epoch:39 step:31003 [D loss: 0.042792, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:39 step:31004 [D loss: 0.177744, acc.: 92.97%] [G loss: 0.321769]\n",
      "epoch:39 step:31005 [D loss: 0.090373, acc.: 95.31%] [G loss: 0.122212]\n",
      "epoch:39 step:31006 [D loss: 0.142490, acc.: 94.53%] [G loss: 0.169820]\n",
      "epoch:39 step:31007 [D loss: 0.017920, acc.: 99.22%] [G loss: 0.000934]\n",
      "epoch:39 step:31008 [D loss: 0.092937, acc.: 96.09%] [G loss: 0.008887]\n",
      "epoch:39 step:31009 [D loss: 0.021139, acc.: 99.22%] [G loss: 0.016903]\n",
      "epoch:39 step:31010 [D loss: 0.019314, acc.: 99.22%] [G loss: 0.342577]\n",
      "epoch:39 step:31011 [D loss: 0.002103, acc.: 100.00%] [G loss: 0.012615]\n",
      "epoch:39 step:31012 [D loss: 0.006527, acc.: 100.00%] [G loss: 0.014992]\n",
      "epoch:39 step:31013 [D loss: 0.012668, acc.: 100.00%] [G loss: 0.013223]\n",
      "epoch:39 step:31014 [D loss: 0.001610, acc.: 100.00%] [G loss: 0.007372]\n",
      "epoch:39 step:31015 [D loss: 0.001024, acc.: 100.00%] [G loss: 0.002402]\n",
      "epoch:39 step:31016 [D loss: 0.013442, acc.: 100.00%] [G loss: 0.004315]\n",
      "epoch:39 step:31017 [D loss: 0.010212, acc.: 100.00%] [G loss: 0.002159]\n",
      "epoch:39 step:31018 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.002138]\n",
      "epoch:39 step:31019 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.085354]\n",
      "epoch:39 step:31020 [D loss: 0.002859, acc.: 100.00%] [G loss: 0.008047]\n",
      "epoch:39 step:31021 [D loss: 0.001814, acc.: 100.00%] [G loss: 0.002158]\n",
      "epoch:39 step:31022 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.007225]\n",
      "epoch:39 step:31023 [D loss: 0.004822, acc.: 100.00%] [G loss: 0.229860]\n",
      "epoch:39 step:31024 [D loss: 0.008567, acc.: 100.00%] [G loss: 0.001319]\n",
      "epoch:39 step:31025 [D loss: 0.014954, acc.: 99.22%] [G loss: 0.000734]\n",
      "epoch:39 step:31026 [D loss: 0.395033, acc.: 76.56%] [G loss: 3.172553]\n",
      "epoch:39 step:31027 [D loss: 0.237271, acc.: 88.28%] [G loss: 2.856084]\n",
      "epoch:39 step:31028 [D loss: 0.108322, acc.: 95.31%] [G loss: 1.173265]\n",
      "epoch:39 step:31029 [D loss: 0.052979, acc.: 98.44%] [G loss: 0.412508]\n",
      "epoch:39 step:31030 [D loss: 0.043847, acc.: 97.66%] [G loss: 0.236614]\n",
      "epoch:39 step:31031 [D loss: 0.303364, acc.: 86.72%] [G loss: 0.798201]\n",
      "epoch:39 step:31032 [D loss: 0.256821, acc.: 89.84%] [G loss: 0.592704]\n",
      "epoch:39 step:31033 [D loss: 0.059508, acc.: 99.22%] [G loss: 1.053412]\n",
      "epoch:39 step:31034 [D loss: 0.026809, acc.: 100.00%] [G loss: 0.283492]\n",
      "epoch:39 step:31035 [D loss: 0.022355, acc.: 99.22%] [G loss: 0.373220]\n",
      "epoch:39 step:31036 [D loss: 0.005285, acc.: 100.00%] [G loss: 0.045432]\n",
      "epoch:39 step:31037 [D loss: 0.036324, acc.: 99.22%] [G loss: 0.013640]\n",
      "epoch:39 step:31038 [D loss: 0.022940, acc.: 99.22%] [G loss: 0.014568]\n",
      "epoch:39 step:31039 [D loss: 0.002106, acc.: 100.00%] [G loss: 0.002078]\n",
      "epoch:39 step:31040 [D loss: 0.010000, acc.: 100.00%] [G loss: 0.005983]\n",
      "epoch:39 step:31041 [D loss: 0.030247, acc.: 99.22%] [G loss: 0.000580]\n",
      "epoch:39 step:31042 [D loss: 0.036198, acc.: 99.22%] [G loss: 0.000955]\n",
      "epoch:39 step:31043 [D loss: 0.002463, acc.: 100.00%] [G loss: 0.001232]\n",
      "epoch:39 step:31044 [D loss: 0.037631, acc.: 99.22%] [G loss: 0.006347]\n",
      "epoch:39 step:31045 [D loss: 0.003413, acc.: 100.00%] [G loss: 0.065862]\n",
      "epoch:39 step:31046 [D loss: 0.009239, acc.: 99.22%] [G loss: 0.019943]\n",
      "epoch:39 step:31047 [D loss: 0.235579, acc.: 88.28%] [G loss: 0.598088]\n",
      "epoch:39 step:31048 [D loss: 0.400418, acc.: 80.47%] [G loss: 3.111028]\n",
      "epoch:39 step:31049 [D loss: 0.006220, acc.: 100.00%] [G loss: 0.055791]\n",
      "epoch:39 step:31050 [D loss: 0.078382, acc.: 96.88%] [G loss: 0.000839]\n",
      "epoch:39 step:31051 [D loss: 0.253829, acc.: 87.50%] [G loss: 0.067207]\n",
      "epoch:39 step:31052 [D loss: 0.098895, acc.: 94.53%] [G loss: 0.357348]\n",
      "epoch:39 step:31053 [D loss: 0.038213, acc.: 98.44%] [G loss: 2.578532]\n",
      "epoch:39 step:31054 [D loss: 0.046516, acc.: 98.44%] [G loss: 0.029301]\n",
      "epoch:39 step:31055 [D loss: 0.009299, acc.: 100.00%] [G loss: 0.317425]\n",
      "epoch:39 step:31056 [D loss: 0.003725, acc.: 100.00%] [G loss: 0.012789]\n",
      "epoch:39 step:31057 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.024111]\n",
      "epoch:39 step:31058 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.009095]\n",
      "epoch:39 step:31059 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.000901]\n",
      "epoch:39 step:31060 [D loss: 0.011206, acc.: 100.00%] [G loss: 0.002955]\n",
      "epoch:39 step:31061 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.008343]\n",
      "epoch:39 step:31062 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.005448]\n",
      "epoch:39 step:31063 [D loss: 0.004263, acc.: 100.00%] [G loss: 0.155614]\n",
      "epoch:39 step:31064 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.001887]\n",
      "epoch:39 step:31065 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.001105]\n",
      "epoch:39 step:31066 [D loss: 0.002185, acc.: 100.00%] [G loss: 0.122755]\n",
      "epoch:39 step:31067 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000854]\n",
      "epoch:39 step:31068 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000423]\n",
      "epoch:39 step:31069 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.001320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31070 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:39 step:31071 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.015000]\n",
      "epoch:39 step:31072 [D loss: 0.004312, acc.: 100.00%] [G loss: 0.000775]\n",
      "epoch:39 step:31073 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.004098]\n",
      "epoch:39 step:31074 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:39 step:31075 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.009585]\n",
      "epoch:39 step:31076 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:39 step:31077 [D loss: 0.001291, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:39 step:31078 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:39 step:31079 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.012479]\n",
      "epoch:39 step:31080 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.009106]\n",
      "epoch:39 step:31081 [D loss: 0.007901, acc.: 100.00%] [G loss: 0.012320]\n",
      "epoch:39 step:31082 [D loss: 0.003745, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:39 step:31083 [D loss: 0.000993, acc.: 100.00%] [G loss: 0.265723]\n",
      "epoch:39 step:31084 [D loss: 0.011906, acc.: 100.00%] [G loss: 0.001105]\n",
      "epoch:39 step:31085 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.013762]\n",
      "epoch:39 step:31086 [D loss: 0.132318, acc.: 96.09%] [G loss: 0.018649]\n",
      "epoch:39 step:31087 [D loss: 0.173479, acc.: 93.75%] [G loss: 0.007773]\n",
      "epoch:39 step:31088 [D loss: 0.021072, acc.: 99.22%] [G loss: 0.005310]\n",
      "epoch:39 step:31089 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.945820]\n",
      "epoch:39 step:31090 [D loss: 0.003058, acc.: 100.00%] [G loss: 0.991200]\n",
      "epoch:39 step:31091 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.002968]\n",
      "epoch:39 step:31092 [D loss: 0.001011, acc.: 100.00%] [G loss: 0.001724]\n",
      "epoch:39 step:31093 [D loss: 0.027635, acc.: 98.44%] [G loss: 0.458983]\n",
      "epoch:39 step:31094 [D loss: 0.272375, acc.: 85.16%] [G loss: 0.041421]\n",
      "epoch:39 step:31095 [D loss: 0.011406, acc.: 100.00%] [G loss: 5.215029]\n",
      "epoch:39 step:31096 [D loss: 0.717771, acc.: 67.97%] [G loss: 0.264100]\n",
      "epoch:39 step:31097 [D loss: 0.283502, acc.: 86.72%] [G loss: 2.380831]\n",
      "epoch:39 step:31098 [D loss: 0.003738, acc.: 100.00%] [G loss: 0.420854]\n",
      "epoch:39 step:31099 [D loss: 0.003905, acc.: 100.00%] [G loss: 0.311083]\n",
      "epoch:39 step:31100 [D loss: 0.014236, acc.: 100.00%] [G loss: 5.065855]\n",
      "epoch:39 step:31101 [D loss: 0.042540, acc.: 99.22%] [G loss: 2.997867]\n",
      "epoch:39 step:31102 [D loss: 0.001715, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:39 step:31103 [D loss: 0.003631, acc.: 100.00%] [G loss: 0.001356]\n",
      "epoch:39 step:31104 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:39 step:31105 [D loss: 0.030040, acc.: 98.44%] [G loss: 0.001044]\n",
      "epoch:39 step:31106 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:39 step:31107 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:39 step:31108 [D loss: 0.002337, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:39 step:31109 [D loss: 0.002372, acc.: 100.00%] [G loss: 0.010241]\n",
      "epoch:39 step:31110 [D loss: 0.013743, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:39 step:31111 [D loss: 0.005805, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:39 step:31112 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.544837]\n",
      "epoch:39 step:31113 [D loss: 0.001918, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:39 step:31114 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.115008]\n",
      "epoch:39 step:31115 [D loss: 0.015467, acc.: 100.00%] [G loss: 0.304494]\n",
      "epoch:39 step:31116 [D loss: 0.005797, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:39 step:31117 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:39 step:31118 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.060043]\n",
      "epoch:39 step:31119 [D loss: 0.002122, acc.: 100.00%] [G loss: 0.053988]\n",
      "epoch:39 step:31120 [D loss: 0.188051, acc.: 92.97%] [G loss: 0.007557]\n",
      "epoch:39 step:31121 [D loss: 0.038298, acc.: 99.22%] [G loss: 0.025466]\n",
      "epoch:39 step:31122 [D loss: 0.123920, acc.: 94.53%] [G loss: 0.003690]\n",
      "epoch:39 step:31123 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.001373]\n",
      "epoch:39 step:31124 [D loss: 0.020126, acc.: 99.22%] [G loss: 0.000059]\n",
      "epoch:39 step:31125 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:39 step:31126 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.001129]\n",
      "epoch:39 step:31127 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:39 step:31128 [D loss: 0.009557, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:39 step:31129 [D loss: 0.009699, acc.: 99.22%] [G loss: 0.000221]\n",
      "epoch:39 step:31130 [D loss: 0.014364, acc.: 100.00%] [G loss: 0.280473]\n",
      "epoch:39 step:31131 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:39 step:31132 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.001255]\n",
      "epoch:39 step:31133 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:39 step:31134 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.001178]\n",
      "epoch:39 step:31135 [D loss: 0.004826, acc.: 100.00%] [G loss: 0.339303]\n",
      "epoch:39 step:31136 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000514]\n",
      "epoch:39 step:31137 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.001264]\n",
      "epoch:39 step:31138 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:39 step:31139 [D loss: 0.000819, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:39 step:31140 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:39 step:31141 [D loss: 0.005816, acc.: 100.00%] [G loss: 0.213787]\n",
      "epoch:39 step:31142 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.390009]\n",
      "epoch:39 step:31143 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.118894]\n",
      "epoch:39 step:31144 [D loss: 0.004637, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:39 step:31145 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.057947]\n",
      "epoch:39 step:31146 [D loss: 0.001551, acc.: 100.00%] [G loss: 0.089676]\n",
      "epoch:39 step:31147 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:39 step:31148 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:39 step:31149 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:39 step:31150 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:39 step:31151 [D loss: 0.008570, acc.: 99.22%] [G loss: 0.000028]\n",
      "epoch:39 step:31152 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:39 step:31153 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:39 step:31154 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:39 step:31155 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.006501]\n",
      "epoch:39 step:31156 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.000828]\n",
      "epoch:39 step:31157 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:39 step:31158 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:39 step:31159 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.003054]\n",
      "epoch:39 step:31160 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.001933]\n",
      "epoch:39 step:31161 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:39 step:31162 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:39 step:31163 [D loss: 0.012135, acc.: 99.22%] [G loss: 0.026658]\n",
      "epoch:39 step:31164 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:39 step:31165 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:39 step:31166 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:39 step:31167 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:39 step:31168 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:39 step:31169 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:39 step:31170 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:39 step:31171 [D loss: 0.009623, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:39 step:31172 [D loss: 0.105057, acc.: 96.88%] [G loss: 0.029801]\n",
      "epoch:39 step:31173 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.162690]\n",
      "epoch:39 step:31174 [D loss: 0.120749, acc.: 93.75%] [G loss: 0.003049]\n",
      "epoch:39 step:31175 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001517]\n",
      "epoch:39 step:31176 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.003285]\n",
      "epoch:39 step:31177 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:39 step:31178 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.001540]\n",
      "epoch:39 step:31179 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:39 step:31180 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:39 step:31181 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:39 step:31182 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:39 step:31183 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002473]\n",
      "epoch:39 step:31184 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:39 step:31185 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31186 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.002013]\n",
      "epoch:39 step:31187 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:39 step:31188 [D loss: 0.001247, acc.: 100.00%] [G loss: 0.000770]\n",
      "epoch:39 step:31189 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:39 step:31190 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:39 step:31191 [D loss: 0.001300, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:39 step:31192 [D loss: 0.008178, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:39 step:31193 [D loss: 0.002295, acc.: 100.00%] [G loss: 0.001295]\n",
      "epoch:39 step:31194 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.002133]\n",
      "epoch:39 step:31195 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:39 step:31196 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:39 step:31197 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001353]\n",
      "epoch:39 step:31198 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:39 step:31199 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000421]\n",
      "epoch:39 step:31200 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.037628]\n",
      "epoch:39 step:31201 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:39 step:31202 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000976]\n",
      "epoch:39 step:31203 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:39 step:31204 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000468]\n",
      "epoch:39 step:31205 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000946]\n",
      "epoch:39 step:31206 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:39 step:31207 [D loss: 0.000772, acc.: 100.00%] [G loss: 0.089994]\n",
      "epoch:39 step:31208 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:39 step:31209 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000956]\n",
      "epoch:39 step:31210 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001153]\n",
      "epoch:39 step:31211 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001136]\n",
      "epoch:39 step:31212 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:39 step:31213 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.001081]\n",
      "epoch:39 step:31214 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:39 step:31215 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:39 step:31216 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:39 step:31217 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.017603]\n",
      "epoch:39 step:31218 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.013613]\n",
      "epoch:39 step:31219 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:39 step:31220 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:39 step:31221 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:39 step:31222 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:39 step:31223 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:39 step:31224 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:39 step:31225 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:39 step:31226 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:39 step:31227 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:39 step:31228 [D loss: 0.001173, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:39 step:31229 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:39 step:31230 [D loss: 0.007083, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:39 step:31231 [D loss: 0.019037, acc.: 99.22%] [G loss: 0.003461]\n",
      "epoch:39 step:31232 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.000880]\n",
      "epoch:39 step:31233 [D loss: 0.001006, acc.: 100.00%] [G loss: 0.001460]\n",
      "epoch:39 step:31234 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.001052]\n",
      "epoch:39 step:31235 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.000452]\n",
      "epoch:39 step:31236 [D loss: 0.000830, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:39 step:31237 [D loss: 0.056169, acc.: 100.00%] [G loss: 0.027440]\n",
      "epoch:39 step:31238 [D loss: 0.001935, acc.: 100.00%] [G loss: 0.087143]\n",
      "epoch:39 step:31239 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.058698]\n",
      "epoch:39 step:31240 [D loss: 0.001411, acc.: 100.00%] [G loss: 0.283586]\n",
      "epoch:40 step:31241 [D loss: 0.002925, acc.: 100.00%] [G loss: 0.047332]\n",
      "epoch:40 step:31242 [D loss: 0.013119, acc.: 100.00%] [G loss: 0.026571]\n",
      "epoch:40 step:31243 [D loss: 0.003585, acc.: 100.00%] [G loss: 0.058302]\n",
      "epoch:40 step:31244 [D loss: 0.002238, acc.: 100.00%] [G loss: 0.011979]\n",
      "epoch:40 step:31245 [D loss: 0.006163, acc.: 99.22%] [G loss: 0.014382]\n",
      "epoch:40 step:31246 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.042036]\n",
      "epoch:40 step:31247 [D loss: 0.001818, acc.: 100.00%] [G loss: 0.001339]\n",
      "epoch:40 step:31248 [D loss: 0.003090, acc.: 100.00%] [G loss: 0.003920]\n",
      "epoch:40 step:31249 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.001906]\n",
      "epoch:40 step:31250 [D loss: 0.004837, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:40 step:31251 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:40 step:31252 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000978]\n",
      "epoch:40 step:31253 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.001116]\n",
      "epoch:40 step:31254 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:40 step:31255 [D loss: 0.001420, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:40 step:31256 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.001151]\n",
      "epoch:40 step:31257 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:40 step:31258 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001903]\n",
      "epoch:40 step:31259 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:40 step:31260 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:40 step:31261 [D loss: 0.002386, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:40 step:31262 [D loss: 0.752461, acc.: 60.16%] [G loss: 8.208138]\n",
      "epoch:40 step:31263 [D loss: 2.059554, acc.: 53.12%] [G loss: 3.716651]\n",
      "epoch:40 step:31264 [D loss: 0.054116, acc.: 100.00%] [G loss: 3.290866]\n",
      "epoch:40 step:31265 [D loss: 0.114779, acc.: 97.66%] [G loss: 3.757462]\n",
      "epoch:40 step:31266 [D loss: 0.049470, acc.: 99.22%] [G loss: 3.641328]\n",
      "epoch:40 step:31267 [D loss: 0.025874, acc.: 100.00%] [G loss: 2.837372]\n",
      "epoch:40 step:31268 [D loss: 0.157584, acc.: 92.97%] [G loss: 0.569041]\n",
      "epoch:40 step:31269 [D loss: 0.011910, acc.: 100.00%] [G loss: 0.476387]\n",
      "epoch:40 step:31270 [D loss: 0.038241, acc.: 99.22%] [G loss: 4.768254]\n",
      "epoch:40 step:31271 [D loss: 0.037148, acc.: 98.44%] [G loss: 0.025037]\n",
      "epoch:40 step:31272 [D loss: 0.011540, acc.: 100.00%] [G loss: 3.225160]\n",
      "epoch:40 step:31273 [D loss: 0.011560, acc.: 100.00%] [G loss: 2.566633]\n",
      "epoch:40 step:31274 [D loss: 0.000625, acc.: 100.00%] [G loss: 2.586884]\n",
      "epoch:40 step:31275 [D loss: 0.010352, acc.: 100.00%] [G loss: 1.224635]\n",
      "epoch:40 step:31276 [D loss: 0.018246, acc.: 100.00%] [G loss: 0.380741]\n",
      "epoch:40 step:31277 [D loss: 0.001151, acc.: 100.00%] [G loss: 0.124314]\n",
      "epoch:40 step:31278 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.248107]\n",
      "epoch:40 step:31279 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.375473]\n",
      "epoch:40 step:31280 [D loss: 0.005868, acc.: 100.00%] [G loss: 0.028094]\n",
      "epoch:40 step:31281 [D loss: 0.005461, acc.: 100.00%] [G loss: 0.055279]\n",
      "epoch:40 step:31282 [D loss: 0.068302, acc.: 97.66%] [G loss: 0.286683]\n",
      "epoch:40 step:31283 [D loss: 0.001079, acc.: 100.00%] [G loss: 0.044260]\n",
      "epoch:40 step:31284 [D loss: 0.037259, acc.: 99.22%] [G loss: 0.057018]\n",
      "epoch:40 step:31285 [D loss: 0.009721, acc.: 100.00%] [G loss: 0.021162]\n",
      "epoch:40 step:31286 [D loss: 0.001781, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:40 step:31287 [D loss: 0.009705, acc.: 100.00%] [G loss: 0.014109]\n",
      "epoch:40 step:31288 [D loss: 0.008354, acc.: 100.00%] [G loss: 0.007023]\n",
      "epoch:40 step:31289 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.007118]\n",
      "epoch:40 step:31290 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.026853]\n",
      "epoch:40 step:31291 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.004732]\n",
      "epoch:40 step:31292 [D loss: 0.004662, acc.: 100.00%] [G loss: 0.002103]\n",
      "epoch:40 step:31293 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.002693]\n",
      "epoch:40 step:31294 [D loss: 0.001164, acc.: 100.00%] [G loss: 0.007986]\n",
      "epoch:40 step:31295 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:40 step:31296 [D loss: 0.002825, acc.: 100.00%] [G loss: 0.001244]\n",
      "epoch:40 step:31297 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.003656]\n",
      "epoch:40 step:31298 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:40 step:31299 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.009925]\n",
      "epoch:40 step:31300 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.002320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31301 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.001734]\n",
      "epoch:40 step:31302 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.003852]\n",
      "epoch:40 step:31303 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.001126]\n",
      "epoch:40 step:31304 [D loss: 0.002504, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:40 step:31305 [D loss: 0.032825, acc.: 99.22%] [G loss: 0.000835]\n",
      "epoch:40 step:31306 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:40 step:31307 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.001932]\n",
      "epoch:40 step:31308 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:40 step:31309 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001516]\n",
      "epoch:40 step:31310 [D loss: 0.000946, acc.: 100.00%] [G loss: 0.002386]\n",
      "epoch:40 step:31311 [D loss: 0.002127, acc.: 100.00%] [G loss: 0.001330]\n",
      "epoch:40 step:31312 [D loss: 0.001779, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:40 step:31313 [D loss: 0.000770, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:40 step:31314 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.004208]\n",
      "epoch:40 step:31315 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:40 step:31316 [D loss: 0.006188, acc.: 100.00%] [G loss: 0.000990]\n",
      "epoch:40 step:31317 [D loss: 0.005164, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:40 step:31318 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.001319]\n",
      "epoch:40 step:31319 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.001643]\n",
      "epoch:40 step:31320 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.000538]\n",
      "epoch:40 step:31321 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.004425]\n",
      "epoch:40 step:31322 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:40 step:31323 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:40 step:31324 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001071]\n",
      "epoch:40 step:31325 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:40 step:31326 [D loss: 0.001108, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:40 step:31327 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.000696]\n",
      "epoch:40 step:31328 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.002278]\n",
      "epoch:40 step:31329 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:40 step:31330 [D loss: 0.006885, acc.: 100.00%] [G loss: 0.001910]\n",
      "epoch:40 step:31331 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.001087]\n",
      "epoch:40 step:31332 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:40 step:31333 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:40 step:31334 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:40 step:31335 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:40 step:31336 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:40 step:31337 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:40 step:31338 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:40 step:31339 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:40 step:31340 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:40 step:31341 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:40 step:31342 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:40 step:31343 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:40 step:31344 [D loss: 0.001529, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:40 step:31345 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:40 step:31346 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:40 step:31347 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:40 step:31348 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:40 step:31349 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:40 step:31350 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:40 step:31351 [D loss: 0.000741, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:40 step:31352 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:40 step:31353 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:40 step:31354 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:40 step:31355 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:40 step:31356 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:40 step:31357 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.001154]\n",
      "epoch:40 step:31358 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:40 step:31359 [D loss: 0.026689, acc.: 100.00%] [G loss: 0.001083]\n",
      "epoch:40 step:31360 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:40 step:31361 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002539]\n",
      "epoch:40 step:31362 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:40 step:31363 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:40 step:31364 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.000960]\n",
      "epoch:40 step:31365 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:40 step:31366 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.001137]\n",
      "epoch:40 step:31367 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:40 step:31368 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:40 step:31369 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001147]\n",
      "epoch:40 step:31370 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:40 step:31371 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.000971]\n",
      "epoch:40 step:31372 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.003044]\n",
      "epoch:40 step:31373 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001180]\n",
      "epoch:40 step:31374 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:40 step:31375 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001079]\n",
      "epoch:40 step:31376 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.007211]\n",
      "epoch:40 step:31377 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000981]\n",
      "epoch:40 step:31378 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.002890]\n",
      "epoch:40 step:31379 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001845]\n",
      "epoch:40 step:31380 [D loss: 0.001633, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:40 step:31381 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:40 step:31382 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000565]\n",
      "epoch:40 step:31383 [D loss: 0.002980, acc.: 100.00%] [G loss: 0.006739]\n",
      "epoch:40 step:31384 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:40 step:31385 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:40 step:31386 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000990]\n",
      "epoch:40 step:31387 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001447]\n",
      "epoch:40 step:31388 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:40 step:31389 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:40 step:31390 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000699]\n",
      "epoch:40 step:31391 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001828]\n",
      "epoch:40 step:31392 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:40 step:31393 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:40 step:31394 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:40 step:31395 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:40 step:31396 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:40 step:31397 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:40 step:31398 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:40 step:31399 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.002530]\n",
      "epoch:40 step:31400 [D loss: 0.001270, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:40 step:31401 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:40 step:31402 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:40 step:31403 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:40 step:31404 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001047]\n",
      "epoch:40 step:31405 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001079]\n",
      "epoch:40 step:31406 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:40 step:31407 [D loss: 0.001385, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:40 step:31408 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:40 step:31409 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003130]\n",
      "epoch:40 step:31410 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000532]\n",
      "epoch:40 step:31411 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:40 step:31412 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:40 step:31413 [D loss: 0.002069, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:40 step:31414 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:40 step:31415 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31416 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001887]\n",
      "epoch:40 step:31417 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.007166]\n",
      "epoch:40 step:31418 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:40 step:31419 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:40 step:31420 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:40 step:31421 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:40 step:31422 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:40 step:31423 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.002437]\n",
      "epoch:40 step:31424 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:40 step:31425 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:40 step:31426 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:40 step:31427 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:40 step:31428 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000520]\n",
      "epoch:40 step:31429 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000624]\n",
      "epoch:40 step:31430 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:40 step:31431 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:40 step:31432 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000513]\n",
      "epoch:40 step:31433 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:40 step:31434 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000502]\n",
      "epoch:40 step:31435 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:40 step:31436 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:40 step:31437 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:40 step:31438 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:40 step:31439 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:40 step:31440 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:40 step:31441 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001061]\n",
      "epoch:40 step:31442 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001518]\n",
      "epoch:40 step:31443 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:40 step:31444 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:40 step:31445 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:40 step:31446 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:40 step:31447 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:40 step:31448 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:40 step:31449 [D loss: 0.000617, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:40 step:31450 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:40 step:31451 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:40 step:31452 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:40 step:31453 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:40 step:31454 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:40 step:31455 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:40 step:31456 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:40 step:31457 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000936]\n",
      "epoch:40 step:31458 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:40 step:31459 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:40 step:31460 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:40 step:31461 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:40 step:31462 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:40 step:31463 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:40 step:31464 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:40 step:31465 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:40 step:31466 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:40 step:31467 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:40 step:31468 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:40 step:31469 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000631]\n",
      "epoch:40 step:31470 [D loss: 0.002587, acc.: 100.00%] [G loss: 0.001092]\n",
      "epoch:40 step:31471 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:40 step:31472 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:40 step:31473 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:40 step:31474 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:40 step:31475 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:40 step:31476 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:40 step:31477 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:40 step:31478 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:40 step:31479 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:40 step:31480 [D loss: 0.002824, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:40 step:31481 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:40 step:31482 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000680]\n",
      "epoch:40 step:31483 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:40 step:31484 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:40 step:31485 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:40 step:31486 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:40 step:31487 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:40 step:31488 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:40 step:31489 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:40 step:31490 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:40 step:31491 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.003473]\n",
      "epoch:40 step:31492 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.003310]\n",
      "epoch:40 step:31493 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:40 step:31494 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:40 step:31495 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:40 step:31496 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001856]\n",
      "epoch:40 step:31497 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:40 step:31498 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:40 step:31499 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003909]\n",
      "epoch:40 step:31500 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:40 step:31501 [D loss: 0.032835, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:40 step:31502 [D loss: 0.081822, acc.: 98.44%] [G loss: 0.021547]\n",
      "epoch:40 step:31503 [D loss: 0.003331, acc.: 100.00%] [G loss: 0.242515]\n",
      "epoch:40 step:31504 [D loss: 0.014251, acc.: 100.00%] [G loss: 0.892262]\n",
      "epoch:40 step:31505 [D loss: 0.169998, acc.: 93.75%] [G loss: 0.001830]\n",
      "epoch:40 step:31506 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000640]\n",
      "epoch:40 step:31507 [D loss: 0.285872, acc.: 85.94%] [G loss: 0.381027]\n",
      "epoch:40 step:31508 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.961120]\n",
      "epoch:40 step:31509 [D loss: 0.262795, acc.: 88.28%] [G loss: 0.001713]\n",
      "epoch:40 step:31510 [D loss: 0.028592, acc.: 100.00%] [G loss: 0.001685]\n",
      "epoch:40 step:31511 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.869176]\n",
      "epoch:40 step:31512 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:40 step:31513 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.306773]\n",
      "epoch:40 step:31514 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:40 step:31515 [D loss: 0.002757, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:40 step:31516 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.105339]\n",
      "epoch:40 step:31517 [D loss: 0.029829, acc.: 99.22%] [G loss: 0.000024]\n",
      "epoch:40 step:31518 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:40 step:31519 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.106871]\n",
      "epoch:40 step:31520 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.012977]\n",
      "epoch:40 step:31521 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003913]\n",
      "epoch:40 step:31522 [D loss: 0.540483, acc.: 78.12%] [G loss: 0.042531]\n",
      "epoch:40 step:31523 [D loss: 0.014917, acc.: 100.00%] [G loss: 9.567189]\n",
      "epoch:40 step:31524 [D loss: 1.383615, acc.: 58.59%] [G loss: 1.385240]\n",
      "epoch:40 step:31525 [D loss: 1.633066, acc.: 54.69%] [G loss: 1.513589]\n",
      "epoch:40 step:31526 [D loss: 0.213049, acc.: 89.06%] [G loss: 6.470748]\n",
      "epoch:40 step:31527 [D loss: 0.332264, acc.: 84.38%] [G loss: 0.445094]\n",
      "epoch:40 step:31528 [D loss: 0.087949, acc.: 96.09%] [G loss: 0.034041]\n",
      "epoch:40 step:31529 [D loss: 0.013011, acc.: 100.00%] [G loss: 0.007622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31530 [D loss: 0.001435, acc.: 100.00%] [G loss: 0.006749]\n",
      "epoch:40 step:31531 [D loss: 0.004827, acc.: 100.00%] [G loss: 0.001152]\n",
      "epoch:40 step:31532 [D loss: 0.007505, acc.: 100.00%] [G loss: 1.910538]\n",
      "epoch:40 step:31533 [D loss: 0.007588, acc.: 100.00%] [G loss: 0.284810]\n",
      "epoch:40 step:31534 [D loss: 0.017509, acc.: 99.22%] [G loss: 0.001408]\n",
      "epoch:40 step:31535 [D loss: 0.009618, acc.: 100.00%] [G loss: 0.001836]\n",
      "epoch:40 step:31536 [D loss: 0.011139, acc.: 100.00%] [G loss: 1.471255]\n",
      "epoch:40 step:31537 [D loss: 0.154522, acc.: 92.19%] [G loss: 0.010464]\n",
      "epoch:40 step:31538 [D loss: 0.009007, acc.: 100.00%] [G loss: 0.039204]\n",
      "epoch:40 step:31539 [D loss: 0.020217, acc.: 100.00%] [G loss: 0.011697]\n",
      "epoch:40 step:31540 [D loss: 0.155010, acc.: 92.19%] [G loss: 0.002999]\n",
      "epoch:40 step:31541 [D loss: 0.006123, acc.: 100.00%] [G loss: 0.001626]\n",
      "epoch:40 step:31542 [D loss: 0.007498, acc.: 100.00%] [G loss: 0.001052]\n",
      "epoch:40 step:31543 [D loss: 0.015746, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:40 step:31544 [D loss: 0.007152, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:40 step:31545 [D loss: 0.008700, acc.: 100.00%] [G loss: 0.001625]\n",
      "epoch:40 step:31546 [D loss: 0.017493, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:40 step:31547 [D loss: 0.013078, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:40 step:31548 [D loss: 0.043977, acc.: 100.00%] [G loss: 1.210018]\n",
      "epoch:40 step:31549 [D loss: 0.002239, acc.: 100.00%] [G loss: 0.003800]\n",
      "epoch:40 step:31550 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.263468]\n",
      "epoch:40 step:31551 [D loss: 0.019830, acc.: 100.00%] [G loss: 0.002483]\n",
      "epoch:40 step:31552 [D loss: 0.114385, acc.: 96.88%] [G loss: 1.520463]\n",
      "epoch:40 step:31553 [D loss: 0.043098, acc.: 99.22%] [G loss: 0.046992]\n",
      "epoch:40 step:31554 [D loss: 0.002864, acc.: 100.00%] [G loss: 0.025401]\n",
      "epoch:40 step:31555 [D loss: 0.833204, acc.: 67.19%] [G loss: 0.001155]\n",
      "epoch:40 step:31556 [D loss: 0.966222, acc.: 64.06%] [G loss: 0.457670]\n",
      "epoch:40 step:31557 [D loss: 0.004446, acc.: 100.00%] [G loss: 1.863662]\n",
      "epoch:40 step:31558 [D loss: 0.049892, acc.: 98.44%] [G loss: 1.139777]\n",
      "epoch:40 step:31559 [D loss: 0.035757, acc.: 98.44%] [G loss: 2.422628]\n",
      "epoch:40 step:31560 [D loss: 0.241951, acc.: 85.94%] [G loss: 0.094845]\n",
      "epoch:40 step:31561 [D loss: 0.035559, acc.: 99.22%] [G loss: 0.306116]\n",
      "epoch:40 step:31562 [D loss: 0.051330, acc.: 100.00%] [G loss: 0.040472]\n",
      "epoch:40 step:31563 [D loss: 0.109444, acc.: 94.53%] [G loss: 0.035130]\n",
      "epoch:40 step:31564 [D loss: 0.014165, acc.: 100.00%] [G loss: 0.454005]\n",
      "epoch:40 step:31565 [D loss: 0.019604, acc.: 100.00%] [G loss: 0.182320]\n",
      "epoch:40 step:31566 [D loss: 0.010173, acc.: 100.00%] [G loss: 0.285910]\n",
      "epoch:40 step:31567 [D loss: 0.024981, acc.: 100.00%] [G loss: 0.035501]\n",
      "epoch:40 step:31568 [D loss: 0.036780, acc.: 100.00%] [G loss: 0.095418]\n",
      "epoch:40 step:31569 [D loss: 0.022831, acc.: 100.00%] [G loss: 0.214112]\n",
      "epoch:40 step:31570 [D loss: 0.005861, acc.: 100.00%] [G loss: 0.048350]\n",
      "epoch:40 step:31571 [D loss: 0.454053, acc.: 75.78%] [G loss: 2.212821]\n",
      "epoch:40 step:31572 [D loss: 0.043887, acc.: 98.44%] [G loss: 1.873531]\n",
      "epoch:40 step:31573 [D loss: 0.342396, acc.: 82.03%] [G loss: 0.345940]\n",
      "epoch:40 step:31574 [D loss: 0.040682, acc.: 99.22%] [G loss: 0.087206]\n",
      "epoch:40 step:31575 [D loss: 0.012298, acc.: 100.00%] [G loss: 0.012669]\n",
      "epoch:40 step:31576 [D loss: 0.001539, acc.: 100.00%] [G loss: 0.002598]\n",
      "epoch:40 step:31577 [D loss: 0.029975, acc.: 98.44%] [G loss: 0.020814]\n",
      "epoch:40 step:31578 [D loss: 0.004186, acc.: 100.00%] [G loss: 0.009938]\n",
      "epoch:40 step:31579 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.001153]\n",
      "epoch:40 step:31580 [D loss: 0.001980, acc.: 100.00%] [G loss: 0.415350]\n",
      "epoch:40 step:31581 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.080423]\n",
      "epoch:40 step:31582 [D loss: 0.002281, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:40 step:31583 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.025980]\n",
      "epoch:40 step:31584 [D loss: 0.001468, acc.: 100.00%] [G loss: 0.001319]\n",
      "epoch:40 step:31585 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:40 step:31586 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.002726]\n",
      "epoch:40 step:31587 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.035960]\n",
      "epoch:40 step:31588 [D loss: 0.002312, acc.: 100.00%] [G loss: 0.008988]\n",
      "epoch:40 step:31589 [D loss: 0.000771, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:40 step:31590 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.000725]\n",
      "epoch:40 step:31591 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.119475]\n",
      "epoch:40 step:31592 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:40 step:31593 [D loss: 0.021150, acc.: 99.22%] [G loss: 0.001874]\n",
      "epoch:40 step:31594 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.025996]\n",
      "epoch:40 step:31595 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:40 step:31596 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.027172]\n",
      "epoch:40 step:31597 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:40 step:31598 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000883]\n",
      "epoch:40 step:31599 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.002794]\n",
      "epoch:40 step:31600 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:40 step:31601 [D loss: 0.001318, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:40 step:31602 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:40 step:31603 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.012137]\n",
      "epoch:40 step:31604 [D loss: 0.001707, acc.: 100.00%] [G loss: 0.019499]\n",
      "epoch:40 step:31605 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000752]\n",
      "epoch:40 step:31606 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:40 step:31607 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:40 step:31608 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.005708]\n",
      "epoch:40 step:31609 [D loss: 0.000648, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:40 step:31610 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.009611]\n",
      "epoch:40 step:31611 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:40 step:31612 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.005670]\n",
      "epoch:40 step:31613 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.007479]\n",
      "epoch:40 step:31614 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.001186]\n",
      "epoch:40 step:31615 [D loss: 0.004195, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:40 step:31616 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.001555]\n",
      "epoch:40 step:31617 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:40 step:31618 [D loss: 0.001774, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:40 step:31619 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.008995]\n",
      "epoch:40 step:31620 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:40 step:31621 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:40 step:31622 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.019913]\n",
      "epoch:40 step:31623 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.004139]\n",
      "epoch:40 step:31624 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.003715]\n",
      "epoch:40 step:31625 [D loss: 0.002993, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:40 step:31626 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:40 step:31627 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.004901]\n",
      "epoch:40 step:31628 [D loss: 0.014378, acc.: 99.22%] [G loss: 0.003048]\n",
      "epoch:40 step:31629 [D loss: 0.001747, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:40 step:31630 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.001323]\n",
      "epoch:40 step:31631 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.004203]\n",
      "epoch:40 step:31632 [D loss: 0.001459, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:40 step:31633 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:40 step:31634 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:40 step:31635 [D loss: 0.007072, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:40 step:31636 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.001644]\n",
      "epoch:40 step:31637 [D loss: 0.001457, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:40 step:31638 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.009266]\n",
      "epoch:40 step:31639 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:40 step:31640 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:40 step:31641 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:40 step:31642 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:40 step:31643 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.006616]\n",
      "epoch:40 step:31644 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31645 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:40 step:31646 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:40 step:31647 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:40 step:31648 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.003768]\n",
      "epoch:40 step:31649 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:40 step:31650 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.003015]\n",
      "epoch:40 step:31651 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.001746]\n",
      "epoch:40 step:31652 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:40 step:31653 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.001312]\n",
      "epoch:40 step:31654 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.001691]\n",
      "epoch:40 step:31655 [D loss: 0.001093, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:40 step:31656 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:40 step:31657 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:40 step:31658 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001464]\n",
      "epoch:40 step:31659 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.002577]\n",
      "epoch:40 step:31660 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:40 step:31661 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:40 step:31662 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.004641]\n",
      "epoch:40 step:31663 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:40 step:31664 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.001085]\n",
      "epoch:40 step:31665 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:40 step:31666 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:40 step:31667 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:40 step:31668 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:40 step:31669 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:40 step:31670 [D loss: 0.010846, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:40 step:31671 [D loss: 0.005487, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:40 step:31672 [D loss: 0.012499, acc.: 99.22%] [G loss: 0.000167]\n",
      "epoch:40 step:31673 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31674 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:40 step:31675 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:40 step:31676 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:40 step:31677 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:40 step:31678 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:40 step:31679 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:40 step:31680 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:40 step:31681 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:40 step:31682 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:40 step:31683 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:40 step:31684 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:40 step:31685 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:40 step:31686 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:40 step:31687 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:40 step:31688 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:40 step:31689 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:40 step:31690 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:40 step:31691 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:40 step:31692 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:40 step:31693 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:40 step:31694 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:40 step:31695 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:40 step:31696 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:40 step:31697 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:40 step:31698 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:40 step:31699 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31700 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:40 step:31701 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:40 step:31702 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:40 step:31703 [D loss: 0.004860, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:40 step:31704 [D loss: 0.714108, acc.: 68.75%] [G loss: 0.787816]\n",
      "epoch:40 step:31705 [D loss: 0.021283, acc.: 100.00%] [G loss: 2.216874]\n",
      "epoch:40 step:31706 [D loss: 0.437780, acc.: 82.03%] [G loss: 1.169282]\n",
      "epoch:40 step:31707 [D loss: 0.095774, acc.: 94.53%] [G loss: 0.243905]\n",
      "epoch:40 step:31708 [D loss: 0.004004, acc.: 100.00%] [G loss: 0.069510]\n",
      "epoch:40 step:31709 [D loss: 0.001106, acc.: 100.00%] [G loss: 0.130873]\n",
      "epoch:40 step:31710 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.062783]\n",
      "epoch:40 step:31711 [D loss: 0.007861, acc.: 99.22%] [G loss: 0.001559]\n",
      "epoch:40 step:31712 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.057606]\n",
      "epoch:40 step:31713 [D loss: 0.003223, acc.: 100.00%] [G loss: 0.005603]\n",
      "epoch:40 step:31714 [D loss: 0.002593, acc.: 100.00%] [G loss: 0.022209]\n",
      "epoch:40 step:31715 [D loss: 0.001301, acc.: 100.00%] [G loss: 0.004398]\n",
      "epoch:40 step:31716 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.020218]\n",
      "epoch:40 step:31717 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.003850]\n",
      "epoch:40 step:31718 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:40 step:31719 [D loss: 0.000944, acc.: 100.00%] [G loss: 0.001333]\n",
      "epoch:40 step:31720 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.004139]\n",
      "epoch:40 step:31721 [D loss: 0.002981, acc.: 100.00%] [G loss: 0.004984]\n",
      "epoch:40 step:31722 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.003580]\n",
      "epoch:40 step:31723 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.003890]\n",
      "epoch:40 step:31724 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.001611]\n",
      "epoch:40 step:31725 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.002935]\n",
      "epoch:40 step:31726 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.004467]\n",
      "epoch:40 step:31727 [D loss: 0.000810, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:40 step:31728 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.002351]\n",
      "epoch:40 step:31729 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:40 step:31730 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.003373]\n",
      "epoch:40 step:31731 [D loss: 0.006632, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:40 step:31732 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:40 step:31733 [D loss: 0.001933, acc.: 100.00%] [G loss: 0.000895]\n",
      "epoch:40 step:31734 [D loss: 0.025696, acc.: 100.00%] [G loss: 0.003391]\n",
      "epoch:40 step:31735 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.066504]\n",
      "epoch:40 step:31736 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.004855]\n",
      "epoch:40 step:31737 [D loss: 0.001279, acc.: 100.00%] [G loss: 0.005784]\n",
      "epoch:40 step:31738 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:40 step:31739 [D loss: 0.002361, acc.: 100.00%] [G loss: 0.012037]\n",
      "epoch:40 step:31740 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.020089]\n",
      "epoch:40 step:31741 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.002598]\n",
      "epoch:40 step:31742 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.002018]\n",
      "epoch:40 step:31743 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.004523]\n",
      "epoch:40 step:31744 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.003496]\n",
      "epoch:40 step:31745 [D loss: 0.002367, acc.: 100.00%] [G loss: 0.001450]\n",
      "epoch:40 step:31746 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:40 step:31747 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.001035]\n",
      "epoch:40 step:31748 [D loss: 0.000985, acc.: 100.00%] [G loss: 0.003142]\n",
      "epoch:40 step:31749 [D loss: 0.013922, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:40 step:31750 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000650]\n",
      "epoch:40 step:31751 [D loss: 0.001637, acc.: 100.00%] [G loss: 0.000896]\n",
      "epoch:40 step:31752 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.056509]\n",
      "epoch:40 step:31753 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:40 step:31754 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000496]\n",
      "epoch:40 step:31755 [D loss: 0.003060, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:40 step:31756 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:40 step:31757 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31758 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:40 step:31759 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:40 step:31760 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:40 step:31761 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:40 step:31762 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:40 step:31763 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:40 step:31764 [D loss: 0.002458, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:40 step:31765 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.001544]\n",
      "epoch:40 step:31766 [D loss: 0.005085, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:40 step:31767 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:40 step:31768 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:40 step:31769 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000915]\n",
      "epoch:40 step:31770 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.009923]\n",
      "epoch:40 step:31771 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:40 step:31772 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:40 step:31773 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.009921]\n",
      "epoch:40 step:31774 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:40 step:31775 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:40 step:31776 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:40 step:31777 [D loss: 0.003482, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:40 step:31778 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:40 step:31779 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:40 step:31780 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:40 step:31781 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:40 step:31782 [D loss: 0.001079, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:40 step:31783 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:40 step:31784 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:40 step:31785 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:40 step:31786 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:40 step:31787 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:40 step:31788 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:40 step:31789 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:40 step:31790 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.000697]\n",
      "epoch:40 step:31791 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:40 step:31792 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:40 step:31793 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:40 step:31794 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:40 step:31795 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:40 step:31796 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:40 step:31797 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:40 step:31798 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:40 step:31799 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:40 step:31800 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:40 step:31801 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:40 step:31802 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:40 step:31803 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:40 step:31804 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:40 step:31805 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:40 step:31806 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:40 step:31807 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001440]\n",
      "epoch:40 step:31808 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:40 step:31809 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:40 step:31810 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:40 step:31811 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:40 step:31812 [D loss: 0.003647, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:40 step:31813 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:40 step:31814 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:40 step:31815 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:40 step:31816 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:40 step:31817 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:40 step:31818 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:40 step:31819 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:40 step:31820 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:40 step:31821 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:40 step:31822 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:40 step:31823 [D loss: 0.002435, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:40 step:31824 [D loss: 0.003183, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:40 step:31825 [D loss: 0.064044, acc.: 97.66%] [G loss: 0.001005]\n",
      "epoch:40 step:31826 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.004710]\n",
      "epoch:40 step:31827 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.007291]\n",
      "epoch:40 step:31828 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.004108]\n",
      "epoch:40 step:31829 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.003796]\n",
      "epoch:40 step:31830 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.002246]\n",
      "epoch:40 step:31831 [D loss: 0.001908, acc.: 100.00%] [G loss: 0.028390]\n",
      "epoch:40 step:31832 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000809]\n",
      "epoch:40 step:31833 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.045415]\n",
      "epoch:40 step:31834 [D loss: 0.001217, acc.: 100.00%] [G loss: 0.004862]\n",
      "epoch:40 step:31835 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.001534]\n",
      "epoch:40 step:31836 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.017609]\n",
      "epoch:40 step:31837 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:40 step:31838 [D loss: 0.000902, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:40 step:31839 [D loss: 0.002768, acc.: 100.00%] [G loss: 0.002754]\n",
      "epoch:40 step:31840 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001889]\n",
      "epoch:40 step:31841 [D loss: 0.003325, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:40 step:31842 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000704]\n",
      "epoch:40 step:31843 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:40 step:31844 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:40 step:31845 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000987]\n",
      "epoch:40 step:31846 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:40 step:31847 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000384]\n",
      "epoch:40 step:31848 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:40 step:31849 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000955]\n",
      "epoch:40 step:31850 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000834]\n",
      "epoch:40 step:31851 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.001042]\n",
      "epoch:40 step:31852 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000940]\n",
      "epoch:40 step:31853 [D loss: 0.006389, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:40 step:31854 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:40 step:31855 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:40 step:31856 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001018]\n",
      "epoch:40 step:31857 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:40 step:31858 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:40 step:31859 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000466]\n",
      "epoch:40 step:31860 [D loss: 0.006226, acc.: 99.22%] [G loss: 0.000384]\n",
      "epoch:40 step:31861 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:40 step:31862 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:40 step:31863 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:40 step:31864 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:40 step:31865 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:40 step:31866 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:40 step:31867 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:40 step:31868 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:40 step:31869 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:40 step:31870 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:40 step:31871 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.000151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31872 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:40 step:31873 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:40 step:31874 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:40 step:31875 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:40 step:31876 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:40 step:31877 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:40 step:31878 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:40 step:31879 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:40 step:31880 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:40 step:31881 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:40 step:31882 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:40 step:31883 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:40 step:31884 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:40 step:31885 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:40 step:31886 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:40 step:31887 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:40 step:31888 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:40 step:31889 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:40 step:31890 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:40 step:31891 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:40 step:31892 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:40 step:31893 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:40 step:31894 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:40 step:31895 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:40 step:31896 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:40 step:31897 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:40 step:31898 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:40 step:31899 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:40 step:31900 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:40 step:31901 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:40 step:31902 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:40 step:31903 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:40 step:31904 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:40 step:31905 [D loss: 0.004398, acc.: 100.00%] [G loss: 0.001589]\n",
      "epoch:40 step:31906 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:40 step:31907 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:40 step:31908 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:40 step:31909 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:40 step:31910 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:40 step:31911 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:40 step:31912 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:40 step:31913 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:40 step:31914 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:40 step:31915 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:40 step:31916 [D loss: 0.002892, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:40 step:31917 [D loss: 0.338965, acc.: 81.25%] [G loss: 0.369159]\n",
      "epoch:40 step:31918 [D loss: 0.023145, acc.: 99.22%] [G loss: 2.409657]\n",
      "epoch:40 step:31919 [D loss: 0.057064, acc.: 98.44%] [G loss: 1.252973]\n",
      "epoch:40 step:31920 [D loss: 0.469802, acc.: 82.03%] [G loss: 0.233804]\n",
      "epoch:40 step:31921 [D loss: 0.196461, acc.: 89.84%] [G loss: 3.088608]\n",
      "epoch:40 step:31922 [D loss: 0.039036, acc.: 100.00%] [G loss: 2.808142]\n",
      "epoch:40 step:31923 [D loss: 0.027739, acc.: 100.00%] [G loss: 3.768014]\n",
      "epoch:40 step:31924 [D loss: 0.144762, acc.: 94.53%] [G loss: 3.567890]\n",
      "epoch:40 step:31925 [D loss: 0.202778, acc.: 93.75%] [G loss: 0.207033]\n",
      "epoch:40 step:31926 [D loss: 0.001033, acc.: 100.00%] [G loss: 2.435160]\n",
      "epoch:40 step:31927 [D loss: 0.028535, acc.: 99.22%] [G loss: 2.010457]\n",
      "epoch:40 step:31928 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.030097]\n",
      "epoch:40 step:31929 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.020949]\n",
      "epoch:40 step:31930 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.187091]\n",
      "epoch:40 step:31931 [D loss: 0.000058, acc.: 100.00%] [G loss: 1.358193]\n",
      "epoch:40 step:31932 [D loss: 0.002231, acc.: 100.00%] [G loss: 0.448405]\n",
      "epoch:40 step:31933 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.004956]\n",
      "epoch:40 step:31934 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.386700]\n",
      "epoch:40 step:31935 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.275527]\n",
      "epoch:40 step:31936 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.077381]\n",
      "epoch:40 step:31937 [D loss: 0.001462, acc.: 100.00%] [G loss: 0.002914]\n",
      "epoch:40 step:31938 [D loss: 0.003179, acc.: 100.00%] [G loss: 0.069283]\n",
      "epoch:40 step:31939 [D loss: 0.006242, acc.: 100.00%] [G loss: 0.062015]\n",
      "epoch:40 step:31940 [D loss: 0.002862, acc.: 100.00%] [G loss: 0.002048]\n",
      "epoch:40 step:31941 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.053945]\n",
      "epoch:40 step:31942 [D loss: 0.002852, acc.: 100.00%] [G loss: 0.086992]\n",
      "epoch:40 step:31943 [D loss: 0.005507, acc.: 100.00%] [G loss: 0.029406]\n",
      "epoch:40 step:31944 [D loss: 0.008266, acc.: 100.00%] [G loss: 0.019118]\n",
      "epoch:40 step:31945 [D loss: 0.073093, acc.: 99.22%] [G loss: 0.006469]\n",
      "epoch:40 step:31946 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.141049]\n",
      "epoch:40 step:31947 [D loss: 0.009387, acc.: 99.22%] [G loss: 0.057365]\n",
      "epoch:40 step:31948 [D loss: 0.001105, acc.: 100.00%] [G loss: 0.009777]\n",
      "epoch:40 step:31949 [D loss: 0.000974, acc.: 100.00%] [G loss: 0.053952]\n",
      "epoch:40 step:31950 [D loss: 0.001939, acc.: 100.00%] [G loss: 0.927248]\n",
      "epoch:40 step:31951 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.033674]\n",
      "epoch:40 step:31952 [D loss: 0.000863, acc.: 100.00%] [G loss: 0.030448]\n",
      "epoch:40 step:31953 [D loss: 0.081812, acc.: 96.88%] [G loss: 0.000045]\n",
      "epoch:40 step:31954 [D loss: 0.103678, acc.: 98.44%] [G loss: 0.003745]\n",
      "epoch:40 step:31955 [D loss: 0.005488, acc.: 100.00%] [G loss: 0.093047]\n",
      "epoch:40 step:31956 [D loss: 0.002576, acc.: 100.00%] [G loss: 0.017479]\n",
      "epoch:40 step:31957 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.036466]\n",
      "epoch:40 step:31958 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.045608]\n",
      "epoch:40 step:31959 [D loss: 1.386378, acc.: 53.91%] [G loss: 7.950673]\n",
      "epoch:40 step:31960 [D loss: 2.297539, acc.: 50.78%] [G loss: 3.138267]\n",
      "epoch:40 step:31961 [D loss: 0.409527, acc.: 82.03%] [G loss: 2.563626]\n",
      "epoch:40 step:31962 [D loss: 0.172951, acc.: 94.53%] [G loss: 1.884099]\n",
      "epoch:40 step:31963 [D loss: 0.112007, acc.: 100.00%] [G loss: 2.063573]\n",
      "epoch:40 step:31964 [D loss: 0.080315, acc.: 99.22%] [G loss: 1.348632]\n",
      "epoch:40 step:31965 [D loss: 0.148584, acc.: 96.88%] [G loss: 0.520323]\n",
      "epoch:40 step:31966 [D loss: 0.035013, acc.: 100.00%] [G loss: 0.323749]\n",
      "epoch:40 step:31967 [D loss: 0.069942, acc.: 99.22%] [G loss: 0.088824]\n",
      "epoch:40 step:31968 [D loss: 0.041302, acc.: 100.00%] [G loss: 0.020274]\n",
      "epoch:40 step:31969 [D loss: 0.020104, acc.: 100.00%] [G loss: 0.024945]\n",
      "epoch:40 step:31970 [D loss: 0.010181, acc.: 100.00%] [G loss: 0.034574]\n",
      "epoch:40 step:31971 [D loss: 0.009658, acc.: 100.00%] [G loss: 0.009601]\n",
      "epoch:40 step:31972 [D loss: 0.042740, acc.: 99.22%] [G loss: 0.937917]\n",
      "epoch:40 step:31973 [D loss: 0.017907, acc.: 100.00%] [G loss: 0.647727]\n",
      "epoch:40 step:31974 [D loss: 0.050518, acc.: 100.00%] [G loss: 0.497053]\n",
      "epoch:40 step:31975 [D loss: 0.042986, acc.: 99.22%] [G loss: 0.272675]\n",
      "epoch:40 step:31976 [D loss: 0.012133, acc.: 100.00%] [G loss: 0.031472]\n",
      "epoch:40 step:31977 [D loss: 0.039781, acc.: 100.00%] [G loss: 0.216987]\n",
      "epoch:40 step:31978 [D loss: 0.018704, acc.: 99.22%] [G loss: 0.035414]\n",
      "epoch:40 step:31979 [D loss: 0.055995, acc.: 99.22%] [G loss: 0.007846]\n",
      "epoch:40 step:31980 [D loss: 0.005639, acc.: 100.00%] [G loss: 0.162250]\n",
      "epoch:40 step:31981 [D loss: 0.052193, acc.: 100.00%] [G loss: 0.031900]\n",
      "epoch:40 step:31982 [D loss: 0.009775, acc.: 100.00%] [G loss: 0.018795]\n",
      "epoch:40 step:31983 [D loss: 0.009430, acc.: 100.00%] [G loss: 0.063935]\n",
      "epoch:40 step:31984 [D loss: 0.009905, acc.: 100.00%] [G loss: 0.136159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31985 [D loss: 0.009816, acc.: 100.00%] [G loss: 0.018370]\n",
      "epoch:40 step:31986 [D loss: 0.027960, acc.: 99.22%] [G loss: 0.126653]\n",
      "epoch:40 step:31987 [D loss: 0.012038, acc.: 100.00%] [G loss: 0.003156]\n",
      "epoch:40 step:31988 [D loss: 0.016067, acc.: 100.00%] [G loss: 0.004745]\n",
      "epoch:40 step:31989 [D loss: 0.003579, acc.: 100.00%] [G loss: 0.017584]\n",
      "epoch:40 step:31990 [D loss: 0.011977, acc.: 100.00%] [G loss: 0.002351]\n",
      "epoch:40 step:31991 [D loss: 0.001205, acc.: 100.00%] [G loss: 0.053099]\n",
      "epoch:40 step:31992 [D loss: 0.011900, acc.: 100.00%] [G loss: 0.002750]\n",
      "epoch:40 step:31993 [D loss: 0.020250, acc.: 100.00%] [G loss: 0.001124]\n",
      "epoch:40 step:31994 [D loss: 0.025322, acc.: 99.22%] [G loss: 0.005387]\n",
      "epoch:40 step:31995 [D loss: 0.009757, acc.: 100.00%] [G loss: 0.000574]\n",
      "epoch:40 step:31996 [D loss: 0.005144, acc.: 100.00%] [G loss: 0.000684]\n",
      "epoch:40 step:31997 [D loss: 0.000851, acc.: 100.00%] [G loss: 0.003925]\n",
      "epoch:40 step:31998 [D loss: 0.000910, acc.: 100.00%] [G loss: 0.005073]\n",
      "epoch:40 step:31999 [D loss: 0.009078, acc.: 100.00%] [G loss: 0.001181]\n",
      "epoch:40 step:32000 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.018741]\n",
      "epoch:40 step:32001 [D loss: 0.005283, acc.: 100.00%] [G loss: 0.001092]\n",
      "epoch:40 step:32002 [D loss: 0.006138, acc.: 100.00%] [G loss: 0.007784]\n",
      "epoch:40 step:32003 [D loss: 0.055126, acc.: 100.00%] [G loss: 0.003716]\n",
      "epoch:40 step:32004 [D loss: 0.003787, acc.: 100.00%] [G loss: 0.001970]\n",
      "epoch:40 step:32005 [D loss: 0.003039, acc.: 100.00%] [G loss: 0.009028]\n",
      "epoch:40 step:32006 [D loss: 0.002089, acc.: 100.00%] [G loss: 0.006743]\n",
      "epoch:40 step:32007 [D loss: 0.002976, acc.: 100.00%] [G loss: 0.002412]\n",
      "epoch:40 step:32008 [D loss: 0.005435, acc.: 100.00%] [G loss: 0.515106]\n",
      "epoch:40 step:32009 [D loss: 0.009632, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:40 step:32010 [D loss: 0.010489, acc.: 100.00%] [G loss: 0.003276]\n",
      "epoch:40 step:32011 [D loss: 0.068533, acc.: 96.88%] [G loss: 0.008365]\n",
      "epoch:40 step:32012 [D loss: 0.002836, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:40 step:32013 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:40 step:32014 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:40 step:32015 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.001103]\n",
      "epoch:40 step:32016 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:40 step:32017 [D loss: 0.040153, acc.: 99.22%] [G loss: 0.053639]\n",
      "epoch:40 step:32018 [D loss: 0.015039, acc.: 100.00%] [G loss: 0.002727]\n",
      "epoch:40 step:32019 [D loss: 0.002638, acc.: 100.00%] [G loss: 0.024036]\n",
      "epoch:40 step:32020 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.005233]\n",
      "epoch:40 step:32021 [D loss: 0.005028, acc.: 100.00%] [G loss: 0.002020]\n",
      "epoch:41 step:32022 [D loss: 0.042347, acc.: 100.00%] [G loss: 0.072720]\n",
      "epoch:41 step:32023 [D loss: 0.007213, acc.: 100.00%] [G loss: 0.017066]\n",
      "epoch:41 step:32024 [D loss: 0.023239, acc.: 99.22%] [G loss: 0.007375]\n",
      "epoch:41 step:32025 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.015149]\n",
      "epoch:41 step:32026 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.016309]\n",
      "epoch:41 step:32027 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.005904]\n",
      "epoch:41 step:32028 [D loss: 0.006101, acc.: 100.00%] [G loss: 0.001720]\n",
      "epoch:41 step:32029 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.031727]\n",
      "epoch:41 step:32030 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.015604]\n",
      "epoch:41 step:32031 [D loss: 0.008456, acc.: 100.00%] [G loss: 0.009122]\n",
      "epoch:41 step:32032 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.000576]\n",
      "epoch:41 step:32033 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.001893]\n",
      "epoch:41 step:32034 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.005022]\n",
      "epoch:41 step:32035 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.000601]\n",
      "epoch:41 step:32036 [D loss: 0.000844, acc.: 100.00%] [G loss: 0.007450]\n",
      "epoch:41 step:32037 [D loss: 0.000953, acc.: 100.00%] [G loss: 0.003522]\n",
      "epoch:41 step:32038 [D loss: 0.001279, acc.: 100.00%] [G loss: 0.004685]\n",
      "epoch:41 step:32039 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.092509]\n",
      "epoch:41 step:32040 [D loss: 0.004701, acc.: 100.00%] [G loss: 0.000658]\n",
      "epoch:41 step:32041 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.000615]\n",
      "epoch:41 step:32042 [D loss: 0.014531, acc.: 100.00%] [G loss: 0.047458]\n",
      "epoch:41 step:32043 [D loss: 0.016745, acc.: 100.00%] [G loss: 0.027646]\n",
      "epoch:41 step:32044 [D loss: 0.009155, acc.: 100.00%] [G loss: 0.001857]\n",
      "epoch:41 step:32045 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.001622]\n",
      "epoch:41 step:32046 [D loss: 0.007016, acc.: 100.00%] [G loss: 0.055496]\n",
      "epoch:41 step:32047 [D loss: 0.001460, acc.: 100.00%] [G loss: 0.032536]\n",
      "epoch:41 step:32048 [D loss: 0.014069, acc.: 100.00%] [G loss: 0.403538]\n",
      "epoch:41 step:32049 [D loss: 0.045850, acc.: 100.00%] [G loss: 0.028751]\n",
      "epoch:41 step:32050 [D loss: 0.047783, acc.: 97.66%] [G loss: 0.015812]\n",
      "epoch:41 step:32051 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.002884]\n",
      "epoch:41 step:32052 [D loss: 0.004160, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:41 step:32053 [D loss: 0.000905, acc.: 100.00%] [G loss: 0.095565]\n",
      "epoch:41 step:32054 [D loss: 0.003670, acc.: 100.00%] [G loss: 0.002871]\n",
      "epoch:41 step:32055 [D loss: 0.002924, acc.: 100.00%] [G loss: 0.230775]\n",
      "epoch:41 step:32056 [D loss: 0.024360, acc.: 99.22%] [G loss: 0.004023]\n",
      "epoch:41 step:32057 [D loss: 0.007706, acc.: 100.00%] [G loss: 0.017367]\n",
      "epoch:41 step:32058 [D loss: 0.001508, acc.: 100.00%] [G loss: 0.009193]\n",
      "epoch:41 step:32059 [D loss: 0.233193, acc.: 89.06%] [G loss: 5.696182]\n",
      "epoch:41 step:32060 [D loss: 0.144926, acc.: 92.19%] [G loss: 5.540187]\n",
      "epoch:41 step:32061 [D loss: 0.120603, acc.: 96.09%] [G loss: 0.048507]\n",
      "epoch:41 step:32062 [D loss: 0.008322, acc.: 100.00%] [G loss: 0.008876]\n",
      "epoch:41 step:32063 [D loss: 0.021729, acc.: 100.00%] [G loss: 0.021758]\n",
      "epoch:41 step:32064 [D loss: 0.007583, acc.: 100.00%] [G loss: 0.001357]\n",
      "epoch:41 step:32065 [D loss: 0.199599, acc.: 91.41%] [G loss: 0.000031]\n",
      "epoch:41 step:32066 [D loss: 0.015718, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:41 step:32067 [D loss: 0.001130, acc.: 100.00%] [G loss: 0.070973]\n",
      "epoch:41 step:32068 [D loss: 0.041488, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:41 step:32069 [D loss: 0.002007, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:41 step:32070 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.000757]\n",
      "epoch:41 step:32071 [D loss: 0.001576, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:41 step:32072 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000943]\n",
      "epoch:41 step:32073 [D loss: 0.010192, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:41 step:32074 [D loss: 0.133211, acc.: 95.31%] [G loss: 3.792454]\n",
      "epoch:41 step:32075 [D loss: 0.026038, acc.: 99.22%] [G loss: 0.238581]\n",
      "epoch:41 step:32076 [D loss: 0.012679, acc.: 99.22%] [G loss: 0.121797]\n",
      "epoch:41 step:32077 [D loss: 0.169985, acc.: 92.19%] [G loss: 0.452158]\n",
      "epoch:41 step:32078 [D loss: 0.006821, acc.: 100.00%] [G loss: 0.039363]\n",
      "epoch:41 step:32079 [D loss: 0.002587, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:41 step:32080 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:41 step:32081 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:41 step:32082 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000423]\n",
      "epoch:41 step:32083 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:41 step:32084 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:41 step:32085 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:41 step:32086 [D loss: 0.000875, acc.: 100.00%] [G loss: 0.042075]\n",
      "epoch:41 step:32087 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.013561]\n",
      "epoch:41 step:32088 [D loss: 0.006714, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:41 step:32089 [D loss: 0.000766, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32090 [D loss: 0.002165, acc.: 100.00%] [G loss: 0.004274]\n",
      "epoch:41 step:32091 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:41 step:32092 [D loss: 0.001010, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:41 step:32093 [D loss: 0.001853, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:41 step:32094 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.092618]\n",
      "epoch:41 step:32095 [D loss: 0.008407, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:41 step:32096 [D loss: 0.007858, acc.: 100.00%] [G loss: 0.020783]\n",
      "epoch:41 step:32097 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:41 step:32098 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:41 step:32099 [D loss: 0.000868, acc.: 100.00%] [G loss: 0.000453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32100 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:41 step:32101 [D loss: 0.002398, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:41 step:32102 [D loss: 0.005827, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:41 step:32103 [D loss: 0.012376, acc.: 100.00%] [G loss: 0.047858]\n",
      "epoch:41 step:32104 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001206]\n",
      "epoch:41 step:32105 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.464597]\n",
      "epoch:41 step:32106 [D loss: 0.000876, acc.: 100.00%] [G loss: 0.008087]\n",
      "epoch:41 step:32107 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:41 step:32108 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.001468]\n",
      "epoch:41 step:32109 [D loss: 0.001891, acc.: 100.00%] [G loss: 0.003103]\n",
      "epoch:41 step:32110 [D loss: 0.002666, acc.: 100.00%] [G loss: 0.026861]\n",
      "epoch:41 step:32111 [D loss: 0.030874, acc.: 98.44%] [G loss: 0.000069]\n",
      "epoch:41 step:32112 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.010368]\n",
      "epoch:41 step:32113 [D loss: 0.002737, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:41 step:32114 [D loss: 0.001355, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:41 step:32115 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:41 step:32116 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.003018]\n",
      "epoch:41 step:32117 [D loss: 0.001204, acc.: 100.00%] [G loss: 0.010594]\n",
      "epoch:41 step:32118 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.017964]\n",
      "epoch:41 step:32119 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:41 step:32120 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:41 step:32121 [D loss: 0.002690, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:41 step:32122 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:41 step:32123 [D loss: 0.030611, acc.: 99.22%] [G loss: 0.070262]\n",
      "epoch:41 step:32124 [D loss: 0.001913, acc.: 100.00%] [G loss: 0.170763]\n",
      "epoch:41 step:32125 [D loss: 0.029081, acc.: 100.00%] [G loss: 0.001374]\n",
      "epoch:41 step:32126 [D loss: 0.001082, acc.: 100.00%] [G loss: 0.886306]\n",
      "epoch:41 step:32127 [D loss: 0.001871, acc.: 100.00%] [G loss: 0.043804]\n",
      "epoch:41 step:32128 [D loss: 0.053767, acc.: 97.66%] [G loss: 0.217078]\n",
      "epoch:41 step:32129 [D loss: 0.040586, acc.: 100.00%] [G loss: 0.394760]\n",
      "epoch:41 step:32130 [D loss: 0.026482, acc.: 98.44%] [G loss: 0.759784]\n",
      "epoch:41 step:32131 [D loss: 0.028163, acc.: 99.22%] [G loss: 0.997967]\n",
      "epoch:41 step:32132 [D loss: 1.475809, acc.: 49.22%] [G loss: 6.503740]\n",
      "epoch:41 step:32133 [D loss: 1.569188, acc.: 54.69%] [G loss: 6.771189]\n",
      "epoch:41 step:32134 [D loss: 0.079782, acc.: 96.88%] [G loss: 0.024397]\n",
      "epoch:41 step:32135 [D loss: 0.051560, acc.: 96.88%] [G loss: 0.000646]\n",
      "epoch:41 step:32136 [D loss: 0.066097, acc.: 97.66%] [G loss: 4.603545]\n",
      "epoch:41 step:32137 [D loss: 0.029835, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:41 step:32138 [D loss: 0.213284, acc.: 92.97%] [G loss: 0.002323]\n",
      "epoch:41 step:32139 [D loss: 0.012467, acc.: 100.00%] [G loss: 5.157811]\n",
      "epoch:41 step:32140 [D loss: 0.070778, acc.: 97.66%] [G loss: 4.634491]\n",
      "epoch:41 step:32141 [D loss: 0.126432, acc.: 94.53%] [G loss: 0.005894]\n",
      "epoch:41 step:32142 [D loss: 0.035738, acc.: 100.00%] [G loss: 2.039496]\n",
      "epoch:41 step:32143 [D loss: 0.061087, acc.: 98.44%] [G loss: 0.010534]\n",
      "epoch:41 step:32144 [D loss: 0.158094, acc.: 94.53%] [G loss: 0.000534]\n",
      "epoch:41 step:32145 [D loss: 0.088129, acc.: 96.88%] [G loss: 0.000264]\n",
      "epoch:41 step:32146 [D loss: 0.005966, acc.: 100.00%] [G loss: 0.004501]\n",
      "epoch:41 step:32147 [D loss: 0.043899, acc.: 100.00%] [G loss: 3.343689]\n",
      "epoch:41 step:32148 [D loss: 0.019437, acc.: 99.22%] [G loss: 2.512211]\n",
      "epoch:41 step:32149 [D loss: 0.014508, acc.: 100.00%] [G loss: 1.824551]\n",
      "epoch:41 step:32150 [D loss: 0.114555, acc.: 95.31%] [G loss: 2.864712]\n",
      "epoch:41 step:32151 [D loss: 0.036129, acc.: 99.22%] [G loss: 0.172625]\n",
      "epoch:41 step:32152 [D loss: 0.050563, acc.: 97.66%] [G loss: 0.080497]\n",
      "epoch:41 step:32153 [D loss: 0.057244, acc.: 98.44%] [G loss: 0.003658]\n",
      "epoch:41 step:32154 [D loss: 0.005200, acc.: 100.00%] [G loss: 0.010732]\n",
      "epoch:41 step:32155 [D loss: 0.009978, acc.: 100.00%] [G loss: 0.000850]\n",
      "epoch:41 step:32156 [D loss: 0.010332, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:41 step:32157 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.000902]\n",
      "epoch:41 step:32158 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:41 step:32159 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.003612]\n",
      "epoch:41 step:32160 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.002992]\n",
      "epoch:41 step:32161 [D loss: 0.011976, acc.: 100.00%] [G loss: 0.004197]\n",
      "epoch:41 step:32162 [D loss: 0.002377, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:41 step:32163 [D loss: 0.008308, acc.: 100.00%] [G loss: 0.101435]\n",
      "epoch:41 step:32164 [D loss: 0.024598, acc.: 100.00%] [G loss: 0.004015]\n",
      "epoch:41 step:32165 [D loss: 0.001096, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:41 step:32166 [D loss: 0.006934, acc.: 100.00%] [G loss: 0.001243]\n",
      "epoch:41 step:32167 [D loss: 0.004604, acc.: 100.00%] [G loss: 0.002532]\n",
      "epoch:41 step:32168 [D loss: 0.008829, acc.: 100.00%] [G loss: 0.088091]\n",
      "epoch:41 step:32169 [D loss: 0.016558, acc.: 100.00%] [G loss: 0.003102]\n",
      "epoch:41 step:32170 [D loss: 0.006594, acc.: 100.00%] [G loss: 1.262243]\n",
      "epoch:41 step:32171 [D loss: 0.007016, acc.: 100.00%] [G loss: 0.134004]\n",
      "epoch:41 step:32172 [D loss: 0.002339, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:41 step:32173 [D loss: 0.002177, acc.: 100.00%] [G loss: 0.002485]\n",
      "epoch:41 step:32174 [D loss: 0.026323, acc.: 98.44%] [G loss: 0.003515]\n",
      "epoch:41 step:32175 [D loss: 0.004120, acc.: 100.00%] [G loss: 0.000855]\n",
      "epoch:41 step:32176 [D loss: 0.097963, acc.: 96.88%] [G loss: 0.062344]\n",
      "epoch:41 step:32177 [D loss: 0.022126, acc.: 99.22%] [G loss: 2.901454]\n",
      "epoch:41 step:32178 [D loss: 0.112591, acc.: 94.53%] [G loss: 0.161308]\n",
      "epoch:41 step:32179 [D loss: 0.040276, acc.: 98.44%] [G loss: 0.001513]\n",
      "epoch:41 step:32180 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:41 step:32181 [D loss: 0.003652, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:41 step:32182 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.025794]\n",
      "epoch:41 step:32183 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:41 step:32184 [D loss: 0.001067, acc.: 100.00%] [G loss: 0.002729]\n",
      "epoch:41 step:32185 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.005679]\n",
      "epoch:41 step:32186 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.008268]\n",
      "epoch:41 step:32187 [D loss: 0.003385, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:41 step:32188 [D loss: 0.001959, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:41 step:32189 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:41 step:32190 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:41 step:32191 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:41 step:32192 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:41 step:32193 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.019880]\n",
      "epoch:41 step:32194 [D loss: 0.038407, acc.: 99.22%] [G loss: 0.006472]\n",
      "epoch:41 step:32195 [D loss: 0.007026, acc.: 99.22%] [G loss: 0.004248]\n",
      "epoch:41 step:32196 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.005339]\n",
      "epoch:41 step:32197 [D loss: 0.000829, acc.: 100.00%] [G loss: 0.033870]\n",
      "epoch:41 step:32198 [D loss: 0.000657, acc.: 100.00%] [G loss: 0.025909]\n",
      "epoch:41 step:32199 [D loss: 0.026521, acc.: 98.44%] [G loss: 0.000542]\n",
      "epoch:41 step:32200 [D loss: 0.018943, acc.: 99.22%] [G loss: 0.000268]\n",
      "epoch:41 step:32201 [D loss: 0.002888, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32202 [D loss: 0.006383, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:41 step:32203 [D loss: 0.004343, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:41 step:32204 [D loss: 0.000731, acc.: 100.00%] [G loss: 0.046624]\n",
      "epoch:41 step:32205 [D loss: 0.003297, acc.: 100.00%] [G loss: 0.009364]\n",
      "epoch:41 step:32206 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000715]\n",
      "epoch:41 step:32207 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:41 step:32208 [D loss: 0.004114, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:41 step:32209 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000764]\n",
      "epoch:41 step:32210 [D loss: 0.005939, acc.: 99.22%] [G loss: 0.000031]\n",
      "epoch:41 step:32211 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:41 step:32212 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001231]\n",
      "epoch:41 step:32213 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.582518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32214 [D loss: 0.009488, acc.: 100.00%] [G loss: 0.031850]\n",
      "epoch:41 step:32215 [D loss: 0.005469, acc.: 100.00%] [G loss: 0.002939]\n",
      "epoch:41 step:32216 [D loss: 1.187865, acc.: 60.16%] [G loss: 4.822989]\n",
      "epoch:41 step:32217 [D loss: 1.363773, acc.: 58.59%] [G loss: 0.696380]\n",
      "epoch:41 step:32218 [D loss: 1.324318, acc.: 54.69%] [G loss: 3.524725]\n",
      "epoch:41 step:32219 [D loss: 0.015947, acc.: 99.22%] [G loss: 1.382076]\n",
      "epoch:41 step:32220 [D loss: 0.113232, acc.: 96.09%] [G loss: 0.000015]\n",
      "epoch:41 step:32221 [D loss: 0.012231, acc.: 100.00%] [G loss: 0.580825]\n",
      "epoch:41 step:32222 [D loss: 0.033010, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:41 step:32223 [D loss: 0.044221, acc.: 100.00%] [G loss: 0.238407]\n",
      "epoch:41 step:32224 [D loss: 0.021289, acc.: 100.00%] [G loss: 0.001617]\n",
      "epoch:41 step:32225 [D loss: 0.018351, acc.: 100.00%] [G loss: 0.003478]\n",
      "epoch:41 step:32226 [D loss: 0.010666, acc.: 100.00%] [G loss: 0.000411]\n",
      "epoch:41 step:32227 [D loss: 0.010007, acc.: 100.00%] [G loss: 0.067188]\n",
      "epoch:41 step:32228 [D loss: 0.012369, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:41 step:32229 [D loss: 0.004971, acc.: 100.00%] [G loss: 0.189581]\n",
      "epoch:41 step:32230 [D loss: 0.125261, acc.: 97.66%] [G loss: 0.004286]\n",
      "epoch:41 step:32231 [D loss: 0.207743, acc.: 89.06%] [G loss: 0.000071]\n",
      "epoch:41 step:32232 [D loss: 0.002795, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:41 step:32233 [D loss: 0.002569, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:41 step:32234 [D loss: 0.001775, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:41 step:32235 [D loss: 0.007521, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:41 step:32236 [D loss: 0.000782, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:41 step:32237 [D loss: 0.007234, acc.: 100.00%] [G loss: 0.156854]\n",
      "epoch:41 step:32238 [D loss: 0.002823, acc.: 100.00%] [G loss: 0.166627]\n",
      "epoch:41 step:32239 [D loss: 0.003786, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:41 step:32240 [D loss: 0.008352, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:41 step:32241 [D loss: 0.006238, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:41 step:32242 [D loss: 0.002268, acc.: 100.00%] [G loss: 0.000399]\n",
      "epoch:41 step:32243 [D loss: 0.002580, acc.: 100.00%] [G loss: 0.032602]\n",
      "epoch:41 step:32244 [D loss: 0.008123, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:41 step:32245 [D loss: 0.016682, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:41 step:32246 [D loss: 0.006650, acc.: 100.00%] [G loss: 0.360617]\n",
      "epoch:41 step:32247 [D loss: 0.129595, acc.: 96.88%] [G loss: 0.000676]\n",
      "epoch:41 step:32248 [D loss: 0.004803, acc.: 100.00%] [G loss: 0.017303]\n",
      "epoch:41 step:32249 [D loss: 0.024675, acc.: 99.22%] [G loss: 0.003099]\n",
      "epoch:41 step:32250 [D loss: 0.156996, acc.: 92.19%] [G loss: 0.000309]\n",
      "epoch:41 step:32251 [D loss: 0.051870, acc.: 96.88%] [G loss: 0.000110]\n",
      "epoch:41 step:32252 [D loss: 0.222958, acc.: 87.50%] [G loss: 0.146925]\n",
      "epoch:41 step:32253 [D loss: 0.005620, acc.: 100.00%] [G loss: 5.664188]\n",
      "epoch:41 step:32254 [D loss: 0.106516, acc.: 96.09%] [G loss: 1.370235]\n",
      "epoch:41 step:32255 [D loss: 0.193650, acc.: 95.31%] [G loss: 0.251880]\n",
      "epoch:41 step:32256 [D loss: 0.007654, acc.: 100.00%] [G loss: 0.031649]\n",
      "epoch:41 step:32257 [D loss: 0.019468, acc.: 100.00%] [G loss: 0.002163]\n",
      "epoch:41 step:32258 [D loss: 0.056466, acc.: 98.44%] [G loss: 0.004019]\n",
      "epoch:41 step:32259 [D loss: 0.029091, acc.: 99.22%] [G loss: 0.046034]\n",
      "epoch:41 step:32260 [D loss: 0.022087, acc.: 100.00%] [G loss: 0.079178]\n",
      "epoch:41 step:32261 [D loss: 0.102031, acc.: 97.66%] [G loss: 0.107827]\n",
      "epoch:41 step:32262 [D loss: 0.017508, acc.: 100.00%] [G loss: 6.822649]\n",
      "epoch:41 step:32263 [D loss: 0.102546, acc.: 96.88%] [G loss: 0.439966]\n",
      "epoch:41 step:32264 [D loss: 0.171501, acc.: 90.62%] [G loss: 4.536320]\n",
      "epoch:41 step:32265 [D loss: 0.225236, acc.: 89.06%] [G loss: 4.305987]\n",
      "epoch:41 step:32266 [D loss: 0.120654, acc.: 96.09%] [G loss: 1.141689]\n",
      "epoch:41 step:32267 [D loss: 0.021750, acc.: 100.00%] [G loss: 2.932655]\n",
      "epoch:41 step:32268 [D loss: 0.090207, acc.: 97.66%] [G loss: 4.738605]\n",
      "epoch:41 step:32269 [D loss: 0.213142, acc.: 90.62%] [G loss: 6.812882]\n",
      "epoch:41 step:32270 [D loss: 0.466890, acc.: 75.78%] [G loss: 2.538851]\n",
      "epoch:41 step:32271 [D loss: 0.844281, acc.: 67.97%] [G loss: 4.811404]\n",
      "epoch:41 step:32272 [D loss: 0.075474, acc.: 97.66%] [G loss: 5.117242]\n",
      "epoch:41 step:32273 [D loss: 0.618072, acc.: 75.00%] [G loss: 0.685148]\n",
      "epoch:41 step:32274 [D loss: 0.260865, acc.: 88.28%] [G loss: 3.605459]\n",
      "epoch:41 step:32275 [D loss: 0.055294, acc.: 97.66%] [G loss: 2.556841]\n",
      "epoch:41 step:32276 [D loss: 0.197998, acc.: 92.97%] [G loss: 0.526909]\n",
      "epoch:41 step:32277 [D loss: 0.009517, acc.: 99.22%] [G loss: 1.348640]\n",
      "epoch:41 step:32278 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.194576]\n",
      "epoch:41 step:32279 [D loss: 0.002743, acc.: 100.00%] [G loss: 0.174213]\n",
      "epoch:41 step:32280 [D loss: 0.006108, acc.: 100.00%] [G loss: 0.027788]\n",
      "epoch:41 step:32281 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.045381]\n",
      "epoch:41 step:32282 [D loss: 0.027412, acc.: 100.00%] [G loss: 0.007256]\n",
      "epoch:41 step:32283 [D loss: 0.003438, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:41 step:32284 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.014783]\n",
      "epoch:41 step:32285 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.000629]\n",
      "epoch:41 step:32286 [D loss: 0.004063, acc.: 100.00%] [G loss: 0.007132]\n",
      "epoch:41 step:32287 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.001710]\n",
      "epoch:41 step:32288 [D loss: 0.001102, acc.: 100.00%] [G loss: 0.002040]\n",
      "epoch:41 step:32289 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.001127]\n",
      "epoch:41 step:32290 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.001592]\n",
      "epoch:41 step:32291 [D loss: 0.006181, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:41 step:32292 [D loss: 0.004013, acc.: 100.00%] [G loss: 0.772416]\n",
      "epoch:41 step:32293 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.000423]\n",
      "epoch:41 step:32294 [D loss: 0.001683, acc.: 100.00%] [G loss: 0.002922]\n",
      "epoch:41 step:32295 [D loss: 0.007528, acc.: 100.00%] [G loss: 0.007725]\n",
      "epoch:41 step:32296 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.004108]\n",
      "epoch:41 step:32297 [D loss: 0.001566, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:41 step:32298 [D loss: 0.000692, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:41 step:32299 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:41 step:32300 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.001496]\n",
      "epoch:41 step:32301 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:41 step:32302 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:41 step:32303 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:41 step:32304 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:41 step:32305 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.001537]\n",
      "epoch:41 step:32306 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:41 step:32307 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:41 step:32308 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.001063]\n",
      "epoch:41 step:32309 [D loss: 0.015069, acc.: 99.22%] [G loss: 0.000238]\n",
      "epoch:41 step:32310 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.137870]\n",
      "epoch:41 step:32311 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:41 step:32312 [D loss: 0.001251, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:41 step:32313 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:41 step:32314 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:41 step:32315 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:41 step:32316 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:41 step:32317 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001467]\n",
      "epoch:41 step:32318 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:41 step:32319 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32320 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:41 step:32321 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:41 step:32322 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:41 step:32323 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:41 step:32324 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:41 step:32325 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.001014]\n",
      "epoch:41 step:32326 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:41 step:32327 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.011839]\n",
      "epoch:41 step:32328 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:41 step:32329 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32330 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:41 step:32331 [D loss: 0.008213, acc.: 99.22%] [G loss: 0.000142]\n",
      "epoch:41 step:32332 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:41 step:32333 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:41 step:32334 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:41 step:32335 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:41 step:32336 [D loss: 0.001273, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:41 step:32337 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:41 step:32338 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:41 step:32339 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:41 step:32340 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:41 step:32341 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:41 step:32342 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32343 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:41 step:32344 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000562]\n",
      "epoch:41 step:32345 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:41 step:32346 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:41 step:32347 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001952]\n",
      "epoch:41 step:32348 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:41 step:32349 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:41 step:32350 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000871]\n",
      "epoch:41 step:32351 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:41 step:32352 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:41 step:32353 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:41 step:32354 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:41 step:32355 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:41 step:32356 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:41 step:32357 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:41 step:32358 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:41 step:32359 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:41 step:32360 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:41 step:32361 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:41 step:32362 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:41 step:32363 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32364 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:41 step:32365 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:41 step:32366 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:41 step:32367 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32368 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:41 step:32369 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32370 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:41 step:32371 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:41 step:32372 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:41 step:32373 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:41 step:32374 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:41 step:32375 [D loss: 0.003161, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:41 step:32376 [D loss: 0.013805, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32377 [D loss: 0.011444, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:41 step:32378 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:41 step:32379 [D loss: 0.001189, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:41 step:32380 [D loss: 0.007642, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:41 step:32381 [D loss: 0.001513, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:41 step:32382 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:41 step:32383 [D loss: 0.012110, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:41 step:32384 [D loss: 0.051467, acc.: 99.22%] [G loss: 0.020263]\n",
      "epoch:41 step:32385 [D loss: 0.075861, acc.: 99.22%] [G loss: 0.813308]\n",
      "epoch:41 step:32386 [D loss: 0.012560, acc.: 100.00%] [G loss: 0.713270]\n",
      "epoch:41 step:32387 [D loss: 0.030440, acc.: 99.22%] [G loss: 0.020963]\n",
      "epoch:41 step:32388 [D loss: 0.002336, acc.: 100.00%] [G loss: 0.043749]\n",
      "epoch:41 step:32389 [D loss: 0.092204, acc.: 98.44%] [G loss: 0.808212]\n",
      "epoch:41 step:32390 [D loss: 0.126844, acc.: 97.66%] [G loss: 1.570420]\n",
      "epoch:41 step:32391 [D loss: 0.025878, acc.: 100.00%] [G loss: 2.962477]\n",
      "epoch:41 step:32392 [D loss: 0.038890, acc.: 99.22%] [G loss: 1.574008]\n",
      "epoch:41 step:32393 [D loss: 0.627553, acc.: 75.00%] [G loss: 7.984323]\n",
      "epoch:41 step:32394 [D loss: 0.619968, acc.: 77.34%] [G loss: 6.536267]\n",
      "epoch:41 step:32395 [D loss: 0.561650, acc.: 76.56%] [G loss: 3.105015]\n",
      "epoch:41 step:32396 [D loss: 0.034337, acc.: 98.44%] [G loss: 1.755420]\n",
      "epoch:41 step:32397 [D loss: 0.703701, acc.: 69.53%] [G loss: 3.265127]\n",
      "epoch:41 step:32398 [D loss: 0.073655, acc.: 97.66%] [G loss: 4.200511]\n",
      "epoch:41 step:32399 [D loss: 0.192725, acc.: 90.62%] [G loss: 2.884048]\n",
      "epoch:41 step:32400 [D loss: 0.019734, acc.: 99.22%] [G loss: 1.647909]\n",
      "epoch:41 step:32401 [D loss: 0.009385, acc.: 100.00%] [G loss: 1.275112]\n",
      "epoch:41 step:32402 [D loss: 0.054244, acc.: 98.44%] [G loss: 0.945380]\n",
      "epoch:41 step:32403 [D loss: 0.009094, acc.: 100.00%] [G loss: 1.181859]\n",
      "epoch:41 step:32404 [D loss: 0.002874, acc.: 100.00%] [G loss: 0.846998]\n",
      "epoch:41 step:32405 [D loss: 0.029646, acc.: 98.44%] [G loss: 0.666133]\n",
      "epoch:41 step:32406 [D loss: 0.006789, acc.: 100.00%] [G loss: 0.533449]\n",
      "epoch:41 step:32407 [D loss: 0.005144, acc.: 100.00%] [G loss: 0.532471]\n",
      "epoch:41 step:32408 [D loss: 0.064084, acc.: 100.00%] [G loss: 0.499390]\n",
      "epoch:41 step:32409 [D loss: 0.515587, acc.: 72.66%] [G loss: 2.564971]\n",
      "epoch:41 step:32410 [D loss: 0.012875, acc.: 100.00%] [G loss: 2.683789]\n",
      "epoch:41 step:32411 [D loss: 0.027984, acc.: 100.00%] [G loss: 0.352105]\n",
      "epoch:41 step:32412 [D loss: 0.092307, acc.: 97.66%] [G loss: 2.058549]\n",
      "epoch:41 step:32413 [D loss: 0.004394, acc.: 100.00%] [G loss: 1.130054]\n",
      "epoch:41 step:32414 [D loss: 0.001678, acc.: 100.00%] [G loss: 0.621267]\n",
      "epoch:41 step:32415 [D loss: 0.003186, acc.: 100.00%] [G loss: 0.245291]\n",
      "epoch:41 step:32416 [D loss: 0.041008, acc.: 99.22%] [G loss: 0.181412]\n",
      "epoch:41 step:32417 [D loss: 0.019263, acc.: 99.22%] [G loss: 0.164542]\n",
      "epoch:41 step:32418 [D loss: 0.006882, acc.: 100.00%] [G loss: 0.066823]\n",
      "epoch:41 step:32419 [D loss: 0.016919, acc.: 100.00%] [G loss: 0.024973]\n",
      "epoch:41 step:32420 [D loss: 0.000963, acc.: 100.00%] [G loss: 0.004943]\n",
      "epoch:41 step:32421 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.045418]\n",
      "epoch:41 step:32422 [D loss: 0.005045, acc.: 100.00%] [G loss: 0.021616]\n",
      "epoch:41 step:32423 [D loss: 0.002430, acc.: 100.00%] [G loss: 0.029343]\n",
      "epoch:41 step:32424 [D loss: 0.008989, acc.: 99.22%] [G loss: 0.044644]\n",
      "epoch:41 step:32425 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.074608]\n",
      "epoch:41 step:32426 [D loss: 0.002544, acc.: 100.00%] [G loss: 0.052208]\n",
      "epoch:41 step:32427 [D loss: 0.028136, acc.: 100.00%] [G loss: 0.009042]\n",
      "epoch:41 step:32428 [D loss: 0.122022, acc.: 97.66%] [G loss: 0.292532]\n",
      "epoch:41 step:32429 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.206267]\n",
      "epoch:41 step:32430 [D loss: 0.014762, acc.: 99.22%] [G loss: 0.513237]\n",
      "epoch:41 step:32431 [D loss: 0.034893, acc.: 100.00%] [G loss: 0.043355]\n",
      "epoch:41 step:32432 [D loss: 0.090139, acc.: 95.31%] [G loss: 0.005495]\n",
      "epoch:41 step:32433 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:41 step:32434 [D loss: 0.000746, acc.: 100.00%] [G loss: 0.002372]\n",
      "epoch:41 step:32435 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.002800]\n",
      "epoch:41 step:32436 [D loss: 0.003344, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:41 step:32437 [D loss: 0.089832, acc.: 96.09%] [G loss: 0.087298]\n",
      "epoch:41 step:32438 [D loss: 0.001517, acc.: 100.00%] [G loss: 0.077233]\n",
      "epoch:41 step:32439 [D loss: 0.002608, acc.: 100.00%] [G loss: 0.094720]\n",
      "epoch:41 step:32440 [D loss: 0.030507, acc.: 100.00%] [G loss: 0.069793]\n",
      "epoch:41 step:32441 [D loss: 0.012970, acc.: 100.00%] [G loss: 0.116034]\n",
      "epoch:41 step:32442 [D loss: 0.054416, acc.: 98.44%] [G loss: 0.032793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32443 [D loss: 0.036874, acc.: 100.00%] [G loss: 0.029632]\n",
      "epoch:41 step:32444 [D loss: 0.111925, acc.: 98.44%] [G loss: 0.138515]\n",
      "epoch:41 step:32445 [D loss: 0.021970, acc.: 100.00%] [G loss: 0.091638]\n",
      "epoch:41 step:32446 [D loss: 0.039893, acc.: 98.44%] [G loss: 0.543506]\n",
      "epoch:41 step:32447 [D loss: 0.002240, acc.: 100.00%] [G loss: 0.331402]\n",
      "epoch:41 step:32448 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.447773]\n",
      "epoch:41 step:32449 [D loss: 0.022393, acc.: 100.00%] [G loss: 0.010992]\n",
      "epoch:41 step:32450 [D loss: 0.009460, acc.: 99.22%] [G loss: 0.002220]\n",
      "epoch:41 step:32451 [D loss: 0.076235, acc.: 96.88%] [G loss: 0.000627]\n",
      "epoch:41 step:32452 [D loss: 0.028514, acc.: 99.22%] [G loss: 0.000040]\n",
      "epoch:41 step:32453 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:41 step:32454 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:41 step:32455 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:41 step:32456 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:41 step:32457 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:41 step:32458 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:41 step:32459 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.042074]\n",
      "epoch:41 step:32460 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.004731]\n",
      "epoch:41 step:32461 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.008502]\n",
      "epoch:41 step:32462 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.001121]\n",
      "epoch:41 step:32463 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:41 step:32464 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:41 step:32465 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:41 step:32466 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:41 step:32467 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:41 step:32468 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.009058]\n",
      "epoch:41 step:32469 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:41 step:32470 [D loss: 0.010035, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:41 step:32471 [D loss: 0.043974, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:41 step:32472 [D loss: 0.004566, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:41 step:32473 [D loss: 0.002333, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:41 step:32474 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000331]\n",
      "epoch:41 step:32475 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.001227]\n",
      "epoch:41 step:32476 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:41 step:32477 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002263]\n",
      "epoch:41 step:32478 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.007409]\n",
      "epoch:41 step:32479 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000889]\n",
      "epoch:41 step:32480 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:41 step:32481 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.001130]\n",
      "epoch:41 step:32482 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:41 step:32483 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:41 step:32484 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:41 step:32485 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.001293]\n",
      "epoch:41 step:32486 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001999]\n",
      "epoch:41 step:32487 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:41 step:32488 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.003283]\n",
      "epoch:41 step:32489 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.653268]\n",
      "epoch:41 step:32490 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000730]\n",
      "epoch:41 step:32491 [D loss: 0.100656, acc.: 98.44%] [G loss: 0.070767]\n",
      "epoch:41 step:32492 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.383213]\n",
      "epoch:41 step:32493 [D loss: 0.009510, acc.: 100.00%] [G loss: 0.226286]\n",
      "epoch:41 step:32494 [D loss: 0.009535, acc.: 100.00%] [G loss: 0.117716]\n",
      "epoch:41 step:32495 [D loss: 0.012491, acc.: 100.00%] [G loss: 0.033545]\n",
      "epoch:41 step:32496 [D loss: 0.003439, acc.: 100.00%] [G loss: 0.030184]\n",
      "epoch:41 step:32497 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.030990]\n",
      "epoch:41 step:32498 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.005894]\n",
      "epoch:41 step:32499 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.015196]\n",
      "epoch:41 step:32500 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.002358]\n",
      "epoch:41 step:32501 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.513384]\n",
      "epoch:41 step:32502 [D loss: 0.005739, acc.: 100.00%] [G loss: 0.006462]\n",
      "epoch:41 step:32503 [D loss: 0.000413, acc.: 100.00%] [G loss: 0.006454]\n",
      "epoch:41 step:32504 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.003750]\n",
      "epoch:41 step:32505 [D loss: 0.000928, acc.: 100.00%] [G loss: 0.007392]\n",
      "epoch:41 step:32506 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.001334]\n",
      "epoch:41 step:32507 [D loss: 0.000855, acc.: 100.00%] [G loss: 0.001785]\n",
      "epoch:41 step:32508 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.005638]\n",
      "epoch:41 step:32509 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.002494]\n",
      "epoch:41 step:32510 [D loss: 0.000806, acc.: 100.00%] [G loss: 0.015773]\n",
      "epoch:41 step:32511 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001716]\n",
      "epoch:41 step:32512 [D loss: 0.016360, acc.: 99.22%] [G loss: 0.000789]\n",
      "epoch:41 step:32513 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:41 step:32514 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001847]\n",
      "epoch:41 step:32515 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001669]\n",
      "epoch:41 step:32516 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000691]\n",
      "epoch:41 step:32517 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.002015]\n",
      "epoch:41 step:32518 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000996]\n",
      "epoch:41 step:32519 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:41 step:32520 [D loss: 0.001436, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:41 step:32521 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:41 step:32522 [D loss: 0.001435, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:41 step:32523 [D loss: 0.000999, acc.: 100.00%] [G loss: 0.004672]\n",
      "epoch:41 step:32524 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:41 step:32525 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000897]\n",
      "epoch:41 step:32526 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.001027]\n",
      "epoch:41 step:32527 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.002059]\n",
      "epoch:41 step:32528 [D loss: 0.005347, acc.: 100.00%] [G loss: 0.002083]\n",
      "epoch:41 step:32529 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:41 step:32530 [D loss: 0.008927, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:41 step:32531 [D loss: 0.007353, acc.: 100.00%] [G loss: 0.014629]\n",
      "epoch:41 step:32532 [D loss: 0.010828, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:41 step:32533 [D loss: 0.004340, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:41 step:32534 [D loss: 0.001721, acc.: 100.00%] [G loss: 0.001105]\n",
      "epoch:41 step:32535 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.011514]\n",
      "epoch:41 step:32536 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.003237]\n",
      "epoch:41 step:32537 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.002588]\n",
      "epoch:41 step:32538 [D loss: 0.000734, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:41 step:32539 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:41 step:32540 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:41 step:32541 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:41 step:32542 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:41 step:32543 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:41 step:32544 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:41 step:32545 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:41 step:32546 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:41 step:32547 [D loss: 0.022183, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:41 step:32548 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:41 step:32549 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:41 step:32550 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:41 step:32551 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:41 step:32552 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:41 step:32553 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:41 step:32554 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:41 step:32555 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:41 step:32556 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32557 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:41 step:32558 [D loss: 0.006012, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:41 step:32559 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32560 [D loss: 0.001144, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:41 step:32561 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:41 step:32562 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:41 step:32563 [D loss: 0.001975, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:41 step:32564 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:41 step:32565 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:41 step:32566 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002912]\n",
      "epoch:41 step:32567 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:41 step:32568 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:41 step:32569 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:41 step:32570 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:41 step:32571 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:41 step:32572 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32573 [D loss: 0.008295, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:41 step:32574 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002637]\n",
      "epoch:41 step:32575 [D loss: 0.074029, acc.: 96.09%] [G loss: 0.004172]\n",
      "epoch:41 step:32576 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.297497]\n",
      "epoch:41 step:32577 [D loss: 0.008513, acc.: 100.00%] [G loss: 0.025369]\n",
      "epoch:41 step:32578 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.006210]\n",
      "epoch:41 step:32579 [D loss: 0.036479, acc.: 97.66%] [G loss: 0.008521]\n",
      "epoch:41 step:32580 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.013990]\n",
      "epoch:41 step:32581 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.013206]\n",
      "epoch:41 step:32582 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:41 step:32583 [D loss: 0.004455, acc.: 100.00%] [G loss: 0.004451]\n",
      "epoch:41 step:32584 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:41 step:32585 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:41 step:32586 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:41 step:32587 [D loss: 0.004439, acc.: 100.00%] [G loss: 0.000932]\n",
      "epoch:41 step:32588 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.001527]\n",
      "epoch:41 step:32589 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.002351]\n",
      "epoch:41 step:32590 [D loss: 0.548835, acc.: 76.56%] [G loss: 8.523005]\n",
      "epoch:41 step:32591 [D loss: 0.910609, acc.: 71.88%] [G loss: 0.768225]\n",
      "epoch:41 step:32592 [D loss: 0.018622, acc.: 100.00%] [G loss: 0.122686]\n",
      "epoch:41 step:32593 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.157071]\n",
      "epoch:41 step:32594 [D loss: 0.038106, acc.: 99.22%] [G loss: 2.441780]\n",
      "epoch:41 step:32595 [D loss: 0.001873, acc.: 100.00%] [G loss: 0.010999]\n",
      "epoch:41 step:32596 [D loss: 0.046647, acc.: 98.44%] [G loss: 0.066583]\n",
      "epoch:41 step:32597 [D loss: 0.002213, acc.: 100.00%] [G loss: 0.005597]\n",
      "epoch:41 step:32598 [D loss: 0.001974, acc.: 100.00%] [G loss: 0.003463]\n",
      "epoch:41 step:32599 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.010310]\n",
      "epoch:41 step:32600 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.004656]\n",
      "epoch:41 step:32601 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.007461]\n",
      "epoch:41 step:32602 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.706483]\n",
      "epoch:41 step:32603 [D loss: 0.009135, acc.: 99.22%] [G loss: 0.821149]\n",
      "epoch:41 step:32604 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.012422]\n",
      "epoch:41 step:32605 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.000586]\n",
      "epoch:41 step:32606 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000514]\n",
      "epoch:41 step:32607 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.238465]\n",
      "epoch:41 step:32608 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.004850]\n",
      "epoch:41 step:32609 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.211741]\n",
      "epoch:41 step:32610 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:41 step:32611 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:41 step:32612 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:41 step:32613 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:41 step:32614 [D loss: 0.002207, acc.: 100.00%] [G loss: 0.054942]\n",
      "epoch:41 step:32615 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:41 step:32616 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:41 step:32617 [D loss: 0.002228, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:41 step:32618 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:41 step:32619 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.002832]\n",
      "epoch:41 step:32620 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:41 step:32621 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:41 step:32622 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.001229]\n",
      "epoch:41 step:32623 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.096955]\n",
      "epoch:41 step:32624 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:41 step:32625 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.002635]\n",
      "epoch:41 step:32626 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:41 step:32627 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:41 step:32628 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:41 step:32629 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.006855]\n",
      "epoch:41 step:32630 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.003526]\n",
      "epoch:41 step:32631 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:41 step:32632 [D loss: 0.000713, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:41 step:32633 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000857]\n",
      "epoch:41 step:32634 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:41 step:32635 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002209]\n",
      "epoch:41 step:32636 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000515]\n",
      "epoch:41 step:32637 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.003014]\n",
      "epoch:41 step:32638 [D loss: 0.001829, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:41 step:32639 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:41 step:32640 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:41 step:32641 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.003663]\n",
      "epoch:41 step:32642 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:41 step:32643 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:41 step:32644 [D loss: 0.004706, acc.: 100.00%] [G loss: 0.002757]\n",
      "epoch:41 step:32645 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32646 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:41 step:32647 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:41 step:32648 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:41 step:32649 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:41 step:32650 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:41 step:32651 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:41 step:32652 [D loss: 0.000997, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:41 step:32653 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000776]\n",
      "epoch:41 step:32654 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:41 step:32655 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.003239]\n",
      "epoch:41 step:32656 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:41 step:32657 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:41 step:32658 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:41 step:32659 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:41 step:32660 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:41 step:32661 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:41 step:32662 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001684]\n",
      "epoch:41 step:32663 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:41 step:32664 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:41 step:32665 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:41 step:32666 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32667 [D loss: 0.001026, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:41 step:32668 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001033]\n",
      "epoch:41 step:32669 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:41 step:32670 [D loss: 0.005596, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:41 step:32671 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32672 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32673 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:41 step:32674 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:41 step:32675 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32676 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001038]\n",
      "epoch:41 step:32677 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:41 step:32678 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:41 step:32679 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:41 step:32680 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000738]\n",
      "epoch:41 step:32681 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:41 step:32682 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:41 step:32683 [D loss: 0.657174, acc.: 76.56%] [G loss: 3.767786]\n",
      "epoch:41 step:32684 [D loss: 0.461222, acc.: 81.25%] [G loss: 1.075621]\n",
      "epoch:41 step:32685 [D loss: 0.008126, acc.: 100.00%] [G loss: 0.119576]\n",
      "epoch:41 step:32686 [D loss: 0.173351, acc.: 92.19%] [G loss: 0.022289]\n",
      "epoch:41 step:32687 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.001415]\n",
      "epoch:41 step:32688 [D loss: 0.002053, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:41 step:32689 [D loss: 0.002492, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:41 step:32690 [D loss: 0.280562, acc.: 88.28%] [G loss: 0.534875]\n",
      "epoch:41 step:32691 [D loss: 0.015589, acc.: 100.00%] [G loss: 0.242959]\n",
      "epoch:41 step:32692 [D loss: 0.156105, acc.: 95.31%] [G loss: 0.005156]\n",
      "epoch:41 step:32693 [D loss: 0.002679, acc.: 100.00%] [G loss: 0.374257]\n",
      "epoch:41 step:32694 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.576902]\n",
      "epoch:41 step:32695 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.090239]\n",
      "epoch:41 step:32696 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:41 step:32697 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.156360]\n",
      "epoch:41 step:32698 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.017100]\n",
      "epoch:41 step:32699 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:41 step:32700 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.034839]\n",
      "epoch:41 step:32701 [D loss: 0.002643, acc.: 100.00%] [G loss: 0.017598]\n",
      "epoch:41 step:32702 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:41 step:32703 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.037280]\n",
      "epoch:41 step:32704 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.031305]\n",
      "epoch:41 step:32705 [D loss: 0.003408, acc.: 100.00%] [G loss: 0.023223]\n",
      "epoch:41 step:32706 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.070101]\n",
      "epoch:41 step:32707 [D loss: 0.001012, acc.: 100.00%] [G loss: 0.005681]\n",
      "epoch:41 step:32708 [D loss: 0.001366, acc.: 100.00%] [G loss: 0.006644]\n",
      "epoch:41 step:32709 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:41 step:32710 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002634]\n",
      "epoch:41 step:32711 [D loss: 0.000861, acc.: 100.00%] [G loss: 0.002693]\n",
      "epoch:41 step:32712 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.436718]\n",
      "epoch:41 step:32713 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.003101]\n",
      "epoch:41 step:32714 [D loss: 0.002868, acc.: 100.00%] [G loss: 0.011315]\n",
      "epoch:41 step:32715 [D loss: 0.002312, acc.: 100.00%] [G loss: 0.001201]\n",
      "epoch:41 step:32716 [D loss: 0.001536, acc.: 100.00%] [G loss: 0.003871]\n",
      "epoch:41 step:32717 [D loss: 0.004684, acc.: 100.00%] [G loss: 0.002451]\n",
      "epoch:41 step:32718 [D loss: 0.003021, acc.: 100.00%] [G loss: 0.010689]\n",
      "epoch:41 step:32719 [D loss: 0.029000, acc.: 100.00%] [G loss: 0.005766]\n",
      "epoch:41 step:32720 [D loss: 0.004807, acc.: 100.00%] [G loss: 0.076078]\n",
      "epoch:41 step:32721 [D loss: 0.114862, acc.: 98.44%] [G loss: 0.355509]\n",
      "epoch:41 step:32722 [D loss: 0.004795, acc.: 100.00%] [G loss: 2.074230]\n",
      "epoch:41 step:32723 [D loss: 0.594829, acc.: 76.56%] [G loss: 0.016610]\n",
      "epoch:41 step:32724 [D loss: 0.004064, acc.: 100.00%] [G loss: 1.702799]\n",
      "epoch:41 step:32725 [D loss: 0.056632, acc.: 99.22%] [G loss: 0.091277]\n",
      "epoch:41 step:32726 [D loss: 0.007609, acc.: 100.00%] [G loss: 0.840304]\n",
      "epoch:41 step:32727 [D loss: 0.001827, acc.: 100.00%] [G loss: 0.269580]\n",
      "epoch:41 step:32728 [D loss: 0.003640, acc.: 100.00%] [G loss: 0.008469]\n",
      "epoch:41 step:32729 [D loss: 0.001846, acc.: 100.00%] [G loss: 0.138306]\n",
      "epoch:41 step:32730 [D loss: 0.001914, acc.: 100.00%] [G loss: 0.005497]\n",
      "epoch:41 step:32731 [D loss: 0.001288, acc.: 100.00%] [G loss: 0.011064]\n",
      "epoch:41 step:32732 [D loss: 0.006236, acc.: 100.00%] [G loss: 0.019134]\n",
      "epoch:41 step:32733 [D loss: 0.001587, acc.: 100.00%] [G loss: 0.059874]\n",
      "epoch:41 step:32734 [D loss: 0.018480, acc.: 100.00%] [G loss: 0.009200]\n",
      "epoch:41 step:32735 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.026730]\n",
      "epoch:41 step:32736 [D loss: 0.000863, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:41 step:32737 [D loss: 0.022035, acc.: 99.22%] [G loss: 0.033475]\n",
      "epoch:41 step:32738 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.003154]\n",
      "epoch:41 step:32739 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.009536]\n",
      "epoch:41 step:32740 [D loss: 0.005089, acc.: 100.00%] [G loss: 0.002904]\n",
      "epoch:41 step:32741 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.020731]\n",
      "epoch:41 step:32742 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.007147]\n",
      "epoch:41 step:32743 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.027369]\n",
      "epoch:41 step:32744 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.051942]\n",
      "epoch:41 step:32745 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.002626]\n",
      "epoch:41 step:32746 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.004194]\n",
      "epoch:41 step:32747 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.947127]\n",
      "epoch:41 step:32748 [D loss: 0.007327, acc.: 100.00%] [G loss: 0.016179]\n",
      "epoch:41 step:32749 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.001174]\n",
      "epoch:41 step:32750 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.008357]\n",
      "epoch:41 step:32751 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:41 step:32752 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.006152]\n",
      "epoch:41 step:32753 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.004379]\n",
      "epoch:41 step:32754 [D loss: 0.025597, acc.: 100.00%] [G loss: 0.008497]\n",
      "epoch:41 step:32755 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.004814]\n",
      "epoch:41 step:32756 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:41 step:32757 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.004819]\n",
      "epoch:41 step:32758 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.005693]\n",
      "epoch:41 step:32759 [D loss: 0.001538, acc.: 100.00%] [G loss: 0.054539]\n",
      "epoch:41 step:32760 [D loss: 0.005649, acc.: 100.00%] [G loss: 0.002458]\n",
      "epoch:41 step:32761 [D loss: 0.057212, acc.: 98.44%] [G loss: 0.075003]\n",
      "epoch:41 step:32762 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.124617]\n",
      "epoch:41 step:32763 [D loss: 0.044845, acc.: 99.22%] [G loss: 0.012626]\n",
      "epoch:41 step:32764 [D loss: 0.001107, acc.: 100.00%] [G loss: 0.003046]\n",
      "epoch:41 step:32765 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.010469]\n",
      "epoch:41 step:32766 [D loss: 0.001304, acc.: 100.00%] [G loss: 0.003826]\n",
      "epoch:41 step:32767 [D loss: 0.208691, acc.: 88.28%] [G loss: 1.312770]\n",
      "epoch:41 step:32768 [D loss: 0.036025, acc.: 97.66%] [G loss: 2.117269]\n",
      "epoch:41 step:32769 [D loss: 0.772937, acc.: 71.09%] [G loss: 0.019690]\n",
      "epoch:41 step:32770 [D loss: 0.016771, acc.: 100.00%] [G loss: 0.001268]\n",
      "epoch:41 step:32771 [D loss: 0.004925, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:41 step:32772 [D loss: 0.034656, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:41 step:32773 [D loss: 0.005587, acc.: 100.00%] [G loss: 0.006082]\n",
      "epoch:41 step:32774 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.001178]\n",
      "epoch:41 step:32775 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:41 step:32776 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:41 step:32777 [D loss: 0.032165, acc.: 100.00%] [G loss: 0.004482]\n",
      "epoch:41 step:32778 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001232]\n",
      "epoch:41 step:32779 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.013928]\n",
      "epoch:41 step:32780 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.001223]\n",
      "epoch:41 step:32781 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001074]\n",
      "epoch:41 step:32782 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.006634]\n",
      "epoch:41 step:32783 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.032552]\n",
      "epoch:41 step:32784 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.304844]\n",
      "epoch:41 step:32785 [D loss: 0.004622, acc.: 100.00%] [G loss: 0.005449]\n",
      "epoch:41 step:32786 [D loss: 0.023940, acc.: 99.22%] [G loss: 0.006436]\n",
      "epoch:41 step:32787 [D loss: 0.000896, acc.: 100.00%] [G loss: 0.038399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32788 [D loss: 0.003879, acc.: 100.00%] [G loss: 0.007307]\n",
      "epoch:41 step:32789 [D loss: 0.108818, acc.: 96.09%] [G loss: 0.719718]\n",
      "epoch:41 step:32790 [D loss: 0.002985, acc.: 100.00%] [G loss: 2.896209]\n",
      "epoch:41 step:32791 [D loss: 0.013314, acc.: 100.00%] [G loss: 2.608553]\n",
      "epoch:41 step:32792 [D loss: 0.338672, acc.: 85.94%] [G loss: 0.117121]\n",
      "epoch:41 step:32793 [D loss: 0.037476, acc.: 99.22%] [G loss: 0.287563]\n",
      "epoch:41 step:32794 [D loss: 0.216611, acc.: 90.62%] [G loss: 2.720352]\n",
      "epoch:41 step:32795 [D loss: 0.030329, acc.: 99.22%] [G loss: 0.000672]\n",
      "epoch:41 step:32796 [D loss: 0.026491, acc.: 99.22%] [G loss: 1.951704]\n",
      "epoch:41 step:32797 [D loss: 0.003517, acc.: 100.00%] [G loss: 1.545574]\n",
      "epoch:41 step:32798 [D loss: 0.104924, acc.: 96.88%] [G loss: 0.419749]\n",
      "epoch:41 step:32799 [D loss: 0.060716, acc.: 98.44%] [G loss: 1.790085]\n",
      "epoch:41 step:32800 [D loss: 0.076992, acc.: 97.66%] [G loss: 0.000337]\n",
      "epoch:41 step:32801 [D loss: 0.057945, acc.: 97.66%] [G loss: 0.001206]\n",
      "epoch:41 step:32802 [D loss: 0.012216, acc.: 99.22%] [G loss: 0.007981]\n",
      "epoch:42 step:32803 [D loss: 0.124432, acc.: 94.53%] [G loss: 0.000055]\n",
      "epoch:42 step:32804 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:42 step:32805 [D loss: 0.001491, acc.: 100.00%] [G loss: 0.131426]\n",
      "epoch:42 step:32806 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.097349]\n",
      "epoch:42 step:32807 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.018633]\n",
      "epoch:42 step:32808 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:42 step:32809 [D loss: 0.001731, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:42 step:32810 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:42 step:32811 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:42 step:32812 [D loss: 0.002336, acc.: 100.00%] [G loss: 0.021036]\n",
      "epoch:42 step:32813 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:42 step:32814 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.004537]\n",
      "epoch:42 step:32815 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:32816 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:42 step:32817 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:42 step:32818 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.007012]\n",
      "epoch:42 step:32819 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:42 step:32820 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.226101]\n",
      "epoch:42 step:32821 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:32822 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:42 step:32823 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:32824 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.066692]\n",
      "epoch:42 step:32825 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.014180]\n",
      "epoch:42 step:32826 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.002346]\n",
      "epoch:42 step:32827 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:32828 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:42 step:32829 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.003106]\n",
      "epoch:42 step:32830 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:32831 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:42 step:32832 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:42 step:32833 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:32834 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:32835 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:42 step:32836 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.002080]\n",
      "epoch:42 step:32837 [D loss: 0.003122, acc.: 100.00%] [G loss: 0.005301]\n",
      "epoch:42 step:32838 [D loss: 0.000921, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:42 step:32839 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:42 step:32840 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:42 step:32841 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:42 step:32842 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:42 step:32843 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.030571]\n",
      "epoch:42 step:32844 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:32845 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002157]\n",
      "epoch:42 step:32846 [D loss: 0.007053, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:42 step:32847 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.001379]\n",
      "epoch:42 step:32848 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:42 step:32849 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000482]\n",
      "epoch:42 step:32850 [D loss: 0.001372, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:32851 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:42 step:32852 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.001603]\n",
      "epoch:42 step:32853 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:42 step:32854 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:32855 [D loss: 0.000937, acc.: 100.00%] [G loss: 0.003605]\n",
      "epoch:42 step:32856 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:42 step:32857 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:32858 [D loss: 0.001200, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:32859 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:42 step:32860 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:42 step:32861 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.005271]\n",
      "epoch:42 step:32862 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:32863 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001605]\n",
      "epoch:42 step:32864 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:42 step:32865 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.002197]\n",
      "epoch:42 step:32866 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:32867 [D loss: 0.016748, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:42 step:32868 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:32869 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:32870 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:32871 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:32872 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:42 step:32873 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:32874 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:32875 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32876 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:32877 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:42 step:32878 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:42 step:32879 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:42 step:32880 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:42 step:32881 [D loss: 0.002020, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:32882 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32883 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32884 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:32885 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:42 step:32886 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:32887 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:42 step:32888 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.002581]\n",
      "epoch:42 step:32889 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32890 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:32891 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:32892 [D loss: 0.002207, acc.: 100.00%] [G loss: 0.001579]\n",
      "epoch:42 step:32893 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:42 step:32894 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32895 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:42 step:32896 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:42 step:32897 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32898 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:42 step:32899 [D loss: 0.004052, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:32900 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:42 step:32901 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000965]\n",
      "epoch:42 step:32902 [D loss: 0.935635, acc.: 63.28%] [G loss: 9.803320]\n",
      "epoch:42 step:32903 [D loss: 2.155485, acc.: 52.34%] [G loss: 1.743938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32904 [D loss: 0.019631, acc.: 100.00%] [G loss: 0.303955]\n",
      "epoch:42 step:32905 [D loss: 0.032046, acc.: 100.00%] [G loss: 0.090861]\n",
      "epoch:42 step:32906 [D loss: 0.009806, acc.: 100.00%] [G loss: 0.026730]\n",
      "epoch:42 step:32907 [D loss: 0.011775, acc.: 100.00%] [G loss: 3.735280]\n",
      "epoch:42 step:32908 [D loss: 0.010067, acc.: 100.00%] [G loss: 0.009106]\n",
      "epoch:42 step:32909 [D loss: 0.007987, acc.: 100.00%] [G loss: 0.012047]\n",
      "epoch:42 step:32910 [D loss: 0.007235, acc.: 100.00%] [G loss: 0.015029]\n",
      "epoch:42 step:32911 [D loss: 0.005523, acc.: 100.00%] [G loss: 0.003166]\n",
      "epoch:42 step:32912 [D loss: 0.004607, acc.: 100.00%] [G loss: 2.315165]\n",
      "epoch:42 step:32913 [D loss: 0.008701, acc.: 100.00%] [G loss: 0.009245]\n",
      "epoch:42 step:32914 [D loss: 0.001521, acc.: 100.00%] [G loss: 0.000786]\n",
      "epoch:42 step:32915 [D loss: 0.002581, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:42 step:32916 [D loss: 0.002158, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:42 step:32917 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:42 step:32918 [D loss: 0.004403, acc.: 100.00%] [G loss: 0.030974]\n",
      "epoch:42 step:32919 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.486314]\n",
      "epoch:42 step:32920 [D loss: 0.002149, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:42 step:32921 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.272048]\n",
      "epoch:42 step:32922 [D loss: 0.001211, acc.: 100.00%] [G loss: 0.002143]\n",
      "epoch:42 step:32923 [D loss: 0.001726, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:42 step:32924 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:42 step:32925 [D loss: 0.000939, acc.: 100.00%] [G loss: 0.060348]\n",
      "epoch:42 step:32926 [D loss: 0.006497, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:42 step:32927 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:42 step:32928 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:42 step:32929 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.000654]\n",
      "epoch:42 step:32930 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:42 step:32931 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:42 step:32932 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.074892]\n",
      "epoch:42 step:32933 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.025486]\n",
      "epoch:42 step:32934 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.002314]\n",
      "epoch:42 step:32935 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:42 step:32936 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.138599]\n",
      "epoch:42 step:32937 [D loss: 0.008270, acc.: 100.00%] [G loss: 0.004029]\n",
      "epoch:42 step:32938 [D loss: 0.012812, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:42 step:32939 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.006437]\n",
      "epoch:42 step:32940 [D loss: 0.004823, acc.: 100.00%] [G loss: 0.000391]\n",
      "epoch:42 step:32941 [D loss: 0.001139, acc.: 100.00%] [G loss: 0.006193]\n",
      "epoch:42 step:32942 [D loss: 0.016613, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:42 step:32943 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:42 step:32944 [D loss: 0.002334, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:42 step:32945 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.003461]\n",
      "epoch:42 step:32946 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.018505]\n",
      "epoch:42 step:32947 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.017338]\n",
      "epoch:42 step:32948 [D loss: 0.014229, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:42 step:32949 [D loss: 0.000928, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:42 step:32950 [D loss: 0.002539, acc.: 100.00%] [G loss: 0.000756]\n",
      "epoch:42 step:32951 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:42 step:32952 [D loss: 0.001143, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:42 step:32953 [D loss: 0.001086, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:42 step:32954 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.012635]\n",
      "epoch:42 step:32955 [D loss: 0.001745, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:42 step:32956 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:42 step:32957 [D loss: 0.000487, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:42 step:32958 [D loss: 0.001675, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:42 step:32959 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.008598]\n",
      "epoch:42 step:32960 [D loss: 0.001114, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:42 step:32961 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.022782]\n",
      "epoch:42 step:32962 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:42 step:32963 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:42 step:32964 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:42 step:32965 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:42 step:32966 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:42 step:32967 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.001670]\n",
      "epoch:42 step:32968 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.006073]\n",
      "epoch:42 step:32969 [D loss: 0.000796, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:42 step:32970 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:42 step:32971 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.026387]\n",
      "epoch:42 step:32972 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000828]\n",
      "epoch:42 step:32973 [D loss: 0.020082, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:42 step:32974 [D loss: 0.000751, acc.: 100.00%] [G loss: 0.001576]\n",
      "epoch:42 step:32975 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:42 step:32976 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:42 step:32977 [D loss: 0.001404, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:42 step:32978 [D loss: 0.003548, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:42 step:32979 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:42 step:32980 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.008836]\n",
      "epoch:42 step:32981 [D loss: 0.004527, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:42 step:32982 [D loss: 0.001819, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:42 step:32983 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:42 step:32984 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.007937]\n",
      "epoch:42 step:32985 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:42 step:32986 [D loss: 0.000446, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:42 step:32987 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:42 step:32988 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:32989 [D loss: 0.001281, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:42 step:32990 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:42 step:32991 [D loss: 0.000759, acc.: 100.00%] [G loss: 0.006122]\n",
      "epoch:42 step:32992 [D loss: 0.000901, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:42 step:32993 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:42 step:32994 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.002423]\n",
      "epoch:42 step:32995 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:42 step:32996 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:32997 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:42 step:32998 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:42 step:32999 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.002811]\n",
      "epoch:42 step:33000 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.002022]\n",
      "epoch:42 step:33001 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:42 step:33002 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.012024]\n",
      "epoch:42 step:33003 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:42 step:33004 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:42 step:33005 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000675]\n",
      "epoch:42 step:33006 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000775]\n",
      "epoch:42 step:33007 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:42 step:33008 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.000991]\n",
      "epoch:42 step:33009 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:42 step:33010 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:42 step:33011 [D loss: 0.002117, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:42 step:33012 [D loss: 0.003000, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:42 step:33013 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:42 step:33014 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:42 step:33015 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:42 step:33016 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.001514]\n",
      "epoch:42 step:33017 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:33018 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:42 step:33019 [D loss: 0.006477, acc.: 100.00%] [G loss: 0.000026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33020 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:33021 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:33022 [D loss: 0.006517, acc.: 100.00%] [G loss: 0.003238]\n",
      "epoch:42 step:33023 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33024 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.001359]\n",
      "epoch:42 step:33025 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:42 step:33026 [D loss: 0.010331, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33027 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.002010]\n",
      "epoch:42 step:33028 [D loss: 0.011631, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:42 step:33029 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:42 step:33030 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.127139]\n",
      "epoch:42 step:33031 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.050039]\n",
      "epoch:42 step:33032 [D loss: 0.013560, acc.: 99.22%] [G loss: 0.000013]\n",
      "epoch:42 step:33033 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:42 step:33034 [D loss: 0.001816, acc.: 100.00%] [G loss: 0.004382]\n",
      "epoch:42 step:33035 [D loss: 0.001077, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:33036 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.002372]\n",
      "epoch:42 step:33037 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.001635]\n",
      "epoch:42 step:33038 [D loss: 0.002864, acc.: 100.00%] [G loss: 0.001233]\n",
      "epoch:42 step:33039 [D loss: 0.035280, acc.: 100.00%] [G loss: 0.025723]\n",
      "epoch:42 step:33040 [D loss: 0.005534, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:42 step:33041 [D loss: 0.000945, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:42 step:33042 [D loss: 0.016841, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:33043 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.015252]\n",
      "epoch:42 step:33044 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33045 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:42 step:33046 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.030086]\n",
      "epoch:42 step:33047 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:33048 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33049 [D loss: 0.002551, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:42 step:33050 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:42 step:33051 [D loss: 0.002019, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33052 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:42 step:33053 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:42 step:33054 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33055 [D loss: 0.000887, acc.: 100.00%] [G loss: 0.001370]\n",
      "epoch:42 step:33056 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33057 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:42 step:33058 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33059 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001259]\n",
      "epoch:42 step:33060 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33061 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.047999]\n",
      "epoch:42 step:33062 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.003639]\n",
      "epoch:42 step:33063 [D loss: 0.009635, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:42 step:33064 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33065 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33066 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33067 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.001004]\n",
      "epoch:42 step:33068 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:42 step:33069 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33070 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:42 step:33071 [D loss: 0.005360, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33072 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33073 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33074 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33075 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33076 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:42 step:33077 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33078 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33079 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33080 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33081 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33082 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000954]\n",
      "epoch:42 step:33083 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:42 step:33084 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:42 step:33085 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:42 step:33086 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:42 step:33087 [D loss: 0.026616, acc.: 100.00%] [G loss: 0.007623]\n",
      "epoch:42 step:33088 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.095733]\n",
      "epoch:42 step:33089 [D loss: 0.005641, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:42 step:33090 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:42 step:33091 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:42 step:33092 [D loss: 0.000642, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:42 step:33093 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:42 step:33094 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.095614]\n",
      "epoch:42 step:33095 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.010110]\n",
      "epoch:42 step:33096 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:42 step:33097 [D loss: 0.007268, acc.: 100.00%] [G loss: 0.018044]\n",
      "epoch:42 step:33098 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:42 step:33099 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.005302]\n",
      "epoch:42 step:33100 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.006333]\n",
      "epoch:42 step:33101 [D loss: 0.007534, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:42 step:33102 [D loss: 0.006815, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:42 step:33103 [D loss: 0.001689, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:42 step:33104 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33105 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33106 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33107 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002811]\n",
      "epoch:42 step:33108 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33109 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:33110 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33111 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:42 step:33112 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:42 step:33113 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:42 step:33114 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000932]\n",
      "epoch:42 step:33115 [D loss: 0.000873, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:42 step:33116 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.003177]\n",
      "epoch:42 step:33117 [D loss: 0.069224, acc.: 96.88%] [G loss: 0.000000]\n",
      "epoch:42 step:33118 [D loss: 0.001533, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33119 [D loss: 0.008344, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:33120 [D loss: 0.002367, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:42 step:33121 [D loss: 0.007969, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:42 step:33122 [D loss: 0.002164, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:42 step:33123 [D loss: 0.024523, acc.: 100.00%] [G loss: 0.003673]\n",
      "epoch:42 step:33124 [D loss: 0.027844, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:42 step:33125 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:42 step:33126 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.371687]\n",
      "epoch:42 step:33127 [D loss: 0.001374, acc.: 100.00%] [G loss: 0.196709]\n",
      "epoch:42 step:33128 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:42 step:33129 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.030954]\n",
      "epoch:42 step:33130 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:42 step:33131 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:42 step:33132 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.134087]\n",
      "epoch:42 step:33133 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:42 step:33134 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.030023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33135 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:42 step:33136 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.015238]\n",
      "epoch:42 step:33137 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.032023]\n",
      "epoch:42 step:33138 [D loss: 0.010036, acc.: 100.00%] [G loss: 0.000758]\n",
      "epoch:42 step:33139 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.017358]\n",
      "epoch:42 step:33140 [D loss: 0.014953, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:42 step:33141 [D loss: 0.000801, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:42 step:33142 [D loss: 0.007070, acc.: 100.00%] [G loss: 0.002259]\n",
      "epoch:42 step:33143 [D loss: 0.002584, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33144 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:42 step:33145 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.014023]\n",
      "epoch:42 step:33146 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:42 step:33147 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.009629]\n",
      "epoch:42 step:33148 [D loss: 0.001285, acc.: 100.00%] [G loss: 0.006258]\n",
      "epoch:42 step:33149 [D loss: 0.001420, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:42 step:33150 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:42 step:33151 [D loss: 0.001788, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33152 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:42 step:33153 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.018930]\n",
      "epoch:42 step:33154 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:33155 [D loss: 0.014539, acc.: 100.00%] [G loss: 0.001290]\n",
      "epoch:42 step:33156 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33157 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33158 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000666]\n",
      "epoch:42 step:33159 [D loss: 0.002163, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33160 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:42 step:33161 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:42 step:33162 [D loss: 0.002308, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:42 step:33163 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33164 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33165 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:42 step:33166 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33167 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:42 step:33168 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000512]\n",
      "epoch:42 step:33169 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:42 step:33170 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:42 step:33171 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33172 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:42 step:33173 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:42 step:33174 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:33175 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:42 step:33176 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33177 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33178 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33179 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33180 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:42 step:33181 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:42 step:33182 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:42 step:33183 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:42 step:33184 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33185 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:42 step:33186 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33187 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33188 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:33189 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33190 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:42 step:33191 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33192 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:42 step:33193 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33194 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:42 step:33195 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:33196 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:42 step:33197 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33198 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33199 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:42 step:33200 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:42 step:33201 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:42 step:33202 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:42 step:33203 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33204 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33205 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001700]\n",
      "epoch:42 step:33206 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:42 step:33207 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33208 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:42 step:33209 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:42 step:33210 [D loss: 0.001751, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33211 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33212 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:42 step:33213 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:42 step:33214 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33215 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:42 step:33216 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:42 step:33217 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33218 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33219 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33220 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:42 step:33221 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33222 [D loss: 0.001425, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:33223 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:42 step:33224 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:42 step:33225 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:42 step:33226 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33227 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:42 step:33228 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:42 step:33229 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:42 step:33230 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33231 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:42 step:33232 [D loss: 0.012825, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33233 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:42 step:33234 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:42 step:33235 [D loss: 0.012402, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33236 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:42 step:33237 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33238 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000697]\n",
      "epoch:42 step:33239 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001883]\n",
      "epoch:42 step:33240 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33241 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:42 step:33242 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:42 step:33243 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33244 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33245 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:42 step:33246 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000841]\n",
      "epoch:42 step:33247 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33248 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33249 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33250 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.002682]\n",
      "epoch:42 step:33251 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33252 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001548]\n",
      "epoch:42 step:33253 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33254 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33255 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33256 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33257 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33258 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:42 step:33259 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33260 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:42 step:33261 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:42 step:33262 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:42 step:33263 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33264 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33265 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:42 step:33266 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33267 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.008894]\n",
      "epoch:42 step:33268 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:42 step:33269 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:42 step:33270 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33271 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33272 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33273 [D loss: 0.002253, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:33274 [D loss: 0.006488, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:33275 [D loss: 0.022902, acc.: 100.00%] [G loss: 0.001531]\n",
      "epoch:42 step:33276 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000877]\n",
      "epoch:42 step:33277 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:42 step:33278 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.144946]\n",
      "epoch:42 step:33279 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.113113]\n",
      "epoch:42 step:33280 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.010010]\n",
      "epoch:42 step:33281 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.001025]\n",
      "epoch:42 step:33282 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.002379]\n",
      "epoch:42 step:33283 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:42 step:33284 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.007410]\n",
      "epoch:42 step:33285 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.010529]\n",
      "epoch:42 step:33286 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:42 step:33287 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:42 step:33288 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:42 step:33289 [D loss: 0.001026, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:42 step:33290 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:42 step:33291 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000819]\n",
      "epoch:42 step:33292 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:42 step:33293 [D loss: 0.002231, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:42 step:33294 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000886]\n",
      "epoch:42 step:33295 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:42 step:33296 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:42 step:33297 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33298 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:42 step:33299 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:42 step:33300 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:42 step:33301 [D loss: 0.001257, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:42 step:33302 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:42 step:33303 [D loss: 0.001458, acc.: 100.00%] [G loss: 0.001141]\n",
      "epoch:42 step:33304 [D loss: 0.005697, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:42 step:33305 [D loss: 0.005318, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:42 step:33306 [D loss: 0.012871, acc.: 100.00%] [G loss: 0.002514]\n",
      "epoch:42 step:33307 [D loss: 0.022731, acc.: 99.22%] [G loss: 0.013986]\n",
      "epoch:42 step:33308 [D loss: 0.006986, acc.: 100.00%] [G loss: 0.297353]\n",
      "epoch:42 step:33309 [D loss: 0.022214, acc.: 99.22%] [G loss: 0.010490]\n",
      "epoch:42 step:33310 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.004974]\n",
      "epoch:42 step:33311 [D loss: 0.052566, acc.: 99.22%] [G loss: 0.178848]\n",
      "epoch:42 step:33312 [D loss: 0.001845, acc.: 100.00%] [G loss: 0.153088]\n",
      "epoch:42 step:33313 [D loss: 0.015328, acc.: 99.22%] [G loss: 0.074290]\n",
      "epoch:42 step:33314 [D loss: 0.003385, acc.: 100.00%] [G loss: 0.004328]\n",
      "epoch:42 step:33315 [D loss: 0.001809, acc.: 100.00%] [G loss: 0.003817]\n",
      "epoch:42 step:33316 [D loss: 0.000842, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:42 step:33317 [D loss: 0.008396, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:42 step:33318 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:42 step:33319 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:42 step:33320 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:42 step:33321 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:42 step:33322 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.006157]\n",
      "epoch:42 step:33323 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:42 step:33324 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.001065]\n",
      "epoch:42 step:33325 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:42 step:33326 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:42 step:33327 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33328 [D loss: 0.020726, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:42 step:33329 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:42 step:33330 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:42 step:33331 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:42 step:33332 [D loss: 0.266723, acc.: 85.16%] [G loss: 4.067880]\n",
      "epoch:42 step:33333 [D loss: 0.139384, acc.: 92.97%] [G loss: 1.861866]\n",
      "epoch:42 step:33334 [D loss: 2.786234, acc.: 26.56%] [G loss: 5.867436]\n",
      "epoch:42 step:33335 [D loss: 0.570959, acc.: 79.69%] [G loss: 2.948548]\n",
      "epoch:42 step:33336 [D loss: 0.013452, acc.: 100.00%] [G loss: 1.877097]\n",
      "epoch:42 step:33337 [D loss: 0.112881, acc.: 96.88%] [G loss: 0.920926]\n",
      "epoch:42 step:33338 [D loss: 0.051217, acc.: 100.00%] [G loss: 1.826691]\n",
      "epoch:42 step:33339 [D loss: 0.050142, acc.: 100.00%] [G loss: 2.208118]\n",
      "epoch:42 step:33340 [D loss: 0.215520, acc.: 94.53%] [G loss: 3.545665]\n",
      "epoch:42 step:33341 [D loss: 0.286403, acc.: 85.16%] [G loss: 0.050237]\n",
      "epoch:42 step:33342 [D loss: 0.014426, acc.: 100.00%] [G loss: 4.613301]\n",
      "epoch:42 step:33343 [D loss: 0.015387, acc.: 100.00%] [G loss: 3.059915]\n",
      "epoch:42 step:33344 [D loss: 0.070897, acc.: 96.88%] [G loss: 0.220655]\n",
      "epoch:42 step:33345 [D loss: 0.018905, acc.: 99.22%] [G loss: 2.167498]\n",
      "epoch:42 step:33346 [D loss: 0.034999, acc.: 100.00%] [G loss: 0.557684]\n",
      "epoch:42 step:33347 [D loss: 0.021274, acc.: 100.00%] [G loss: 1.492007]\n",
      "epoch:42 step:33348 [D loss: 0.034639, acc.: 100.00%] [G loss: 0.961201]\n",
      "epoch:42 step:33349 [D loss: 0.079201, acc.: 97.66%] [G loss: 1.319204]\n",
      "epoch:42 step:33350 [D loss: 0.023219, acc.: 100.00%] [G loss: 0.066952]\n",
      "epoch:42 step:33351 [D loss: 0.061583, acc.: 99.22%] [G loss: 0.008558]\n",
      "epoch:42 step:33352 [D loss: 0.011509, acc.: 100.00%] [G loss: 0.006199]\n",
      "epoch:42 step:33353 [D loss: 0.003322, acc.: 100.00%] [G loss: 0.059962]\n",
      "epoch:42 step:33354 [D loss: 0.002301, acc.: 100.00%] [G loss: 0.001121]\n",
      "epoch:42 step:33355 [D loss: 0.020328, acc.: 100.00%] [G loss: 0.171012]\n",
      "epoch:42 step:33356 [D loss: 0.005123, acc.: 100.00%] [G loss: 0.091146]\n",
      "epoch:42 step:33357 [D loss: 0.006337, acc.: 99.22%] [G loss: 0.019591]\n",
      "epoch:42 step:33358 [D loss: 0.003472, acc.: 100.00%] [G loss: 0.007243]\n",
      "epoch:42 step:33359 [D loss: 0.003748, acc.: 100.00%] [G loss: 0.010330]\n",
      "epoch:42 step:33360 [D loss: 0.005295, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:42 step:33361 [D loss: 0.002344, acc.: 100.00%] [G loss: 0.015386]\n",
      "epoch:42 step:33362 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:42 step:33363 [D loss: 0.100927, acc.: 97.66%] [G loss: 3.531999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33364 [D loss: 0.028074, acc.: 98.44%] [G loss: 0.380786]\n",
      "epoch:42 step:33365 [D loss: 0.098598, acc.: 95.31%] [G loss: 0.016902]\n",
      "epoch:42 step:33366 [D loss: 0.003378, acc.: 100.00%] [G loss: 0.666104]\n",
      "epoch:42 step:33367 [D loss: 0.005558, acc.: 100.00%] [G loss: 0.001483]\n",
      "epoch:42 step:33368 [D loss: 0.040075, acc.: 99.22%] [G loss: 0.003687]\n",
      "epoch:42 step:33369 [D loss: 0.012210, acc.: 99.22%] [G loss: 0.146467]\n",
      "epoch:42 step:33370 [D loss: 0.018684, acc.: 100.00%] [G loss: 0.014669]\n",
      "epoch:42 step:33371 [D loss: 0.000945, acc.: 100.00%] [G loss: 0.024899]\n",
      "epoch:42 step:33372 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.128124]\n",
      "epoch:42 step:33373 [D loss: 0.055958, acc.: 98.44%] [G loss: 0.003670]\n",
      "epoch:42 step:33374 [D loss: 0.001901, acc.: 100.00%] [G loss: 0.000896]\n",
      "epoch:42 step:33375 [D loss: 0.002432, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:42 step:33376 [D loss: 0.000852, acc.: 100.00%] [G loss: 0.001105]\n",
      "epoch:42 step:33377 [D loss: 0.029689, acc.: 99.22%] [G loss: 0.001360]\n",
      "epoch:42 step:33378 [D loss: 0.003187, acc.: 100.00%] [G loss: 0.007246]\n",
      "epoch:42 step:33379 [D loss: 0.001178, acc.: 100.00%] [G loss: 0.015092]\n",
      "epoch:42 step:33380 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.005426]\n",
      "epoch:42 step:33381 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.001945]\n",
      "epoch:42 step:33382 [D loss: 0.007474, acc.: 100.00%] [G loss: 0.010576]\n",
      "epoch:42 step:33383 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.001206]\n",
      "epoch:42 step:33384 [D loss: 0.002910, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:42 step:33385 [D loss: 0.027238, acc.: 100.00%] [G loss: 0.004066]\n",
      "epoch:42 step:33386 [D loss: 0.001196, acc.: 100.00%] [G loss: 0.007459]\n",
      "epoch:42 step:33387 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.002309]\n",
      "epoch:42 step:33388 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:42 step:33389 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.014069]\n",
      "epoch:42 step:33390 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.004826]\n",
      "epoch:42 step:33391 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.001820]\n",
      "epoch:42 step:33392 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:42 step:33393 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:42 step:33394 [D loss: 0.001529, acc.: 100.00%] [G loss: 0.068037]\n",
      "epoch:42 step:33395 [D loss: 0.004875, acc.: 100.00%] [G loss: 0.058997]\n",
      "epoch:42 step:33396 [D loss: 0.001460, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:42 step:33397 [D loss: 0.002570, acc.: 100.00%] [G loss: 0.000806]\n",
      "epoch:42 step:33398 [D loss: 0.003956, acc.: 100.00%] [G loss: 0.000576]\n",
      "epoch:42 step:33399 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:42 step:33400 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:42 step:33401 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.002533]\n",
      "epoch:42 step:33402 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:42 step:33403 [D loss: 0.002398, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:42 step:33404 [D loss: 0.007924, acc.: 100.00%] [G loss: 0.011503]\n",
      "epoch:42 step:33405 [D loss: 0.030764, acc.: 100.00%] [G loss: 0.038219]\n",
      "epoch:42 step:33406 [D loss: 0.089383, acc.: 96.09%] [G loss: 0.003202]\n",
      "epoch:42 step:33407 [D loss: 0.007113, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:42 step:33408 [D loss: 0.000887, acc.: 100.00%] [G loss: 0.002478]\n",
      "epoch:42 step:33409 [D loss: 0.035952, acc.: 100.00%] [G loss: 0.009375]\n",
      "epoch:42 step:33410 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.014558]\n",
      "epoch:42 step:33411 [D loss: 0.006857, acc.: 100.00%] [G loss: 0.033880]\n",
      "epoch:42 step:33412 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.003492]\n",
      "epoch:42 step:33413 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.016714]\n",
      "epoch:42 step:33414 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.013157]\n",
      "epoch:42 step:33415 [D loss: 0.114823, acc.: 94.53%] [G loss: 0.001513]\n",
      "epoch:42 step:33416 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:33417 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33418 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33419 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33420 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33421 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:42 step:33422 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003144]\n",
      "epoch:42 step:33423 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:42 step:33424 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:42 step:33425 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33426 [D loss: 0.209242, acc.: 92.97%] [G loss: 3.114563]\n",
      "epoch:42 step:33427 [D loss: 0.001858, acc.: 100.00%] [G loss: 0.500247]\n",
      "epoch:42 step:33428 [D loss: 1.042727, acc.: 73.44%] [G loss: 0.000027]\n",
      "epoch:42 step:33429 [D loss: 0.024108, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:42 step:33430 [D loss: 0.737648, acc.: 72.66%] [G loss: 4.678626]\n",
      "epoch:42 step:33431 [D loss: 0.000816, acc.: 100.00%] [G loss: 2.819338]\n",
      "epoch:42 step:33432 [D loss: 0.519725, acc.: 79.69%] [G loss: 4.221717]\n",
      "epoch:42 step:33433 [D loss: 0.007084, acc.: 100.00%] [G loss: 0.071728]\n",
      "epoch:42 step:33434 [D loss: 0.008709, acc.: 100.00%] [G loss: 0.003169]\n",
      "epoch:42 step:33435 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.014517]\n",
      "epoch:42 step:33436 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.004864]\n",
      "epoch:42 step:33437 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.619328]\n",
      "epoch:42 step:33438 [D loss: 0.007652, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:42 step:33439 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.331194]\n",
      "epoch:42 step:33440 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.002662]\n",
      "epoch:42 step:33441 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.407336]\n",
      "epoch:42 step:33442 [D loss: 0.036100, acc.: 99.22%] [G loss: 0.000308]\n",
      "epoch:42 step:33443 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.003280]\n",
      "epoch:42 step:33444 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.655961]\n",
      "epoch:42 step:33445 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.232089]\n",
      "epoch:42 step:33446 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:42 step:33447 [D loss: 0.002313, acc.: 100.00%] [G loss: 0.078251]\n",
      "epoch:42 step:33448 [D loss: 0.021439, acc.: 100.00%] [G loss: 0.010305]\n",
      "epoch:42 step:33449 [D loss: 0.036524, acc.: 99.22%] [G loss: 0.014437]\n",
      "epoch:42 step:33450 [D loss: 0.005299, acc.: 100.00%] [G loss: 0.007098]\n",
      "epoch:42 step:33451 [D loss: 0.057252, acc.: 98.44%] [G loss: 0.005874]\n",
      "epoch:42 step:33452 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.007190]\n",
      "epoch:42 step:33453 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.001553]\n",
      "epoch:42 step:33454 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.004866]\n",
      "epoch:42 step:33455 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.001780]\n",
      "epoch:42 step:33456 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:42 step:33457 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.000891]\n",
      "epoch:42 step:33458 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.002508]\n",
      "epoch:42 step:33459 [D loss: 0.179371, acc.: 92.97%] [G loss: 0.867085]\n",
      "epoch:42 step:33460 [D loss: 0.004274, acc.: 100.00%] [G loss: 0.611316]\n",
      "epoch:42 step:33461 [D loss: 0.051361, acc.: 97.66%] [G loss: 0.299971]\n",
      "epoch:42 step:33462 [D loss: 0.003105, acc.: 100.00%] [G loss: 0.365839]\n",
      "epoch:42 step:33463 [D loss: 0.150395, acc.: 94.53%] [G loss: 0.000705]\n",
      "epoch:42 step:33464 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000688]\n",
      "epoch:42 step:33465 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001122]\n",
      "epoch:42 step:33466 [D loss: 0.000738, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:42 step:33467 [D loss: 0.007141, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:42 step:33468 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:42 step:33469 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:42 step:33470 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:42 step:33471 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:42 step:33472 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:42 step:33473 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:42 step:33474 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:42 step:33475 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:42 step:33476 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33477 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:33478 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:42 step:33479 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:42 step:33480 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.004523]\n",
      "epoch:42 step:33481 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.000684]\n",
      "epoch:42 step:33482 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.008701]\n",
      "epoch:42 step:33483 [D loss: 0.164422, acc.: 93.75%] [G loss: 0.017936]\n",
      "epoch:42 step:33484 [D loss: 0.077256, acc.: 97.66%] [G loss: 0.087262]\n",
      "epoch:42 step:33485 [D loss: 0.024499, acc.: 99.22%] [G loss: 0.046824]\n",
      "epoch:42 step:33486 [D loss: 0.001327, acc.: 100.00%] [G loss: 0.952454]\n",
      "epoch:42 step:33487 [D loss: 0.045649, acc.: 98.44%] [G loss: 0.042823]\n",
      "epoch:42 step:33488 [D loss: 0.014220, acc.: 100.00%] [G loss: 0.345183]\n",
      "epoch:42 step:33489 [D loss: 0.650897, acc.: 67.19%] [G loss: 1.469167]\n",
      "epoch:42 step:33490 [D loss: 0.029314, acc.: 99.22%] [G loss: 2.068627]\n",
      "epoch:42 step:33491 [D loss: 0.525751, acc.: 78.91%] [G loss: 6.272673]\n",
      "epoch:42 step:33492 [D loss: 0.009819, acc.: 100.00%] [G loss: 4.289604]\n",
      "epoch:42 step:33493 [D loss: 0.000271, acc.: 100.00%] [G loss: 3.422105]\n",
      "epoch:42 step:33494 [D loss: 0.022386, acc.: 99.22%] [G loss: 1.455334]\n",
      "epoch:42 step:33495 [D loss: 0.176473, acc.: 92.19%] [G loss: 0.000583]\n",
      "epoch:42 step:33496 [D loss: 0.001089, acc.: 100.00%] [G loss: 2.159666]\n",
      "epoch:42 step:33497 [D loss: 0.000207, acc.: 100.00%] [G loss: 1.628777]\n",
      "epoch:42 step:33498 [D loss: 0.063883, acc.: 98.44%] [G loss: 0.563171]\n",
      "epoch:42 step:33499 [D loss: 0.008193, acc.: 100.00%] [G loss: 0.040652]\n",
      "epoch:42 step:33500 [D loss: 0.004763, acc.: 100.00%] [G loss: 0.026727]\n",
      "epoch:42 step:33501 [D loss: 0.002052, acc.: 100.00%] [G loss: 0.015780]\n",
      "epoch:42 step:33502 [D loss: 0.005093, acc.: 100.00%] [G loss: 0.001704]\n",
      "epoch:42 step:33503 [D loss: 0.021142, acc.: 100.00%] [G loss: 0.004309]\n",
      "epoch:42 step:33504 [D loss: 0.001290, acc.: 100.00%] [G loss: 0.005244]\n",
      "epoch:42 step:33505 [D loss: 0.002577, acc.: 100.00%] [G loss: 0.008865]\n",
      "epoch:42 step:33506 [D loss: 0.225626, acc.: 90.62%] [G loss: 0.000015]\n",
      "epoch:42 step:33507 [D loss: 0.000886, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33508 [D loss: 0.028572, acc.: 99.22%] [G loss: 0.000008]\n",
      "epoch:42 step:33509 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:42 step:33510 [D loss: 0.001001, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:42 step:33511 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:42 step:33512 [D loss: 0.000793, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:42 step:33513 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000475]\n",
      "epoch:42 step:33514 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33515 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:42 step:33516 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:42 step:33517 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:42 step:33518 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:42 step:33519 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:42 step:33520 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:42 step:33521 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:42 step:33522 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:42 step:33523 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33524 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:42 step:33525 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:42 step:33526 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:42 step:33527 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:42 step:33528 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:42 step:33529 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:42 step:33530 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:42 step:33531 [D loss: 0.119237, acc.: 95.31%] [G loss: 0.028068]\n",
      "epoch:42 step:33532 [D loss: 0.003247, acc.: 100.00%] [G loss: 1.452785]\n",
      "epoch:42 step:33533 [D loss: 0.003221, acc.: 100.00%] [G loss: 0.212399]\n",
      "epoch:42 step:33534 [D loss: 0.003619, acc.: 100.00%] [G loss: 0.162814]\n",
      "epoch:42 step:33535 [D loss: 0.020235, acc.: 100.00%] [G loss: 0.841331]\n",
      "epoch:42 step:33536 [D loss: 0.059965, acc.: 100.00%] [G loss: 0.104055]\n",
      "epoch:42 step:33537 [D loss: 0.001099, acc.: 100.00%] [G loss: 1.252291]\n",
      "epoch:42 step:33538 [D loss: 0.001646, acc.: 100.00%] [G loss: 0.118886]\n",
      "epoch:42 step:33539 [D loss: 0.027180, acc.: 99.22%] [G loss: 0.061180]\n",
      "epoch:42 step:33540 [D loss: 0.086093, acc.: 98.44%] [G loss: 0.132468]\n",
      "epoch:42 step:33541 [D loss: 0.053299, acc.: 96.88%] [G loss: 0.271234]\n",
      "epoch:42 step:33542 [D loss: 0.019639, acc.: 100.00%] [G loss: 0.092525]\n",
      "epoch:42 step:33543 [D loss: 0.003504, acc.: 100.00%] [G loss: 0.232460]\n",
      "epoch:42 step:33544 [D loss: 0.189703, acc.: 92.19%] [G loss: 2.862242]\n",
      "epoch:42 step:33545 [D loss: 0.077727, acc.: 97.66%] [G loss: 1.825861]\n",
      "epoch:42 step:33546 [D loss: 0.038999, acc.: 100.00%] [G loss: 1.198063]\n",
      "epoch:42 step:33547 [D loss: 0.009028, acc.: 100.00%] [G loss: 0.214850]\n",
      "epoch:42 step:33548 [D loss: 0.078036, acc.: 96.09%] [G loss: 0.021678]\n",
      "epoch:42 step:33549 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.319900]\n",
      "epoch:42 step:33550 [D loss: 0.002629, acc.: 100.00%] [G loss: 0.028121]\n",
      "epoch:42 step:33551 [D loss: 0.001653, acc.: 100.00%] [G loss: 0.009384]\n",
      "epoch:42 step:33552 [D loss: 0.012287, acc.: 99.22%] [G loss: 0.001139]\n",
      "epoch:42 step:33553 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:42 step:33554 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000519]\n",
      "epoch:42 step:33555 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33556 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.010541]\n",
      "epoch:42 step:33557 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.001731]\n",
      "epoch:42 step:33558 [D loss: 0.002912, acc.: 100.00%] [G loss: 0.024086]\n",
      "epoch:42 step:33559 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:42 step:33560 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:42 step:33561 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:42 step:33562 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:42 step:33563 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.011651]\n",
      "epoch:42 step:33564 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:42 step:33565 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:42 step:33566 [D loss: 0.010692, acc.: 100.00%] [G loss: 0.001197]\n",
      "epoch:42 step:33567 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001526]\n",
      "epoch:42 step:33568 [D loss: 0.006990, acc.: 100.00%] [G loss: 0.001058]\n",
      "epoch:42 step:33569 [D loss: 0.664882, acc.: 68.75%] [G loss: 8.955215]\n",
      "epoch:42 step:33570 [D loss: 2.340732, acc.: 52.34%] [G loss: 5.569141]\n",
      "epoch:42 step:33571 [D loss: 0.016367, acc.: 100.00%] [G loss: 0.207218]\n",
      "epoch:42 step:33572 [D loss: 0.008007, acc.: 100.00%] [G loss: 3.618260]\n",
      "epoch:42 step:33573 [D loss: 0.031439, acc.: 99.22%] [G loss: 2.242358]\n",
      "epoch:42 step:33574 [D loss: 0.053661, acc.: 99.22%] [G loss: 1.968541]\n",
      "epoch:42 step:33575 [D loss: 0.091619, acc.: 99.22%] [G loss: 1.735623]\n",
      "epoch:42 step:33576 [D loss: 0.041672, acc.: 99.22%] [G loss: 2.485452]\n",
      "epoch:42 step:33577 [D loss: 0.053461, acc.: 97.66%] [G loss: 2.823373]\n",
      "epoch:42 step:33578 [D loss: 0.185031, acc.: 92.19%] [G loss: 5.309120]\n",
      "epoch:42 step:33579 [D loss: 0.156852, acc.: 92.97%] [G loss: 6.123569]\n",
      "epoch:42 step:33580 [D loss: 0.023744, acc.: 99.22%] [G loss: 0.112884]\n",
      "epoch:42 step:33581 [D loss: 0.313503, acc.: 83.59%] [G loss: 6.923244]\n",
      "epoch:42 step:33582 [D loss: 0.046379, acc.: 98.44%] [G loss: 7.380892]\n",
      "epoch:42 step:33583 [D loss: 0.010038, acc.: 100.00%] [G loss: 7.067118]\n",
      "epoch:43 step:33584 [D loss: 0.091931, acc.: 96.88%] [G loss: 4.417791]\n",
      "epoch:43 step:33585 [D loss: 0.007221, acc.: 100.00%] [G loss: 4.628273]\n",
      "epoch:43 step:33586 [D loss: 0.011760, acc.: 100.00%] [G loss: 4.527874]\n",
      "epoch:43 step:33587 [D loss: 0.008667, acc.: 100.00%] [G loss: 3.990271]\n",
      "epoch:43 step:33588 [D loss: 0.040945, acc.: 99.22%] [G loss: 4.985648]\n",
      "epoch:43 step:33589 [D loss: 0.005410, acc.: 100.00%] [G loss: 6.224111]\n",
      "epoch:43 step:33590 [D loss: 0.020571, acc.: 100.00%] [G loss: 5.252150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33591 [D loss: 0.052544, acc.: 99.22%] [G loss: 6.319906]\n",
      "epoch:43 step:33592 [D loss: 0.034927, acc.: 99.22%] [G loss: 8.714703]\n",
      "epoch:43 step:33593 [D loss: 0.135110, acc.: 93.75%] [G loss: 0.347085]\n",
      "epoch:43 step:33594 [D loss: 0.142905, acc.: 95.31%] [G loss: 6.639642]\n",
      "epoch:43 step:33595 [D loss: 0.028226, acc.: 99.22%] [G loss: 0.275090]\n",
      "epoch:43 step:33596 [D loss: 0.024926, acc.: 100.00%] [G loss: 6.783489]\n",
      "epoch:43 step:33597 [D loss: 0.071208, acc.: 99.22%] [G loss: 4.582414]\n",
      "epoch:43 step:33598 [D loss: 0.019863, acc.: 100.00%] [G loss: 3.596479]\n",
      "epoch:43 step:33599 [D loss: 0.020932, acc.: 100.00%] [G loss: 3.236656]\n",
      "epoch:43 step:33600 [D loss: 0.055721, acc.: 99.22%] [G loss: 4.678775]\n",
      "epoch:43 step:33601 [D loss: 0.366848, acc.: 82.03%] [G loss: 8.162797]\n",
      "epoch:43 step:33602 [D loss: 0.623715, acc.: 75.78%] [G loss: 0.324946]\n",
      "epoch:43 step:33603 [D loss: 0.008394, acc.: 100.00%] [G loss: 1.953677]\n",
      "epoch:43 step:33604 [D loss: 0.100813, acc.: 96.09%] [G loss: 0.017344]\n",
      "epoch:43 step:33605 [D loss: 0.036536, acc.: 98.44%] [G loss: 2.337618]\n",
      "epoch:43 step:33606 [D loss: 0.136295, acc.: 93.75%] [G loss: 0.342687]\n",
      "epoch:43 step:33607 [D loss: 0.024788, acc.: 100.00%] [G loss: 0.259525]\n",
      "epoch:43 step:33608 [D loss: 0.397761, acc.: 82.03%] [G loss: 4.618032]\n",
      "epoch:43 step:33609 [D loss: 0.320427, acc.: 85.94%] [G loss: 4.033484]\n",
      "epoch:43 step:33610 [D loss: 0.042473, acc.: 98.44%] [G loss: 0.467879]\n",
      "epoch:43 step:33611 [D loss: 0.024918, acc.: 99.22%] [G loss: 1.075366]\n",
      "epoch:43 step:33612 [D loss: 0.007090, acc.: 100.00%] [G loss: 0.450746]\n",
      "epoch:43 step:33613 [D loss: 0.055665, acc.: 98.44%] [G loss: 0.004024]\n",
      "epoch:43 step:33614 [D loss: 0.034580, acc.: 99.22%] [G loss: 0.000800]\n",
      "epoch:43 step:33615 [D loss: 0.002547, acc.: 100.00%] [G loss: 0.911895]\n",
      "epoch:43 step:33616 [D loss: 0.001453, acc.: 100.00%] [G loss: 0.522008]\n",
      "epoch:43 step:33617 [D loss: 0.000985, acc.: 100.00%] [G loss: 0.188310]\n",
      "epoch:43 step:33618 [D loss: 0.034319, acc.: 99.22%] [G loss: 0.041621]\n",
      "epoch:43 step:33619 [D loss: 0.022930, acc.: 99.22%] [G loss: 0.029760]\n",
      "epoch:43 step:33620 [D loss: 0.036216, acc.: 99.22%] [G loss: 0.000096]\n",
      "epoch:43 step:33621 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:43 step:33622 [D loss: 0.000723, acc.: 100.00%] [G loss: 0.027822]\n",
      "epoch:43 step:33623 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:43 step:33624 [D loss: 0.009283, acc.: 100.00%] [G loss: 0.030559]\n",
      "epoch:43 step:33625 [D loss: 0.005369, acc.: 100.00%] [G loss: 0.000843]\n",
      "epoch:43 step:33626 [D loss: 0.003896, acc.: 100.00%] [G loss: 0.054397]\n",
      "epoch:43 step:33627 [D loss: 0.012946, acc.: 100.00%] [G loss: 0.020922]\n",
      "epoch:43 step:33628 [D loss: 0.017541, acc.: 100.00%] [G loss: 0.000801]\n",
      "epoch:43 step:33629 [D loss: 0.000936, acc.: 100.00%] [G loss: 0.001285]\n",
      "epoch:43 step:33630 [D loss: 0.001866, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:43 step:33631 [D loss: 0.825087, acc.: 65.62%] [G loss: 3.210325]\n",
      "epoch:43 step:33632 [D loss: 0.600098, acc.: 74.22%] [G loss: 2.131961]\n",
      "epoch:43 step:33633 [D loss: 0.282947, acc.: 86.72%] [G loss: 0.014406]\n",
      "epoch:43 step:33634 [D loss: 0.337455, acc.: 84.38%] [G loss: 1.520502]\n",
      "epoch:43 step:33635 [D loss: 0.077798, acc.: 96.88%] [G loss: 0.174154]\n",
      "epoch:43 step:33636 [D loss: 0.024900, acc.: 99.22%] [G loss: 1.518520]\n",
      "epoch:43 step:33637 [D loss: 0.140686, acc.: 95.31%] [G loss: 0.005981]\n",
      "epoch:43 step:33638 [D loss: 0.001642, acc.: 100.00%] [G loss: 0.002863]\n",
      "epoch:43 step:33639 [D loss: 0.146692, acc.: 95.31%] [G loss: 0.070049]\n",
      "epoch:43 step:33640 [D loss: 0.001546, acc.: 100.00%] [G loss: 0.002427]\n",
      "epoch:43 step:33641 [D loss: 0.003235, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:43 step:33642 [D loss: 0.148712, acc.: 95.31%] [G loss: 0.258031]\n",
      "epoch:43 step:33643 [D loss: 0.001475, acc.: 100.00%] [G loss: 0.003436]\n",
      "epoch:43 step:33644 [D loss: 0.014047, acc.: 99.22%] [G loss: 0.002245]\n",
      "epoch:43 step:33645 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.002499]\n",
      "epoch:43 step:33646 [D loss: 0.002573, acc.: 100.00%] [G loss: 0.001569]\n",
      "epoch:43 step:33647 [D loss: 0.058993, acc.: 99.22%] [G loss: 0.000876]\n",
      "epoch:43 step:33648 [D loss: 0.076217, acc.: 96.88%] [G loss: 0.000019]\n",
      "epoch:43 step:33649 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:33650 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:33651 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:33652 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.011017]\n",
      "epoch:43 step:33653 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:33654 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:43 step:33655 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:33656 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.001963]\n",
      "epoch:43 step:33657 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:43 step:33658 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:33659 [D loss: 0.000741, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:33660 [D loss: 0.003290, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33661 [D loss: 0.001230, acc.: 100.00%] [G loss: 0.001364]\n",
      "epoch:43 step:33662 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:33663 [D loss: 0.001516, acc.: 100.00%] [G loss: 0.003733]\n",
      "epoch:43 step:33664 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000991]\n",
      "epoch:43 step:33665 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:33666 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:43 step:33667 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:43 step:33668 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002277]\n",
      "epoch:43 step:33669 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:33670 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33671 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:33672 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:43 step:33673 [D loss: 0.004220, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33674 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:33675 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:43 step:33676 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.001612]\n",
      "epoch:43 step:33677 [D loss: 0.274205, acc.: 88.28%] [G loss: 0.001508]\n",
      "epoch:43 step:33678 [D loss: 0.002145, acc.: 100.00%] [G loss: 1.627254]\n",
      "epoch:43 step:33679 [D loss: 0.058408, acc.: 97.66%] [G loss: 0.018524]\n",
      "epoch:43 step:33680 [D loss: 0.027002, acc.: 99.22%] [G loss: 0.003971]\n",
      "epoch:43 step:33681 [D loss: 0.044231, acc.: 98.44%] [G loss: 0.000286]\n",
      "epoch:43 step:33682 [D loss: 0.006498, acc.: 100.00%] [G loss: 0.002174]\n",
      "epoch:43 step:33683 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.013086]\n",
      "epoch:43 step:33684 [D loss: 0.001263, acc.: 100.00%] [G loss: 0.001511]\n",
      "epoch:43 step:33685 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.001636]\n",
      "epoch:43 step:33686 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.001441]\n",
      "epoch:43 step:33687 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:43 step:33688 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001410]\n",
      "epoch:43 step:33689 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.015930]\n",
      "epoch:43 step:33690 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:43 step:33691 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:43 step:33692 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000698]\n",
      "epoch:43 step:33693 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.005235]\n",
      "epoch:43 step:33694 [D loss: 0.006328, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:43 step:33695 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:43 step:33696 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:43 step:33697 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.003000]\n",
      "epoch:43 step:33698 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:43 step:33699 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.001346]\n",
      "epoch:43 step:33700 [D loss: 0.004821, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:43 step:33701 [D loss: 0.004478, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:43 step:33702 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:43 step:33703 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.005983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33704 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:43 step:33705 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:43 step:33706 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000918]\n",
      "epoch:43 step:33707 [D loss: 0.004482, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:43 step:33708 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:43 step:33709 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.002316]\n",
      "epoch:43 step:33710 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:43 step:33711 [D loss: 0.003008, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:43 step:33712 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:43 step:33713 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.008237]\n",
      "epoch:43 step:33714 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000804]\n",
      "epoch:43 step:33715 [D loss: 0.031166, acc.: 100.00%] [G loss: 0.002796]\n",
      "epoch:43 step:33716 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:43 step:33717 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:43 step:33718 [D loss: 0.004697, acc.: 100.00%] [G loss: 0.025960]\n",
      "epoch:43 step:33719 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003620]\n",
      "epoch:43 step:33720 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:43 step:33721 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:43 step:33722 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:43 step:33723 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.001675]\n",
      "epoch:43 step:33724 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:43 step:33725 [D loss: 0.004180, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:43 step:33726 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:43 step:33727 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.300466]\n",
      "epoch:43 step:33728 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:43 step:33729 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:43 step:33730 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:43 step:33731 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.001092]\n",
      "epoch:43 step:33732 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003102]\n",
      "epoch:43 step:33733 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000577]\n",
      "epoch:43 step:33734 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.002540]\n",
      "epoch:43 step:33735 [D loss: 0.001889, acc.: 100.00%] [G loss: 0.001005]\n",
      "epoch:43 step:33736 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:43 step:33737 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000836]\n",
      "epoch:43 step:33738 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:43 step:33739 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:43 step:33740 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:43 step:33741 [D loss: 0.003388, acc.: 100.00%] [G loss: 0.060150]\n",
      "epoch:43 step:33742 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:43 step:33743 [D loss: 0.021351, acc.: 99.22%] [G loss: 0.000017]\n",
      "epoch:43 step:33744 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.001990]\n",
      "epoch:43 step:33745 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:43 step:33746 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:43 step:33747 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002533]\n",
      "epoch:43 step:33748 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:43 step:33749 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.002051]\n",
      "epoch:43 step:33750 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:43 step:33751 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.004344]\n",
      "epoch:43 step:33752 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:43 step:33753 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:43 step:33754 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:43 step:33755 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000385]\n",
      "epoch:43 step:33756 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:43 step:33757 [D loss: 0.007186, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:43 step:33758 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:43 step:33759 [D loss: 0.002476, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:43 step:33760 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.017408]\n",
      "epoch:43 step:33761 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000709]\n",
      "epoch:43 step:33762 [D loss: 0.007514, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:43 step:33763 [D loss: 0.001075, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:43 step:33764 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:43 step:33765 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:43 step:33766 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:43 step:33767 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:43 step:33768 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:43 step:33769 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:33770 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:43 step:33771 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:43 step:33772 [D loss: 0.001926, acc.: 100.00%] [G loss: 0.000629]\n",
      "epoch:43 step:33773 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.003044]\n",
      "epoch:43 step:33774 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:43 step:33775 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000507]\n",
      "epoch:43 step:33776 [D loss: 0.290936, acc.: 85.16%] [G loss: 0.139626]\n",
      "epoch:43 step:33777 [D loss: 0.048647, acc.: 97.66%] [G loss: 0.838837]\n",
      "epoch:43 step:33778 [D loss: 0.236984, acc.: 86.72%] [G loss: 2.120200]\n",
      "epoch:43 step:33779 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.015360]\n",
      "epoch:43 step:33780 [D loss: 0.027648, acc.: 99.22%] [G loss: 0.367884]\n",
      "epoch:43 step:33781 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.092104]\n",
      "epoch:43 step:33782 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.313470]\n",
      "epoch:43 step:33783 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:43 step:33784 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.084687]\n",
      "epoch:43 step:33785 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:43 step:33786 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:43 step:33787 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001557]\n",
      "epoch:43 step:33788 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:43 step:33789 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:33790 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:43 step:33791 [D loss: 0.008930, acc.: 99.22%] [G loss: 0.000010]\n",
      "epoch:43 step:33792 [D loss: 0.010018, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:43 step:33793 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.008918]\n",
      "epoch:43 step:33794 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.006617]\n",
      "epoch:43 step:33795 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:33796 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:43 step:33797 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.007578]\n",
      "epoch:43 step:33798 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:43 step:33799 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.007330]\n",
      "epoch:43 step:33800 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.105179]\n",
      "epoch:43 step:33801 [D loss: 0.000798, acc.: 100.00%] [G loss: 0.002253]\n",
      "epoch:43 step:33802 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:43 step:33803 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:43 step:33804 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:43 step:33805 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:43 step:33806 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001250]\n",
      "epoch:43 step:33807 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.001874]\n",
      "epoch:43 step:33808 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:43 step:33809 [D loss: 0.001170, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33810 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:43 step:33811 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:43 step:33812 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:43 step:33813 [D loss: 0.007609, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:43 step:33814 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:43 step:33815 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000897]\n",
      "epoch:43 step:33816 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:33817 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33818 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:43 step:33819 [D loss: 0.017835, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:43 step:33820 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:43 step:33821 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003989]\n",
      "epoch:43 step:33822 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:33823 [D loss: 0.009306, acc.: 99.22%] [G loss: 0.000003]\n",
      "epoch:43 step:33824 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33825 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:43 step:33826 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:43 step:33827 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:43 step:33828 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:43 step:33829 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:33830 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:43 step:33831 [D loss: 0.004746, acc.: 100.00%] [G loss: 0.001108]\n",
      "epoch:43 step:33832 [D loss: 0.001151, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:43 step:33833 [D loss: 0.004011, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:43 step:33834 [D loss: 0.162089, acc.: 92.97%] [G loss: 0.000068]\n",
      "epoch:43 step:33835 [D loss: 0.001484, acc.: 100.00%] [G loss: 1.373658]\n",
      "epoch:43 step:33836 [D loss: 0.041250, acc.: 98.44%] [G loss: 0.019865]\n",
      "epoch:43 step:33837 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.017356]\n",
      "epoch:43 step:33838 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.059535]\n",
      "epoch:43 step:33839 [D loss: 0.001828, acc.: 100.00%] [G loss: 0.007795]\n",
      "epoch:43 step:33840 [D loss: 0.613598, acc.: 75.00%] [G loss: 1.827378]\n",
      "epoch:43 step:33841 [D loss: 0.610676, acc.: 72.66%] [G loss: 3.604286]\n",
      "epoch:43 step:33842 [D loss: 0.065761, acc.: 96.88%] [G loss: 0.545569]\n",
      "epoch:43 step:33843 [D loss: 0.042449, acc.: 98.44%] [G loss: 0.072409]\n",
      "epoch:43 step:33844 [D loss: 0.110134, acc.: 93.75%] [G loss: 0.000702]\n",
      "epoch:43 step:33845 [D loss: 0.004177, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:43 step:33846 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.092660]\n",
      "epoch:43 step:33847 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.036288]\n",
      "epoch:43 step:33848 [D loss: 0.026718, acc.: 100.00%] [G loss: 0.162573]\n",
      "epoch:43 step:33849 [D loss: 0.009168, acc.: 100.00%] [G loss: 0.001289]\n",
      "epoch:43 step:33850 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:43 step:33851 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:43 step:33852 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.089190]\n",
      "epoch:43 step:33853 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:43 step:33854 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.032198]\n",
      "epoch:43 step:33855 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:43 step:33856 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:43 step:33857 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.014463]\n",
      "epoch:43 step:33858 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.020141]\n",
      "epoch:43 step:33859 [D loss: 0.001456, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:43 step:33860 [D loss: 0.038345, acc.: 99.22%] [G loss: 0.000024]\n",
      "epoch:43 step:33861 [D loss: 0.056162, acc.: 99.22%] [G loss: 0.000037]\n",
      "epoch:43 step:33862 [D loss: 0.007829, acc.: 99.22%] [G loss: 1.193305]\n",
      "epoch:43 step:33863 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.530666]\n",
      "epoch:43 step:33864 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.000460]\n",
      "epoch:43 step:33865 [D loss: 0.004246, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:43 step:33866 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:43 step:33867 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.051456]\n",
      "epoch:43 step:33868 [D loss: 0.002045, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:43 step:33869 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.016472]\n",
      "epoch:43 step:33870 [D loss: 0.003803, acc.: 100.00%] [G loss: 0.000759]\n",
      "epoch:43 step:33871 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:43 step:33872 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:43 step:33873 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001398]\n",
      "epoch:43 step:33874 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:43 step:33875 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:43 step:33876 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:43 step:33877 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:43 step:33878 [D loss: 0.002303, acc.: 100.00%] [G loss: 0.004065]\n",
      "epoch:43 step:33879 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.018488]\n",
      "epoch:43 step:33880 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.004598]\n",
      "epoch:43 step:33881 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:43 step:33882 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:43 step:33883 [D loss: 0.002591, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:43 step:33884 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:43 step:33885 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:43 step:33886 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:43 step:33887 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:43 step:33888 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:43 step:33889 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001713]\n",
      "epoch:43 step:33890 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:43 step:33891 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:43 step:33892 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:43 step:33893 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.042464]\n",
      "epoch:43 step:33894 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:43 step:33895 [D loss: 0.001644, acc.: 100.00%] [G loss: 0.167859]\n",
      "epoch:43 step:33896 [D loss: 0.002018, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:43 step:33897 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:43 step:33898 [D loss: 0.259899, acc.: 90.62%] [G loss: 0.000004]\n",
      "epoch:43 step:33899 [D loss: 0.015842, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:33900 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:33901 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:43 step:33902 [D loss: 0.002138, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:33903 [D loss: 0.002665, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:43 step:33904 [D loss: 0.003369, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:33905 [D loss: 0.010124, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:33906 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:43 step:33907 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33908 [D loss: 0.000810, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:43 step:33909 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:33910 [D loss: 0.001728, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33911 [D loss: 0.376159, acc.: 84.38%] [G loss: 2.087174]\n",
      "epoch:43 step:33912 [D loss: 0.003495, acc.: 100.00%] [G loss: 1.498020]\n",
      "epoch:43 step:33913 [D loss: 0.279959, acc.: 87.50%] [G loss: 0.721156]\n",
      "epoch:43 step:33914 [D loss: 0.043278, acc.: 99.22%] [G loss: 0.871247]\n",
      "epoch:43 step:33915 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.005883]\n",
      "epoch:43 step:33916 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.002422]\n",
      "epoch:43 step:33917 [D loss: 0.002350, acc.: 100.00%] [G loss: 0.083528]\n",
      "epoch:43 step:33918 [D loss: 0.003746, acc.: 100.00%] [G loss: 0.109672]\n",
      "epoch:43 step:33919 [D loss: 0.007038, acc.: 100.00%] [G loss: 0.014855]\n",
      "epoch:43 step:33920 [D loss: 0.043604, acc.: 99.22%] [G loss: 0.002641]\n",
      "epoch:43 step:33921 [D loss: 0.165788, acc.: 93.75%] [G loss: 0.005345]\n",
      "epoch:43 step:33922 [D loss: 0.001491, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:43 step:33923 [D loss: 0.088188, acc.: 95.31%] [G loss: 0.001544]\n",
      "epoch:43 step:33924 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.002399]\n",
      "epoch:43 step:33925 [D loss: 0.007551, acc.: 100.00%] [G loss: 0.001822]\n",
      "epoch:43 step:33926 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.005422]\n",
      "epoch:43 step:33927 [D loss: 0.002385, acc.: 100.00%] [G loss: 0.659931]\n",
      "epoch:43 step:33928 [D loss: 0.000648, acc.: 100.00%] [G loss: 1.386899]\n",
      "epoch:43 step:33929 [D loss: 0.001618, acc.: 100.00%] [G loss: 0.595490]\n",
      "epoch:43 step:33930 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.001164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33931 [D loss: 0.004411, acc.: 100.00%] [G loss: 0.000894]\n",
      "epoch:43 step:33932 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.016839]\n",
      "epoch:43 step:33933 [D loss: 0.028285, acc.: 99.22%] [G loss: 0.016921]\n",
      "epoch:43 step:33934 [D loss: 0.001820, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:43 step:33935 [D loss: 0.007275, acc.: 99.22%] [G loss: 0.000029]\n",
      "epoch:43 step:33936 [D loss: 0.168791, acc.: 92.97%] [G loss: 0.000922]\n",
      "epoch:43 step:33937 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.066576]\n",
      "epoch:43 step:33938 [D loss: 0.001841, acc.: 100.00%] [G loss: 0.005818]\n",
      "epoch:43 step:33939 [D loss: 0.002327, acc.: 100.00%] [G loss: 0.034201]\n",
      "epoch:43 step:33940 [D loss: 0.003166, acc.: 100.00%] [G loss: 0.004973]\n",
      "epoch:43 step:33941 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.089829]\n",
      "epoch:43 step:33942 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.016803]\n",
      "epoch:43 step:33943 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.548062]\n",
      "epoch:43 step:33944 [D loss: 0.040980, acc.: 100.00%] [G loss: 0.001252]\n",
      "epoch:43 step:33945 [D loss: 0.002080, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:43 step:33946 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.002382]\n",
      "epoch:43 step:33947 [D loss: 0.001012, acc.: 100.00%] [G loss: 0.000993]\n",
      "epoch:43 step:33948 [D loss: 0.000819, acc.: 100.00%] [G loss: 0.001073]\n",
      "epoch:43 step:33949 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.056627]\n",
      "epoch:43 step:33950 [D loss: 0.002451, acc.: 100.00%] [G loss: 0.000392]\n",
      "epoch:43 step:33951 [D loss: 0.001425, acc.: 100.00%] [G loss: 0.049127]\n",
      "epoch:43 step:33952 [D loss: 0.002850, acc.: 100.00%] [G loss: 0.005794]\n",
      "epoch:43 step:33953 [D loss: 0.001514, acc.: 100.00%] [G loss: 0.000999]\n",
      "epoch:43 step:33954 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.001458]\n",
      "epoch:43 step:33955 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.002472]\n",
      "epoch:43 step:33956 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.001407]\n",
      "epoch:43 step:33957 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000629]\n",
      "epoch:43 step:33958 [D loss: 0.006052, acc.: 100.00%] [G loss: 0.005312]\n",
      "epoch:43 step:33959 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:43 step:33960 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000489]\n",
      "epoch:43 step:33961 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:43 step:33962 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:43 step:33963 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.005196]\n",
      "epoch:43 step:33964 [D loss: 0.005197, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:43 step:33965 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:43 step:33966 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000469]\n",
      "epoch:43 step:33967 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002036]\n",
      "epoch:43 step:33968 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000496]\n",
      "epoch:43 step:33969 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:43 step:33970 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002986]\n",
      "epoch:43 step:33971 [D loss: 0.038136, acc.: 99.22%] [G loss: 0.000288]\n",
      "epoch:43 step:33972 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:43 step:33973 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:43 step:33974 [D loss: 0.048193, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:43 step:33975 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:43 step:33976 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.164551]\n",
      "epoch:43 step:33977 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:43 step:33978 [D loss: 0.020449, acc.: 100.00%] [G loss: 0.129726]\n",
      "epoch:43 step:33979 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:43 step:33980 [D loss: 0.002059, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:43 step:33981 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:43 step:33982 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.043347]\n",
      "epoch:43 step:33983 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:43 step:33984 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.040495]\n",
      "epoch:43 step:33985 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.004831]\n",
      "epoch:43 step:33986 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:43 step:33987 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:43 step:33988 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:43 step:33989 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:43 step:33990 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001600]\n",
      "epoch:43 step:33991 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001026]\n",
      "epoch:43 step:33992 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:43 step:33993 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:43 step:33994 [D loss: 0.002385, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:43 step:33995 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001361]\n",
      "epoch:43 step:33996 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:43 step:33997 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.019709]\n",
      "epoch:43 step:33998 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.003868]\n",
      "epoch:43 step:33999 [D loss: 0.001690, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:43 step:34000 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:43 step:34001 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.016160]\n",
      "epoch:43 step:34002 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:43 step:34003 [D loss: 0.002253, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:43 step:34004 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002827]\n",
      "epoch:43 step:34005 [D loss: 0.004123, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:43 step:34006 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.004070]\n",
      "epoch:43 step:34007 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:43 step:34008 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:43 step:34009 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:43 step:34010 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:43 step:34011 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:43 step:34012 [D loss: 0.001332, acc.: 100.00%] [G loss: 0.031329]\n",
      "epoch:43 step:34013 [D loss: 0.054322, acc.: 97.66%] [G loss: 0.000012]\n",
      "epoch:43 step:34014 [D loss: 0.002580, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:43 step:34015 [D loss: 0.002120, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34016 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:43 step:34017 [D loss: 0.259569, acc.: 86.72%] [G loss: 0.084714]\n",
      "epoch:43 step:34018 [D loss: 0.000700, acc.: 100.00%] [G loss: 2.812172]\n",
      "epoch:43 step:34019 [D loss: 0.745314, acc.: 67.97%] [G loss: 0.258081]\n",
      "epoch:43 step:34020 [D loss: 0.001807, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:43 step:34021 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.003751]\n",
      "epoch:43 step:34022 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:43 step:34023 [D loss: 0.015410, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:43 step:34024 [D loss: 0.008125, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:43 step:34025 [D loss: 0.007127, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:34026 [D loss: 0.007261, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:43 step:34027 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34028 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:43 step:34029 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.007596]\n",
      "epoch:43 step:34030 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.013482]\n",
      "epoch:43 step:34031 [D loss: 0.022550, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:43 step:34032 [D loss: 0.001085, acc.: 100.00%] [G loss: 0.031351]\n",
      "epoch:43 step:34033 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.039328]\n",
      "epoch:43 step:34034 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:43 step:34035 [D loss: 0.001793, acc.: 100.00%] [G loss: 0.056378]\n",
      "epoch:43 step:34036 [D loss: 0.011172, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:43 step:34037 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.037014]\n",
      "epoch:43 step:34038 [D loss: 0.014303, acc.: 100.00%] [G loss: 0.000927]\n",
      "epoch:43 step:34039 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.031388]\n",
      "epoch:43 step:34040 [D loss: 0.016520, acc.: 99.22%] [G loss: 0.000456]\n",
      "epoch:43 step:34041 [D loss: 0.006397, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:43 step:34042 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:43 step:34043 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.001404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34044 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:43 step:34045 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.090275]\n",
      "epoch:43 step:34046 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:43 step:34047 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.057390]\n",
      "epoch:43 step:34048 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.002330]\n",
      "epoch:43 step:34049 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:43 step:34050 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.105805]\n",
      "epoch:43 step:34051 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.076118]\n",
      "epoch:43 step:34052 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.079070]\n",
      "epoch:43 step:34053 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.016179]\n",
      "epoch:43 step:34054 [D loss: 0.018119, acc.: 100.00%] [G loss: 0.012636]\n",
      "epoch:43 step:34055 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:43 step:34056 [D loss: 0.002896, acc.: 100.00%] [G loss: 0.008043]\n",
      "epoch:43 step:34057 [D loss: 0.049517, acc.: 100.00%] [G loss: 0.002073]\n",
      "epoch:43 step:34058 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.693183]\n",
      "epoch:43 step:34059 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.034243]\n",
      "epoch:43 step:34060 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.005786]\n",
      "epoch:43 step:34061 [D loss: 0.002433, acc.: 100.00%] [G loss: 0.003841]\n",
      "epoch:43 step:34062 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.290586]\n",
      "epoch:43 step:34063 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.007721]\n",
      "epoch:43 step:34064 [D loss: 0.004006, acc.: 100.00%] [G loss: 0.160663]\n",
      "epoch:43 step:34065 [D loss: 0.004911, acc.: 100.00%] [G loss: 0.587900]\n",
      "epoch:43 step:34066 [D loss: 0.003017, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:43 step:34067 [D loss: 0.007206, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:43 step:34068 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.050762]\n",
      "epoch:43 step:34069 [D loss: 0.012662, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:43 step:34070 [D loss: 0.001516, acc.: 100.00%] [G loss: 0.014684]\n",
      "epoch:43 step:34071 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:43 step:34072 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:43 step:34073 [D loss: 0.001404, acc.: 100.00%] [G loss: 0.005689]\n",
      "epoch:43 step:34074 [D loss: 0.004108, acc.: 100.00%] [G loss: 0.002625]\n",
      "epoch:43 step:34075 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:43 step:34076 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:43 step:34077 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:43 step:34078 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:43 step:34079 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:43 step:34080 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:43 step:34081 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.006324]\n",
      "epoch:43 step:34082 [D loss: 0.033278, acc.: 98.44%] [G loss: 0.000006]\n",
      "epoch:43 step:34083 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:43 step:34084 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:43 step:34085 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002920]\n",
      "epoch:43 step:34086 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:43 step:34087 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:43 step:34088 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:43 step:34089 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001944]\n",
      "epoch:43 step:34090 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34091 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.009460]\n",
      "epoch:43 step:34092 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34093 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000718]\n",
      "epoch:43 step:34094 [D loss: 0.002067, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:43 step:34095 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34096 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:43 step:34097 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:43 step:34098 [D loss: 0.001795, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:34099 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34100 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34101 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34102 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34103 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001115]\n",
      "epoch:43 step:34104 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:43 step:34105 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.005458]\n",
      "epoch:43 step:34106 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:34107 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:43 step:34108 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34109 [D loss: 0.015408, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34110 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:43 step:34111 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34112 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:43 step:34113 [D loss: 0.002417, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34114 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34115 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:43 step:34116 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:43 step:34117 [D loss: 0.000782, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:43 step:34118 [D loss: 0.000935, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34119 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:34120 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34121 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:43 step:34122 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34123 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:43 step:34124 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.004200]\n",
      "epoch:43 step:34125 [D loss: 0.006192, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34126 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34127 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34128 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34129 [D loss: 0.005185, acc.: 100.00%] [G loss: 0.001284]\n",
      "epoch:43 step:34130 [D loss: 0.007394, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:43 step:34131 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34132 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:43 step:34133 [D loss: 0.472590, acc.: 74.22%] [G loss: 0.618048]\n",
      "epoch:43 step:34134 [D loss: 0.318002, acc.: 85.94%] [G loss: 0.241032]\n",
      "epoch:43 step:34135 [D loss: 0.019346, acc.: 100.00%] [G loss: 0.006770]\n",
      "epoch:43 step:34136 [D loss: 0.010318, acc.: 99.22%] [G loss: 3.422639]\n",
      "epoch:43 step:34137 [D loss: 0.168306, acc.: 93.75%] [G loss: 0.235590]\n",
      "epoch:43 step:34138 [D loss: 0.045522, acc.: 96.88%] [G loss: 3.542213]\n",
      "epoch:43 step:34139 [D loss: 0.122237, acc.: 94.53%] [G loss: 0.667951]\n",
      "epoch:43 step:34140 [D loss: 0.007268, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:43 step:34141 [D loss: 0.007690, acc.: 100.00%] [G loss: 0.078349]\n",
      "epoch:43 step:34142 [D loss: 0.088500, acc.: 96.09%] [G loss: 0.004950]\n",
      "epoch:43 step:34143 [D loss: 0.005010, acc.: 100.00%] [G loss: 0.002911]\n",
      "epoch:43 step:34144 [D loss: 0.006378, acc.: 100.00%] [G loss: 2.481182]\n",
      "epoch:43 step:34145 [D loss: 0.052781, acc.: 98.44%] [G loss: 0.000109]\n",
      "epoch:43 step:34146 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.020747]\n",
      "epoch:43 step:34147 [D loss: 0.001161, acc.: 100.00%] [G loss: 0.141095]\n",
      "epoch:43 step:34148 [D loss: 0.004286, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:43 step:34149 [D loss: 0.004047, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:43 step:34150 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.005771]\n",
      "epoch:43 step:34151 [D loss: 0.017687, acc.: 100.00%] [G loss: 0.038331]\n",
      "epoch:43 step:34152 [D loss: 0.009868, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:43 step:34153 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.062194]\n",
      "epoch:43 step:34154 [D loss: 0.003681, acc.: 100.00%] [G loss: 0.042842]\n",
      "epoch:43 step:34155 [D loss: 0.032057, acc.: 99.22%] [G loss: 0.174582]\n",
      "epoch:43 step:34156 [D loss: 0.037514, acc.: 99.22%] [G loss: 0.006119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34157 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.017735]\n",
      "epoch:43 step:34158 [D loss: 0.030127, acc.: 98.44%] [G loss: 0.106306]\n",
      "epoch:43 step:34159 [D loss: 0.005429, acc.: 100.00%] [G loss: 0.144419]\n",
      "epoch:43 step:34160 [D loss: 0.002716, acc.: 100.00%] [G loss: 0.033328]\n",
      "epoch:43 step:34161 [D loss: 0.003073, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34162 [D loss: 0.001208, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:43 step:34163 [D loss: 0.013958, acc.: 99.22%] [G loss: 0.000008]\n",
      "epoch:43 step:34164 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:43 step:34165 [D loss: 0.003032, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:43 step:34166 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.038923]\n",
      "epoch:43 step:34167 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34168 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34169 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:43 step:34170 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34171 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:43 step:34172 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.015728]\n",
      "epoch:43 step:34173 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.030011]\n",
      "epoch:43 step:34174 [D loss: 0.003920, acc.: 100.00%] [G loss: 0.001406]\n",
      "epoch:43 step:34175 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.003898]\n",
      "epoch:43 step:34176 [D loss: 0.011382, acc.: 99.22%] [G loss: 0.001640]\n",
      "epoch:43 step:34177 [D loss: 0.009339, acc.: 100.00%] [G loss: 0.001994]\n",
      "epoch:43 step:34178 [D loss: 0.038482, acc.: 99.22%] [G loss: 0.258942]\n",
      "epoch:43 step:34179 [D loss: 0.011043, acc.: 99.22%] [G loss: 1.325393]\n",
      "epoch:43 step:34180 [D loss: 0.076416, acc.: 97.66%] [G loss: 0.065927]\n",
      "epoch:43 step:34181 [D loss: 0.286816, acc.: 87.50%] [G loss: 0.000152]\n",
      "epoch:43 step:34182 [D loss: 0.191286, acc.: 92.19%] [G loss: 4.287306]\n",
      "epoch:43 step:34183 [D loss: 0.005009, acc.: 100.00%] [G loss: 5.051645]\n",
      "epoch:43 step:34184 [D loss: 0.327847, acc.: 91.41%] [G loss: 0.012116]\n",
      "epoch:43 step:34185 [D loss: 0.033908, acc.: 99.22%] [G loss: 2.919733]\n",
      "epoch:43 step:34186 [D loss: 0.048098, acc.: 100.00%] [G loss: 3.497246]\n",
      "epoch:43 step:34187 [D loss: 0.016381, acc.: 100.00%] [G loss: 0.006884]\n",
      "epoch:43 step:34188 [D loss: 0.002641, acc.: 100.00%] [G loss: 2.386405]\n",
      "epoch:43 step:34189 [D loss: 0.048798, acc.: 99.22%] [G loss: 0.025505]\n",
      "epoch:43 step:34190 [D loss: 0.090614, acc.: 96.88%] [G loss: 0.016779]\n",
      "epoch:43 step:34191 [D loss: 0.022728, acc.: 98.44%] [G loss: 0.065454]\n",
      "epoch:43 step:34192 [D loss: 0.011271, acc.: 100.00%] [G loss: 3.151498]\n",
      "epoch:43 step:34193 [D loss: 0.017029, acc.: 99.22%] [G loss: 1.321376]\n",
      "epoch:43 step:34194 [D loss: 0.053567, acc.: 99.22%] [G loss: 1.590267]\n",
      "epoch:43 step:34195 [D loss: 0.004815, acc.: 100.00%] [G loss: 1.038603]\n",
      "epoch:43 step:34196 [D loss: 4.514812, acc.: 35.16%] [G loss: 1.804656]\n",
      "epoch:43 step:34197 [D loss: 1.071300, acc.: 59.38%] [G loss: 0.604986]\n",
      "epoch:43 step:34198 [D loss: 0.408596, acc.: 82.81%] [G loss: 3.676552]\n",
      "epoch:43 step:34199 [D loss: 0.023613, acc.: 100.00%] [G loss: 0.003931]\n",
      "epoch:43 step:34200 [D loss: 0.044701, acc.: 99.22%] [G loss: 1.834053]\n",
      "epoch:43 step:34201 [D loss: 0.009220, acc.: 100.00%] [G loss: 0.000688]\n",
      "epoch:43 step:34202 [D loss: 0.024779, acc.: 99.22%] [G loss: 0.000239]\n",
      "epoch:43 step:34203 [D loss: 0.005621, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:43 step:34204 [D loss: 0.004201, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:43 step:34205 [D loss: 0.004518, acc.: 100.00%] [G loss: 0.756444]\n",
      "epoch:43 step:34206 [D loss: 0.022862, acc.: 99.22%] [G loss: 0.509809]\n",
      "epoch:43 step:34207 [D loss: 0.045609, acc.: 99.22%] [G loss: 0.000081]\n",
      "epoch:43 step:34208 [D loss: 0.011114, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:43 step:34209 [D loss: 0.005390, acc.: 100.00%] [G loss: 0.536550]\n",
      "epoch:43 step:34210 [D loss: 0.006544, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:43 step:34211 [D loss: 0.009626, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:43 step:34212 [D loss: 0.005497, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:43 step:34213 [D loss: 0.002912, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34214 [D loss: 0.005636, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:43 step:34215 [D loss: 0.001657, acc.: 100.00%] [G loss: 0.107466]\n",
      "epoch:43 step:34216 [D loss: 0.007500, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:43 step:34217 [D loss: 0.025911, acc.: 99.22%] [G loss: 0.000037]\n",
      "epoch:43 step:34218 [D loss: 0.089213, acc.: 99.22%] [G loss: 0.000020]\n",
      "epoch:43 step:34219 [D loss: 0.004390, acc.: 100.00%] [G loss: 0.268382]\n",
      "epoch:43 step:34220 [D loss: 0.030078, acc.: 98.44%] [G loss: 0.000086]\n",
      "epoch:43 step:34221 [D loss: 0.023151, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:43 step:34222 [D loss: 0.008318, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:43 step:34223 [D loss: 0.005848, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:43 step:34224 [D loss: 0.121180, acc.: 97.66%] [G loss: 0.000204]\n",
      "epoch:43 step:34225 [D loss: 0.030108, acc.: 99.22%] [G loss: 0.000224]\n",
      "epoch:43 step:34226 [D loss: 0.015684, acc.: 100.00%] [G loss: 0.549145]\n",
      "epoch:43 step:34227 [D loss: 0.043612, acc.: 99.22%] [G loss: 0.285734]\n",
      "epoch:43 step:34228 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:43 step:34229 [D loss: 0.023387, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34230 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.042772]\n",
      "epoch:43 step:34231 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:43 step:34232 [D loss: 0.003380, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:43 step:34233 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.019281]\n",
      "epoch:43 step:34234 [D loss: 0.004134, acc.: 100.00%] [G loss: 0.024932]\n",
      "epoch:43 step:34235 [D loss: 0.001167, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:43 step:34236 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.008816]\n",
      "epoch:43 step:34237 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.018934]\n",
      "epoch:43 step:34238 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.082817]\n",
      "epoch:43 step:34239 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.025309]\n",
      "epoch:43 step:34240 [D loss: 0.001246, acc.: 100.00%] [G loss: 0.025499]\n",
      "epoch:43 step:34241 [D loss: 0.010476, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:34242 [D loss: 0.000942, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34243 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.015451]\n",
      "epoch:43 step:34244 [D loss: 0.014875, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:34245 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34246 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.004693]\n",
      "epoch:43 step:34247 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.003646]\n",
      "epoch:43 step:34248 [D loss: 0.020184, acc.: 100.00%] [G loss: 0.001458]\n",
      "epoch:43 step:34249 [D loss: 0.004561, acc.: 100.00%] [G loss: 0.008033]\n",
      "epoch:43 step:34250 [D loss: 0.008094, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:43 step:34251 [D loss: 0.004675, acc.: 100.00%] [G loss: 0.001334]\n",
      "epoch:43 step:34252 [D loss: 0.013228, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34253 [D loss: 0.002669, acc.: 100.00%] [G loss: 0.004340]\n",
      "epoch:43 step:34254 [D loss: 0.058794, acc.: 100.00%] [G loss: 0.013157]\n",
      "epoch:43 step:34255 [D loss: 0.006117, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:43 step:34256 [D loss: 0.001368, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:43 step:34257 [D loss: 0.007082, acc.: 100.00%] [G loss: 0.059268]\n",
      "epoch:43 step:34258 [D loss: 0.001601, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:34259 [D loss: 0.025207, acc.: 99.22%] [G loss: 0.026935]\n",
      "epoch:43 step:34260 [D loss: 0.001387, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34261 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:43 step:34262 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34263 [D loss: 0.002538, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:34264 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:43 step:34265 [D loss: 0.005478, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:43 step:34266 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:43 step:34267 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:43 step:34268 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34269 [D loss: 0.001027, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34270 [D loss: 0.004880, acc.: 100.00%] [G loss: 0.003252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34271 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:43 step:34272 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34273 [D loss: 0.001159, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:43 step:34274 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34275 [D loss: 0.004581, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34276 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.007582]\n",
      "epoch:43 step:34277 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34278 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34279 [D loss: 0.003327, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34280 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.007325]\n",
      "epoch:43 step:34281 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:34282 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.015332]\n",
      "epoch:43 step:34283 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34284 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.007630]\n",
      "epoch:43 step:34285 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:43 step:34286 [D loss: 0.002444, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34287 [D loss: 0.001475, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34288 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.012000]\n",
      "epoch:43 step:34289 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34290 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.002509]\n",
      "epoch:43 step:34291 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.002072]\n",
      "epoch:43 step:34292 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34293 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34294 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34295 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.005431]\n",
      "epoch:43 step:34296 [D loss: 0.002340, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:43 step:34297 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34298 [D loss: 0.003388, acc.: 100.00%] [G loss: 0.001670]\n",
      "epoch:43 step:34299 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.004799]\n",
      "epoch:43 step:34300 [D loss: 0.004916, acc.: 100.00%] [G loss: 0.007607]\n",
      "epoch:43 step:34301 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34302 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.001472]\n",
      "epoch:43 step:34303 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34304 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.003973]\n",
      "epoch:43 step:34305 [D loss: 0.004946, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34306 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.002160]\n",
      "epoch:43 step:34307 [D loss: 0.003726, acc.: 100.00%] [G loss: 0.005203]\n",
      "epoch:43 step:34308 [D loss: 0.001652, acc.: 100.00%] [G loss: 0.004577]\n",
      "epoch:43 step:34309 [D loss: 0.006186, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34310 [D loss: 0.000900, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34311 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34312 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34313 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:43 step:34314 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34315 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.001664]\n",
      "epoch:43 step:34316 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:43 step:34317 [D loss: 0.003132, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34318 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:43 step:34319 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34320 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34321 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:43 step:34322 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.012235]\n",
      "epoch:43 step:34323 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34324 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34325 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34326 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34327 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34328 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001845]\n",
      "epoch:43 step:34329 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34330 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.001704]\n",
      "epoch:43 step:34331 [D loss: 0.001380, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34332 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34333 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34334 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34335 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.005403]\n",
      "epoch:43 step:34336 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34337 [D loss: 0.005654, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34338 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.002375]\n",
      "epoch:43 step:34339 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34340 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34341 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.014725]\n",
      "epoch:43 step:34342 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.001895]\n",
      "epoch:43 step:34343 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34344 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34345 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:34346 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34347 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002075]\n",
      "epoch:43 step:34348 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.002016]\n",
      "epoch:43 step:34349 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000671]\n",
      "epoch:43 step:34350 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.003740]\n",
      "epoch:43 step:34351 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34352 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34353 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.002665]\n",
      "epoch:43 step:34354 [D loss: 0.008112, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34355 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34356 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34357 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001422]\n",
      "epoch:43 step:34358 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34359 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.001028]\n",
      "epoch:43 step:34360 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.001616]\n",
      "epoch:43 step:34361 [D loss: 0.002200, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34362 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34363 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:43 step:34364 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34365 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34366 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.002247]\n",
      "epoch:44 step:34367 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34368 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34369 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34370 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.005561]\n",
      "epoch:44 step:34371 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34372 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34373 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:44 step:34374 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34375 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001552]\n",
      "epoch:44 step:34376 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34377 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34378 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:44 step:34379 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34380 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001142]\n",
      "epoch:44 step:34381 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34382 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34383 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34384 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.006791]\n",
      "epoch:44 step:34385 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34386 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34387 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34388 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34389 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:44 step:34390 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:44 step:34391 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34392 [D loss: 0.002127, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34393 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34394 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34395 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:44 step:34396 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34397 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34398 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001306]\n",
      "epoch:44 step:34399 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34400 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001601]\n",
      "epoch:44 step:34401 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:44 step:34402 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.001161]\n",
      "epoch:44 step:34403 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000662]\n",
      "epoch:44 step:34404 [D loss: 0.002086, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34405 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34406 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34407 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:44 step:34408 [D loss: 0.005101, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34409 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34410 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34411 [D loss: 0.004453, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34412 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34413 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:44 step:34414 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34415 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34416 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34417 [D loss: 0.001746, acc.: 100.00%] [G loss: 0.001022]\n",
      "epoch:44 step:34418 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.002830]\n",
      "epoch:44 step:34419 [D loss: 0.001375, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34420 [D loss: 0.001425, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34421 [D loss: 0.138028, acc.: 94.53%] [G loss: 0.000771]\n",
      "epoch:44 step:34422 [D loss: 0.009449, acc.: 99.22%] [G loss: 0.314285]\n",
      "epoch:44 step:34423 [D loss: 0.100188, acc.: 96.09%] [G loss: 0.165770]\n",
      "epoch:44 step:34424 [D loss: 0.002453, acc.: 100.00%] [G loss: 0.008918]\n",
      "epoch:44 step:34425 [D loss: 0.001369, acc.: 100.00%] [G loss: 0.002391]\n",
      "epoch:44 step:34426 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.092470]\n",
      "epoch:44 step:34427 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.145311]\n",
      "epoch:44 step:34428 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.008562]\n",
      "epoch:44 step:34429 [D loss: 0.018760, acc.: 99.22%] [G loss: 0.069337]\n",
      "epoch:44 step:34430 [D loss: 0.005484, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:44 step:34431 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:44 step:34432 [D loss: 0.000810, acc.: 100.00%] [G loss: 0.001225]\n",
      "epoch:44 step:34433 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.001707]\n",
      "epoch:44 step:34434 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000415]\n",
      "epoch:44 step:34435 [D loss: 0.005511, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:44 step:34436 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:44 step:34437 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.004100]\n",
      "epoch:44 step:34438 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:44 step:34439 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:44 step:34440 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:44 step:34441 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:44 step:34442 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:44 step:34443 [D loss: 0.002327, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:44 step:34444 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:44 step:34445 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:44 step:34446 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.000577]\n",
      "epoch:44 step:34447 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:44 step:34448 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:44 step:34449 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34450 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:44 step:34451 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:44 step:34452 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:44 step:34453 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:44 step:34454 [D loss: 0.003204, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:34455 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.007887]\n",
      "epoch:44 step:34456 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:44 step:34457 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:44 step:34458 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:44 step:34459 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:44 step:34460 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:44 step:34461 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:44 step:34462 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:44 step:34463 [D loss: 0.000665, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34464 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:44 step:34465 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:44 step:34466 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001281]\n",
      "epoch:44 step:34467 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34468 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34469 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:44 step:34470 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:44 step:34471 [D loss: 0.001366, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:44 step:34472 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:44 step:34473 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:44 step:34474 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:44 step:34475 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:44 step:34476 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:44 step:34477 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:44 step:34478 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:44 step:34479 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:44 step:34480 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000758]\n",
      "epoch:44 step:34481 [D loss: 0.006078, acc.: 99.22%] [G loss: 0.000010]\n",
      "epoch:44 step:34482 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001441]\n",
      "epoch:44 step:34483 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34484 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:44 step:34485 [D loss: 0.014908, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:44 step:34486 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.034203]\n",
      "epoch:44 step:34487 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34488 [D loss: 0.003255, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:44 step:34489 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:44 step:34490 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:34491 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002182]\n",
      "epoch:44 step:34492 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:44 step:34493 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34494 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.033328]\n",
      "epoch:44 step:34495 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.039708]\n",
      "epoch:44 step:34496 [D loss: 0.002254, acc.: 100.00%] [G loss: 0.002382]\n",
      "epoch:44 step:34497 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.003541]\n",
      "epoch:44 step:34498 [D loss: 0.024280, acc.: 99.22%] [G loss: 0.000006]\n",
      "epoch:44 step:34499 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34500 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:44 step:34501 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:34502 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:44 step:34503 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003065]\n",
      "epoch:44 step:34504 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004381]\n",
      "epoch:44 step:34505 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002595]\n",
      "epoch:44 step:34506 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:44 step:34507 [D loss: 0.001867, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:44 step:34508 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34509 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000492]\n",
      "epoch:44 step:34510 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:44 step:34511 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.001699]\n",
      "epoch:44 step:34512 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.001071]\n",
      "epoch:44 step:34513 [D loss: 0.003339, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34514 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.027299]\n",
      "epoch:44 step:34515 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.013375]\n",
      "epoch:44 step:34516 [D loss: 0.002105, acc.: 100.00%] [G loss: 0.008019]\n",
      "epoch:44 step:34517 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:44 step:34518 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:44 step:34519 [D loss: 0.005867, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34520 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:44 step:34521 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:44 step:34522 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:44 step:34523 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:44 step:34524 [D loss: 0.004976, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:44 step:34525 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:44 step:34526 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:44 step:34527 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:34528 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:44 step:34529 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000688]\n",
      "epoch:44 step:34530 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:44 step:34531 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000492]\n",
      "epoch:44 step:34532 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000640]\n",
      "epoch:44 step:34533 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34534 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:34535 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:44 step:34536 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34537 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:44 step:34538 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:44 step:34539 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001736]\n",
      "epoch:44 step:34540 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34541 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:34542 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:44 step:34543 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.006165]\n",
      "epoch:44 step:34544 [D loss: 0.004781, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:44 step:34545 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000512]\n",
      "epoch:44 step:34546 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001148]\n",
      "epoch:44 step:34547 [D loss: 0.006582, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:44 step:34548 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34549 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.004331]\n",
      "epoch:44 step:34550 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001689]\n",
      "epoch:44 step:34551 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.002860]\n",
      "epoch:44 step:34552 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:44 step:34553 [D loss: 0.000927, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:44 step:34554 [D loss: 0.003675, acc.: 100.00%] [G loss: 0.004868]\n",
      "epoch:44 step:34555 [D loss: 0.039294, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:44 step:34556 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:44 step:34557 [D loss: 0.063430, acc.: 96.88%] [G loss: 0.003989]\n",
      "epoch:44 step:34558 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.008480]\n",
      "epoch:44 step:34559 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34560 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34561 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.011645]\n",
      "epoch:44 step:34562 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:44 step:34563 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001163]\n",
      "epoch:44 step:34564 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:34565 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:44 step:34566 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34567 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:44 step:34568 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:44 step:34569 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001074]\n",
      "epoch:44 step:34570 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34571 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.007158]\n",
      "epoch:44 step:34572 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.005397]\n",
      "epoch:44 step:34573 [D loss: 0.010635, acc.: 99.22%] [G loss: 0.000002]\n",
      "epoch:44 step:34574 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:44 step:34575 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002848]\n",
      "epoch:44 step:34576 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:44 step:34577 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34578 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:44 step:34579 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34580 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34581 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34582 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:44 step:34583 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:44 step:34584 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34585 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000785]\n",
      "epoch:44 step:34586 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34587 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34588 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34589 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.007461]\n",
      "epoch:44 step:34590 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.001739]\n",
      "epoch:44 step:34591 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34592 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34593 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:44 step:34594 [D loss: 0.001115, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:34595 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:44 step:34596 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:44 step:34597 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002300]\n",
      "epoch:44 step:34598 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34599 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34600 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34601 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34602 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34603 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34604 [D loss: 0.006592, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:44 step:34605 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:44 step:34606 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34607 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:44 step:34608 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34609 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34610 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34611 [D loss: 0.003015, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34612 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34613 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34614 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:44 step:34615 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34616 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34617 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34618 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34619 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:44 step:34620 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000758]\n",
      "epoch:44 step:34621 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34622 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:44 step:34623 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34624 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34625 [D loss: 0.001264, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:34626 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:44 step:34627 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:44 step:34628 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34629 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34630 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34631 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:44 step:34632 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:44 step:34633 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34634 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:44 step:34635 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34636 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34637 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:44 step:34638 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34639 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34640 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34641 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:44 step:34642 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000709]\n",
      "epoch:44 step:34643 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34644 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34645 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34646 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:44 step:34647 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34648 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:44 step:34649 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:44 step:34650 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:44 step:34651 [D loss: 0.004644, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34652 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34653 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34654 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:44 step:34655 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:44 step:34656 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34657 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34658 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34659 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34660 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:44 step:34661 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34662 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34663 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34664 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34665 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34666 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:44 step:34667 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34668 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34669 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:44 step:34670 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:44 step:34671 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34672 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34673 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34674 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34675 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:44 step:34676 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34677 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:44 step:34678 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34679 [D loss: 0.001613, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34680 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:44 step:34681 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34682 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34683 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:44 step:34684 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34685 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001323]\n",
      "epoch:44 step:34686 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34687 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34688 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34689 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:44 step:34690 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34691 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:44 step:34692 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:44 step:34693 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34694 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34695 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:44 step:34696 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34697 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34698 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:44 step:34699 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:44 step:34700 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34701 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34702 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34703 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34704 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000906]\n",
      "epoch:44 step:34705 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34706 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:44 step:34707 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34708 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34709 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34710 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34711 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34712 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:44 step:34713 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34714 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:34715 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:44 step:34716 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34717 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34718 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:44 step:34719 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34720 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001025]\n",
      "epoch:44 step:34721 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:44 step:34722 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34723 [D loss: 0.000889, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:44 step:34724 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:44 step:34725 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34726 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.001506]\n",
      "epoch:44 step:34727 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34728 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34729 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34730 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34731 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34732 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:44 step:34733 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34734 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:44 step:34735 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34736 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34737 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:44 step:34738 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:44 step:34739 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34740 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34741 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34742 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34743 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:44 step:34744 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34745 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34746 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:44 step:34747 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:44 step:34748 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:44 step:34749 [D loss: 0.000791, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:44 step:34750 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34751 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:44 step:34752 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.000792]\n",
      "epoch:44 step:34753 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:44 step:34754 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:44 step:34755 [D loss: 0.292229, acc.: 82.81%] [G loss: 5.104352]\n",
      "epoch:44 step:34756 [D loss: 0.123142, acc.: 96.09%] [G loss: 1.804250]\n",
      "epoch:44 step:34757 [D loss: 0.286208, acc.: 92.19%] [G loss: 0.000336]\n",
      "epoch:44 step:34758 [D loss: 0.298797, acc.: 86.72%] [G loss: 0.015792]\n",
      "epoch:44 step:34759 [D loss: 1.219721, acc.: 57.81%] [G loss: 0.789755]\n",
      "epoch:44 step:34760 [D loss: 0.015559, acc.: 100.00%] [G loss: 6.228787]\n",
      "epoch:44 step:34761 [D loss: 0.012565, acc.: 100.00%] [G loss: 5.357452]\n",
      "epoch:44 step:34762 [D loss: 0.055308, acc.: 98.44%] [G loss: 4.322046]\n",
      "epoch:44 step:34763 [D loss: 0.125017, acc.: 93.75%] [G loss: 0.969818]\n",
      "epoch:44 step:34764 [D loss: 0.012583, acc.: 100.00%] [G loss: 0.118737]\n",
      "epoch:44 step:34765 [D loss: 0.084281, acc.: 96.88%] [G loss: 2.249595]\n",
      "epoch:44 step:34766 [D loss: 0.002614, acc.: 100.00%] [G loss: 0.776090]\n",
      "epoch:44 step:34767 [D loss: 0.645574, acc.: 72.66%] [G loss: 6.107768]\n",
      "epoch:44 step:34768 [D loss: 0.108566, acc.: 95.31%] [G loss: 0.060367]\n",
      "epoch:44 step:34769 [D loss: 0.149769, acc.: 94.53%] [G loss: 4.832080]\n",
      "epoch:44 step:34770 [D loss: 0.092303, acc.: 97.66%] [G loss: 3.149746]\n",
      "epoch:44 step:34771 [D loss: 0.133184, acc.: 92.97%] [G loss: 0.093684]\n",
      "epoch:44 step:34772 [D loss: 0.155293, acc.: 91.41%] [G loss: 3.509724]\n",
      "epoch:44 step:34773 [D loss: 0.082660, acc.: 97.66%] [G loss: 3.210128]\n",
      "epoch:44 step:34774 [D loss: 0.066365, acc.: 97.66%] [G loss: 0.000116]\n",
      "epoch:44 step:34775 [D loss: 0.557913, acc.: 77.34%] [G loss: 0.033595]\n",
      "epoch:44 step:34776 [D loss: 0.055122, acc.: 97.66%] [G loss: 0.819344]\n",
      "epoch:44 step:34777 [D loss: 0.159416, acc.: 91.41%] [G loss: 4.707493]\n",
      "epoch:44 step:34778 [D loss: 0.048523, acc.: 98.44%] [G loss: 0.000050]\n",
      "epoch:44 step:34779 [D loss: 0.007988, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34780 [D loss: 0.006481, acc.: 100.00%] [G loss: 3.358653]\n",
      "epoch:44 step:34781 [D loss: 0.002860, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:44 step:34782 [D loss: 0.000827, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34783 [D loss: 0.001635, acc.: 100.00%] [G loss: 1.780817]\n",
      "epoch:44 step:34784 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34785 [D loss: 0.002032, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:44 step:34786 [D loss: 0.002247, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:44 step:34787 [D loss: 0.007344, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34788 [D loss: 0.001930, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34789 [D loss: 0.000790, acc.: 100.00%] [G loss: 1.071536]\n",
      "epoch:44 step:34790 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.495801]\n",
      "epoch:44 step:34791 [D loss: 0.012127, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34792 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34793 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.029876]\n",
      "epoch:44 step:34794 [D loss: 0.013400, acc.: 100.00%] [G loss: 0.004729]\n",
      "epoch:44 step:34795 [D loss: 0.009914, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34796 [D loss: 0.007297, acc.: 100.00%] [G loss: 0.065985]\n",
      "epoch:44 step:34797 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.090548]\n",
      "epoch:44 step:34798 [D loss: 0.002438, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:44 step:34799 [D loss: 0.001556, acc.: 100.00%] [G loss: 0.005170]\n",
      "epoch:44 step:34800 [D loss: 0.702260, acc.: 68.75%] [G loss: 0.763856]\n",
      "epoch:44 step:34801 [D loss: 0.431592, acc.: 79.69%] [G loss: 3.775693]\n",
      "epoch:44 step:34802 [D loss: 0.649260, acc.: 75.00%] [G loss: 0.000921]\n",
      "epoch:44 step:34803 [D loss: 0.101296, acc.: 96.09%] [G loss: 0.000236]\n",
      "epoch:44 step:34804 [D loss: 0.001776, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:44 step:34805 [D loss: 0.011853, acc.: 99.22%] [G loss: 0.000778]\n",
      "epoch:44 step:34806 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:44 step:34807 [D loss: 0.001853, acc.: 100.00%] [G loss: 0.872113]\n",
      "epoch:44 step:34808 [D loss: 0.008537, acc.: 100.00%] [G loss: 0.000442]\n",
      "epoch:44 step:34809 [D loss: 0.498100, acc.: 77.34%] [G loss: 0.175830]\n",
      "epoch:44 step:34810 [D loss: 0.039888, acc.: 98.44%] [G loss: 0.588749]\n",
      "epoch:44 step:34811 [D loss: 0.193858, acc.: 91.41%] [G loss: 2.654789]\n",
      "epoch:44 step:34812 [D loss: 0.100429, acc.: 96.88%] [G loss: 0.002623]\n",
      "epoch:44 step:34813 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.656016]\n",
      "epoch:44 step:34814 [D loss: 0.052745, acc.: 97.66%] [G loss: 0.001833]\n",
      "epoch:44 step:34815 [D loss: 0.001163, acc.: 100.00%] [G loss: 0.019119]\n",
      "epoch:44 step:34816 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.232532]\n",
      "epoch:44 step:34817 [D loss: 0.012991, acc.: 99.22%] [G loss: 0.001162]\n",
      "epoch:44 step:34818 [D loss: 0.039267, acc.: 98.44%] [G loss: 0.245144]\n",
      "epoch:44 step:34819 [D loss: 0.003257, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:44 step:34820 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.025836]\n",
      "epoch:44 step:34821 [D loss: 0.002650, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:44 step:34822 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.003153]\n",
      "epoch:44 step:34823 [D loss: 0.005269, acc.: 100.00%] [G loss: 0.000594]\n",
      "epoch:44 step:34824 [D loss: 0.002957, acc.: 100.00%] [G loss: 0.125685]\n",
      "epoch:44 step:34825 [D loss: 0.002746, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:44 step:34826 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:44 step:34827 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:44 step:34828 [D loss: 0.036054, acc.: 99.22%] [G loss: 0.001147]\n",
      "epoch:44 step:34829 [D loss: 0.002752, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:44 step:34830 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.061854]\n",
      "epoch:44 step:34831 [D loss: 0.006590, acc.: 100.00%] [G loss: 0.081512]\n",
      "epoch:44 step:34832 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:44 step:34833 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:44 step:34834 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.002046]\n",
      "epoch:44 step:34835 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:44 step:34836 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.001450]\n",
      "epoch:44 step:34837 [D loss: 0.009186, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:44 step:34838 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:44 step:34839 [D loss: 0.002714, acc.: 100.00%] [G loss: 0.053235]\n",
      "epoch:44 step:34840 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34841 [D loss: 0.002112, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:44 step:34842 [D loss: 0.029969, acc.: 100.00%] [G loss: 0.001581]\n",
      "epoch:44 step:34843 [D loss: 0.001768, acc.: 100.00%] [G loss: 0.018514]\n",
      "epoch:44 step:34844 [D loss: 0.006895, acc.: 100.00%] [G loss: 0.037861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34845 [D loss: 0.014159, acc.: 99.22%] [G loss: 0.006343]\n",
      "epoch:44 step:34846 [D loss: 0.003948, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:44 step:34847 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.019060]\n",
      "epoch:44 step:34848 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.005498]\n",
      "epoch:44 step:34849 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.005854]\n",
      "epoch:44 step:34850 [D loss: 0.001047, acc.: 100.00%] [G loss: 0.034901]\n",
      "epoch:44 step:34851 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:44 step:34852 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:44 step:34853 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.007318]\n",
      "epoch:44 step:34854 [D loss: 0.001692, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:44 step:34855 [D loss: 0.016399, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34856 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:44 step:34857 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:44 step:34858 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:44 step:34859 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:44 step:34860 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:44 step:34861 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:44 step:34862 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:44 step:34863 [D loss: 0.002444, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:44 step:34864 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:44 step:34865 [D loss: 0.263646, acc.: 86.72%] [G loss: 2.818434]\n",
      "epoch:44 step:34866 [D loss: 0.017926, acc.: 99.22%] [G loss: 1.519019]\n",
      "epoch:44 step:34867 [D loss: 0.021217, acc.: 99.22%] [G loss: 0.422225]\n",
      "epoch:44 step:34868 [D loss: 0.784236, acc.: 57.81%] [G loss: 0.368812]\n",
      "epoch:44 step:34869 [D loss: 0.035799, acc.: 98.44%] [G loss: 0.666040]\n",
      "epoch:44 step:34870 [D loss: 0.073760, acc.: 99.22%] [G loss: 1.598403]\n",
      "epoch:44 step:34871 [D loss: 1.593693, acc.: 53.91%] [G loss: 5.126806]\n",
      "epoch:44 step:34872 [D loss: 0.946468, acc.: 62.50%] [G loss: 3.366824]\n",
      "epoch:44 step:34873 [D loss: 1.366811, acc.: 51.56%] [G loss: 1.990988]\n",
      "epoch:44 step:34874 [D loss: 0.045181, acc.: 100.00%] [G loss: 0.760460]\n",
      "epoch:44 step:34875 [D loss: 0.027431, acc.: 99.22%] [G loss: 0.541784]\n",
      "epoch:44 step:34876 [D loss: 0.064565, acc.: 98.44%] [G loss: 0.262972]\n",
      "epoch:44 step:34877 [D loss: 0.159180, acc.: 94.53%] [G loss: 0.485930]\n",
      "epoch:44 step:34878 [D loss: 0.038562, acc.: 98.44%] [G loss: 0.445216]\n",
      "epoch:44 step:34879 [D loss: 0.053792, acc.: 100.00%] [G loss: 0.345903]\n",
      "epoch:44 step:34880 [D loss: 0.042397, acc.: 100.00%] [G loss: 0.182592]\n",
      "epoch:44 step:34881 [D loss: 0.042638, acc.: 99.22%] [G loss: 0.087356]\n",
      "epoch:44 step:34882 [D loss: 0.077299, acc.: 98.44%] [G loss: 1.052436]\n",
      "epoch:44 step:34883 [D loss: 0.020261, acc.: 99.22%] [G loss: 0.198359]\n",
      "epoch:44 step:34884 [D loss: 0.046133, acc.: 99.22%] [G loss: 0.043307]\n",
      "epoch:44 step:34885 [D loss: 0.025801, acc.: 100.00%] [G loss: 0.062376]\n",
      "epoch:44 step:34886 [D loss: 0.027383, acc.: 100.00%] [G loss: 0.018656]\n",
      "epoch:44 step:34887 [D loss: 0.036588, acc.: 100.00%] [G loss: 0.032841]\n",
      "epoch:44 step:34888 [D loss: 0.063434, acc.: 98.44%] [G loss: 0.788794]\n",
      "epoch:44 step:34889 [D loss: 0.030937, acc.: 100.00%] [G loss: 0.011865]\n",
      "epoch:44 step:34890 [D loss: 0.070622, acc.: 99.22%] [G loss: 0.001730]\n",
      "epoch:44 step:34891 [D loss: 0.097061, acc.: 99.22%] [G loss: 0.030737]\n",
      "epoch:44 step:34892 [D loss: 0.037080, acc.: 99.22%] [G loss: 0.452884]\n",
      "epoch:44 step:34893 [D loss: 0.007582, acc.: 100.00%] [G loss: 0.082906]\n",
      "epoch:44 step:34894 [D loss: 0.004350, acc.: 100.00%] [G loss: 0.151460]\n",
      "epoch:44 step:34895 [D loss: 0.007023, acc.: 100.00%] [G loss: 0.139804]\n",
      "epoch:44 step:34896 [D loss: 0.050021, acc.: 97.66%] [G loss: 0.016818]\n",
      "epoch:44 step:34897 [D loss: 0.007028, acc.: 100.00%] [G loss: 0.015787]\n",
      "epoch:44 step:34898 [D loss: 0.005248, acc.: 100.00%] [G loss: 0.004799]\n",
      "epoch:44 step:34899 [D loss: 0.009320, acc.: 100.00%] [G loss: 0.000936]\n",
      "epoch:44 step:34900 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.004302]\n",
      "epoch:44 step:34901 [D loss: 0.023825, acc.: 100.00%] [G loss: 0.001791]\n",
      "epoch:44 step:34902 [D loss: 0.002219, acc.: 100.00%] [G loss: 0.001234]\n",
      "epoch:44 step:34903 [D loss: 0.004845, acc.: 100.00%] [G loss: 0.001602]\n",
      "epoch:44 step:34904 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.002035]\n",
      "epoch:44 step:34905 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:44 step:34906 [D loss: 0.004176, acc.: 100.00%] [G loss: 0.005554]\n",
      "epoch:44 step:34907 [D loss: 0.016891, acc.: 100.00%] [G loss: 0.002211]\n",
      "epoch:44 step:34908 [D loss: 0.006253, acc.: 100.00%] [G loss: 0.001116]\n",
      "epoch:44 step:34909 [D loss: 0.002913, acc.: 100.00%] [G loss: 0.012632]\n",
      "epoch:44 step:34910 [D loss: 0.169923, acc.: 90.62%] [G loss: 0.043081]\n",
      "epoch:44 step:34911 [D loss: 0.105287, acc.: 96.09%] [G loss: 0.100797]\n",
      "epoch:44 step:34912 [D loss: 0.026658, acc.: 99.22%] [G loss: 1.258535]\n",
      "epoch:44 step:34913 [D loss: 0.025228, acc.: 99.22%] [G loss: 0.013934]\n",
      "epoch:44 step:34914 [D loss: 0.025278, acc.: 99.22%] [G loss: 0.010473]\n",
      "epoch:44 step:34915 [D loss: 0.066724, acc.: 96.88%] [G loss: 0.092190]\n",
      "epoch:44 step:34916 [D loss: 0.002829, acc.: 100.00%] [G loss: 0.378785]\n",
      "epoch:44 step:34917 [D loss: 0.009945, acc.: 100.00%] [G loss: 0.094252]\n",
      "epoch:44 step:34918 [D loss: 0.044186, acc.: 99.22%] [G loss: 0.068935]\n",
      "epoch:44 step:34919 [D loss: 0.029848, acc.: 100.00%] [G loss: 0.062853]\n",
      "epoch:44 step:34920 [D loss: 0.072419, acc.: 97.66%] [G loss: 0.169025]\n",
      "epoch:44 step:34921 [D loss: 0.002467, acc.: 100.00%] [G loss: 0.165109]\n",
      "epoch:44 step:34922 [D loss: 0.054421, acc.: 98.44%] [G loss: 0.149143]\n",
      "epoch:44 step:34923 [D loss: 0.011655, acc.: 100.00%] [G loss: 0.020609]\n",
      "epoch:44 step:34924 [D loss: 0.008354, acc.: 100.00%] [G loss: 0.005885]\n",
      "epoch:44 step:34925 [D loss: 0.027390, acc.: 99.22%] [G loss: 0.002380]\n",
      "epoch:44 step:34926 [D loss: 0.005299, acc.: 100.00%] [G loss: 0.002873]\n",
      "epoch:44 step:34927 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.011287]\n",
      "epoch:44 step:34928 [D loss: 0.009692, acc.: 99.22%] [G loss: 0.000264]\n",
      "epoch:44 step:34929 [D loss: 0.005406, acc.: 100.00%] [G loss: 0.000881]\n",
      "epoch:44 step:34930 [D loss: 0.021475, acc.: 100.00%] [G loss: 0.001526]\n",
      "epoch:44 step:34931 [D loss: 0.016226, acc.: 99.22%] [G loss: 0.004909]\n",
      "epoch:44 step:34932 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.002722]\n",
      "epoch:44 step:34933 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.003801]\n",
      "epoch:44 step:34934 [D loss: 0.001946, acc.: 100.00%] [G loss: 0.004167]\n",
      "epoch:44 step:34935 [D loss: 0.001603, acc.: 100.00%] [G loss: 0.024933]\n",
      "epoch:44 step:34936 [D loss: 0.006906, acc.: 100.00%] [G loss: 0.001107]\n",
      "epoch:44 step:34937 [D loss: 0.011135, acc.: 100.00%] [G loss: 0.002279]\n",
      "epoch:44 step:34938 [D loss: 0.073228, acc.: 97.66%] [G loss: 0.012937]\n",
      "epoch:44 step:34939 [D loss: 0.005449, acc.: 100.00%] [G loss: 0.140677]\n",
      "epoch:44 step:34940 [D loss: 0.008258, acc.: 100.00%] [G loss: 0.096570]\n",
      "epoch:44 step:34941 [D loss: 0.014628, acc.: 99.22%] [G loss: 0.014756]\n",
      "epoch:44 step:34942 [D loss: 0.009516, acc.: 100.00%] [G loss: 0.013798]\n",
      "epoch:44 step:34943 [D loss: 0.005116, acc.: 100.00%] [G loss: 0.008903]\n",
      "epoch:44 step:34944 [D loss: 0.082890, acc.: 96.88%] [G loss: 0.366328]\n",
      "epoch:44 step:34945 [D loss: 0.037156, acc.: 99.22%] [G loss: 0.100781]\n",
      "epoch:44 step:34946 [D loss: 0.090559, acc.: 95.31%] [G loss: 0.001198]\n",
      "epoch:44 step:34947 [D loss: 0.016003, acc.: 99.22%] [G loss: 0.000398]\n",
      "epoch:44 step:34948 [D loss: 0.004562, acc.: 100.00%] [G loss: 0.005414]\n",
      "epoch:44 step:34949 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:44 step:34950 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:44 step:34951 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:44 step:34952 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.003568]\n",
      "epoch:44 step:34953 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:44 step:34954 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:44 step:34955 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:44 step:34956 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:44 step:34957 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34958 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:44 step:34959 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:44 step:34960 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:44 step:34961 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000723]\n",
      "epoch:44 step:34962 [D loss: 0.000929, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:44 step:34963 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:44 step:34964 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003929]\n",
      "epoch:44 step:34965 [D loss: 0.001732, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34966 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:44 step:34967 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:44 step:34968 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34969 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:44 step:34970 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.006103]\n",
      "epoch:44 step:34971 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:44 step:34972 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003182]\n",
      "epoch:44 step:34973 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:44 step:34974 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:44 step:34975 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:44 step:34976 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:44 step:34977 [D loss: 0.003842, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:44 step:34978 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:44 step:34979 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:44 step:34980 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:44 step:34981 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:44 step:34982 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:44 step:34983 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.024795]\n",
      "epoch:44 step:34984 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000565]\n",
      "epoch:44 step:34985 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:44 step:34986 [D loss: 0.002415, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:44 step:34987 [D loss: 0.004166, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:44 step:34988 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.002016]\n",
      "epoch:44 step:34989 [D loss: 0.080994, acc.: 97.66%] [G loss: 0.001042]\n",
      "epoch:44 step:34990 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.006122]\n",
      "epoch:44 step:34991 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.013940]\n",
      "epoch:44 step:34992 [D loss: 0.318517, acc.: 85.94%] [G loss: 0.000040]\n",
      "epoch:44 step:34993 [D loss: 0.002103, acc.: 100.00%] [G loss: 0.005582]\n",
      "epoch:44 step:34994 [D loss: 0.001024, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:44 step:34995 [D loss: 0.001157, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:44 step:34996 [D loss: 0.735170, acc.: 78.12%] [G loss: 2.048490]\n",
      "epoch:44 step:34997 [D loss: 0.018242, acc.: 99.22%] [G loss: 4.139657]\n",
      "epoch:44 step:34998 [D loss: 1.519852, acc.: 53.12%] [G loss: 1.581800]\n",
      "epoch:44 step:34999 [D loss: 0.438208, acc.: 80.47%] [G loss: 0.767633]\n",
      "epoch:44 step:35000 [D loss: 0.042807, acc.: 100.00%] [G loss: 0.788573]\n",
      "epoch:44 step:35001 [D loss: 0.036753, acc.: 99.22%] [G loss: 0.244451]\n",
      "epoch:44 step:35002 [D loss: 0.056586, acc.: 98.44%] [G loss: 0.101101]\n",
      "epoch:44 step:35003 [D loss: 0.028214, acc.: 100.00%] [G loss: 0.016870]\n",
      "epoch:44 step:35004 [D loss: 0.009363, acc.: 100.00%] [G loss: 0.009711]\n",
      "epoch:44 step:35005 [D loss: 0.016290, acc.: 100.00%] [G loss: 1.991873]\n",
      "epoch:44 step:35006 [D loss: 0.019999, acc.: 100.00%] [G loss: 0.010695]\n",
      "epoch:44 step:35007 [D loss: 0.025359, acc.: 100.00%] [G loss: 1.053388]\n",
      "epoch:44 step:35008 [D loss: 0.044540, acc.: 100.00%] [G loss: 0.074846]\n",
      "epoch:44 step:35009 [D loss: 0.009794, acc.: 100.00%] [G loss: 1.275677]\n",
      "epoch:44 step:35010 [D loss: 0.166678, acc.: 94.53%] [G loss: 0.001631]\n",
      "epoch:44 step:35011 [D loss: 0.028931, acc.: 100.00%] [G loss: 0.003661]\n",
      "epoch:44 step:35012 [D loss: 0.024302, acc.: 100.00%] [G loss: 0.717322]\n",
      "epoch:44 step:35013 [D loss: 0.084090, acc.: 97.66%] [G loss: 0.003042]\n",
      "epoch:44 step:35014 [D loss: 0.008339, acc.: 100.00%] [G loss: 0.006757]\n",
      "epoch:44 step:35015 [D loss: 0.009719, acc.: 100.00%] [G loss: 0.080266]\n",
      "epoch:44 step:35016 [D loss: 0.049220, acc.: 100.00%] [G loss: 0.329988]\n",
      "epoch:44 step:35017 [D loss: 0.109668, acc.: 96.88%] [G loss: 0.040607]\n",
      "epoch:44 step:35018 [D loss: 0.021239, acc.: 99.22%] [G loss: 0.074336]\n",
      "epoch:44 step:35019 [D loss: 0.012584, acc.: 100.00%] [G loss: 0.558692]\n",
      "epoch:44 step:35020 [D loss: 0.004413, acc.: 100.00%] [G loss: 0.021549]\n",
      "epoch:44 step:35021 [D loss: 0.015936, acc.: 99.22%] [G loss: 0.212981]\n",
      "epoch:44 step:35022 [D loss: 0.004639, acc.: 100.00%] [G loss: 0.031570]\n",
      "epoch:44 step:35023 [D loss: 0.002600, acc.: 100.00%] [G loss: 0.015800]\n",
      "epoch:44 step:35024 [D loss: 0.002396, acc.: 100.00%] [G loss: 0.022001]\n",
      "epoch:44 step:35025 [D loss: 0.013306, acc.: 100.00%] [G loss: 0.005075]\n",
      "epoch:44 step:35026 [D loss: 0.006300, acc.: 100.00%] [G loss: 0.029851]\n",
      "epoch:44 step:35027 [D loss: 0.005817, acc.: 100.00%] [G loss: 0.068512]\n",
      "epoch:44 step:35028 [D loss: 0.014262, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:44 step:35029 [D loss: 0.104498, acc.: 97.66%] [G loss: 0.006388]\n",
      "epoch:44 step:35030 [D loss: 0.703007, acc.: 58.59%] [G loss: 4.704685]\n",
      "epoch:44 step:35031 [D loss: 0.061273, acc.: 99.22%] [G loss: 2.434648]\n",
      "epoch:44 step:35032 [D loss: 0.699789, acc.: 69.53%] [G loss: 0.018458]\n",
      "epoch:44 step:35033 [D loss: 0.021341, acc.: 99.22%] [G loss: 0.000154]\n",
      "epoch:44 step:35034 [D loss: 0.015566, acc.: 99.22%] [G loss: 0.000076]\n",
      "epoch:44 step:35035 [D loss: 0.011287, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:35036 [D loss: 0.001711, acc.: 100.00%] [G loss: 2.925690]\n",
      "epoch:44 step:35037 [D loss: 0.069551, acc.: 98.44%] [G loss: 1.547705]\n",
      "epoch:44 step:35038 [D loss: 0.022083, acc.: 100.00%] [G loss: 0.000423]\n",
      "epoch:44 step:35039 [D loss: 0.003469, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:44 step:35040 [D loss: 0.030557, acc.: 99.22%] [G loss: 0.000249]\n",
      "epoch:44 step:35041 [D loss: 0.004535, acc.: 100.00%] [G loss: 1.231229]\n",
      "epoch:44 step:35042 [D loss: 0.027438, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:44 step:35043 [D loss: 0.314757, acc.: 85.94%] [G loss: 0.039658]\n",
      "epoch:44 step:35044 [D loss: 0.176590, acc.: 94.53%] [G loss: 0.023603]\n",
      "epoch:44 step:35045 [D loss: 0.005247, acc.: 100.00%] [G loss: 0.059679]\n",
      "epoch:44 step:35046 [D loss: 0.058667, acc.: 98.44%] [G loss: 0.082389]\n",
      "epoch:44 step:35047 [D loss: 0.078165, acc.: 97.66%] [G loss: 0.008920]\n",
      "epoch:44 step:35048 [D loss: 0.031775, acc.: 100.00%] [G loss: 0.004696]\n",
      "epoch:44 step:35049 [D loss: 0.020003, acc.: 100.00%] [G loss: 2.327065]\n",
      "epoch:44 step:35050 [D loss: 0.012751, acc.: 100.00%] [G loss: 0.003044]\n",
      "epoch:44 step:35051 [D loss: 0.024777, acc.: 99.22%] [G loss: 0.003076]\n",
      "epoch:44 step:35052 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.001093]\n",
      "epoch:44 step:35053 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.001338]\n",
      "epoch:44 step:35054 [D loss: 0.001120, acc.: 100.00%] [G loss: 0.000649]\n",
      "epoch:44 step:35055 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.001111]\n",
      "epoch:44 step:35056 [D loss: 0.023657, acc.: 99.22%] [G loss: 0.000122]\n",
      "epoch:44 step:35057 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.121367]\n",
      "epoch:44 step:35058 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.051853]\n",
      "epoch:44 step:35059 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:44 step:35060 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.021325]\n",
      "epoch:44 step:35061 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:44 step:35062 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:44 step:35063 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.027831]\n",
      "epoch:44 step:35064 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.002668]\n",
      "epoch:44 step:35065 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:44 step:35066 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:44 step:35067 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.014109]\n",
      "epoch:44 step:35068 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.000882]\n",
      "epoch:44 step:35069 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:44 step:35070 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002461]\n",
      "epoch:44 step:35071 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:35072 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:44 step:35073 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.080790]\n",
      "epoch:44 step:35074 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.006519]\n",
      "epoch:44 step:35075 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:44 step:35076 [D loss: 0.000715, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:44 step:35077 [D loss: 0.076518, acc.: 97.66%] [G loss: 0.000070]\n",
      "epoch:44 step:35078 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:44 step:35079 [D loss: 0.002388, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:44 step:35080 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.001472]\n",
      "epoch:44 step:35081 [D loss: 0.000833, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:44 step:35082 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:44 step:35083 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:35084 [D loss: 0.000697, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:44 step:35085 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.009961]\n",
      "epoch:44 step:35086 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:35087 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:44 step:35088 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:44 step:35089 [D loss: 0.116069, acc.: 93.75%] [G loss: 0.000984]\n",
      "epoch:44 step:35090 [D loss: 0.001324, acc.: 100.00%] [G loss: 0.082123]\n",
      "epoch:44 step:35091 [D loss: 0.008358, acc.: 100.00%] [G loss: 0.035212]\n",
      "epoch:44 step:35092 [D loss: 0.004064, acc.: 100.00%] [G loss: 0.016220]\n",
      "epoch:44 step:35093 [D loss: 0.004212, acc.: 100.00%] [G loss: 0.015294]\n",
      "epoch:44 step:35094 [D loss: 0.003727, acc.: 100.00%] [G loss: 0.017856]\n",
      "epoch:44 step:35095 [D loss: 0.006996, acc.: 100.00%] [G loss: 0.007219]\n",
      "epoch:44 step:35096 [D loss: 0.010372, acc.: 100.00%] [G loss: 0.020438]\n",
      "epoch:44 step:35097 [D loss: 0.148795, acc.: 93.75%] [G loss: 0.701257]\n",
      "epoch:44 step:35098 [D loss: 0.327813, acc.: 89.06%] [G loss: 0.068264]\n",
      "epoch:44 step:35099 [D loss: 0.003694, acc.: 100.00%] [G loss: 0.357559]\n",
      "epoch:44 step:35100 [D loss: 0.022702, acc.: 99.22%] [G loss: 0.095350]\n",
      "epoch:44 step:35101 [D loss: 0.000950, acc.: 100.00%] [G loss: 0.094859]\n",
      "epoch:44 step:35102 [D loss: 0.002865, acc.: 100.00%] [G loss: 0.023752]\n",
      "epoch:44 step:35103 [D loss: 0.004414, acc.: 100.00%] [G loss: 0.015011]\n",
      "epoch:44 step:35104 [D loss: 0.000955, acc.: 100.00%] [G loss: 0.004291]\n",
      "epoch:44 step:35105 [D loss: 0.004579, acc.: 100.00%] [G loss: 0.448097]\n",
      "epoch:44 step:35106 [D loss: 0.008141, acc.: 100.00%] [G loss: 0.002656]\n",
      "epoch:44 step:35107 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.005242]\n",
      "epoch:44 step:35108 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.002847]\n",
      "epoch:44 step:35109 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.001019]\n",
      "epoch:44 step:35110 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.006032]\n",
      "epoch:44 step:35111 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.002368]\n",
      "epoch:44 step:35112 [D loss: 0.004161, acc.: 100.00%] [G loss: 0.007770]\n",
      "epoch:44 step:35113 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.001112]\n",
      "epoch:44 step:35114 [D loss: 0.001750, acc.: 100.00%] [G loss: 0.001373]\n",
      "epoch:44 step:35115 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.003575]\n",
      "epoch:44 step:35116 [D loss: 0.004273, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:44 step:35117 [D loss: 0.003578, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:44 step:35118 [D loss: 0.018888, acc.: 100.00%] [G loss: 0.001717]\n",
      "epoch:44 step:35119 [D loss: 0.003343, acc.: 100.00%] [G loss: 0.001542]\n",
      "epoch:44 step:35120 [D loss: 0.002575, acc.: 100.00%] [G loss: 0.003163]\n",
      "epoch:44 step:35121 [D loss: 0.002339, acc.: 100.00%] [G loss: 0.003969]\n",
      "epoch:44 step:35122 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.002424]\n",
      "epoch:44 step:35123 [D loss: 0.004575, acc.: 100.00%] [G loss: 0.001558]\n",
      "epoch:44 step:35124 [D loss: 0.001254, acc.: 100.00%] [G loss: 0.004671]\n",
      "epoch:44 step:35125 [D loss: 0.001479, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:44 step:35126 [D loss: 0.007531, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:44 step:35127 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.001453]\n",
      "epoch:44 step:35128 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001685]\n",
      "epoch:44 step:35129 [D loss: 0.001815, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:44 step:35130 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.006376]\n",
      "epoch:44 step:35131 [D loss: 0.001830, acc.: 100.00%] [G loss: 0.002082]\n",
      "epoch:44 step:35132 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.002808]\n",
      "epoch:44 step:35133 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.410004]\n",
      "epoch:44 step:35134 [D loss: 0.005385, acc.: 100.00%] [G loss: 0.001626]\n",
      "epoch:44 step:35135 [D loss: 0.008005, acc.: 100.00%] [G loss: 0.002203]\n",
      "epoch:44 step:35136 [D loss: 0.081410, acc.: 98.44%] [G loss: 0.032965]\n",
      "epoch:44 step:35137 [D loss: 0.016107, acc.: 100.00%] [G loss: 0.066148]\n",
      "epoch:44 step:35138 [D loss: 0.007686, acc.: 100.00%] [G loss: 0.073213]\n",
      "epoch:44 step:35139 [D loss: 0.013602, acc.: 100.00%] [G loss: 0.012186]\n",
      "epoch:44 step:35140 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.016018]\n",
      "epoch:44 step:35141 [D loss: 0.005068, acc.: 100.00%] [G loss: 0.002761]\n",
      "epoch:44 step:35142 [D loss: 0.001372, acc.: 100.00%] [G loss: 0.006078]\n",
      "epoch:44 step:35143 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.000772]\n",
      "epoch:44 step:35144 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.009932]\n",
      "epoch:44 step:35145 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.026858]\n",
      "epoch:45 step:35146 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:45 step:35147 [D loss: 0.000782, acc.: 100.00%] [G loss: 0.001373]\n",
      "epoch:45 step:35148 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.046406]\n",
      "epoch:45 step:35149 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000891]\n",
      "epoch:45 step:35150 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.001421]\n",
      "epoch:45 step:35151 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:45 step:35152 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:45 step:35153 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.002206]\n",
      "epoch:45 step:35154 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:45 step:35155 [D loss: 0.002409, acc.: 100.00%] [G loss: 0.000953]\n",
      "epoch:45 step:35156 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.000946]\n",
      "epoch:45 step:35157 [D loss: 0.004671, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:45 step:35158 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000586]\n",
      "epoch:45 step:35159 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:45 step:35160 [D loss: 0.001723, acc.: 100.00%] [G loss: 0.004077]\n",
      "epoch:45 step:35161 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.016030]\n",
      "epoch:45 step:35162 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:45 step:35163 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000788]\n",
      "epoch:45 step:35164 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.001536]\n",
      "epoch:45 step:35165 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:45 step:35166 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:45 step:35167 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:45 step:35168 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:45 step:35169 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:45 step:35170 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.002684]\n",
      "epoch:45 step:35171 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:45 step:35172 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000526]\n",
      "epoch:45 step:35173 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:45 step:35174 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.000576]\n",
      "epoch:45 step:35175 [D loss: 0.006526, acc.: 100.00%] [G loss: 0.001849]\n",
      "epoch:45 step:35176 [D loss: 0.019902, acc.: 99.22%] [G loss: 0.000343]\n",
      "epoch:45 step:35177 [D loss: 0.066005, acc.: 99.22%] [G loss: 0.012434]\n",
      "epoch:45 step:35178 [D loss: 0.001141, acc.: 100.00%] [G loss: 0.442297]\n",
      "epoch:45 step:35179 [D loss: 0.001874, acc.: 100.00%] [G loss: 0.039772]\n",
      "epoch:45 step:35180 [D loss: 0.256653, acc.: 86.72%] [G loss: 0.000385]\n",
      "epoch:45 step:35181 [D loss: 0.003447, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:45 step:35182 [D loss: 0.019553, acc.: 99.22%] [G loss: 0.000054]\n",
      "epoch:45 step:35183 [D loss: 0.001930, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:45 step:35184 [D loss: 0.298170, acc.: 86.72%] [G loss: 0.669774]\n",
      "epoch:45 step:35185 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.657001]\n",
      "epoch:45 step:35186 [D loss: 0.164400, acc.: 93.75%] [G loss: 5.374045]\n",
      "epoch:45 step:35187 [D loss: 0.001612, acc.: 100.00%] [G loss: 0.002458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35188 [D loss: 0.048949, acc.: 98.44%] [G loss: 0.633898]\n",
      "epoch:45 step:35189 [D loss: 0.006692, acc.: 100.00%] [G loss: 0.265901]\n",
      "epoch:45 step:35190 [D loss: 0.007698, acc.: 100.00%] [G loss: 0.085729]\n",
      "epoch:45 step:35191 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.001369]\n",
      "epoch:45 step:35192 [D loss: 0.091208, acc.: 96.88%] [G loss: 0.422599]\n",
      "epoch:45 step:35193 [D loss: 0.053527, acc.: 99.22%] [G loss: 0.036087]\n",
      "epoch:45 step:35194 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.097946]\n",
      "epoch:45 step:35195 [D loss: 0.147253, acc.: 92.19%] [G loss: 0.012078]\n",
      "epoch:45 step:35196 [D loss: 0.026700, acc.: 100.00%] [G loss: 0.003133]\n",
      "epoch:45 step:35197 [D loss: 0.010898, acc.: 100.00%] [G loss: 0.033093]\n",
      "epoch:45 step:35198 [D loss: 0.007037, acc.: 100.00%] [G loss: 0.006484]\n",
      "epoch:45 step:35199 [D loss: 0.003066, acc.: 100.00%] [G loss: 0.014674]\n",
      "epoch:45 step:35200 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.011020]\n",
      "epoch:45 step:35201 [D loss: 0.086628, acc.: 96.09%] [G loss: 0.003875]\n",
      "epoch:45 step:35202 [D loss: 0.006205, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:45 step:35203 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:45 step:35204 [D loss: 0.018544, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:45 step:35205 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:45 step:35206 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:45 step:35207 [D loss: 0.002665, acc.: 100.00%] [G loss: 0.112318]\n",
      "epoch:45 step:35208 [D loss: 0.023329, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:45 step:35209 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.000514]\n",
      "epoch:45 step:35210 [D loss: 0.051716, acc.: 97.66%] [G loss: 0.000192]\n",
      "epoch:45 step:35211 [D loss: 0.057110, acc.: 99.22%] [G loss: 0.005281]\n",
      "epoch:45 step:35212 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.007048]\n",
      "epoch:45 step:35213 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.019530]\n",
      "epoch:45 step:35214 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.398626]\n",
      "epoch:45 step:35215 [D loss: 0.001159, acc.: 100.00%] [G loss: 0.001997]\n",
      "epoch:45 step:35216 [D loss: 0.003507, acc.: 100.00%] [G loss: 0.001494]\n",
      "epoch:45 step:35217 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:45 step:35218 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:45 step:35219 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.002975]\n",
      "epoch:45 step:35220 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.001034]\n",
      "epoch:45 step:35221 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:45 step:35222 [D loss: 0.007537, acc.: 100.00%] [G loss: 0.003570]\n",
      "epoch:45 step:35223 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:45 step:35224 [D loss: 0.006693, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:45 step:35225 [D loss: 0.010836, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:45 step:35226 [D loss: 0.126225, acc.: 96.88%] [G loss: 0.057394]\n",
      "epoch:45 step:35227 [D loss: 0.023226, acc.: 99.22%] [G loss: 0.741132]\n",
      "epoch:45 step:35228 [D loss: 0.035181, acc.: 99.22%] [G loss: 0.160059]\n",
      "epoch:45 step:35229 [D loss: 0.009928, acc.: 100.00%] [G loss: 0.060799]\n",
      "epoch:45 step:35230 [D loss: 0.110893, acc.: 97.66%] [G loss: 2.938671]\n",
      "epoch:45 step:35231 [D loss: 0.057093, acc.: 98.44%] [G loss: 0.376987]\n",
      "epoch:45 step:35232 [D loss: 0.007967, acc.: 100.00%] [G loss: 0.102230]\n",
      "epoch:45 step:35233 [D loss: 0.030852, acc.: 99.22%] [G loss: 0.017997]\n",
      "epoch:45 step:35234 [D loss: 0.270129, acc.: 92.19%] [G loss: 4.085433]\n",
      "epoch:45 step:35235 [D loss: 2.556326, acc.: 42.19%] [G loss: 3.625067]\n",
      "epoch:45 step:35236 [D loss: 0.530625, acc.: 80.47%] [G loss: 5.042455]\n",
      "epoch:45 step:35237 [D loss: 0.787707, acc.: 71.88%] [G loss: 1.345194]\n",
      "epoch:45 step:35238 [D loss: 0.018124, acc.: 100.00%] [G loss: 1.440350]\n",
      "epoch:45 step:35239 [D loss: 0.129901, acc.: 95.31%] [G loss: 1.665923]\n",
      "epoch:45 step:35240 [D loss: 0.256401, acc.: 86.72%] [G loss: 3.867373]\n",
      "epoch:45 step:35241 [D loss: 0.295032, acc.: 85.94%] [G loss: 2.211534]\n",
      "epoch:45 step:35242 [D loss: 0.155639, acc.: 96.09%] [G loss: 2.114575]\n",
      "epoch:45 step:35243 [D loss: 0.097996, acc.: 96.88%] [G loss: 0.985545]\n",
      "epoch:45 step:35244 [D loss: 0.097444, acc.: 95.31%] [G loss: 0.016871]\n",
      "epoch:45 step:35245 [D loss: 0.035565, acc.: 99.22%] [G loss: 0.025392]\n",
      "epoch:45 step:35246 [D loss: 0.012603, acc.: 100.00%] [G loss: 1.850598]\n",
      "epoch:45 step:35247 [D loss: 0.045359, acc.: 99.22%] [G loss: 0.003029]\n",
      "epoch:45 step:35248 [D loss: 0.005018, acc.: 100.00%] [G loss: 0.012975]\n",
      "epoch:45 step:35249 [D loss: 0.008789, acc.: 100.00%] [G loss: 1.323727]\n",
      "epoch:45 step:35250 [D loss: 0.006884, acc.: 100.00%] [G loss: 0.008134]\n",
      "epoch:45 step:35251 [D loss: 0.038304, acc.: 100.00%] [G loss: 0.013092]\n",
      "epoch:45 step:35252 [D loss: 0.044750, acc.: 98.44%] [G loss: 0.006754]\n",
      "epoch:45 step:35253 [D loss: 0.018949, acc.: 100.00%] [G loss: 0.001202]\n",
      "epoch:45 step:35254 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:45 step:35255 [D loss: 0.004887, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:45 step:35256 [D loss: 0.019246, acc.: 99.22%] [G loss: 0.120744]\n",
      "epoch:45 step:35257 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:45 step:35258 [D loss: 0.004023, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:45 step:35259 [D loss: 0.005846, acc.: 100.00%] [G loss: 0.050915]\n",
      "epoch:45 step:35260 [D loss: 0.004451, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:45 step:35261 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.005572]\n",
      "epoch:45 step:35262 [D loss: 0.005076, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:45 step:35263 [D loss: 0.003706, acc.: 100.00%] [G loss: 0.015895]\n",
      "epoch:45 step:35264 [D loss: 0.000980, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:45 step:35265 [D loss: 0.025331, acc.: 100.00%] [G loss: 0.053106]\n",
      "epoch:45 step:35266 [D loss: 0.016936, acc.: 100.00%] [G loss: 0.148163]\n",
      "epoch:45 step:35267 [D loss: 0.058242, acc.: 100.00%] [G loss: 0.000925]\n",
      "epoch:45 step:35268 [D loss: 0.011493, acc.: 99.22%] [G loss: 0.008581]\n",
      "epoch:45 step:35269 [D loss: 0.182153, acc.: 93.75%] [G loss: 0.264310]\n",
      "epoch:45 step:35270 [D loss: 0.013256, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:45 step:35271 [D loss: 0.015403, acc.: 100.00%] [G loss: 0.091996]\n",
      "epoch:45 step:35272 [D loss: 0.013306, acc.: 100.00%] [G loss: 0.001914]\n",
      "epoch:45 step:35273 [D loss: 0.021187, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:45 step:35274 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:45 step:35275 [D loss: 0.001439, acc.: 100.00%] [G loss: 0.432044]\n",
      "epoch:45 step:35276 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.529752]\n",
      "epoch:45 step:35277 [D loss: 0.011505, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:45 step:35278 [D loss: 0.011240, acc.: 99.22%] [G loss: 0.000168]\n",
      "epoch:45 step:35279 [D loss: 0.001340, acc.: 100.00%] [G loss: 0.007123]\n",
      "epoch:45 step:35280 [D loss: 0.015500, acc.: 100.00%] [G loss: 0.556599]\n",
      "epoch:45 step:35281 [D loss: 0.016200, acc.: 100.00%] [G loss: 0.024377]\n",
      "epoch:45 step:35282 [D loss: 0.295469, acc.: 87.50%] [G loss: 0.504914]\n",
      "epoch:45 step:35283 [D loss: 0.282286, acc.: 87.50%] [G loss: 4.455946]\n",
      "epoch:45 step:35284 [D loss: 0.023161, acc.: 99.22%] [G loss: 3.539240]\n",
      "epoch:45 step:35285 [D loss: 0.049156, acc.: 100.00%] [G loss: 1.093220]\n",
      "epoch:45 step:35286 [D loss: 0.296224, acc.: 88.28%] [G loss: 1.432497]\n",
      "epoch:45 step:35287 [D loss: 0.370670, acc.: 83.59%] [G loss: 3.115216]\n",
      "epoch:45 step:35288 [D loss: 0.039345, acc.: 99.22%] [G loss: 1.747275]\n",
      "epoch:45 step:35289 [D loss: 0.032896, acc.: 100.00%] [G loss: 0.875068]\n",
      "epoch:45 step:35290 [D loss: 0.040165, acc.: 100.00%] [G loss: 0.471684]\n",
      "epoch:45 step:35291 [D loss: 0.064799, acc.: 99.22%] [G loss: 0.026353]\n",
      "epoch:45 step:35292 [D loss: 0.011405, acc.: 100.00%] [G loss: 0.133859]\n",
      "epoch:45 step:35293 [D loss: 0.170048, acc.: 92.19%] [G loss: 0.021279]\n",
      "epoch:45 step:35294 [D loss: 0.030460, acc.: 99.22%] [G loss: 0.002264]\n",
      "epoch:45 step:35295 [D loss: 0.018466, acc.: 100.00%] [G loss: 0.750899]\n",
      "epoch:45 step:35296 [D loss: 0.037240, acc.: 98.44%] [G loss: 0.001148]\n",
      "epoch:45 step:35297 [D loss: 0.003504, acc.: 100.00%] [G loss: 0.462736]\n",
      "epoch:45 step:35298 [D loss: 0.010869, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:45 step:35299 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:45 step:35300 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:45 step:35301 [D loss: 0.017899, acc.: 99.22%] [G loss: 0.000018]\n",
      "epoch:45 step:35302 [D loss: 0.009092, acc.: 99.22%] [G loss: 0.000105]\n",
      "epoch:45 step:35303 [D loss: 0.001490, acc.: 100.00%] [G loss: 0.000207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35304 [D loss: 0.002190, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:45 step:35305 [D loss: 0.003031, acc.: 100.00%] [G loss: 0.133026]\n",
      "epoch:45 step:35306 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.017353]\n",
      "epoch:45 step:35307 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:45 step:35308 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:45 step:35309 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:45 step:35310 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.008193]\n",
      "epoch:45 step:35311 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.017326]\n",
      "epoch:45 step:35312 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:45 step:35313 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35314 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:45 step:35315 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35316 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35317 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.003903]\n",
      "epoch:45 step:35318 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:45 step:35319 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35320 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:45 step:35321 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.003118]\n",
      "epoch:45 step:35322 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35323 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35324 [D loss: 0.002742, acc.: 100.00%] [G loss: 0.001722]\n",
      "epoch:45 step:35325 [D loss: 0.001614, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35326 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35327 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35328 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35329 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35330 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:45 step:35331 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35332 [D loss: 0.006537, acc.: 100.00%] [G loss: 0.001110]\n",
      "epoch:45 step:35333 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35334 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:45 step:35335 [D loss: 0.001699, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:45 step:35336 [D loss: 0.001125, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35337 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35338 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:45 step:35339 [D loss: 0.001874, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35340 [D loss: 0.007147, acc.: 100.00%] [G loss: 0.007498]\n",
      "epoch:45 step:35341 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35342 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:45 step:35343 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:45 step:35344 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:45 step:35345 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:45 step:35346 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35347 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35348 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:45 step:35349 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35350 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:45 step:35351 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35352 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001148]\n",
      "epoch:45 step:35353 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35354 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.001460]\n",
      "epoch:45 step:35355 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.021502]\n",
      "epoch:45 step:35356 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001034]\n",
      "epoch:45 step:35357 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35358 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:45 step:35359 [D loss: 0.001212, acc.: 100.00%] [G loss: 0.002022]\n",
      "epoch:45 step:35360 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:45 step:35361 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35362 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:45 step:35363 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:45 step:35364 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:45 step:35365 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:45 step:35366 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35367 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:45 step:35368 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.000906]\n",
      "epoch:45 step:35369 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:45 step:35370 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35371 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35372 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35373 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35374 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.003091]\n",
      "epoch:45 step:35375 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35376 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.027348]\n",
      "epoch:45 step:35377 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35378 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35379 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35380 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001723]\n",
      "epoch:45 step:35381 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35382 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35383 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35384 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35385 [D loss: 0.006599, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35386 [D loss: 0.002023, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:45 step:35387 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:45 step:35388 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:45 step:35389 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35390 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35391 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:45 step:35392 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35393 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:45 step:35394 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35395 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35396 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35397 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35398 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:45 step:35399 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35400 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35401 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:45 step:35402 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35403 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35404 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35405 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35406 [D loss: 0.021716, acc.: 99.22%] [G loss: 0.000001]\n",
      "epoch:45 step:35407 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35408 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35409 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35410 [D loss: 0.021951, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35411 [D loss: 0.101387, acc.: 96.88%] [G loss: 0.000288]\n",
      "epoch:45 step:35412 [D loss: 0.000965, acc.: 100.00%] [G loss: 0.274172]\n",
      "epoch:45 step:35413 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.040807]\n",
      "epoch:45 step:35414 [D loss: 0.003230, acc.: 100.00%] [G loss: 2.240027]\n",
      "epoch:45 step:35415 [D loss: 0.043535, acc.: 98.44%] [G loss: 0.000922]\n",
      "epoch:45 step:35416 [D loss: 0.002463, acc.: 100.00%] [G loss: 0.000524]\n",
      "epoch:45 step:35417 [D loss: 0.402707, acc.: 76.56%] [G loss: 2.879575]\n",
      "epoch:45 step:35418 [D loss: 0.365058, acc.: 79.69%] [G loss: 0.810772]\n",
      "epoch:45 step:35419 [D loss: 0.135024, acc.: 93.75%] [G loss: 1.487445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35420 [D loss: 0.293489, acc.: 87.50%] [G loss: 0.031790]\n",
      "epoch:45 step:35421 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.392357]\n",
      "epoch:45 step:35422 [D loss: 0.004392, acc.: 100.00%] [G loss: 0.223044]\n",
      "epoch:45 step:35423 [D loss: 0.231321, acc.: 92.19%] [G loss: 0.824533]\n",
      "epoch:45 step:35424 [D loss: 0.001300, acc.: 100.00%] [G loss: 3.092583]\n",
      "epoch:45 step:35425 [D loss: 0.013399, acc.: 100.00%] [G loss: 2.399191]\n",
      "epoch:45 step:35426 [D loss: 0.096505, acc.: 97.66%] [G loss: 3.422247]\n",
      "epoch:45 step:35427 [D loss: 1.476275, acc.: 45.31%] [G loss: 7.832305]\n",
      "epoch:45 step:35428 [D loss: 0.015050, acc.: 99.22%] [G loss: 10.762207]\n",
      "epoch:45 step:35429 [D loss: 0.422354, acc.: 82.03%] [G loss: 8.496185]\n",
      "epoch:45 step:35430 [D loss: 0.002993, acc.: 100.00%] [G loss: 7.534494]\n",
      "epoch:45 step:35431 [D loss: 0.024419, acc.: 99.22%] [G loss: 6.315205]\n",
      "epoch:45 step:35432 [D loss: 0.025041, acc.: 99.22%] [G loss: 6.442827]\n",
      "epoch:45 step:35433 [D loss: 0.013219, acc.: 100.00%] [G loss: 4.325903]\n",
      "epoch:45 step:35434 [D loss: 0.037577, acc.: 100.00%] [G loss: 7.496328]\n",
      "epoch:45 step:35435 [D loss: 0.008109, acc.: 100.00%] [G loss: 4.605177]\n",
      "epoch:45 step:35436 [D loss: 0.132136, acc.: 95.31%] [G loss: 7.269280]\n",
      "epoch:45 step:35437 [D loss: 0.066299, acc.: 98.44%] [G loss: 0.644207]\n",
      "epoch:45 step:35438 [D loss: 0.106847, acc.: 96.88%] [G loss: 0.093536]\n",
      "epoch:45 step:35439 [D loss: 0.062894, acc.: 98.44%] [G loss: 7.477580]\n",
      "epoch:45 step:35440 [D loss: 0.098171, acc.: 96.88%] [G loss: 0.034351]\n",
      "epoch:45 step:35441 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.008965]\n",
      "epoch:45 step:35442 [D loss: 0.002560, acc.: 100.00%] [G loss: 0.463269]\n",
      "epoch:45 step:35443 [D loss: 0.089548, acc.: 98.44%] [G loss: 0.039754]\n",
      "epoch:45 step:35444 [D loss: 0.161723, acc.: 95.31%] [G loss: 1.064900]\n",
      "epoch:45 step:35445 [D loss: 0.278501, acc.: 92.19%] [G loss: 5.023107]\n",
      "epoch:45 step:35446 [D loss: 0.009647, acc.: 100.00%] [G loss: 0.182157]\n",
      "epoch:45 step:35447 [D loss: 0.004286, acc.: 100.00%] [G loss: 0.155306]\n",
      "epoch:45 step:35448 [D loss: 0.068758, acc.: 98.44%] [G loss: 0.094538]\n",
      "epoch:45 step:35449 [D loss: 0.003355, acc.: 100.00%] [G loss: 2.827495]\n",
      "epoch:45 step:35450 [D loss: 0.006830, acc.: 100.00%] [G loss: 0.198081]\n",
      "epoch:45 step:35451 [D loss: 0.023235, acc.: 99.22%] [G loss: 0.084692]\n",
      "epoch:45 step:35452 [D loss: 0.002434, acc.: 100.00%] [G loss: 0.011301]\n",
      "epoch:45 step:35453 [D loss: 0.075052, acc.: 99.22%] [G loss: 0.802620]\n",
      "epoch:45 step:35454 [D loss: 0.040899, acc.: 98.44%] [G loss: 0.867074]\n",
      "epoch:45 step:35455 [D loss: 0.004253, acc.: 100.00%] [G loss: 0.001636]\n",
      "epoch:45 step:35456 [D loss: 0.005076, acc.: 100.00%] [G loss: 0.003380]\n",
      "epoch:45 step:35457 [D loss: 0.012104, acc.: 99.22%] [G loss: 0.001249]\n",
      "epoch:45 step:35458 [D loss: 0.007273, acc.: 100.00%] [G loss: 0.168201]\n",
      "epoch:45 step:35459 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:45 step:35460 [D loss: 0.498290, acc.: 77.34%] [G loss: 0.000000]\n",
      "epoch:45 step:35461 [D loss: 0.003233, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35462 [D loss: 0.001211, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:45 step:35463 [D loss: 0.041160, acc.: 97.66%] [G loss: 0.000002]\n",
      "epoch:45 step:35464 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:45 step:35465 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35466 [D loss: 0.002254, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35467 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:45 step:35468 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:45 step:35469 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:45 step:35470 [D loss: 0.006454, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35471 [D loss: 0.003703, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:45 step:35472 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:45 step:35473 [D loss: 1.414713, acc.: 54.69%] [G loss: 3.746033]\n",
      "epoch:45 step:35474 [D loss: 0.184240, acc.: 93.75%] [G loss: 9.861189]\n",
      "epoch:45 step:35475 [D loss: 1.058074, acc.: 60.94%] [G loss: 3.925911]\n",
      "epoch:45 step:35476 [D loss: 0.119967, acc.: 96.09%] [G loss: 1.453452]\n",
      "epoch:45 step:35477 [D loss: 0.011342, acc.: 100.00%] [G loss: 0.091570]\n",
      "epoch:45 step:35478 [D loss: 0.059333, acc.: 97.66%] [G loss: 0.096235]\n",
      "epoch:45 step:35479 [D loss: 0.003827, acc.: 100.00%] [G loss: 0.077615]\n",
      "epoch:45 step:35480 [D loss: 0.001767, acc.: 100.00%] [G loss: 0.065780]\n",
      "epoch:45 step:35481 [D loss: 0.005309, acc.: 100.00%] [G loss: 1.115420]\n",
      "epoch:45 step:35482 [D loss: 0.002728, acc.: 100.00%] [G loss: 0.287002]\n",
      "epoch:45 step:35483 [D loss: 0.042104, acc.: 99.22%] [G loss: 0.010143]\n",
      "epoch:45 step:35484 [D loss: 0.003106, acc.: 100.00%] [G loss: 0.061274]\n",
      "epoch:45 step:35485 [D loss: 0.006480, acc.: 100.00%] [G loss: 0.017881]\n",
      "epoch:45 step:35486 [D loss: 0.208595, acc.: 92.19%] [G loss: 0.286199]\n",
      "epoch:45 step:35487 [D loss: 0.020665, acc.: 100.00%] [G loss: 0.023695]\n",
      "epoch:45 step:35488 [D loss: 0.016088, acc.: 100.00%] [G loss: 0.006745]\n",
      "epoch:45 step:35489 [D loss: 0.020597, acc.: 99.22%] [G loss: 0.014838]\n",
      "epoch:45 step:35490 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.805221]\n",
      "epoch:45 step:35491 [D loss: 0.013973, acc.: 100.00%] [G loss: 0.128059]\n",
      "epoch:45 step:35492 [D loss: 0.017009, acc.: 99.22%] [G loss: 0.001180]\n",
      "epoch:45 step:35493 [D loss: 0.001232, acc.: 100.00%] [G loss: 0.001191]\n",
      "epoch:45 step:35494 [D loss: 0.011345, acc.: 99.22%] [G loss: 0.005163]\n",
      "epoch:45 step:35495 [D loss: 0.014296, acc.: 99.22%] [G loss: 0.041826]\n",
      "epoch:45 step:35496 [D loss: 0.002053, acc.: 100.00%] [G loss: 0.006364]\n",
      "epoch:45 step:35497 [D loss: 0.047280, acc.: 97.66%] [G loss: 0.024661]\n",
      "epoch:45 step:35498 [D loss: 0.100817, acc.: 96.88%] [G loss: 0.012398]\n",
      "epoch:45 step:35499 [D loss: 0.028740, acc.: 98.44%] [G loss: 0.000637]\n",
      "epoch:45 step:35500 [D loss: 0.003310, acc.: 100.00%] [G loss: 0.058266]\n",
      "epoch:45 step:35501 [D loss: 0.012996, acc.: 100.00%] [G loss: 0.161887]\n",
      "epoch:45 step:35502 [D loss: 0.001656, acc.: 100.00%] [G loss: 0.001062]\n",
      "epoch:45 step:35503 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.038069]\n",
      "epoch:45 step:35504 [D loss: 0.005680, acc.: 100.00%] [G loss: 0.039869]\n",
      "epoch:45 step:35505 [D loss: 0.020258, acc.: 100.00%] [G loss: 0.055517]\n",
      "epoch:45 step:35506 [D loss: 0.036815, acc.: 99.22%] [G loss: 0.082057]\n",
      "epoch:45 step:35507 [D loss: 0.023806, acc.: 100.00%] [G loss: 0.169079]\n",
      "epoch:45 step:35508 [D loss: 0.073261, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:45 step:35509 [D loss: 0.022379, acc.: 99.22%] [G loss: 1.383046]\n",
      "epoch:45 step:35510 [D loss: 0.030809, acc.: 99.22%] [G loss: 0.342489]\n",
      "epoch:45 step:35511 [D loss: 0.060786, acc.: 98.44%] [G loss: 0.000737]\n",
      "epoch:45 step:35512 [D loss: 0.018304, acc.: 100.00%] [G loss: 0.001935]\n",
      "epoch:45 step:35513 [D loss: 0.057394, acc.: 97.66%] [G loss: 0.985730]\n",
      "epoch:45 step:35514 [D loss: 0.011479, acc.: 100.00%] [G loss: 0.166162]\n",
      "epoch:45 step:35515 [D loss: 0.013294, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:45 step:35516 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:45 step:35517 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:45 step:35518 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:45 step:35519 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:45 step:35520 [D loss: 0.032319, acc.: 97.66%] [G loss: 0.027572]\n",
      "epoch:45 step:35521 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:45 step:35522 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.005178]\n",
      "epoch:45 step:35523 [D loss: 0.000459, acc.: 100.00%] [G loss: 0.105027]\n",
      "epoch:45 step:35524 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.009271]\n",
      "epoch:45 step:35525 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35526 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.002565]\n",
      "epoch:45 step:35527 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.016339]\n",
      "epoch:45 step:35528 [D loss: 0.003292, acc.: 100.00%] [G loss: 0.062348]\n",
      "epoch:45 step:35529 [D loss: 0.004429, acc.: 100.00%] [G loss: 0.034488]\n",
      "epoch:45 step:35530 [D loss: 0.041053, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35531 [D loss: 0.004536, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:45 step:35532 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.136914]\n",
      "epoch:45 step:35533 [D loss: 0.022960, acc.: 99.22%] [G loss: 0.000032]\n",
      "epoch:45 step:35534 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.054390]\n",
      "epoch:45 step:35535 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.028601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35536 [D loss: 0.002664, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35537 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35538 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.087928]\n",
      "epoch:45 step:35539 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.022373]\n",
      "epoch:45 step:35540 [D loss: 0.019786, acc.: 99.22%] [G loss: 0.000035]\n",
      "epoch:45 step:35541 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.028805]\n",
      "epoch:45 step:35542 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35543 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:45 step:35544 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35545 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.004406]\n",
      "epoch:45 step:35546 [D loss: 0.004259, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35547 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.005774]\n",
      "epoch:45 step:35548 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.002557]\n",
      "epoch:45 step:35549 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35550 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.012473]\n",
      "epoch:45 step:35551 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35552 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001920]\n",
      "epoch:45 step:35553 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:45 step:35554 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35555 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35556 [D loss: 0.001631, acc.: 100.00%] [G loss: 0.008400]\n",
      "epoch:45 step:35557 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.002448]\n",
      "epoch:45 step:35558 [D loss: 0.006703, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35559 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35560 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35561 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35562 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35563 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000747]\n",
      "epoch:45 step:35564 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35565 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35566 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:45 step:35567 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:45 step:35568 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35569 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35570 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:45 step:35571 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.003393]\n",
      "epoch:45 step:35572 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.005049]\n",
      "epoch:45 step:35573 [D loss: 0.001240, acc.: 100.00%] [G loss: 0.005090]\n",
      "epoch:45 step:35574 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.001952]\n",
      "epoch:45 step:35575 [D loss: 0.011268, acc.: 100.00%] [G loss: 0.001672]\n",
      "epoch:45 step:35576 [D loss: 0.013362, acc.: 100.00%] [G loss: 0.001419]\n",
      "epoch:45 step:35577 [D loss: 0.005676, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35578 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35579 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001587]\n",
      "epoch:45 step:35580 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002973]\n",
      "epoch:45 step:35581 [D loss: 0.002556, acc.: 100.00%] [G loss: 0.005342]\n",
      "epoch:45 step:35582 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.002240]\n",
      "epoch:45 step:35583 [D loss: 0.000949, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35584 [D loss: 0.002580, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35585 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.002496]\n",
      "epoch:45 step:35586 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35587 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35588 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35589 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.003857]\n",
      "epoch:45 step:35590 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.003553]\n",
      "epoch:45 step:35591 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.002062]\n",
      "epoch:45 step:35592 [D loss: 0.002777, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35593 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.004743]\n",
      "epoch:45 step:35594 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35595 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:45 step:35596 [D loss: 0.005888, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35597 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.009991]\n",
      "epoch:45 step:35598 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.010526]\n",
      "epoch:45 step:35599 [D loss: 0.003282, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35600 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.003961]\n",
      "epoch:45 step:35601 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35602 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.007704]\n",
      "epoch:45 step:35603 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.001762]\n",
      "epoch:45 step:35604 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.008760]\n",
      "epoch:45 step:35605 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.002717]\n",
      "epoch:45 step:35606 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.005864]\n",
      "epoch:45 step:35607 [D loss: 0.041847, acc.: 99.22%] [G loss: 0.000003]\n",
      "epoch:45 step:35608 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.945033]\n",
      "epoch:45 step:35609 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.229580]\n",
      "epoch:45 step:35610 [D loss: 0.003134, acc.: 100.00%] [G loss: 0.061095]\n",
      "epoch:45 step:35611 [D loss: 0.053686, acc.: 98.44%] [G loss: 0.461844]\n",
      "epoch:45 step:35612 [D loss: 0.038201, acc.: 99.22%] [G loss: 0.004192]\n",
      "epoch:45 step:35613 [D loss: 0.006296, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:45 step:35614 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.618867]\n",
      "epoch:45 step:35615 [D loss: 0.005488, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:45 step:35616 [D loss: 0.002476, acc.: 100.00%] [G loss: 0.151968]\n",
      "epoch:45 step:35617 [D loss: 0.003965, acc.: 100.00%] [G loss: 0.000550]\n",
      "epoch:45 step:35618 [D loss: 0.018309, acc.: 99.22%] [G loss: 0.000217]\n",
      "epoch:45 step:35619 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.000863]\n",
      "epoch:45 step:35620 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.013743]\n",
      "epoch:45 step:35621 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:45 step:35622 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.018591]\n",
      "epoch:45 step:35623 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.009391]\n",
      "epoch:45 step:35624 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.045081]\n",
      "epoch:45 step:35625 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:45 step:35626 [D loss: 0.001092, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:45 step:35627 [D loss: 0.002561, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:45 step:35628 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.002819]\n",
      "epoch:45 step:35629 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:45 step:35630 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:45 step:35631 [D loss: 0.001105, acc.: 100.00%] [G loss: 0.010957]\n",
      "epoch:45 step:35632 [D loss: 0.000874, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:45 step:35633 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:45 step:35634 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:45 step:35635 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.003286]\n",
      "epoch:45 step:35636 [D loss: 0.001797, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:45 step:35637 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35638 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.002679]\n",
      "epoch:45 step:35639 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.002154]\n",
      "epoch:45 step:35640 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.008939]\n",
      "epoch:45 step:35641 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.011926]\n",
      "epoch:45 step:35642 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.001401]\n",
      "epoch:45 step:35643 [D loss: 0.001270, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35644 [D loss: 0.001004, acc.: 100.00%] [G loss: 0.001347]\n",
      "epoch:45 step:35645 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35646 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35647 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001565]\n",
      "epoch:45 step:35648 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35649 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.001589]\n",
      "epoch:45 step:35650 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35651 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:45 step:35652 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35653 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.003208]\n",
      "epoch:45 step:35654 [D loss: 0.004674, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:45 step:35655 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.003006]\n",
      "epoch:45 step:35656 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35657 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.001091]\n",
      "epoch:45 step:35658 [D loss: 0.005481, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35659 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35660 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35661 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35662 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35663 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001288]\n",
      "epoch:45 step:35664 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:45 step:35665 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:45 step:35666 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35667 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35668 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:45 step:35669 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35670 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:45 step:35671 [D loss: 0.014390, acc.: 99.22%] [G loss: 0.000927]\n",
      "epoch:45 step:35672 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35673 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35674 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35675 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001690]\n",
      "epoch:45 step:35676 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35677 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:45 step:35678 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35679 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:45 step:35680 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:45 step:35681 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35682 [D loss: 0.005834, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35683 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:45 step:35684 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:45 step:35685 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35686 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35687 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:45 step:35688 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:45 step:35689 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35690 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35691 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35692 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001432]\n",
      "epoch:45 step:35693 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35694 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:45 step:35695 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:45 step:35696 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35697 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35698 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35699 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35700 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:45 step:35701 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35702 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35703 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35704 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:45 step:35705 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:45 step:35706 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:45 step:35707 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:45 step:35708 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:45 step:35709 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35710 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:45 step:35711 [D loss: 0.004655, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:45 step:35712 [D loss: 0.011480, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:45 step:35713 [D loss: 0.034845, acc.: 99.22%] [G loss: 0.004573]\n",
      "epoch:45 step:35714 [D loss: 0.006444, acc.: 100.00%] [G loss: 0.364756]\n",
      "epoch:45 step:35715 [D loss: 0.002553, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:45 step:35716 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.110669]\n",
      "epoch:45 step:35717 [D loss: 0.000806, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:45 step:35718 [D loss: 0.008572, acc.: 100.00%] [G loss: 0.089420]\n",
      "epoch:45 step:35719 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35720 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.013467]\n",
      "epoch:45 step:35721 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:45 step:35722 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:45 step:35723 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:45 step:35724 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:45 step:35725 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:45 step:35726 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:45 step:35727 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.003763]\n",
      "epoch:45 step:35728 [D loss: 0.000960, acc.: 100.00%] [G loss: 0.001780]\n",
      "epoch:45 step:35729 [D loss: 0.001759, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35730 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001841]\n",
      "epoch:45 step:35731 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:45 step:35732 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:45 step:35733 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000747]\n",
      "epoch:45 step:35734 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.002223]\n",
      "epoch:45 step:35735 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:45 step:35736 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.001851]\n",
      "epoch:45 step:35737 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:45 step:35738 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35739 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35740 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:45 step:35741 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.003979]\n",
      "epoch:45 step:35742 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002411]\n",
      "epoch:45 step:35743 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.006239]\n",
      "epoch:45 step:35744 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:45 step:35745 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35746 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35747 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000311]\n",
      "epoch:45 step:35748 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004932]\n",
      "epoch:45 step:35749 [D loss: 0.001004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35750 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:45 step:35751 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35752 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000988]\n",
      "epoch:45 step:35753 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:45 step:35754 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:45 step:35755 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35756 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35757 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35758 [D loss: 0.000827, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35759 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35760 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35761 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003673]\n",
      "epoch:45 step:35762 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:45 step:35763 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35764 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35765 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000839]\n",
      "epoch:45 step:35766 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35767 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35768 [D loss: 0.013847, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35769 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35770 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35771 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:45 step:35772 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:45 step:35773 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:45 step:35774 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:45 step:35775 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:45 step:35776 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35777 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35778 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:45 step:35779 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35780 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:45 step:35781 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35782 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:45 step:35783 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:45 step:35784 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:45 step:35785 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:45 step:35786 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35787 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000595]\n",
      "epoch:45 step:35788 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:45 step:35789 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:45 step:35790 [D loss: 0.003059, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:45 step:35791 [D loss: 0.097576, acc.: 97.66%] [G loss: 0.002107]\n",
      "epoch:45 step:35792 [D loss: 0.000168, acc.: 100.00%] [G loss: 6.426411]\n",
      "epoch:45 step:35793 [D loss: 0.118144, acc.: 94.53%] [G loss: 1.387052]\n",
      "epoch:45 step:35794 [D loss: 0.016100, acc.: 99.22%] [G loss: 0.540021]\n",
      "epoch:45 step:35795 [D loss: 0.025426, acc.: 99.22%] [G loss: 0.928409]\n",
      "epoch:45 step:35796 [D loss: 0.011412, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:45 step:35797 [D loss: 0.002563, acc.: 100.00%] [G loss: 1.026588]\n",
      "epoch:45 step:35798 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.271500]\n",
      "epoch:45 step:35799 [D loss: 0.002449, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:45 step:35800 [D loss: 0.001853, acc.: 100.00%] [G loss: 0.250955]\n",
      "epoch:45 step:35801 [D loss: 0.012114, acc.: 100.00%] [G loss: 0.002792]\n",
      "epoch:45 step:35802 [D loss: 0.001038, acc.: 100.00%] [G loss: 0.351706]\n",
      "epoch:45 step:35803 [D loss: 0.005879, acc.: 100.00%] [G loss: 0.018678]\n",
      "epoch:45 step:35804 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.003219]\n",
      "epoch:45 step:35805 [D loss: 0.001494, acc.: 100.00%] [G loss: 0.010461]\n",
      "epoch:45 step:35806 [D loss: 0.008963, acc.: 100.00%] [G loss: 0.001970]\n",
      "epoch:45 step:35807 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.002753]\n",
      "epoch:45 step:35808 [D loss: 0.001445, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:45 step:35809 [D loss: 0.001932, acc.: 100.00%] [G loss: 0.001275]\n",
      "epoch:45 step:35810 [D loss: 0.022244, acc.: 99.22%] [G loss: 0.000199]\n",
      "epoch:45 step:35811 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000779]\n",
      "epoch:45 step:35812 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35813 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:45 step:35814 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:45 step:35815 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35816 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000407]\n",
      "epoch:45 step:35817 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:45 step:35818 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35819 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35820 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:45 step:35821 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35822 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:45 step:35823 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:45 step:35824 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35825 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:45 step:35826 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:45 step:35827 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:45 step:35828 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001028]\n",
      "epoch:45 step:35829 [D loss: 0.015868, acc.: 100.00%] [G loss: 0.001650]\n",
      "epoch:45 step:35830 [D loss: 0.009082, acc.: 100.00%] [G loss: 0.019502]\n",
      "epoch:45 step:35831 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:45 step:35832 [D loss: 0.014098, acc.: 99.22%] [G loss: 0.000075]\n",
      "epoch:45 step:35833 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.024933]\n",
      "epoch:45 step:35834 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:45 step:35835 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:45 step:35836 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:45 step:35837 [D loss: 0.027742, acc.: 99.22%] [G loss: 0.002328]\n",
      "epoch:45 step:35838 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:45 step:35839 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:45 step:35840 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35841 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35842 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35843 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35844 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000731]\n",
      "epoch:45 step:35845 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:45 step:35846 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35847 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35848 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35849 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.002402]\n",
      "epoch:45 step:35850 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35851 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35852 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35853 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:45 step:35854 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:45 step:35855 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:45 step:35856 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000442]\n",
      "epoch:45 step:35857 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:45 step:35858 [D loss: 0.001140, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:45 step:35859 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:45 step:35860 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:45 step:35861 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:45 step:35862 [D loss: 0.007105, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:45 step:35863 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.010451]\n",
      "epoch:45 step:35864 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:45 step:35865 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35866 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.003305]\n",
      "epoch:45 step:35867 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35868 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:45 step:35869 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:45 step:35870 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001259]\n",
      "epoch:45 step:35871 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:45 step:35872 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.005127]\n",
      "epoch:45 step:35873 [D loss: 0.000874, acc.: 100.00%] [G loss: 0.000899]\n",
      "epoch:45 step:35874 [D loss: 0.668740, acc.: 65.62%] [G loss: 14.101972]\n",
      "epoch:45 step:35875 [D loss: 4.417963, acc.: 50.00%] [G loss: 6.123773]\n",
      "epoch:45 step:35876 [D loss: 0.023493, acc.: 99.22%] [G loss: 0.844410]\n",
      "epoch:45 step:35877 [D loss: 0.017967, acc.: 100.00%] [G loss: 0.082501]\n",
      "epoch:45 step:35878 [D loss: 0.015047, acc.: 100.00%] [G loss: 0.091101]\n",
      "epoch:45 step:35879 [D loss: 0.014273, acc.: 100.00%] [G loss: 0.941724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35880 [D loss: 0.020872, acc.: 100.00%] [G loss: 4.416101]\n",
      "epoch:45 step:35881 [D loss: 0.027468, acc.: 99.22%] [G loss: 0.000667]\n",
      "epoch:45 step:35882 [D loss: 0.003317, acc.: 100.00%] [G loss: 2.734773]\n",
      "epoch:45 step:35883 [D loss: 0.091533, acc.: 98.44%] [G loss: 2.934894]\n",
      "epoch:45 step:35884 [D loss: 0.067122, acc.: 99.22%] [G loss: 0.019477]\n",
      "epoch:45 step:35885 [D loss: 0.007658, acc.: 100.00%] [G loss: 0.000903]\n",
      "epoch:45 step:35886 [D loss: 0.002826, acc.: 100.00%] [G loss: 2.732912]\n",
      "epoch:45 step:35887 [D loss: 0.025521, acc.: 100.00%] [G loss: 0.772988]\n",
      "epoch:45 step:35888 [D loss: 0.255444, acc.: 89.84%] [G loss: 2.638101]\n",
      "epoch:45 step:35889 [D loss: 0.040269, acc.: 98.44%] [G loss: 0.194075]\n",
      "epoch:45 step:35890 [D loss: 0.187523, acc.: 92.97%] [G loss: 0.003444]\n",
      "epoch:45 step:35891 [D loss: 0.013765, acc.: 100.00%] [G loss: 0.395709]\n",
      "epoch:45 step:35892 [D loss: 0.004987, acc.: 100.00%] [G loss: 0.298712]\n",
      "epoch:45 step:35893 [D loss: 0.193522, acc.: 92.19%] [G loss: 0.002713]\n",
      "epoch:45 step:35894 [D loss: 0.038566, acc.: 99.22%] [G loss: 0.009101]\n",
      "epoch:45 step:35895 [D loss: 0.063094, acc.: 97.66%] [G loss: 0.000678]\n",
      "epoch:45 step:35896 [D loss: 0.003021, acc.: 100.00%] [G loss: 1.376137]\n",
      "epoch:45 step:35897 [D loss: 0.004007, acc.: 100.00%] [G loss: 0.607082]\n",
      "epoch:45 step:35898 [D loss: 0.027820, acc.: 99.22%] [G loss: 0.000159]\n",
      "epoch:45 step:35899 [D loss: 0.011036, acc.: 100.00%] [G loss: 0.228733]\n",
      "epoch:45 step:35900 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.322364]\n",
      "epoch:45 step:35901 [D loss: 0.019837, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:45 step:35902 [D loss: 0.002465, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:45 step:35903 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.001049]\n",
      "epoch:45 step:35904 [D loss: 0.014550, acc.: 100.00%] [G loss: 0.017209]\n",
      "epoch:45 step:35905 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.042798]\n",
      "epoch:45 step:35906 [D loss: 0.001754, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:45 step:35907 [D loss: 0.015847, acc.: 100.00%] [G loss: 0.049406]\n",
      "epoch:45 step:35908 [D loss: 0.004233, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:45 step:35909 [D loss: 0.001135, acc.: 100.00%] [G loss: 0.004619]\n",
      "epoch:45 step:35910 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.009053]\n",
      "epoch:45 step:35911 [D loss: 0.005746, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:45 step:35912 [D loss: 0.002368, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:45 step:35913 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.084119]\n",
      "epoch:45 step:35914 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:45 step:35915 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.003910]\n",
      "epoch:45 step:35916 [D loss: 0.040445, acc.: 99.22%] [G loss: 0.001059]\n",
      "epoch:45 step:35917 [D loss: 0.004291, acc.: 100.00%] [G loss: 0.002658]\n",
      "epoch:45 step:35918 [D loss: 0.005308, acc.: 100.00%] [G loss: 0.001955]\n",
      "epoch:45 step:35919 [D loss: 0.086730, acc.: 99.22%] [G loss: 0.028235]\n",
      "epoch:45 step:35920 [D loss: 0.004283, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:45 step:35921 [D loss: 0.003760, acc.: 100.00%] [G loss: 0.472797]\n",
      "epoch:45 step:35922 [D loss: 0.020365, acc.: 99.22%] [G loss: 0.043313]\n",
      "epoch:45 step:35923 [D loss: 0.025507, acc.: 99.22%] [G loss: 0.000139]\n",
      "epoch:45 step:35924 [D loss: 0.002782, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:45 step:35925 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.234464]\n",
      "epoch:45 step:35926 [D loss: 0.001863, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:35927 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:35928 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.021638]\n",
      "epoch:46 step:35929 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:46 step:35930 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.046101]\n",
      "epoch:46 step:35931 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:46 step:35932 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:46 step:35933 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:46 step:35934 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:46 step:35935 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.075322]\n",
      "epoch:46 step:35936 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:35937 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:35938 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.003613]\n",
      "epoch:46 step:35939 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:35940 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002905]\n",
      "epoch:46 step:35941 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:46 step:35942 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.007367]\n",
      "epoch:46 step:35943 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.004669]\n",
      "epoch:46 step:35944 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:35945 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:46 step:35946 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:46 step:35947 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:35948 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:35949 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.017033]\n",
      "epoch:46 step:35950 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:46 step:35951 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:46 step:35952 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:35953 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001599]\n",
      "epoch:46 step:35954 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.005762]\n",
      "epoch:46 step:35955 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:46 step:35956 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:35957 [D loss: 0.003888, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:35958 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.007397]\n",
      "epoch:46 step:35959 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.006433]\n",
      "epoch:46 step:35960 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:35961 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.002914]\n",
      "epoch:46 step:35962 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.010179]\n",
      "epoch:46 step:35963 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:35964 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.003096]\n",
      "epoch:46 step:35965 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:35966 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35967 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:46 step:35968 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:35969 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:35970 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:35971 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.002368]\n",
      "epoch:46 step:35972 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:46 step:35973 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:46 step:35974 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:46 step:35975 [D loss: 0.002236, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35976 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.007179]\n",
      "epoch:46 step:35977 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002164]\n",
      "epoch:46 step:35978 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35979 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:35980 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35981 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:35982 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:46 step:35983 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:46 step:35984 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.003107]\n",
      "epoch:46 step:35985 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:46 step:35986 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:35987 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35988 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35989 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:35990 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:46 step:35991 [D loss: 0.003092, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:46 step:35992 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:35993 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:35994 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:46 step:35995 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:35996 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:46 step:35997 [D loss: 0.010425, acc.: 99.22%] [G loss: 0.000001]\n",
      "epoch:46 step:35998 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:35999 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36000 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:46 step:36001 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36002 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36003 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000451]\n",
      "epoch:46 step:36004 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36005 [D loss: 0.003851, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36006 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:46 step:36007 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36008 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36009 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36010 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36011 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36012 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36013 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36014 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36015 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36016 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:46 step:36017 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36018 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:46 step:36019 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36020 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:46 step:36021 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:46 step:36022 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36023 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000672]\n",
      "epoch:46 step:36024 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000483]\n",
      "epoch:46 step:36025 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:46 step:36026 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36027 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:46 step:36028 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:36029 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36030 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36031 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:46 step:36032 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36033 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36034 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:46 step:36035 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:46 step:36036 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36037 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36038 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:46 step:36039 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:46 step:36040 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:46 step:36041 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36042 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36043 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36044 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36045 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:46 step:36046 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36047 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36048 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36049 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:46 step:36050 [D loss: 0.001327, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36051 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36052 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:46 step:36053 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000330]\n",
      "epoch:46 step:36054 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:46 step:36055 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36056 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36057 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36058 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36059 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36060 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36061 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36062 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:46 step:36063 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:46 step:36064 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36065 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36066 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36067 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36068 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36069 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000650]\n",
      "epoch:46 step:36070 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:46 step:36071 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36072 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36073 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36074 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36075 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001340]\n",
      "epoch:46 step:36076 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:46 step:36077 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36078 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:36079 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:46 step:36080 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36081 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36082 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36083 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36084 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36085 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36086 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36087 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36088 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36089 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36090 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36091 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:46 step:36092 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36093 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36094 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36095 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36096 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36097 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36098 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:46 step:36099 [D loss: 0.002551, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36100 [D loss: 0.021801, acc.: 100.00%] [G loss: 0.003305]\n",
      "epoch:46 step:36101 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:46 step:36102 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.005937]\n",
      "epoch:46 step:36103 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:46 step:36104 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:46 step:36105 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.001976]\n",
      "epoch:46 step:36106 [D loss: 0.024309, acc.: 99.22%] [G loss: 0.000003]\n",
      "epoch:46 step:36107 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:46 step:36108 [D loss: 0.093429, acc.: 97.66%] [G loss: 0.107581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36109 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.991606]\n",
      "epoch:46 step:36110 [D loss: 0.075340, acc.: 97.66%] [G loss: 0.112038]\n",
      "epoch:46 step:36111 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.078441]\n",
      "epoch:46 step:36112 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.050656]\n",
      "epoch:46 step:36113 [D loss: 0.001570, acc.: 100.00%] [G loss: 0.007114]\n",
      "epoch:46 step:36114 [D loss: 0.001427, acc.: 100.00%] [G loss: 0.007767]\n",
      "epoch:46 step:36115 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.002021]\n",
      "epoch:46 step:36116 [D loss: 0.003497, acc.: 100.00%] [G loss: 0.007273]\n",
      "epoch:46 step:36117 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.003986]\n",
      "epoch:46 step:36118 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000378]\n",
      "epoch:46 step:36119 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.004280]\n",
      "epoch:46 step:36120 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:46 step:36121 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:46 step:36122 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.007186]\n",
      "epoch:46 step:36123 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:46 step:36124 [D loss: 0.001531, acc.: 100.00%] [G loss: 0.001738]\n",
      "epoch:46 step:36125 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:46 step:36126 [D loss: 0.002262, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:46 step:36127 [D loss: 0.000924, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:46 step:36128 [D loss: 0.001057, acc.: 100.00%] [G loss: 0.002936]\n",
      "epoch:46 step:36129 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.005598]\n",
      "epoch:46 step:36130 [D loss: 0.003917, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:46 step:36131 [D loss: 0.003116, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:46 step:36132 [D loss: 0.031479, acc.: 98.44%] [G loss: 0.000260]\n",
      "epoch:46 step:36133 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:46 step:36134 [D loss: 0.005473, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:46 step:36135 [D loss: 0.001985, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:46 step:36136 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:46 step:36137 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.003534]\n",
      "epoch:46 step:36138 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.005839]\n",
      "epoch:46 step:36139 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:46 step:36140 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:46 step:36141 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:46 step:36142 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:46 step:36143 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:46 step:36144 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:46 step:36145 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.001395]\n",
      "epoch:46 step:36146 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.001201]\n",
      "epoch:46 step:36147 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000411]\n",
      "epoch:46 step:36148 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:46 step:36149 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:46 step:36150 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002200]\n",
      "epoch:46 step:36151 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:46 step:36152 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.002853]\n",
      "epoch:46 step:36153 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:46 step:36154 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:46 step:36155 [D loss: 0.000883, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:46 step:36156 [D loss: 0.007755, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:46 step:36157 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:46 step:36158 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:46 step:36159 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:46 step:36160 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:46 step:36161 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:46 step:36162 [D loss: 0.001253, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:46 step:36163 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:46 step:36164 [D loss: 0.001152, acc.: 100.00%] [G loss: 0.002590]\n",
      "epoch:46 step:36165 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:46 step:36166 [D loss: 0.019845, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:46 step:36167 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36168 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:46 step:36169 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36170 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:36171 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36172 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:46 step:36173 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:46 step:36174 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36175 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36176 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36177 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:46 step:36178 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36179 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:46 step:36180 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:46 step:36181 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36182 [D loss: 0.002755, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:46 step:36183 [D loss: 0.005318, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:46 step:36184 [D loss: 0.001346, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:46 step:36185 [D loss: 0.169899, acc.: 92.19%] [G loss: 1.160661]\n",
      "epoch:46 step:36186 [D loss: 0.056635, acc.: 97.66%] [G loss: 0.992074]\n",
      "epoch:46 step:36187 [D loss: 1.154040, acc.: 62.50%] [G loss: 0.000009]\n",
      "epoch:46 step:36188 [D loss: 2.837195, acc.: 55.47%] [G loss: 5.823913]\n",
      "epoch:46 step:36189 [D loss: 0.140346, acc.: 93.75%] [G loss: 6.191526]\n",
      "epoch:46 step:36190 [D loss: 0.733426, acc.: 66.41%] [G loss: 2.394593]\n",
      "epoch:46 step:36191 [D loss: 0.205518, acc.: 92.19%] [G loss: 0.944427]\n",
      "epoch:46 step:36192 [D loss: 0.318893, acc.: 87.50%] [G loss: 2.614116]\n",
      "epoch:46 step:36193 [D loss: 0.018034, acc.: 100.00%] [G loss: 2.724517]\n",
      "epoch:46 step:36194 [D loss: 0.017715, acc.: 100.00%] [G loss: 2.671175]\n",
      "epoch:46 step:36195 [D loss: 0.107238, acc.: 100.00%] [G loss: 0.435592]\n",
      "epoch:46 step:36196 [D loss: 0.436546, acc.: 82.81%] [G loss: 2.832486]\n",
      "epoch:46 step:36197 [D loss: 0.058108, acc.: 98.44%] [G loss: 0.310849]\n",
      "epoch:46 step:36198 [D loss: 0.023284, acc.: 99.22%] [G loss: 0.053990]\n",
      "epoch:46 step:36199 [D loss: 0.013578, acc.: 100.00%] [G loss: 3.015872]\n",
      "epoch:46 step:36200 [D loss: 0.010632, acc.: 100.00%] [G loss: 1.495932]\n",
      "epoch:46 step:36201 [D loss: 0.033802, acc.: 100.00%] [G loss: 0.017103]\n",
      "epoch:46 step:36202 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.138658]\n",
      "epoch:46 step:36203 [D loss: 0.123606, acc.: 95.31%] [G loss: 0.000065]\n",
      "epoch:46 step:36204 [D loss: 0.021843, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:46 step:36205 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.002197]\n",
      "epoch:46 step:36206 [D loss: 0.002028, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:46 step:36207 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:46 step:36208 [D loss: 0.000914, acc.: 100.00%] [G loss: 0.133753]\n",
      "epoch:46 step:36209 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.030058]\n",
      "epoch:46 step:36210 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:46 step:36211 [D loss: 0.001217, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:46 step:36212 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:46 step:36213 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:46 step:36214 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.004252]\n",
      "epoch:46 step:36215 [D loss: 0.001215, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:46 step:36216 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.018819]\n",
      "epoch:46 step:36217 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.005935]\n",
      "epoch:46 step:36218 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:46 step:36219 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.005484]\n",
      "epoch:46 step:36220 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:46 step:36221 [D loss: 0.001530, acc.: 100.00%] [G loss: 0.001852]\n",
      "epoch:46 step:36222 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:46 step:36223 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36224 [D loss: 0.002417, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:46 step:36225 [D loss: 0.001980, acc.: 100.00%] [G loss: 0.008786]\n",
      "epoch:46 step:36226 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36227 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.000933]\n",
      "epoch:46 step:36228 [D loss: 0.311049, acc.: 82.81%] [G loss: 2.762155]\n",
      "epoch:46 step:36229 [D loss: 0.015396, acc.: 99.22%] [G loss: 4.580228]\n",
      "epoch:46 step:36230 [D loss: 0.201467, acc.: 88.28%] [G loss: 0.103704]\n",
      "epoch:46 step:36231 [D loss: 0.009231, acc.: 100.00%] [G loss: 0.008932]\n",
      "epoch:46 step:36232 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.005448]\n",
      "epoch:46 step:36233 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.421918]\n",
      "epoch:46 step:36234 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.001419]\n",
      "epoch:46 step:36235 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:46 step:36236 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.001244]\n",
      "epoch:46 step:36237 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:46 step:36238 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.051352]\n",
      "epoch:46 step:36239 [D loss: 0.006934, acc.: 100.00%] [G loss: 0.025283]\n",
      "epoch:46 step:36240 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:46 step:36241 [D loss: 0.349895, acc.: 82.03%] [G loss: 0.000001]\n",
      "epoch:46 step:36242 [D loss: 0.789907, acc.: 71.09%] [G loss: 1.316002]\n",
      "epoch:46 step:36243 [D loss: 0.000411, acc.: 100.00%] [G loss: 4.039904]\n",
      "epoch:46 step:36244 [D loss: 0.300957, acc.: 85.94%] [G loss: 0.429188]\n",
      "epoch:46 step:36245 [D loss: 0.002415, acc.: 100.00%] [G loss: 0.085989]\n",
      "epoch:46 step:36246 [D loss: 0.192410, acc.: 91.41%] [G loss: 0.003711]\n",
      "epoch:46 step:36247 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.000889]\n",
      "epoch:46 step:36248 [D loss: 0.001785, acc.: 100.00%] [G loss: 0.001276]\n",
      "epoch:46 step:36249 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:46 step:36250 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.093391]\n",
      "epoch:46 step:36251 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.001916]\n",
      "epoch:46 step:36252 [D loss: 0.007589, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:46 step:36253 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.044235]\n",
      "epoch:46 step:36254 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:46 step:36255 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:46 step:36256 [D loss: 0.001209, acc.: 100.00%] [G loss: 0.017436]\n",
      "epoch:46 step:36257 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:46 step:36258 [D loss: 0.002840, acc.: 100.00%] [G loss: 0.009608]\n",
      "epoch:46 step:36259 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:46 step:36260 [D loss: 0.000999, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:46 step:36261 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:46 step:36262 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:46 step:36263 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:46 step:36264 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.031704]\n",
      "epoch:46 step:36265 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:46 step:36266 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.004494]\n",
      "epoch:46 step:36267 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.007219]\n",
      "epoch:46 step:36268 [D loss: 0.030387, acc.: 99.22%] [G loss: 0.000717]\n",
      "epoch:46 step:36269 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.001214]\n",
      "epoch:46 step:36270 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:46 step:36271 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:46 step:36272 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.020446]\n",
      "epoch:46 step:36273 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:46 step:36274 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:46 step:36275 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:46 step:36276 [D loss: 0.007583, acc.: 100.00%] [G loss: 0.041768]\n",
      "epoch:46 step:36277 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.015454]\n",
      "epoch:46 step:36278 [D loss: 0.001738, acc.: 100.00%] [G loss: 0.012828]\n",
      "epoch:46 step:36279 [D loss: 0.002128, acc.: 100.00%] [G loss: 0.000882]\n",
      "epoch:46 step:36280 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.010894]\n",
      "epoch:46 step:36281 [D loss: 0.036069, acc.: 99.22%] [G loss: 0.057235]\n",
      "epoch:46 step:36282 [D loss: 0.059939, acc.: 100.00%] [G loss: 0.265373]\n",
      "epoch:46 step:36283 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.013542]\n",
      "epoch:46 step:36284 [D loss: 0.002578, acc.: 100.00%] [G loss: 0.486757]\n",
      "epoch:46 step:36285 [D loss: 0.002158, acc.: 100.00%] [G loss: 0.002135]\n",
      "epoch:46 step:36286 [D loss: 0.011317, acc.: 99.22%] [G loss: 0.001257]\n",
      "epoch:46 step:36287 [D loss: 0.021184, acc.: 99.22%] [G loss: 0.331207]\n",
      "epoch:46 step:36288 [D loss: 0.001395, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:46 step:36289 [D loss: 0.001045, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:46 step:36290 [D loss: 0.009860, acc.: 99.22%] [G loss: 0.018359]\n",
      "epoch:46 step:36291 [D loss: 0.001686, acc.: 100.00%] [G loss: 0.012489]\n",
      "epoch:46 step:36292 [D loss: 0.004314, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:46 step:36293 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:46 step:36294 [D loss: 0.002372, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:46 step:36295 [D loss: 0.003495, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:46 step:36296 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.006410]\n",
      "epoch:46 step:36297 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:46 step:36298 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:46 step:36299 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.002101]\n",
      "epoch:46 step:36300 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:46 step:36301 [D loss: 0.001851, acc.: 100.00%] [G loss: 0.007303]\n",
      "epoch:46 step:36302 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.003083]\n",
      "epoch:46 step:36303 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.005676]\n",
      "epoch:46 step:36304 [D loss: 0.070008, acc.: 99.22%] [G loss: 0.144449]\n",
      "epoch:46 step:36305 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.347325]\n",
      "epoch:46 step:36306 [D loss: 0.003200, acc.: 100.00%] [G loss: 0.002061]\n",
      "epoch:46 step:36307 [D loss: 0.015167, acc.: 99.22%] [G loss: 0.002238]\n",
      "epoch:46 step:36308 [D loss: 0.008650, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:46 step:36309 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:46 step:36310 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000702]\n",
      "epoch:46 step:36311 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.001009]\n",
      "epoch:46 step:36312 [D loss: 0.004209, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:46 step:36313 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:46 step:36314 [D loss: 0.132028, acc.: 94.53%] [G loss: 0.000003]\n",
      "epoch:46 step:36315 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36316 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36317 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36318 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:46 step:36319 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36320 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36321 [D loss: 0.027514, acc.: 99.22%] [G loss: 0.000005]\n",
      "epoch:46 step:36322 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:46 step:36323 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:46 step:36324 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36325 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002438]\n",
      "epoch:46 step:36326 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:46 step:36327 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:46 step:36328 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:46 step:36329 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:46 step:36330 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:36331 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:46 step:36332 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000668]\n",
      "epoch:46 step:36333 [D loss: 0.018765, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36334 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36335 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:46 step:36336 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.011310]\n",
      "epoch:46 step:36337 [D loss: 0.002308, acc.: 100.00%] [G loss: 0.000016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36338 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:46 step:36339 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:46 step:36340 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:46 step:36341 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.003702]\n",
      "epoch:46 step:36342 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.001938]\n",
      "epoch:46 step:36343 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.021125]\n",
      "epoch:46 step:36344 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.011052]\n",
      "epoch:46 step:36345 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.002643]\n",
      "epoch:46 step:36346 [D loss: 0.005143, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36347 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36348 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:46 step:36349 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:36350 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.004078]\n",
      "epoch:46 step:36351 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001086]\n",
      "epoch:46 step:36352 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36353 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:46 step:36354 [D loss: 0.009760, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36355 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.001334]\n",
      "epoch:46 step:36356 [D loss: 0.067455, acc.: 96.88%] [G loss: 0.000846]\n",
      "epoch:46 step:36357 [D loss: 0.008870, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:46 step:36358 [D loss: 0.005886, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36359 [D loss: 0.055625, acc.: 99.22%] [G loss: 0.000062]\n",
      "epoch:46 step:36360 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.014419]\n",
      "epoch:46 step:36361 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:36362 [D loss: 0.001105, acc.: 100.00%] [G loss: 0.028620]\n",
      "epoch:46 step:36363 [D loss: 0.002129, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:46 step:36364 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.008097]\n",
      "epoch:46 step:36365 [D loss: 0.001402, acc.: 100.00%] [G loss: 0.009843]\n",
      "epoch:46 step:36366 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.015261]\n",
      "epoch:46 step:36367 [D loss: 0.001909, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36368 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:46 step:36369 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.027015]\n",
      "epoch:46 step:36370 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:36371 [D loss: 0.011114, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36372 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.003674]\n",
      "epoch:46 step:36373 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36374 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.011544]\n",
      "epoch:46 step:36375 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:36376 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36377 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001428]\n",
      "epoch:46 step:36378 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.004399]\n",
      "epoch:46 step:36379 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:36380 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:46 step:36381 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.005539]\n",
      "epoch:46 step:36382 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004975]\n",
      "epoch:46 step:36383 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.003592]\n",
      "epoch:46 step:36384 [D loss: 0.003309, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36385 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.001879]\n",
      "epoch:46 step:36386 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:46 step:36387 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36388 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.012268]\n",
      "epoch:46 step:36389 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.002120]\n",
      "epoch:46 step:36390 [D loss: 0.002299, acc.: 100.00%] [G loss: 0.007003]\n",
      "epoch:46 step:36391 [D loss: 0.000825, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36392 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36393 [D loss: 0.010125, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:36394 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.013053]\n",
      "epoch:46 step:36395 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:46 step:36396 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.024891]\n",
      "epoch:46 step:36397 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.000945]\n",
      "epoch:46 step:36398 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:36399 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.014670]\n",
      "epoch:46 step:36400 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:46 step:36401 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36402 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.052794]\n",
      "epoch:46 step:36403 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.002566]\n",
      "epoch:46 step:36404 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:46 step:36405 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:46 step:36406 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.021611]\n",
      "epoch:46 step:36407 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:46 step:36408 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.017782]\n",
      "epoch:46 step:36409 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.003906]\n",
      "epoch:46 step:36410 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002431]\n",
      "epoch:46 step:36411 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36412 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:46 step:36413 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.004530]\n",
      "epoch:46 step:36414 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:36415 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.011586]\n",
      "epoch:46 step:36416 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.002854]\n",
      "epoch:46 step:36417 [D loss: 0.001401, acc.: 100.00%] [G loss: 0.004399]\n",
      "epoch:46 step:36418 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003556]\n",
      "epoch:46 step:36419 [D loss: 0.003076, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36420 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36421 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36422 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36423 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36424 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36425 [D loss: 0.003566, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:46 step:36426 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36427 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.001531]\n",
      "epoch:46 step:36428 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36429 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.004519]\n",
      "epoch:46 step:36430 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002364]\n",
      "epoch:46 step:36431 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.013722]\n",
      "epoch:46 step:36432 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.036877]\n",
      "epoch:46 step:36433 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36434 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36435 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.016925]\n",
      "epoch:46 step:36436 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36437 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36438 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36439 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36440 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.003462]\n",
      "epoch:46 step:36441 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.004705]\n",
      "epoch:46 step:36442 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:46 step:36443 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36444 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36445 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36446 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:46 step:36447 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36448 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36449 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.010749]\n",
      "epoch:46 step:36450 [D loss: 0.004023, acc.: 100.00%] [G loss: 0.004608]\n",
      "epoch:46 step:36451 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36452 [D loss: 0.012955, acc.: 100.00%] [G loss: 0.000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36453 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36454 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36455 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001138]\n",
      "epoch:46 step:36456 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36457 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36458 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36459 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:46 step:36460 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:46 step:36461 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36462 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36463 [D loss: 0.004083, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:46 step:36464 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36465 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36466 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36467 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36468 [D loss: 0.003683, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:46 step:36469 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:46 step:36470 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36471 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:46 step:36472 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000375]\n",
      "epoch:46 step:36473 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:46 step:36474 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:46 step:36475 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:46 step:36476 [D loss: 0.010624, acc.: 100.00%] [G loss: 0.001653]\n",
      "epoch:46 step:36477 [D loss: 0.001909, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36478 [D loss: 0.126776, acc.: 94.53%] [G loss: 0.001798]\n",
      "epoch:46 step:36479 [D loss: 0.000024, acc.: 100.00%] [G loss: 4.553521]\n",
      "epoch:46 step:36480 [D loss: 0.012505, acc.: 100.00%] [G loss: 0.214118]\n",
      "epoch:46 step:36481 [D loss: 0.069251, acc.: 96.88%] [G loss: 0.001232]\n",
      "epoch:46 step:36482 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:46 step:36483 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:46 step:36484 [D loss: 0.025430, acc.: 99.22%] [G loss: 0.000087]\n",
      "epoch:46 step:36485 [D loss: 0.006523, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:46 step:36486 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.802474]\n",
      "epoch:46 step:36487 [D loss: 0.007245, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:46 step:36488 [D loss: 0.010069, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:46 step:36489 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:46 step:36490 [D loss: 0.001870, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:46 step:36491 [D loss: 0.009540, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:46 step:36492 [D loss: 0.084419, acc.: 97.66%] [G loss: 0.004569]\n",
      "epoch:46 step:36493 [D loss: 0.023776, acc.: 100.00%] [G loss: 1.379596]\n",
      "epoch:46 step:36494 [D loss: 0.000705, acc.: 100.00%] [G loss: 0.751612]\n",
      "epoch:46 step:36495 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.262153]\n",
      "epoch:46 step:36496 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.950403]\n",
      "epoch:46 step:36497 [D loss: 0.043736, acc.: 98.44%] [G loss: 0.042139]\n",
      "epoch:46 step:36498 [D loss: 0.058958, acc.: 98.44%] [G loss: 0.002208]\n",
      "epoch:46 step:36499 [D loss: 0.096438, acc.: 97.66%] [G loss: 0.000184]\n",
      "epoch:46 step:36500 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:46 step:36501 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:46 step:36502 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.059722]\n",
      "epoch:46 step:36503 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.001043]\n",
      "epoch:46 step:36504 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.067120]\n",
      "epoch:46 step:36505 [D loss: 0.002226, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:46 step:36506 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.062092]\n",
      "epoch:46 step:36507 [D loss: 0.000759, acc.: 100.00%] [G loss: 0.064874]\n",
      "epoch:46 step:36508 [D loss: 0.031172, acc.: 99.22%] [G loss: 0.000098]\n",
      "epoch:46 step:36509 [D loss: 0.008197, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:46 step:36510 [D loss: 0.003577, acc.: 100.00%] [G loss: 0.488633]\n",
      "epoch:46 step:36511 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:46 step:36512 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:46 step:36513 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:46 step:36514 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:46 step:36515 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.026954]\n",
      "epoch:46 step:36516 [D loss: 0.006683, acc.: 100.00%] [G loss: 0.025226]\n",
      "epoch:46 step:36517 [D loss: 0.029375, acc.: 99.22%] [G loss: 0.000119]\n",
      "epoch:46 step:36518 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:46 step:36519 [D loss: 0.003514, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:46 step:36520 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:46 step:36521 [D loss: 0.000964, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:46 step:36522 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:46 step:36523 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:46 step:36524 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:46 step:36525 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.026024]\n",
      "epoch:46 step:36526 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:46 step:36527 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:36528 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:46 step:36529 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.076983]\n",
      "epoch:46 step:36530 [D loss: 0.002206, acc.: 100.00%] [G loss: 0.011443]\n",
      "epoch:46 step:36531 [D loss: 0.002870, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:46 step:36532 [D loss: 0.001320, acc.: 100.00%] [G loss: 0.001085]\n",
      "epoch:46 step:36533 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:46 step:36534 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:46 step:36535 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:46 step:36536 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.015922]\n",
      "epoch:46 step:36537 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:46 step:36538 [D loss: 0.003777, acc.: 100.00%] [G loss: 0.007015]\n",
      "epoch:46 step:36539 [D loss: 0.020791, acc.: 99.22%] [G loss: 0.001803]\n",
      "epoch:46 step:36540 [D loss: 0.011289, acc.: 100.00%] [G loss: 0.010867]\n",
      "epoch:46 step:36541 [D loss: 0.043634, acc.: 100.00%] [G loss: 0.359097]\n",
      "epoch:46 step:36542 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.022632]\n",
      "epoch:46 step:36543 [D loss: 0.001208, acc.: 100.00%] [G loss: 2.339352]\n",
      "epoch:46 step:36544 [D loss: 0.045521, acc.: 98.44%] [G loss: 0.000073]\n",
      "epoch:46 step:36545 [D loss: 0.009848, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:46 step:36546 [D loss: 0.020104, acc.: 100.00%] [G loss: 0.262177]\n",
      "epoch:46 step:36547 [D loss: 0.003290, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:46 step:36548 [D loss: 0.021932, acc.: 100.00%] [G loss: 0.254425]\n",
      "epoch:46 step:36549 [D loss: 3.984131, acc.: 32.03%] [G loss: 4.882446]\n",
      "epoch:46 step:36550 [D loss: 1.232454, acc.: 56.25%] [G loss: 3.134290]\n",
      "epoch:46 step:36551 [D loss: 0.418745, acc.: 80.47%] [G loss: 0.217835]\n",
      "epoch:46 step:36552 [D loss: 0.118336, acc.: 95.31%] [G loss: 1.045450]\n",
      "epoch:46 step:36553 [D loss: 0.097263, acc.: 96.88%] [G loss: 0.091489]\n",
      "epoch:46 step:36554 [D loss: 0.076555, acc.: 99.22%] [G loss: 0.006673]\n",
      "epoch:46 step:36555 [D loss: 0.419645, acc.: 79.69%] [G loss: 4.746602]\n",
      "epoch:46 step:36556 [D loss: 0.114990, acc.: 97.66%] [G loss: 4.549160]\n",
      "epoch:46 step:36557 [D loss: 0.392744, acc.: 80.47%] [G loss: 1.272230]\n",
      "epoch:46 step:36558 [D loss: 0.231935, acc.: 88.28%] [G loss: 0.107552]\n",
      "epoch:46 step:36559 [D loss: 0.018738, acc.: 100.00%] [G loss: 2.140556]\n",
      "epoch:46 step:36560 [D loss: 0.257276, acc.: 94.53%] [G loss: 0.260623]\n",
      "epoch:46 step:36561 [D loss: 0.047827, acc.: 100.00%] [G loss: 0.264734]\n",
      "epoch:46 step:36562 [D loss: 0.067243, acc.: 99.22%] [G loss: 0.083926]\n",
      "epoch:46 step:36563 [D loss: 0.211815, acc.: 90.62%] [G loss: 0.736392]\n",
      "epoch:46 step:36564 [D loss: 0.009605, acc.: 100.00%] [G loss: 0.016775]\n",
      "epoch:46 step:36565 [D loss: 0.015101, acc.: 99.22%] [G loss: 0.007908]\n",
      "epoch:46 step:36566 [D loss: 0.084192, acc.: 97.66%] [G loss: 0.488921]\n",
      "epoch:46 step:36567 [D loss: 0.003587, acc.: 100.00%] [G loss: 0.038127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36568 [D loss: 0.009488, acc.: 100.00%] [G loss: 0.050568]\n",
      "epoch:46 step:36569 [D loss: 0.008009, acc.: 100.00%] [G loss: 0.007030]\n",
      "epoch:46 step:36570 [D loss: 0.041511, acc.: 98.44%] [G loss: 0.020526]\n",
      "epoch:46 step:36571 [D loss: 0.002841, acc.: 100.00%] [G loss: 0.003615]\n",
      "epoch:46 step:36572 [D loss: 0.065634, acc.: 98.44%] [G loss: 0.012865]\n",
      "epoch:46 step:36573 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.000916]\n",
      "epoch:46 step:36574 [D loss: 0.002765, acc.: 100.00%] [G loss: 0.135596]\n",
      "epoch:46 step:36575 [D loss: 0.004909, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:46 step:36576 [D loss: 0.005832, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:46 step:36577 [D loss: 0.001211, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:46 step:36578 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.081653]\n",
      "epoch:46 step:36579 [D loss: 0.019900, acc.: 100.00%] [G loss: 0.125559]\n",
      "epoch:46 step:36580 [D loss: 0.006907, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:46 step:36581 [D loss: 0.033853, acc.: 99.22%] [G loss: 0.001529]\n",
      "epoch:46 step:36582 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.008493]\n",
      "epoch:46 step:36583 [D loss: 0.001401, acc.: 100.00%] [G loss: 0.001417]\n",
      "epoch:46 step:36584 [D loss: 0.002772, acc.: 100.00%] [G loss: 0.002292]\n",
      "epoch:46 step:36585 [D loss: 0.001769, acc.: 100.00%] [G loss: 0.004375]\n",
      "epoch:46 step:36586 [D loss: 0.011690, acc.: 100.00%] [G loss: 0.003261]\n",
      "epoch:46 step:36587 [D loss: 0.016016, acc.: 100.00%] [G loss: 0.002903]\n",
      "epoch:46 step:36588 [D loss: 0.004872, acc.: 100.00%] [G loss: 0.037935]\n",
      "epoch:46 step:36589 [D loss: 0.001511, acc.: 100.00%] [G loss: 0.015789]\n",
      "epoch:46 step:36590 [D loss: 0.001539, acc.: 100.00%] [G loss: 0.003446]\n",
      "epoch:46 step:36591 [D loss: 0.134242, acc.: 94.53%] [G loss: 0.000402]\n",
      "epoch:46 step:36592 [D loss: 0.011747, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:46 step:36593 [D loss: 0.001067, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:46 step:36594 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:46 step:36595 [D loss: 0.015997, acc.: 100.00%] [G loss: 0.002641]\n",
      "epoch:46 step:36596 [D loss: 0.001970, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:46 step:36597 [D loss: 0.001648, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:46 step:36598 [D loss: 0.142584, acc.: 92.97%] [G loss: 0.170890]\n",
      "epoch:46 step:36599 [D loss: 0.000271, acc.: 100.00%] [G loss: 1.177059]\n",
      "epoch:46 step:36600 [D loss: 0.024456, acc.: 99.22%] [G loss: 0.372409]\n",
      "epoch:46 step:36601 [D loss: 0.021535, acc.: 99.22%] [G loss: 1.784840]\n",
      "epoch:46 step:36602 [D loss: 0.048641, acc.: 98.44%] [G loss: 0.046804]\n",
      "epoch:46 step:36603 [D loss: 0.485037, acc.: 76.56%] [G loss: 1.480675]\n",
      "epoch:46 step:36604 [D loss: 0.576785, acc.: 78.12%] [G loss: 3.375884]\n",
      "epoch:46 step:36605 [D loss: 0.069631, acc.: 98.44%] [G loss: 1.195043]\n",
      "epoch:46 step:36606 [D loss: 0.141731, acc.: 95.31%] [G loss: 0.895621]\n",
      "epoch:46 step:36607 [D loss: 0.003044, acc.: 100.00%] [G loss: 0.482049]\n",
      "epoch:46 step:36608 [D loss: 0.014378, acc.: 100.00%] [G loss: 0.066393]\n",
      "epoch:46 step:36609 [D loss: 0.202141, acc.: 87.50%] [G loss: 1.434679]\n",
      "epoch:46 step:36610 [D loss: 0.046313, acc.: 97.66%] [G loss: 0.243253]\n",
      "epoch:46 step:36611 [D loss: 0.125109, acc.: 93.75%] [G loss: 0.865053]\n",
      "epoch:46 step:36612 [D loss: 0.047877, acc.: 99.22%] [G loss: 0.025530]\n",
      "epoch:46 step:36613 [D loss: 0.032583, acc.: 99.22%] [G loss: 0.013318]\n",
      "epoch:46 step:36614 [D loss: 0.007838, acc.: 100.00%] [G loss: 0.126605]\n",
      "epoch:46 step:36615 [D loss: 0.141103, acc.: 94.53%] [G loss: 0.094707]\n",
      "epoch:46 step:36616 [D loss: 0.082077, acc.: 98.44%] [G loss: 0.087800]\n",
      "epoch:46 step:36617 [D loss: 0.009916, acc.: 100.00%] [G loss: 0.024149]\n",
      "epoch:46 step:36618 [D loss: 0.246278, acc.: 91.41%] [G loss: 0.062183]\n",
      "epoch:46 step:36619 [D loss: 0.015703, acc.: 100.00%] [G loss: 0.004686]\n",
      "epoch:46 step:36620 [D loss: 0.121956, acc.: 95.31%] [G loss: 0.069316]\n",
      "epoch:46 step:36621 [D loss: 0.009285, acc.: 100.00%] [G loss: 0.353196]\n",
      "epoch:46 step:36622 [D loss: 0.014484, acc.: 100.00%] [G loss: 0.338056]\n",
      "epoch:46 step:36623 [D loss: 0.006567, acc.: 100.00%] [G loss: 0.117692]\n",
      "epoch:46 step:36624 [D loss: 0.001219, acc.: 100.00%] [G loss: 0.046053]\n",
      "epoch:46 step:36625 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.025743]\n",
      "epoch:46 step:36626 [D loss: 0.005608, acc.: 100.00%] [G loss: 0.049575]\n",
      "epoch:46 step:36627 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.041574]\n",
      "epoch:46 step:36628 [D loss: 0.087458, acc.: 97.66%] [G loss: 0.035631]\n",
      "epoch:46 step:36629 [D loss: 0.024768, acc.: 99.22%] [G loss: 0.033834]\n",
      "epoch:46 step:36630 [D loss: 0.114144, acc.: 95.31%] [G loss: 0.005866]\n",
      "epoch:46 step:36631 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.005430]\n",
      "epoch:46 step:36632 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.148402]\n",
      "epoch:46 step:36633 [D loss: 0.001053, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:46 step:36634 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.001348]\n",
      "epoch:46 step:36635 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001268]\n",
      "epoch:46 step:36636 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000706]\n",
      "epoch:46 step:36637 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.019510]\n",
      "epoch:46 step:36638 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.003078]\n",
      "epoch:46 step:36639 [D loss: 0.006964, acc.: 100.00%] [G loss: 0.028953]\n",
      "epoch:46 step:36640 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.005104]\n",
      "epoch:46 step:36641 [D loss: 0.031130, acc.: 99.22%] [G loss: 0.000515]\n",
      "epoch:46 step:36642 [D loss: 0.000741, acc.: 100.00%] [G loss: 0.002698]\n",
      "epoch:46 step:36643 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:46 step:36644 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001002]\n",
      "epoch:46 step:36645 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:46 step:36646 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:46 step:36647 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:46 step:36648 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:46 step:36649 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:46 step:36650 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.001265]\n",
      "epoch:46 step:36651 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:46 step:36652 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:46 step:36653 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.001273]\n",
      "epoch:46 step:36654 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:46 step:36655 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:46 step:36656 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:46 step:36657 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.003122]\n",
      "epoch:46 step:36658 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.036183]\n",
      "epoch:46 step:36659 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:46 step:36660 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.003466]\n",
      "epoch:46 step:36661 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:46 step:36662 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.017272]\n",
      "epoch:46 step:36663 [D loss: 0.000750, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:46 step:36664 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:46 step:36665 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.002025]\n",
      "epoch:46 step:36666 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:46 step:36667 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:46 step:36668 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:46 step:36669 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.005254]\n",
      "epoch:46 step:36670 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.020684]\n",
      "epoch:46 step:36671 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.001149]\n",
      "epoch:46 step:36672 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.010341]\n",
      "epoch:46 step:36673 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:46 step:36674 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:46 step:36675 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:46 step:36676 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.004612]\n",
      "epoch:46 step:36677 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000849]\n",
      "epoch:46 step:36678 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.002793]\n",
      "epoch:46 step:36679 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:46 step:36680 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.001283]\n",
      "epoch:46 step:36681 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:46 step:36682 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36683 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:46 step:36684 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001953]\n",
      "epoch:46 step:36685 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:46 step:36686 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:46 step:36687 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.004534]\n",
      "epoch:46 step:36688 [D loss: 0.001294, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:46 step:36689 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:46 step:36690 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:46 step:36691 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:46 step:36692 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:46 step:36693 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.005380]\n",
      "epoch:46 step:36694 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:46 step:36695 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:46 step:36696 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:46 step:36697 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:46 step:36698 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:46 step:36699 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:46 step:36700 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:46 step:36701 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:46 step:36702 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:46 step:36703 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:46 step:36704 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000475]\n",
      "epoch:46 step:36705 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000378]\n",
      "epoch:46 step:36706 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:46 step:36707 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:47 step:36708 [D loss: 0.009305, acc.: 99.22%] [G loss: 0.002410]\n",
      "epoch:47 step:36709 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:47 step:36710 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:47 step:36711 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:47 step:36712 [D loss: 0.007524, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:47 step:36713 [D loss: 0.001572, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:47 step:36714 [D loss: 0.001601, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:47 step:36715 [D loss: 0.001475, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:47 step:36716 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:47 step:36717 [D loss: 0.696521, acc.: 62.50%] [G loss: 0.958530]\n",
      "epoch:47 step:36718 [D loss: 0.363266, acc.: 82.81%] [G loss: 0.743054]\n",
      "epoch:47 step:36719 [D loss: 0.117663, acc.: 97.66%] [G loss: 0.144937]\n",
      "epoch:47 step:36720 [D loss: 0.122437, acc.: 95.31%] [G loss: 0.774226]\n",
      "epoch:47 step:36721 [D loss: 0.032854, acc.: 98.44%] [G loss: 0.233440]\n",
      "epoch:47 step:36722 [D loss: 0.006281, acc.: 100.00%] [G loss: 0.038708]\n",
      "epoch:47 step:36723 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.030220]\n",
      "epoch:47 step:36724 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.020510]\n",
      "epoch:47 step:36725 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.008768]\n",
      "epoch:47 step:36726 [D loss: 0.002346, acc.: 100.00%] [G loss: 0.010164]\n",
      "epoch:47 step:36727 [D loss: 0.001430, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:47 step:36728 [D loss: 0.003578, acc.: 100.00%] [G loss: 0.001237]\n",
      "epoch:47 step:36729 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.003310]\n",
      "epoch:47 step:36730 [D loss: 0.001043, acc.: 100.00%] [G loss: 0.009656]\n",
      "epoch:47 step:36731 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.005593]\n",
      "epoch:47 step:36732 [D loss: 0.004086, acc.: 100.00%] [G loss: 0.005048]\n",
      "epoch:47 step:36733 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.004403]\n",
      "epoch:47 step:36734 [D loss: 0.001307, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:47 step:36735 [D loss: 0.001131, acc.: 100.00%] [G loss: 0.011108]\n",
      "epoch:47 step:36736 [D loss: 0.002680, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:47 step:36737 [D loss: 0.000887, acc.: 100.00%] [G loss: 0.001085]\n",
      "epoch:47 step:36738 [D loss: 0.002305, acc.: 100.00%] [G loss: 0.002036]\n",
      "epoch:47 step:36739 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:47 step:36740 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:47 step:36741 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.002319]\n",
      "epoch:47 step:36742 [D loss: 0.003143, acc.: 100.00%] [G loss: 0.000702]\n",
      "epoch:47 step:36743 [D loss: 0.033889, acc.: 98.44%] [G loss: 0.000126]\n",
      "epoch:47 step:36744 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.001942]\n",
      "epoch:47 step:36745 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.018942]\n",
      "epoch:47 step:36746 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:47 step:36747 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:47 step:36748 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:47 step:36749 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.024723]\n",
      "epoch:47 step:36750 [D loss: 0.015476, acc.: 99.22%] [G loss: 0.000347]\n",
      "epoch:47 step:36751 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:47 step:36752 [D loss: 0.007537, acc.: 100.00%] [G loss: 0.001797]\n",
      "epoch:47 step:36753 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:47 step:36754 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:47 step:36755 [D loss: 0.001610, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:47 step:36756 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:47 step:36757 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:47 step:36758 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:47 step:36759 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000617]\n",
      "epoch:47 step:36760 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000304]\n",
      "epoch:47 step:36761 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:47 step:36762 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:47 step:36763 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:47 step:36764 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:47 step:36765 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000391]\n",
      "epoch:47 step:36766 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000758]\n",
      "epoch:47 step:36767 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:47 step:36768 [D loss: 0.002275, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:47 step:36769 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:47 step:36770 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:47 step:36771 [D loss: 0.006927, acc.: 99.22%] [G loss: 0.000084]\n",
      "epoch:47 step:36772 [D loss: 0.003054, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:47 step:36773 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:47 step:36774 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:47 step:36775 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:47 step:36776 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:47 step:36777 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001829]\n",
      "epoch:47 step:36778 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000392]\n",
      "epoch:47 step:36779 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:47 step:36780 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:47 step:36781 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.153607]\n",
      "epoch:47 step:36782 [D loss: 0.003142, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:47 step:36783 [D loss: 0.002645, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:47 step:36784 [D loss: 0.001910, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:47 step:36785 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:47 step:36786 [D loss: 0.003527, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:47 step:36787 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:47 step:36788 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:47 step:36789 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:47 step:36790 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:47 step:36791 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:47 step:36792 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:47 step:36793 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:47 step:36794 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:47 step:36795 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:47 step:36796 [D loss: 0.005824, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:47 step:36797 [D loss: 0.027514, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:47 step:36798 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36799 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000950]\n",
      "epoch:47 step:36800 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:47 step:36801 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:47 step:36802 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:47 step:36803 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:47 step:36804 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:47 step:36805 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:47 step:36806 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.000739]\n",
      "epoch:47 step:36807 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:47 step:36808 [D loss: 0.001563, acc.: 100.00%] [G loss: 0.001672]\n",
      "epoch:47 step:36809 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.003711]\n",
      "epoch:47 step:36810 [D loss: 0.002578, acc.: 100.00%] [G loss: 0.000828]\n",
      "epoch:47 step:36811 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:47 step:36812 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:47 step:36813 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.006402]\n",
      "epoch:47 step:36814 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:47 step:36815 [D loss: 0.001114, acc.: 100.00%] [G loss: 0.000424]\n",
      "epoch:47 step:36816 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000782]\n",
      "epoch:47 step:36817 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:47 step:36818 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:47 step:36819 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.020904]\n",
      "epoch:47 step:36820 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000751]\n",
      "epoch:47 step:36821 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:47 step:36822 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:47 step:36823 [D loss: 0.002695, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:47 step:36824 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:47 step:36825 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:47 step:36826 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:47 step:36827 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:47 step:36828 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:47 step:36829 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000877]\n",
      "epoch:47 step:36830 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000637]\n",
      "epoch:47 step:36831 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:47 step:36832 [D loss: 0.000945, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:47 step:36833 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:47 step:36834 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:47 step:36835 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:47 step:36836 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001005]\n",
      "epoch:47 step:36837 [D loss: 0.002112, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:47 step:36838 [D loss: 0.003188, acc.: 100.00%] [G loss: 0.001063]\n",
      "epoch:47 step:36839 [D loss: 0.003793, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:47 step:36840 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:47 step:36841 [D loss: 0.002223, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:47 step:36842 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:47 step:36843 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:47 step:36844 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.002438]\n",
      "epoch:47 step:36845 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:47 step:36846 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:47 step:36847 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.001404]\n",
      "epoch:47 step:36848 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001620]\n",
      "epoch:47 step:36849 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:47 step:36850 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:47 step:36851 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:47 step:36852 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000415]\n",
      "epoch:47 step:36853 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:47 step:36854 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:47 step:36855 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:47 step:36856 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:47 step:36857 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:47 step:36858 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000442]\n",
      "epoch:47 step:36859 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002048]\n",
      "epoch:47 step:36860 [D loss: 0.001498, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:47 step:36861 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:47 step:36862 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:47 step:36863 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:47 step:36864 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000801]\n",
      "epoch:47 step:36865 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:47 step:36866 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000561]\n",
      "epoch:47 step:36867 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:47 step:36868 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:47 step:36869 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:47 step:36870 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:47 step:36871 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:47 step:36872 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:47 step:36873 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.001116]\n",
      "epoch:47 step:36874 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:47 step:36875 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:47 step:36876 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:47 step:36877 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:47 step:36878 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:47 step:36879 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.005717]\n",
      "epoch:47 step:36880 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:47 step:36881 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:47 step:36882 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:47 step:36883 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:47 step:36884 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:47 step:36885 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:47 step:36886 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:47 step:36887 [D loss: 0.003103, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:47 step:36888 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:47 step:36889 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:47 step:36890 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:47 step:36891 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:47 step:36892 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:47 step:36893 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:47 step:36894 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:47 step:36895 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:47 step:36896 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000643]\n",
      "epoch:47 step:36897 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:47 step:36898 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.002190]\n",
      "epoch:47 step:36899 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:47 step:36900 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:47 step:36901 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:47 step:36902 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:47 step:36903 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:47 step:36904 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:47 step:36905 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:47 step:36906 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:47 step:36907 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:47 step:36908 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:47 step:36909 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:47 step:36910 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:47 step:36911 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:47 step:36912 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:47 step:36913 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:47 step:36914 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36915 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:47 step:36916 [D loss: 0.003269, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:47 step:36917 [D loss: 0.000692, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:47 step:36918 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:47 step:36919 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:47 step:36920 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:47 step:36921 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:47 step:36922 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:47 step:36923 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:47 step:36924 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:47 step:36925 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:47 step:36926 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:47 step:36927 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:47 step:36928 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:47 step:36929 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:47 step:36930 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:47 step:36931 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:47 step:36932 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:47 step:36933 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:47 step:36934 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:47 step:36935 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:47 step:36936 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:47 step:36937 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:47 step:36938 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:47 step:36939 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:47 step:36940 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:47 step:36941 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:47 step:36942 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:47 step:36943 [D loss: 0.001274, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:47 step:36944 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:47 step:36945 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:47 step:36946 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:47 step:36947 [D loss: 0.032759, acc.: 99.22%] [G loss: 0.000002]\n",
      "epoch:47 step:36948 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:47 step:36949 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:47 step:36950 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:47 step:36951 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:47 step:36952 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:47 step:36953 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:47 step:36954 [D loss: 0.000702, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:47 step:36955 [D loss: 0.003203, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:47 step:36956 [D loss: 0.590538, acc.: 71.09%] [G loss: 5.814846]\n",
      "epoch:47 step:36957 [D loss: 0.707013, acc.: 69.53%] [G loss: 3.557747]\n",
      "epoch:47 step:36958 [D loss: 0.006476, acc.: 100.00%] [G loss: 1.084763]\n",
      "epoch:47 step:36959 [D loss: 0.039790, acc.: 98.44%] [G loss: 0.041593]\n",
      "epoch:47 step:36960 [D loss: 0.002152, acc.: 100.00%] [G loss: 0.422481]\n",
      "epoch:47 step:36961 [D loss: 0.013881, acc.: 100.00%] [G loss: 0.210997]\n",
      "epoch:47 step:36962 [D loss: 0.008161, acc.: 100.00%] [G loss: 0.098299]\n",
      "epoch:47 step:36963 [D loss: 0.009699, acc.: 100.00%] [G loss: 0.025966]\n",
      "epoch:47 step:36964 [D loss: 0.322104, acc.: 82.81%] [G loss: 2.168766]\n",
      "epoch:47 step:36965 [D loss: 0.315466, acc.: 89.06%] [G loss: 2.189062]\n",
      "epoch:47 step:36966 [D loss: 0.190311, acc.: 92.19%] [G loss: 0.632445]\n",
      "epoch:47 step:36967 [D loss: 0.022830, acc.: 99.22%] [G loss: 0.003999]\n",
      "epoch:47 step:36968 [D loss: 0.008976, acc.: 100.00%] [G loss: 0.077303]\n",
      "epoch:47 step:36969 [D loss: 0.001020, acc.: 100.00%] [G loss: 0.047717]\n",
      "epoch:47 step:36970 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.018298]\n",
      "epoch:47 step:36971 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.010483]\n",
      "epoch:47 step:36972 [D loss: 0.002982, acc.: 100.00%] [G loss: 0.013212]\n",
      "epoch:47 step:36973 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.019938]\n",
      "epoch:47 step:36974 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.004774]\n",
      "epoch:47 step:36975 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.033152]\n",
      "epoch:47 step:36976 [D loss: 0.004822, acc.: 100.00%] [G loss: 0.005690]\n",
      "epoch:47 step:36977 [D loss: 0.006004, acc.: 100.00%] [G loss: 0.104277]\n",
      "epoch:47 step:36978 [D loss: 0.024992, acc.: 100.00%] [G loss: 0.126502]\n",
      "epoch:47 step:36979 [D loss: 0.227851, acc.: 87.50%] [G loss: 1.123000]\n",
      "epoch:47 step:36980 [D loss: 0.070764, acc.: 97.66%] [G loss: 1.757675]\n",
      "epoch:47 step:36981 [D loss: 0.037688, acc.: 99.22%] [G loss: 0.101275]\n",
      "epoch:47 step:36982 [D loss: 0.200068, acc.: 89.06%] [G loss: 0.015048]\n",
      "epoch:47 step:36983 [D loss: 0.008258, acc.: 100.00%] [G loss: 0.005842]\n",
      "epoch:47 step:36984 [D loss: 0.008422, acc.: 100.00%] [G loss: 0.010071]\n",
      "epoch:47 step:36985 [D loss: 0.081025, acc.: 98.44%] [G loss: 0.000327]\n",
      "epoch:47 step:36986 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.017874]\n",
      "epoch:47 step:36987 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.027055]\n",
      "epoch:47 step:36988 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.008760]\n",
      "epoch:47 step:36989 [D loss: 0.002227, acc.: 100.00%] [G loss: 0.008406]\n",
      "epoch:47 step:36990 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.044093]\n",
      "epoch:47 step:36991 [D loss: 0.001208, acc.: 100.00%] [G loss: 0.021161]\n",
      "epoch:47 step:36992 [D loss: 0.013040, acc.: 100.00%] [G loss: 0.019068]\n",
      "epoch:47 step:36993 [D loss: 0.002180, acc.: 100.00%] [G loss: 0.008116]\n",
      "epoch:47 step:36994 [D loss: 0.019553, acc.: 99.22%] [G loss: 0.013900]\n",
      "epoch:47 step:36995 [D loss: 0.036025, acc.: 100.00%] [G loss: 0.027183]\n",
      "epoch:47 step:36996 [D loss: 0.015886, acc.: 99.22%] [G loss: 0.003736]\n",
      "epoch:47 step:36997 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.099584]\n",
      "epoch:47 step:36998 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.009875]\n",
      "epoch:47 step:36999 [D loss: 0.012468, acc.: 100.00%] [G loss: 0.034026]\n",
      "epoch:47 step:37000 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.034679]\n",
      "epoch:47 step:37001 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.002441]\n",
      "epoch:47 step:37002 [D loss: 0.007910, acc.: 100.00%] [G loss: 0.007018]\n",
      "epoch:47 step:37003 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:47 step:37004 [D loss: 0.000443, acc.: 100.00%] [G loss: 0.003850]\n",
      "epoch:47 step:37005 [D loss: 0.001120, acc.: 100.00%] [G loss: 0.006763]\n",
      "epoch:47 step:37006 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.003556]\n",
      "epoch:47 step:37007 [D loss: 0.017421, acc.: 99.22%] [G loss: 0.000684]\n",
      "epoch:47 step:37008 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:47 step:37009 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000498]\n",
      "epoch:47 step:37010 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.031584]\n",
      "epoch:47 step:37011 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001582]\n",
      "epoch:47 step:37012 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.022059]\n",
      "epoch:47 step:37013 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.001597]\n",
      "epoch:47 step:37014 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:47 step:37015 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.001310]\n",
      "epoch:47 step:37016 [D loss: 0.003765, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:47 step:37017 [D loss: 0.002084, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:47 step:37018 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:47 step:37019 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.002994]\n",
      "epoch:47 step:37020 [D loss: 0.004420, acc.: 100.00%] [G loss: 0.004671]\n",
      "epoch:47 step:37021 [D loss: 0.054272, acc.: 98.44%] [G loss: 0.051364]\n",
      "epoch:47 step:37022 [D loss: 0.521524, acc.: 78.12%] [G loss: 0.005296]\n",
      "epoch:47 step:37023 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.027654]\n",
      "epoch:47 step:37024 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.020018]\n",
      "epoch:47 step:37025 [D loss: 0.005338, acc.: 100.00%] [G loss: 0.057656]\n",
      "epoch:47 step:37026 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.012595]\n",
      "epoch:47 step:37027 [D loss: 0.063989, acc.: 96.88%] [G loss: 0.009917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37028 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:47 step:37029 [D loss: 0.008813, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:47 step:37030 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002316]\n",
      "epoch:47 step:37031 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.181486]\n",
      "epoch:47 step:37032 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.002337]\n",
      "epoch:47 step:37033 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.006606]\n",
      "epoch:47 step:37034 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.006617]\n",
      "epoch:47 step:37035 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001145]\n",
      "epoch:47 step:37036 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.099870]\n",
      "epoch:47 step:37037 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.001427]\n",
      "epoch:47 step:37038 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002152]\n",
      "epoch:47 step:37039 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004319]\n",
      "epoch:47 step:37040 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000734]\n",
      "epoch:47 step:37041 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001419]\n",
      "epoch:47 step:37042 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.001200]\n",
      "epoch:47 step:37043 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:47 step:37044 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000771]\n",
      "epoch:47 step:37045 [D loss: 0.000542, acc.: 100.00%] [G loss: 0.001574]\n",
      "epoch:47 step:37046 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000906]\n",
      "epoch:47 step:37047 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.002993]\n",
      "epoch:47 step:37048 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:47 step:37049 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001266]\n",
      "epoch:47 step:37050 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:47 step:37051 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:47 step:37052 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.001222]\n",
      "epoch:47 step:37053 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.032971]\n",
      "epoch:47 step:37054 [D loss: 0.003703, acc.: 100.00%] [G loss: 0.000878]\n",
      "epoch:47 step:37055 [D loss: 0.000960, acc.: 100.00%] [G loss: 0.002284]\n",
      "epoch:47 step:37056 [D loss: 0.003585, acc.: 100.00%] [G loss: 0.001894]\n",
      "epoch:47 step:37057 [D loss: 0.001673, acc.: 100.00%] [G loss: 0.006015]\n",
      "epoch:47 step:37058 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:47 step:37059 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:47 step:37060 [D loss: 0.004660, acc.: 100.00%] [G loss: 0.001203]\n",
      "epoch:47 step:37061 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:47 step:37062 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.001397]\n",
      "epoch:47 step:37063 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000632]\n",
      "epoch:47 step:37064 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001923]\n",
      "epoch:47 step:37065 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.003774]\n",
      "epoch:47 step:37066 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001770]\n",
      "epoch:47 step:37067 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000568]\n",
      "epoch:47 step:37068 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:47 step:37069 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:47 step:37070 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:47 step:37071 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:47 step:37072 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.002119]\n",
      "epoch:47 step:37073 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:47 step:37074 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:47 step:37075 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:47 step:37076 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.003838]\n",
      "epoch:47 step:37077 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.003156]\n",
      "epoch:47 step:37078 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:47 step:37079 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.003535]\n",
      "epoch:47 step:37080 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:47 step:37081 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000939]\n",
      "epoch:47 step:37082 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:47 step:37083 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.014235]\n",
      "epoch:47 step:37084 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:47 step:37085 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002807]\n",
      "epoch:47 step:37086 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:47 step:37087 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000586]\n",
      "epoch:47 step:37088 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:47 step:37089 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000845]\n",
      "epoch:47 step:37090 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:47 step:37091 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:47 step:37092 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000716]\n",
      "epoch:47 step:37093 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:47 step:37094 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001580]\n",
      "epoch:47 step:37095 [D loss: 0.004394, acc.: 100.00%] [G loss: 0.003220]\n",
      "epoch:47 step:37096 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:47 step:37097 [D loss: 0.000665, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:47 step:37098 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:47 step:37099 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:47 step:37100 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:47 step:37101 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:47 step:37102 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001840]\n",
      "epoch:47 step:37103 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002596]\n",
      "epoch:47 step:37104 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:47 step:37105 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001492]\n",
      "epoch:47 step:37106 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:47 step:37107 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.002835]\n",
      "epoch:47 step:37108 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000700]\n",
      "epoch:47 step:37109 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:47 step:37110 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:47 step:37111 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.003559]\n",
      "epoch:47 step:37112 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.001016]\n",
      "epoch:47 step:37113 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001648]\n",
      "epoch:47 step:37114 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:47 step:37115 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:47 step:37116 [D loss: 0.000835, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:47 step:37117 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.001142]\n",
      "epoch:47 step:37118 [D loss: 0.003328, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:47 step:37119 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:47 step:37120 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:47 step:37121 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:47 step:37122 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:47 step:37123 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000572]\n",
      "epoch:47 step:37124 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:47 step:37125 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:47 step:37126 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:47 step:37127 [D loss: 0.001693, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:47 step:37128 [D loss: 0.000639, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:47 step:37129 [D loss: 0.002176, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:47 step:37130 [D loss: 0.450121, acc.: 78.12%] [G loss: 3.984936]\n",
      "epoch:47 step:37131 [D loss: 0.360854, acc.: 84.38%] [G loss: 0.214810]\n",
      "epoch:47 step:37132 [D loss: 0.228977, acc.: 92.19%] [G loss: 0.013178]\n",
      "epoch:47 step:37133 [D loss: 0.000675, acc.: 100.00%] [G loss: 1.221616]\n",
      "epoch:47 step:37134 [D loss: 0.002498, acc.: 100.00%] [G loss: 0.826553]\n",
      "epoch:47 step:37135 [D loss: 0.008452, acc.: 100.00%] [G loss: 0.203891]\n",
      "epoch:47 step:37136 [D loss: 0.004994, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:47 step:37137 [D loss: 0.027459, acc.: 99.22%] [G loss: 0.012865]\n",
      "epoch:47 step:37138 [D loss: 0.011078, acc.: 99.22%] [G loss: 0.000042]\n",
      "epoch:47 step:37139 [D loss: 0.004912, acc.: 100.00%] [G loss: 0.020406]\n",
      "epoch:47 step:37140 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.002856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37141 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:47 step:37142 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.001966]\n",
      "epoch:47 step:37143 [D loss: 0.002361, acc.: 100.00%] [G loss: 0.003717]\n",
      "epoch:47 step:37144 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.094115]\n",
      "epoch:47 step:37145 [D loss: 0.013718, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:47 step:37146 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:47 step:37147 [D loss: 0.003825, acc.: 100.00%] [G loss: 0.004129]\n",
      "epoch:47 step:37148 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:47 step:37149 [D loss: 0.051349, acc.: 99.22%] [G loss: 0.000932]\n",
      "epoch:47 step:37150 [D loss: 0.001587, acc.: 100.00%] [G loss: 0.010199]\n",
      "epoch:47 step:37151 [D loss: 0.011057, acc.: 100.00%] [G loss: 0.031225]\n",
      "epoch:47 step:37152 [D loss: 0.006736, acc.: 100.00%] [G loss: 0.008154]\n",
      "epoch:47 step:37153 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.175166]\n",
      "epoch:47 step:37154 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002858]\n",
      "epoch:47 step:37155 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.009359]\n",
      "epoch:47 step:37156 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.002276]\n",
      "epoch:47 step:37157 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.001611]\n",
      "epoch:47 step:37158 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002827]\n",
      "epoch:47 step:37159 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.002375]\n",
      "epoch:47 step:37160 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.001841]\n",
      "epoch:47 step:37161 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.012700]\n",
      "epoch:47 step:37162 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.001194]\n",
      "epoch:47 step:37163 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.001428]\n",
      "epoch:47 step:37164 [D loss: 0.039360, acc.: 99.22%] [G loss: 0.002461]\n",
      "epoch:47 step:37165 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.010835]\n",
      "epoch:47 step:37166 [D loss: 0.002935, acc.: 100.00%] [G loss: 0.128494]\n",
      "epoch:47 step:37167 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.007733]\n",
      "epoch:47 step:37168 [D loss: 0.002396, acc.: 100.00%] [G loss: 0.017571]\n",
      "epoch:47 step:37169 [D loss: 0.005939, acc.: 100.00%] [G loss: 0.031383]\n",
      "epoch:47 step:37170 [D loss: 0.004010, acc.: 100.00%] [G loss: 0.001329]\n",
      "epoch:47 step:37171 [D loss: 0.001166, acc.: 100.00%] [G loss: 0.028948]\n",
      "epoch:47 step:37172 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.014951]\n",
      "epoch:47 step:37173 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001071]\n",
      "epoch:47 step:37174 [D loss: 0.004231, acc.: 100.00%] [G loss: 0.009966]\n",
      "epoch:47 step:37175 [D loss: 0.015578, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:47 step:37176 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.062573]\n",
      "epoch:47 step:37177 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.064329]\n",
      "epoch:47 step:37178 [D loss: 0.004161, acc.: 100.00%] [G loss: 0.010665]\n",
      "epoch:47 step:37179 [D loss: 0.003753, acc.: 100.00%] [G loss: 0.010917]\n",
      "epoch:47 step:37180 [D loss: 0.015313, acc.: 99.22%] [G loss: 0.036460]\n",
      "epoch:47 step:37181 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.000694]\n",
      "epoch:47 step:37182 [D loss: 0.002339, acc.: 100.00%] [G loss: 0.002551]\n",
      "epoch:47 step:37183 [D loss: 0.003386, acc.: 100.00%] [G loss: 0.003304]\n",
      "epoch:47 step:37184 [D loss: 0.004802, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:47 step:37185 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.001696]\n",
      "epoch:47 step:37186 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.004321]\n",
      "epoch:47 step:37187 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:47 step:37188 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.002464]\n",
      "epoch:47 step:37189 [D loss: 0.003853, acc.: 100.00%] [G loss: 0.006303]\n",
      "epoch:47 step:37190 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.005115]\n",
      "epoch:47 step:37191 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.004258]\n",
      "epoch:47 step:37192 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.003462]\n",
      "epoch:47 step:37193 [D loss: 0.002484, acc.: 100.00%] [G loss: 0.002432]\n",
      "epoch:47 step:37194 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.006101]\n",
      "epoch:47 step:37195 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:47 step:37196 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.049810]\n",
      "epoch:47 step:37197 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.006408]\n",
      "epoch:47 step:37198 [D loss: 0.001588, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:47 step:37199 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001373]\n",
      "epoch:47 step:37200 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001860]\n",
      "epoch:47 step:37201 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001428]\n",
      "epoch:47 step:37202 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001634]\n",
      "epoch:47 step:37203 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:47 step:37204 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.002608]\n",
      "epoch:47 step:37205 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.012900]\n",
      "epoch:47 step:37206 [D loss: 0.028272, acc.: 98.44%] [G loss: 0.000036]\n",
      "epoch:47 step:37207 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:47 step:37208 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:47 step:37209 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:47 step:37210 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:47 step:37211 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:47 step:37212 [D loss: 0.003139, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:47 step:37213 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:47 step:37214 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:47 step:37215 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:47 step:37216 [D loss: 0.001123, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:47 step:37217 [D loss: 0.010055, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:47 step:37218 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:47 step:37219 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:47 step:37220 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:47 step:37221 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:47 step:37222 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:47 step:37223 [D loss: 0.004462, acc.: 100.00%] [G loss: 0.000597]\n",
      "epoch:47 step:37224 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.001219]\n",
      "epoch:47 step:37225 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:47 step:37226 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:47 step:37227 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:47 step:37228 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:47 step:37229 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:47 step:37230 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.003175]\n",
      "epoch:47 step:37231 [D loss: 0.001053, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:47 step:37232 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000733]\n",
      "epoch:47 step:37233 [D loss: 0.060547, acc.: 97.66%] [G loss: 0.000143]\n",
      "epoch:47 step:37234 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:47 step:37235 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:47 step:37236 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:47 step:37237 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.002241]\n",
      "epoch:47 step:37238 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:47 step:37239 [D loss: 0.004048, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:47 step:37240 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:47 step:37241 [D loss: 0.001197, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:47 step:37242 [D loss: 0.021299, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:47 step:37243 [D loss: 0.045812, acc.: 100.00%] [G loss: 0.007750]\n",
      "epoch:47 step:37244 [D loss: 0.120929, acc.: 94.53%] [G loss: 0.000791]\n",
      "epoch:47 step:37245 [D loss: 0.081635, acc.: 97.66%] [G loss: 0.620942]\n",
      "epoch:47 step:37246 [D loss: 0.003887, acc.: 100.00%] [G loss: 2.108398]\n",
      "epoch:47 step:37247 [D loss: 0.031087, acc.: 100.00%] [G loss: 2.338305]\n",
      "epoch:47 step:37248 [D loss: 0.030928, acc.: 100.00%] [G loss: 1.759549]\n",
      "epoch:47 step:37249 [D loss: 0.224062, acc.: 90.62%] [G loss: 0.005953]\n",
      "epoch:47 step:37250 [D loss: 0.009379, acc.: 99.22%] [G loss: 1.330081]\n",
      "epoch:47 step:37251 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.217076]\n",
      "epoch:47 step:37252 [D loss: 0.002557, acc.: 100.00%] [G loss: 0.210551]\n",
      "epoch:47 step:37253 [D loss: 0.002260, acc.: 100.00%] [G loss: 0.106301]\n",
      "epoch:47 step:37254 [D loss: 0.008325, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:47 step:37255 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.005999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37256 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.019583]\n",
      "epoch:47 step:37257 [D loss: 0.000487, acc.: 100.00%] [G loss: 0.092221]\n",
      "epoch:47 step:37258 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.024907]\n",
      "epoch:47 step:37259 [D loss: 0.071104, acc.: 96.88%] [G loss: 0.277316]\n",
      "epoch:47 step:37260 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.024666]\n",
      "epoch:47 step:37261 [D loss: 0.005572, acc.: 100.00%] [G loss: 0.760421]\n",
      "epoch:47 step:37262 [D loss: 0.015712, acc.: 99.22%] [G loss: 0.314351]\n",
      "epoch:47 step:37263 [D loss: 0.017270, acc.: 99.22%] [G loss: 0.130686]\n",
      "epoch:47 step:37264 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.024246]\n",
      "epoch:47 step:37265 [D loss: 0.001726, acc.: 100.00%] [G loss: 1.678961]\n",
      "epoch:47 step:37266 [D loss: 0.674988, acc.: 77.34%] [G loss: 5.205824]\n",
      "epoch:47 step:37267 [D loss: 0.803602, acc.: 67.97%] [G loss: 2.234084]\n",
      "epoch:47 step:37268 [D loss: 0.006545, acc.: 100.00%] [G loss: 1.218622]\n",
      "epoch:47 step:37269 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.641301]\n",
      "epoch:47 step:37270 [D loss: 0.119869, acc.: 96.09%] [G loss: 1.087036]\n",
      "epoch:47 step:37271 [D loss: 0.020646, acc.: 100.00%] [G loss: 1.413127]\n",
      "epoch:47 step:37272 [D loss: 0.021547, acc.: 100.00%] [G loss: 0.195827]\n",
      "epoch:47 step:37273 [D loss: 0.062251, acc.: 97.66%] [G loss: 1.144057]\n",
      "epoch:47 step:37274 [D loss: 0.077336, acc.: 97.66%] [G loss: 0.196721]\n",
      "epoch:47 step:37275 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.026504]\n",
      "epoch:47 step:37276 [D loss: 0.018359, acc.: 99.22%] [G loss: 0.119679]\n",
      "epoch:47 step:37277 [D loss: 0.010026, acc.: 100.00%] [G loss: 0.007202]\n",
      "epoch:47 step:37278 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.106893]\n",
      "epoch:47 step:37279 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.084594]\n",
      "epoch:47 step:37280 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.064079]\n",
      "epoch:47 step:37281 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000407]\n",
      "epoch:47 step:37282 [D loss: 0.003816, acc.: 100.00%] [G loss: 0.025512]\n",
      "epoch:47 step:37283 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:47 step:37284 [D loss: 0.001457, acc.: 100.00%] [G loss: 0.007343]\n",
      "epoch:47 step:37285 [D loss: 0.005804, acc.: 100.00%] [G loss: 0.405821]\n",
      "epoch:47 step:37286 [D loss: 0.037408, acc.: 98.44%] [G loss: 0.127607]\n",
      "epoch:47 step:37287 [D loss: 0.003363, acc.: 100.00%] [G loss: 1.564046]\n",
      "epoch:47 step:37288 [D loss: 0.012936, acc.: 99.22%] [G loss: 0.134842]\n",
      "epoch:47 step:37289 [D loss: 0.008391, acc.: 100.00%] [G loss: 0.626640]\n",
      "epoch:47 step:37290 [D loss: 0.126170, acc.: 95.31%] [G loss: 0.040688]\n",
      "epoch:47 step:37291 [D loss: 0.047047, acc.: 98.44%] [G loss: 0.191725]\n",
      "epoch:47 step:37292 [D loss: 0.002151, acc.: 100.00%] [G loss: 0.156959]\n",
      "epoch:47 step:37293 [D loss: 0.023486, acc.: 99.22%] [G loss: 0.027908]\n",
      "epoch:47 step:37294 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.045669]\n",
      "epoch:47 step:37295 [D loss: 0.006090, acc.: 100.00%] [G loss: 0.017094]\n",
      "epoch:47 step:37296 [D loss: 0.002714, acc.: 100.00%] [G loss: 0.003971]\n",
      "epoch:47 step:37297 [D loss: 0.001667, acc.: 100.00%] [G loss: 0.008180]\n",
      "epoch:47 step:37298 [D loss: 0.114291, acc.: 95.31%] [G loss: 0.020049]\n",
      "epoch:47 step:37299 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.089327]\n",
      "epoch:47 step:37300 [D loss: 0.113917, acc.: 93.75%] [G loss: 0.036648]\n",
      "epoch:47 step:37301 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.014483]\n",
      "epoch:47 step:37302 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.001627]\n",
      "epoch:47 step:37303 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.025571]\n",
      "epoch:47 step:37304 [D loss: 0.000950, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:47 step:37305 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.011623]\n",
      "epoch:47 step:37306 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.006588]\n",
      "epoch:47 step:37307 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:47 step:37308 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:47 step:37309 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.002768]\n",
      "epoch:47 step:37310 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:47 step:37311 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.001291]\n",
      "epoch:47 step:37312 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:47 step:37313 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.001306]\n",
      "epoch:47 step:37314 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:47 step:37315 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001069]\n",
      "epoch:47 step:37316 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:47 step:37317 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.131280]\n",
      "epoch:47 step:37318 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:47 step:37319 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.009012]\n",
      "epoch:47 step:37320 [D loss: 0.004051, acc.: 100.00%] [G loss: 0.001249]\n",
      "epoch:47 step:37321 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.002729]\n",
      "epoch:47 step:37322 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.001921]\n",
      "epoch:47 step:37323 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:47 step:37324 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000957]\n",
      "epoch:47 step:37325 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:47 step:37326 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.000921]\n",
      "epoch:47 step:37327 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:47 step:37328 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:47 step:37329 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.018178]\n",
      "epoch:47 step:37330 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.001045]\n",
      "epoch:47 step:37331 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:47 step:37332 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:47 step:37333 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:47 step:37334 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:47 step:37335 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.001322]\n",
      "epoch:47 step:37336 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:47 step:37337 [D loss: 0.001745, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:47 step:37338 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:47 step:37339 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.003841]\n",
      "epoch:47 step:37340 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001431]\n",
      "epoch:47 step:37341 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:47 step:37342 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000977]\n",
      "epoch:47 step:37343 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:47 step:37344 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000832]\n",
      "epoch:47 step:37345 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:47 step:37346 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:47 step:37347 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000809]\n",
      "epoch:47 step:37348 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:47 step:37349 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:47 step:37350 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002278]\n",
      "epoch:47 step:37351 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:47 step:37352 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:47 step:37353 [D loss: 0.003734, acc.: 100.00%] [G loss: 0.000543]\n",
      "epoch:47 step:37354 [D loss: 0.111580, acc.: 96.88%] [G loss: 0.279797]\n",
      "epoch:47 step:37355 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.488281]\n",
      "epoch:47 step:37356 [D loss: 0.493059, acc.: 79.69%] [G loss: 0.014171]\n",
      "epoch:47 step:37357 [D loss: 0.045576, acc.: 98.44%] [G loss: 0.073301]\n",
      "epoch:47 step:37358 [D loss: 0.008681, acc.: 100.00%] [G loss: 0.193083]\n",
      "epoch:47 step:37359 [D loss: 0.022073, acc.: 99.22%] [G loss: 0.389600]\n",
      "epoch:47 step:37360 [D loss: 0.016296, acc.: 100.00%] [G loss: 0.417242]\n",
      "epoch:47 step:37361 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.451613]\n",
      "epoch:47 step:37362 [D loss: 0.002286, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:47 step:37363 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.002702]\n",
      "epoch:47 step:37364 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.585017]\n",
      "epoch:47 step:37365 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:47 step:37366 [D loss: 0.001098, acc.: 100.00%] [G loss: 0.313539]\n",
      "epoch:47 step:37367 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.222900]\n",
      "epoch:47 step:37368 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.275031]\n",
      "epoch:47 step:37369 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.224146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37370 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.027281]\n",
      "epoch:47 step:37371 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.001127]\n",
      "epoch:47 step:37372 [D loss: 0.036466, acc.: 99.22%] [G loss: 0.002954]\n",
      "epoch:47 step:37373 [D loss: 0.010747, acc.: 100.00%] [G loss: 0.011448]\n",
      "epoch:47 step:37374 [D loss: 0.117837, acc.: 96.09%] [G loss: 0.014322]\n",
      "epoch:47 step:37375 [D loss: 0.004317, acc.: 100.00%] [G loss: 0.149009]\n",
      "epoch:47 step:37376 [D loss: 0.006449, acc.: 100.00%] [G loss: 0.036239]\n",
      "epoch:47 step:37377 [D loss: 0.084040, acc.: 96.88%] [G loss: 0.023704]\n",
      "epoch:47 step:37378 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.005399]\n",
      "epoch:47 step:37379 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001498]\n",
      "epoch:47 step:37380 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.003205]\n",
      "epoch:47 step:37381 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.003492]\n",
      "epoch:47 step:37382 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000882]\n",
      "epoch:47 step:37383 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.001070]\n",
      "epoch:47 step:37384 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.014419]\n",
      "epoch:47 step:37385 [D loss: 0.000966, acc.: 100.00%] [G loss: 0.001737]\n",
      "epoch:47 step:37386 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000514]\n",
      "epoch:47 step:37387 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000561]\n",
      "epoch:47 step:37388 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:47 step:37389 [D loss: 0.027988, acc.: 100.00%] [G loss: 0.001362]\n",
      "epoch:47 step:37390 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.002387]\n",
      "epoch:47 step:37391 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.005102]\n",
      "epoch:47 step:37392 [D loss: 0.000642, acc.: 100.00%] [G loss: 0.185946]\n",
      "epoch:47 step:37393 [D loss: 0.002595, acc.: 100.00%] [G loss: 0.003541]\n",
      "epoch:47 step:37394 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.004275]\n",
      "epoch:47 step:37395 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.001511]\n",
      "epoch:47 step:37396 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.006868]\n",
      "epoch:47 step:37397 [D loss: 0.002356, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:47 step:37398 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001489]\n",
      "epoch:47 step:37399 [D loss: 0.003963, acc.: 100.00%] [G loss: 0.000500]\n",
      "epoch:47 step:37400 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002218]\n",
      "epoch:47 step:37401 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.002988]\n",
      "epoch:47 step:37402 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001557]\n",
      "epoch:47 step:37403 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.004261]\n",
      "epoch:47 step:37404 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.002556]\n",
      "epoch:47 step:37405 [D loss: 0.000731, acc.: 100.00%] [G loss: 0.003887]\n",
      "epoch:47 step:37406 [D loss: 0.021408, acc.: 100.00%] [G loss: 0.005864]\n",
      "epoch:47 step:37407 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.018392]\n",
      "epoch:47 step:37408 [D loss: 0.005055, acc.: 100.00%] [G loss: 0.098109]\n",
      "epoch:47 step:37409 [D loss: 0.002607, acc.: 100.00%] [G loss: 0.026699]\n",
      "epoch:47 step:37410 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.001355]\n",
      "epoch:47 step:37411 [D loss: 0.044011, acc.: 99.22%] [G loss: 0.014066]\n",
      "epoch:47 step:37412 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001884]\n",
      "epoch:47 step:37413 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004777]\n",
      "epoch:47 step:37414 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:47 step:37415 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:47 step:37416 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.003459]\n",
      "epoch:47 step:37417 [D loss: 0.001233, acc.: 100.00%] [G loss: 0.005810]\n",
      "epoch:47 step:37418 [D loss: 0.016277, acc.: 100.00%] [G loss: 0.649042]\n",
      "epoch:47 step:37419 [D loss: 0.036885, acc.: 98.44%] [G loss: 0.060599]\n",
      "epoch:47 step:37420 [D loss: 0.109505, acc.: 96.09%] [G loss: 0.247458]\n",
      "epoch:47 step:37421 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.007847]\n",
      "epoch:47 step:37422 [D loss: 0.290075, acc.: 85.94%] [G loss: 1.951286]\n",
      "epoch:47 step:37423 [D loss: 1.285978, acc.: 56.25%] [G loss: 4.786926]\n",
      "epoch:47 step:37424 [D loss: 0.025380, acc.: 100.00%] [G loss: 4.821382]\n",
      "epoch:47 step:37425 [D loss: 1.619170, acc.: 56.25%] [G loss: 0.092617]\n",
      "epoch:47 step:37426 [D loss: 0.000639, acc.: 100.00%] [G loss: 0.017348]\n",
      "epoch:47 step:37427 [D loss: 0.151648, acc.: 92.97%] [G loss: 0.032220]\n",
      "epoch:47 step:37428 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.016131]\n",
      "epoch:47 step:37429 [D loss: 0.004670, acc.: 100.00%] [G loss: 0.023549]\n",
      "epoch:47 step:37430 [D loss: 0.003440, acc.: 100.00%] [G loss: 0.103689]\n",
      "epoch:47 step:37431 [D loss: 0.006305, acc.: 100.00%] [G loss: 0.010174]\n",
      "epoch:47 step:37432 [D loss: 0.022847, acc.: 100.00%] [G loss: 0.024403]\n",
      "epoch:47 step:37433 [D loss: 0.029950, acc.: 99.22%] [G loss: 0.073453]\n",
      "epoch:47 step:37434 [D loss: 0.011873, acc.: 100.00%] [G loss: 0.609528]\n",
      "epoch:47 step:37435 [D loss: 0.010485, acc.: 100.00%] [G loss: 0.081407]\n",
      "epoch:47 step:37436 [D loss: 0.020232, acc.: 100.00%] [G loss: 0.213138]\n",
      "epoch:47 step:37437 [D loss: 0.015766, acc.: 100.00%] [G loss: 0.727972]\n",
      "epoch:47 step:37438 [D loss: 0.004098, acc.: 100.00%] [G loss: 0.253511]\n",
      "epoch:47 step:37439 [D loss: 0.071028, acc.: 96.88%] [G loss: 2.765010]\n",
      "epoch:47 step:37440 [D loss: 0.038255, acc.: 98.44%] [G loss: 8.907986]\n",
      "epoch:47 step:37441 [D loss: 0.027345, acc.: 100.00%] [G loss: 8.043530]\n",
      "epoch:47 step:37442 [D loss: 0.057709, acc.: 97.66%] [G loss: 6.406413]\n",
      "epoch:47 step:37443 [D loss: 0.028106, acc.: 98.44%] [G loss: 5.305354]\n",
      "epoch:47 step:37444 [D loss: 0.056415, acc.: 98.44%] [G loss: 6.666582]\n",
      "epoch:47 step:37445 [D loss: 0.004443, acc.: 100.00%] [G loss: 7.603165]\n",
      "epoch:47 step:37446 [D loss: 0.034527, acc.: 98.44%] [G loss: 6.346118]\n",
      "epoch:47 step:37447 [D loss: 0.003887, acc.: 100.00%] [G loss: 5.186612]\n",
      "epoch:47 step:37448 [D loss: 0.019220, acc.: 100.00%] [G loss: 4.387082]\n",
      "epoch:47 step:37449 [D loss: 0.036207, acc.: 99.22%] [G loss: 7.417745]\n",
      "epoch:47 step:37450 [D loss: 0.002392, acc.: 100.00%] [G loss: 7.058337]\n",
      "epoch:47 step:37451 [D loss: 0.010365, acc.: 100.00%] [G loss: 6.101706]\n",
      "epoch:47 step:37452 [D loss: 0.008947, acc.: 100.00%] [G loss: 8.670167]\n",
      "epoch:47 step:37453 [D loss: 0.009626, acc.: 100.00%] [G loss: 6.894361]\n",
      "epoch:47 step:37454 [D loss: 0.009607, acc.: 100.00%] [G loss: 6.320246]\n",
      "epoch:47 step:37455 [D loss: 0.005829, acc.: 100.00%] [G loss: 6.876659]\n",
      "epoch:47 step:37456 [D loss: 0.042363, acc.: 100.00%] [G loss: 9.487642]\n",
      "epoch:47 step:37457 [D loss: 0.064180, acc.: 96.88%] [G loss: 8.345491]\n",
      "epoch:47 step:37458 [D loss: 0.003298, acc.: 100.00%] [G loss: 7.155170]\n",
      "epoch:47 step:37459 [D loss: 0.021100, acc.: 99.22%] [G loss: 7.737575]\n",
      "epoch:47 step:37460 [D loss: 0.002622, acc.: 100.00%] [G loss: 5.657618]\n",
      "epoch:47 step:37461 [D loss: 0.059334, acc.: 97.66%] [G loss: 6.341393]\n",
      "epoch:47 step:37462 [D loss: 0.004736, acc.: 100.00%] [G loss: 4.609173]\n",
      "epoch:47 step:37463 [D loss: 0.007757, acc.: 100.00%] [G loss: 0.468772]\n",
      "epoch:47 step:37464 [D loss: 0.209364, acc.: 91.41%] [G loss: 10.542187]\n",
      "epoch:47 step:37465 [D loss: 0.674882, acc.: 72.66%] [G loss: 0.896549]\n",
      "epoch:47 step:37466 [D loss: 4.192171, acc.: 53.91%] [G loss: 9.311380]\n",
      "epoch:47 step:37467 [D loss: 0.308464, acc.: 85.16%] [G loss: 6.815805]\n",
      "epoch:47 step:37468 [D loss: 0.056923, acc.: 97.66%] [G loss: 4.918306]\n",
      "epoch:47 step:37469 [D loss: 0.388858, acc.: 83.59%] [G loss: 3.352570]\n",
      "epoch:47 step:37470 [D loss: 0.106677, acc.: 96.88%] [G loss: 0.458140]\n",
      "epoch:47 step:37471 [D loss: 0.116161, acc.: 98.44%] [G loss: 0.552259]\n",
      "epoch:47 step:37472 [D loss: 0.056460, acc.: 100.00%] [G loss: 0.520035]\n",
      "epoch:47 step:37473 [D loss: 0.047307, acc.: 98.44%] [G loss: 0.578821]\n",
      "epoch:47 step:37474 [D loss: 0.036750, acc.: 100.00%] [G loss: 0.185762]\n",
      "epoch:47 step:37475 [D loss: 0.040061, acc.: 99.22%] [G loss: 0.075797]\n",
      "epoch:47 step:37476 [D loss: 0.137022, acc.: 96.88%] [G loss: 0.739054]\n",
      "epoch:47 step:37477 [D loss: 0.022603, acc.: 99.22%] [G loss: 1.048584]\n",
      "epoch:47 step:37478 [D loss: 0.536101, acc.: 69.53%] [G loss: 2.268471]\n",
      "epoch:47 step:37479 [D loss: 0.077147, acc.: 97.66%] [G loss: 1.688044]\n",
      "epoch:47 step:37480 [D loss: 0.088176, acc.: 97.66%] [G loss: 1.413436]\n",
      "epoch:47 step:37481 [D loss: 0.235226, acc.: 89.06%] [G loss: 1.855227]\n",
      "epoch:47 step:37482 [D loss: 0.077176, acc.: 96.88%] [G loss: 0.797360]\n",
      "epoch:47 step:37483 [D loss: 0.180218, acc.: 93.75%] [G loss: 2.801636]\n",
      "epoch:47 step:37484 [D loss: 0.405562, acc.: 78.91%] [G loss: 2.450364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37485 [D loss: 0.054594, acc.: 97.66%] [G loss: 8.672781]\n",
      "epoch:47 step:37486 [D loss: 0.383269, acc.: 85.16%] [G loss: 4.948520]\n",
      "epoch:47 step:37487 [D loss: 0.091555, acc.: 95.31%] [G loss: 0.791699]\n",
      "epoch:47 step:37488 [D loss: 0.044951, acc.: 98.44%] [G loss: 1.004748]\n",
      "epoch:48 step:37489 [D loss: 0.063152, acc.: 99.22%] [G loss: 0.102035]\n",
      "epoch:48 step:37490 [D loss: 0.043993, acc.: 99.22%] [G loss: 1.002590]\n",
      "epoch:48 step:37491 [D loss: 0.113371, acc.: 92.97%] [G loss: 0.336784]\n",
      "epoch:48 step:37492 [D loss: 0.028828, acc.: 100.00%] [G loss: 0.466539]\n",
      "epoch:48 step:37493 [D loss: 0.013806, acc.: 100.00%] [G loss: 0.213397]\n",
      "epoch:48 step:37494 [D loss: 0.034114, acc.: 99.22%] [G loss: 0.550669]\n",
      "epoch:48 step:37495 [D loss: 0.036435, acc.: 99.22%] [G loss: 0.133182]\n",
      "epoch:48 step:37496 [D loss: 0.034205, acc.: 100.00%] [G loss: 0.071545]\n",
      "epoch:48 step:37497 [D loss: 0.012867, acc.: 100.00%] [G loss: 0.108508]\n",
      "epoch:48 step:37498 [D loss: 0.104491, acc.: 99.22%] [G loss: 1.494218]\n",
      "epoch:48 step:37499 [D loss: 0.015690, acc.: 100.00%] [G loss: 0.976000]\n",
      "epoch:48 step:37500 [D loss: 0.088313, acc.: 96.88%] [G loss: 0.192997]\n",
      "epoch:48 step:37501 [D loss: 0.160601, acc.: 95.31%] [G loss: 4.585558]\n",
      "epoch:48 step:37502 [D loss: 0.396365, acc.: 83.59%] [G loss: 1.933698]\n",
      "epoch:48 step:37503 [D loss: 0.183344, acc.: 94.53%] [G loss: 1.127997]\n",
      "epoch:48 step:37504 [D loss: 0.300452, acc.: 89.84%] [G loss: 5.228327]\n",
      "epoch:48 step:37505 [D loss: 0.168900, acc.: 91.41%] [G loss: 2.903248]\n",
      "epoch:48 step:37506 [D loss: 0.263257, acc.: 88.28%] [G loss: 2.318348]\n",
      "epoch:48 step:37507 [D loss: 0.117161, acc.: 97.66%] [G loss: 4.710990]\n",
      "epoch:48 step:37508 [D loss: 0.467443, acc.: 82.03%] [G loss: 3.691232]\n",
      "epoch:48 step:37509 [D loss: 0.569956, acc.: 74.22%] [G loss: 5.886823]\n",
      "epoch:48 step:37510 [D loss: 0.102418, acc.: 96.09%] [G loss: 4.495281]\n",
      "epoch:48 step:37511 [D loss: 0.258602, acc.: 89.84%] [G loss: 3.695637]\n",
      "epoch:48 step:37512 [D loss: 0.226699, acc.: 95.31%] [G loss: 5.665054]\n",
      "epoch:48 step:37513 [D loss: 0.100901, acc.: 96.88%] [G loss: 4.255993]\n",
      "epoch:48 step:37514 [D loss: 0.201127, acc.: 93.75%] [G loss: 1.835340]\n",
      "epoch:48 step:37515 [D loss: 0.017231, acc.: 100.00%] [G loss: 1.018934]\n",
      "epoch:48 step:37516 [D loss: 0.839716, acc.: 64.06%] [G loss: 8.866503]\n",
      "epoch:48 step:37517 [D loss: 1.467635, acc.: 53.91%] [G loss: 5.260652]\n",
      "epoch:48 step:37518 [D loss: 0.306420, acc.: 88.28%] [G loss: 2.510063]\n",
      "epoch:48 step:37519 [D loss: 0.250806, acc.: 85.16%] [G loss: 2.244625]\n",
      "epoch:48 step:37520 [D loss: 0.125796, acc.: 95.31%] [G loss: 3.200332]\n",
      "epoch:48 step:37521 [D loss: 0.182184, acc.: 92.19%] [G loss: 2.454617]\n",
      "epoch:48 step:37522 [D loss: 0.091074, acc.: 97.66%] [G loss: 2.332363]\n",
      "epoch:48 step:37523 [D loss: 0.819812, acc.: 56.25%] [G loss: 5.356632]\n",
      "epoch:48 step:37524 [D loss: 0.406967, acc.: 82.03%] [G loss: 6.037127]\n",
      "epoch:48 step:37525 [D loss: 0.160174, acc.: 96.09%] [G loss: 3.193836]\n",
      "epoch:48 step:37526 [D loss: 0.150813, acc.: 93.75%] [G loss: 3.480602]\n",
      "epoch:48 step:37527 [D loss: 0.017247, acc.: 100.00%] [G loss: 3.924991]\n",
      "epoch:48 step:37528 [D loss: 0.078720, acc.: 97.66%] [G loss: 3.042125]\n",
      "epoch:48 step:37529 [D loss: 0.084135, acc.: 96.09%] [G loss: 1.362125]\n",
      "epoch:48 step:37530 [D loss: 0.575404, acc.: 69.53%] [G loss: 5.786100]\n",
      "epoch:48 step:37531 [D loss: 0.711217, acc.: 69.53%] [G loss: 4.240486]\n",
      "epoch:48 step:37532 [D loss: 0.329525, acc.: 88.28%] [G loss: 3.443773]\n",
      "epoch:48 step:37533 [D loss: 0.029612, acc.: 99.22%] [G loss: 2.122022]\n",
      "epoch:48 step:37534 [D loss: 0.254116, acc.: 87.50%] [G loss: 4.138696]\n",
      "epoch:48 step:37535 [D loss: 0.206400, acc.: 92.19%] [G loss: 4.529403]\n",
      "epoch:48 step:37536 [D loss: 0.182286, acc.: 92.97%] [G loss: 2.768380]\n",
      "epoch:48 step:37537 [D loss: 0.094009, acc.: 96.88%] [G loss: 1.987126]\n",
      "epoch:48 step:37538 [D loss: 0.046718, acc.: 99.22%] [G loss: 3.322531]\n",
      "epoch:48 step:37539 [D loss: 0.031079, acc.: 100.00%] [G loss: 2.268790]\n",
      "epoch:48 step:37540 [D loss: 0.059719, acc.: 98.44%] [G loss: 0.587837]\n",
      "epoch:48 step:37541 [D loss: 0.910816, acc.: 59.38%] [G loss: 6.484769]\n",
      "epoch:48 step:37542 [D loss: 0.765661, acc.: 68.75%] [G loss: 5.710423]\n",
      "epoch:48 step:37543 [D loss: 0.395578, acc.: 78.12%] [G loss: 0.757496]\n",
      "epoch:48 step:37544 [D loss: 0.188300, acc.: 92.19%] [G loss: 1.405923]\n",
      "epoch:48 step:37545 [D loss: 0.020003, acc.: 100.00%] [G loss: 0.477595]\n",
      "epoch:48 step:37546 [D loss: 0.003212, acc.: 100.00%] [G loss: 1.111124]\n",
      "epoch:48 step:37547 [D loss: 0.291473, acc.: 83.59%] [G loss: 0.166520]\n",
      "epoch:48 step:37548 [D loss: 0.015192, acc.: 100.00%] [G loss: 2.028435]\n",
      "epoch:48 step:37549 [D loss: 0.171217, acc.: 92.97%] [G loss: 0.913316]\n",
      "epoch:48 step:37550 [D loss: 0.007336, acc.: 100.00%] [G loss: 0.488903]\n",
      "epoch:48 step:37551 [D loss: 0.014530, acc.: 100.00%] [G loss: 0.169422]\n",
      "epoch:48 step:37552 [D loss: 0.161086, acc.: 96.09%] [G loss: 0.321306]\n",
      "epoch:48 step:37553 [D loss: 0.318253, acc.: 86.72%] [G loss: 0.727418]\n",
      "epoch:48 step:37554 [D loss: 0.011078, acc.: 100.00%] [G loss: 2.246754]\n",
      "epoch:48 step:37555 [D loss: 0.143188, acc.: 96.88%] [G loss: 1.595835]\n",
      "epoch:48 step:37556 [D loss: 0.079807, acc.: 97.66%] [G loss: 0.749253]\n",
      "epoch:48 step:37557 [D loss: 0.028930, acc.: 100.00%] [G loss: 0.883979]\n",
      "epoch:48 step:37558 [D loss: 0.013719, acc.: 100.00%] [G loss: 2.205945]\n",
      "epoch:48 step:37559 [D loss: 1.181006, acc.: 52.34%] [G loss: 4.071508]\n",
      "epoch:48 step:37560 [D loss: 0.347143, acc.: 82.03%] [G loss: 4.881990]\n",
      "epoch:48 step:37561 [D loss: 0.472266, acc.: 75.78%] [G loss: 2.077665]\n",
      "epoch:48 step:37562 [D loss: 0.253386, acc.: 89.84%] [G loss: 1.927517]\n",
      "epoch:48 step:37563 [D loss: 0.227151, acc.: 90.62%] [G loss: 2.852601]\n",
      "epoch:48 step:37564 [D loss: 0.168019, acc.: 96.09%] [G loss: 2.787248]\n",
      "epoch:48 step:37565 [D loss: 0.655068, acc.: 71.09%] [G loss: 1.949224]\n",
      "epoch:48 step:37566 [D loss: 0.142544, acc.: 96.09%] [G loss: 2.301048]\n",
      "epoch:48 step:37567 [D loss: 0.150171, acc.: 97.66%] [G loss: 2.825818]\n",
      "epoch:48 step:37568 [D loss: 0.170299, acc.: 94.53%] [G loss: 2.220447]\n",
      "epoch:48 step:37569 [D loss: 0.194354, acc.: 95.31%] [G loss: 2.681761]\n",
      "epoch:48 step:37570 [D loss: 0.089724, acc.: 96.09%] [G loss: 1.701833]\n",
      "epoch:48 step:37571 [D loss: 0.071372, acc.: 98.44%] [G loss: 2.832890]\n",
      "epoch:48 step:37572 [D loss: 0.186137, acc.: 96.88%] [G loss: 2.033677]\n",
      "epoch:48 step:37573 [D loss: 0.194301, acc.: 92.97%] [G loss: 3.381459]\n",
      "epoch:48 step:37574 [D loss: 0.496528, acc.: 75.00%] [G loss: 4.733953]\n",
      "epoch:48 step:37575 [D loss: 0.200537, acc.: 92.19%] [G loss: 5.060326]\n",
      "epoch:48 step:37576 [D loss: 0.550694, acc.: 70.31%] [G loss: 0.877180]\n",
      "epoch:48 step:37577 [D loss: 0.248033, acc.: 91.41%] [G loss: 1.228229]\n",
      "epoch:48 step:37578 [D loss: 0.069487, acc.: 98.44%] [G loss: 0.324349]\n",
      "epoch:48 step:37579 [D loss: 0.009025, acc.: 100.00%] [G loss: 1.763889]\n",
      "epoch:48 step:37580 [D loss: 0.031094, acc.: 99.22%] [G loss: 1.692580]\n",
      "epoch:48 step:37581 [D loss: 0.006410, acc.: 100.00%] [G loss: 0.792934]\n",
      "epoch:48 step:37582 [D loss: 0.150743, acc.: 95.31%] [G loss: 2.178747]\n",
      "epoch:48 step:37583 [D loss: 0.062924, acc.: 100.00%] [G loss: 2.607780]\n",
      "epoch:48 step:37584 [D loss: 0.061387, acc.: 97.66%] [G loss: 1.819860]\n",
      "epoch:48 step:37585 [D loss: 0.359457, acc.: 83.59%] [G loss: 2.244662]\n",
      "epoch:48 step:37586 [D loss: 0.028712, acc.: 99.22%] [G loss: 1.517319]\n",
      "epoch:48 step:37587 [D loss: 0.303554, acc.: 89.06%] [G loss: 0.405466]\n",
      "epoch:48 step:37588 [D loss: 0.067337, acc.: 97.66%] [G loss: 3.420438]\n",
      "epoch:48 step:37589 [D loss: 0.052090, acc.: 98.44%] [G loss: 1.386088]\n",
      "epoch:48 step:37590 [D loss: 0.026583, acc.: 100.00%] [G loss: 0.934962]\n",
      "epoch:48 step:37591 [D loss: 0.025919, acc.: 98.44%] [G loss: 0.088811]\n",
      "epoch:48 step:37592 [D loss: 0.007636, acc.: 100.00%] [G loss: 0.079843]\n",
      "epoch:48 step:37593 [D loss: 0.047776, acc.: 98.44%] [G loss: 0.076719]\n",
      "epoch:48 step:37594 [D loss: 0.039746, acc.: 98.44%] [G loss: 0.070829]\n",
      "epoch:48 step:37595 [D loss: 0.083828, acc.: 96.09%] [G loss: 0.790661]\n",
      "epoch:48 step:37596 [D loss: 0.186588, acc.: 93.75%] [G loss: 0.346820]\n",
      "epoch:48 step:37597 [D loss: 0.029586, acc.: 99.22%] [G loss: 0.515580]\n",
      "epoch:48 step:37598 [D loss: 0.108515, acc.: 96.88%] [G loss: 0.135167]\n",
      "epoch:48 step:37599 [D loss: 0.205698, acc.: 92.19%] [G loss: 0.619552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37600 [D loss: 0.013087, acc.: 100.00%] [G loss: 0.528552]\n",
      "epoch:48 step:37601 [D loss: 1.002566, acc.: 57.81%] [G loss: 2.326916]\n",
      "epoch:48 step:37602 [D loss: 1.349027, acc.: 57.81%] [G loss: 0.937453]\n",
      "epoch:48 step:37603 [D loss: 0.260273, acc.: 88.28%] [G loss: 2.150925]\n",
      "epoch:48 step:37604 [D loss: 0.168925, acc.: 92.97%] [G loss: 0.004043]\n",
      "epoch:48 step:37605 [D loss: 0.162827, acc.: 94.53%] [G loss: 2.003939]\n",
      "epoch:48 step:37606 [D loss: 0.007548, acc.: 100.00%] [G loss: 2.003295]\n",
      "epoch:48 step:37607 [D loss: 0.027600, acc.: 98.44%] [G loss: 1.940926]\n",
      "epoch:48 step:37608 [D loss: 0.003184, acc.: 100.00%] [G loss: 0.496806]\n",
      "epoch:48 step:37609 [D loss: 0.016245, acc.: 100.00%] [G loss: 0.032869]\n",
      "epoch:48 step:37610 [D loss: 0.035369, acc.: 99.22%] [G loss: 0.241016]\n",
      "epoch:48 step:37611 [D loss: 0.017191, acc.: 100.00%] [G loss: 0.071065]\n",
      "epoch:48 step:37612 [D loss: 0.033535, acc.: 99.22%] [G loss: 0.018259]\n",
      "epoch:48 step:37613 [D loss: 0.034664, acc.: 100.00%] [G loss: 0.030044]\n",
      "epoch:48 step:37614 [D loss: 0.004094, acc.: 100.00%] [G loss: 0.026365]\n",
      "epoch:48 step:37615 [D loss: 0.001393, acc.: 100.00%] [G loss: 0.012381]\n",
      "epoch:48 step:37616 [D loss: 0.002054, acc.: 100.00%] [G loss: 0.028902]\n",
      "epoch:48 step:37617 [D loss: 0.001014, acc.: 100.00%] [G loss: 0.003023]\n",
      "epoch:48 step:37618 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.031740]\n",
      "epoch:48 step:37619 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.003586]\n",
      "epoch:48 step:37620 [D loss: 0.019757, acc.: 100.00%] [G loss: 0.022433]\n",
      "epoch:48 step:37621 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.033629]\n",
      "epoch:48 step:37622 [D loss: 0.010181, acc.: 100.00%] [G loss: 0.018754]\n",
      "epoch:48 step:37623 [D loss: 0.009750, acc.: 100.00%] [G loss: 0.028657]\n",
      "epoch:48 step:37624 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.018534]\n",
      "epoch:48 step:37625 [D loss: 0.005706, acc.: 100.00%] [G loss: 0.005474]\n",
      "epoch:48 step:37626 [D loss: 0.002974, acc.: 100.00%] [G loss: 0.016178]\n",
      "epoch:48 step:37627 [D loss: 0.004747, acc.: 100.00%] [G loss: 0.007954]\n",
      "epoch:48 step:37628 [D loss: 0.071503, acc.: 98.44%] [G loss: 0.005167]\n",
      "epoch:48 step:37629 [D loss: 0.001073, acc.: 100.00%] [G loss: 0.005027]\n",
      "epoch:48 step:37630 [D loss: 0.018362, acc.: 100.00%] [G loss: 0.015848]\n",
      "epoch:48 step:37631 [D loss: 0.032411, acc.: 98.44%] [G loss: 0.036192]\n",
      "epoch:48 step:37632 [D loss: 0.010645, acc.: 100.00%] [G loss: 0.039416]\n",
      "epoch:48 step:37633 [D loss: 0.009692, acc.: 99.22%] [G loss: 0.054148]\n",
      "epoch:48 step:37634 [D loss: 0.020240, acc.: 100.00%] [G loss: 0.013887]\n",
      "epoch:48 step:37635 [D loss: 0.003689, acc.: 100.00%] [G loss: 0.070809]\n",
      "epoch:48 step:37636 [D loss: 0.057526, acc.: 99.22%] [G loss: 0.021066]\n",
      "epoch:48 step:37637 [D loss: 0.001597, acc.: 100.00%] [G loss: 0.769852]\n",
      "epoch:48 step:37638 [D loss: 0.222121, acc.: 89.84%] [G loss: 0.353798]\n",
      "epoch:48 step:37639 [D loss: 0.060397, acc.: 96.88%] [G loss: 0.679590]\n",
      "epoch:48 step:37640 [D loss: 0.023107, acc.: 100.00%] [G loss: 0.206232]\n",
      "epoch:48 step:37641 [D loss: 0.108563, acc.: 96.09%] [G loss: 0.890781]\n",
      "epoch:48 step:37642 [D loss: 0.003380, acc.: 100.00%] [G loss: 0.009258]\n",
      "epoch:48 step:37643 [D loss: 0.009770, acc.: 100.00%] [G loss: 0.022308]\n",
      "epoch:48 step:37644 [D loss: 0.042027, acc.: 99.22%] [G loss: 0.012837]\n",
      "epoch:48 step:37645 [D loss: 0.007240, acc.: 100.00%] [G loss: 0.017677]\n",
      "epoch:48 step:37646 [D loss: 0.042626, acc.: 99.22%] [G loss: 0.008305]\n",
      "epoch:48 step:37647 [D loss: 0.054757, acc.: 99.22%] [G loss: 0.010895]\n",
      "epoch:48 step:37648 [D loss: 0.143883, acc.: 93.75%] [G loss: 0.013136]\n",
      "epoch:48 step:37649 [D loss: 0.348488, acc.: 85.16%] [G loss: 2.934027]\n",
      "epoch:48 step:37650 [D loss: 0.009229, acc.: 100.00%] [G loss: 5.358465]\n",
      "epoch:48 step:37651 [D loss: 0.252191, acc.: 88.28%] [G loss: 4.149143]\n",
      "epoch:48 step:37652 [D loss: 0.033325, acc.: 98.44%] [G loss: 1.903882]\n",
      "epoch:48 step:37653 [D loss: 0.465201, acc.: 79.69%] [G loss: 3.307256]\n",
      "epoch:48 step:37654 [D loss: 0.476534, acc.: 82.03%] [G loss: 0.345849]\n",
      "epoch:48 step:37655 [D loss: 0.115392, acc.: 92.97%] [G loss: 1.254306]\n",
      "epoch:48 step:37656 [D loss: 0.006819, acc.: 100.00%] [G loss: 0.698678]\n",
      "epoch:48 step:37657 [D loss: 0.016441, acc.: 100.00%] [G loss: 0.425320]\n",
      "epoch:48 step:37658 [D loss: 0.003758, acc.: 100.00%] [G loss: 0.020992]\n",
      "epoch:48 step:37659 [D loss: 0.006339, acc.: 100.00%] [G loss: 0.003746]\n",
      "epoch:48 step:37660 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.224470]\n",
      "epoch:48 step:37661 [D loss: 0.006807, acc.: 100.00%] [G loss: 0.296034]\n",
      "epoch:48 step:37662 [D loss: 0.001788, acc.: 100.00%] [G loss: 0.100958]\n",
      "epoch:48 step:37663 [D loss: 0.006147, acc.: 100.00%] [G loss: 0.004466]\n",
      "epoch:48 step:37664 [D loss: 0.030098, acc.: 100.00%] [G loss: 0.013892]\n",
      "epoch:48 step:37665 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.033123]\n",
      "epoch:48 step:37666 [D loss: 0.016026, acc.: 100.00%] [G loss: 0.041089]\n",
      "epoch:48 step:37667 [D loss: 0.023253, acc.: 99.22%] [G loss: 0.001484]\n",
      "epoch:48 step:37668 [D loss: 0.017733, acc.: 99.22%] [G loss: 0.002107]\n",
      "epoch:48 step:37669 [D loss: 0.006402, acc.: 100.00%] [G loss: 0.005256]\n",
      "epoch:48 step:37670 [D loss: 0.006243, acc.: 100.00%] [G loss: 0.002425]\n",
      "epoch:48 step:37671 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.010326]\n",
      "epoch:48 step:37672 [D loss: 0.028169, acc.: 100.00%] [G loss: 0.008413]\n",
      "epoch:48 step:37673 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.066565]\n",
      "epoch:48 step:37674 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.012404]\n",
      "epoch:48 step:37675 [D loss: 0.009318, acc.: 100.00%] [G loss: 0.027970]\n",
      "epoch:48 step:37676 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.607158]\n",
      "epoch:48 step:37677 [D loss: 0.004073, acc.: 100.00%] [G loss: 0.002183]\n",
      "epoch:48 step:37678 [D loss: 0.008254, acc.: 100.00%] [G loss: 0.001678]\n",
      "epoch:48 step:37679 [D loss: 0.022966, acc.: 100.00%] [G loss: 0.003674]\n",
      "epoch:48 step:37680 [D loss: 0.003667, acc.: 100.00%] [G loss: 0.011082]\n",
      "epoch:48 step:37681 [D loss: 0.005440, acc.: 100.00%] [G loss: 0.001479]\n",
      "epoch:48 step:37682 [D loss: 0.004475, acc.: 100.00%] [G loss: 0.006305]\n",
      "epoch:48 step:37683 [D loss: 0.009497, acc.: 100.00%] [G loss: 0.001533]\n",
      "epoch:48 step:37684 [D loss: 0.004170, acc.: 100.00%] [G loss: 0.023274]\n",
      "epoch:48 step:37685 [D loss: 0.562160, acc.: 69.53%] [G loss: 3.907862]\n",
      "epoch:48 step:37686 [D loss: 0.557728, acc.: 75.78%] [G loss: 2.486390]\n",
      "epoch:48 step:37687 [D loss: 0.087638, acc.: 97.66%] [G loss: 0.017759]\n",
      "epoch:48 step:37688 [D loss: 0.079401, acc.: 96.09%] [G loss: 0.776553]\n",
      "epoch:48 step:37689 [D loss: 0.002994, acc.: 100.00%] [G loss: 0.019279]\n",
      "epoch:48 step:37690 [D loss: 0.008643, acc.: 100.00%] [G loss: 0.250808]\n",
      "epoch:48 step:37691 [D loss: 0.006752, acc.: 100.00%] [G loss: 0.216217]\n",
      "epoch:48 step:37692 [D loss: 0.010539, acc.: 100.00%] [G loss: 0.001304]\n",
      "epoch:48 step:37693 [D loss: 0.108619, acc.: 95.31%] [G loss: 0.118180]\n",
      "epoch:48 step:37694 [D loss: 0.020565, acc.: 100.00%] [G loss: 0.270696]\n",
      "epoch:48 step:37695 [D loss: 0.013206, acc.: 100.00%] [G loss: 2.041143]\n",
      "epoch:48 step:37696 [D loss: 0.007971, acc.: 100.00%] [G loss: 0.208670]\n",
      "epoch:48 step:37697 [D loss: 0.114090, acc.: 95.31%] [G loss: 0.033460]\n",
      "epoch:48 step:37698 [D loss: 0.169387, acc.: 90.62%] [G loss: 0.247708]\n",
      "epoch:48 step:37699 [D loss: 0.002565, acc.: 100.00%] [G loss: 1.154698]\n",
      "epoch:48 step:37700 [D loss: 0.015141, acc.: 100.00%] [G loss: 1.303528]\n",
      "epoch:48 step:37701 [D loss: 0.029085, acc.: 99.22%] [G loss: 0.470959]\n",
      "epoch:48 step:37702 [D loss: 0.087073, acc.: 96.09%] [G loss: 0.162248]\n",
      "epoch:48 step:37703 [D loss: 0.264772, acc.: 87.50%] [G loss: 0.802543]\n",
      "epoch:48 step:37704 [D loss: 0.103515, acc.: 96.88%] [G loss: 1.463251]\n",
      "epoch:48 step:37705 [D loss: 0.016541, acc.: 100.00%] [G loss: 0.889064]\n",
      "epoch:48 step:37706 [D loss: 0.152336, acc.: 96.88%] [G loss: 0.769924]\n",
      "epoch:48 step:37707 [D loss: 0.100134, acc.: 96.88%] [G loss: 3.226129]\n",
      "epoch:48 step:37708 [D loss: 0.406692, acc.: 82.03%] [G loss: 2.495142]\n",
      "epoch:48 step:37709 [D loss: 0.035124, acc.: 98.44%] [G loss: 3.643237]\n",
      "epoch:48 step:37710 [D loss: 0.357445, acc.: 84.38%] [G loss: 0.052468]\n",
      "epoch:48 step:37711 [D loss: 0.011175, acc.: 100.00%] [G loss: 1.058469]\n",
      "epoch:48 step:37712 [D loss: 0.021321, acc.: 100.00%] [G loss: 0.445148]\n",
      "epoch:48 step:37713 [D loss: 0.028235, acc.: 100.00%] [G loss: 0.018710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37714 [D loss: 0.019962, acc.: 100.00%] [G loss: 0.409765]\n",
      "epoch:48 step:37715 [D loss: 0.033096, acc.: 99.22%] [G loss: 0.321660]\n",
      "epoch:48 step:37716 [D loss: 0.009477, acc.: 100.00%] [G loss: 0.594616]\n",
      "epoch:48 step:37717 [D loss: 0.051347, acc.: 98.44%] [G loss: 0.058699]\n",
      "epoch:48 step:37718 [D loss: 0.395790, acc.: 78.91%] [G loss: 0.254006]\n",
      "epoch:48 step:37719 [D loss: 0.016629, acc.: 100.00%] [G loss: 1.997505]\n",
      "epoch:48 step:37720 [D loss: 0.039148, acc.: 98.44%] [G loss: 0.519760]\n",
      "epoch:48 step:37721 [D loss: 0.014005, acc.: 100.00%] [G loss: 0.268916]\n",
      "epoch:48 step:37722 [D loss: 0.046087, acc.: 99.22%] [G loss: 0.367634]\n",
      "epoch:48 step:37723 [D loss: 0.075382, acc.: 99.22%] [G loss: 0.257750]\n",
      "epoch:48 step:37724 [D loss: 1.205013, acc.: 57.81%] [G loss: 2.861434]\n",
      "epoch:48 step:37725 [D loss: 0.256362, acc.: 89.84%] [G loss: 5.719044]\n",
      "epoch:48 step:37726 [D loss: 0.395468, acc.: 80.47%] [G loss: 3.078401]\n",
      "epoch:48 step:37727 [D loss: 0.078725, acc.: 97.66%] [G loss: 1.621256]\n",
      "epoch:48 step:37728 [D loss: 0.370867, acc.: 82.81%] [G loss: 2.658551]\n",
      "epoch:48 step:37729 [D loss: 0.045874, acc.: 99.22%] [G loss: 2.501603]\n",
      "epoch:48 step:37730 [D loss: 0.128563, acc.: 93.75%] [G loss: 1.601672]\n",
      "epoch:48 step:37731 [D loss: 0.066879, acc.: 99.22%] [G loss: 3.383182]\n",
      "epoch:48 step:37732 [D loss: 0.082738, acc.: 98.44%] [G loss: 2.302351]\n",
      "epoch:48 step:37733 [D loss: 0.118216, acc.: 96.09%] [G loss: 3.304075]\n",
      "epoch:48 step:37734 [D loss: 0.087514, acc.: 99.22%] [G loss: 2.340508]\n",
      "epoch:48 step:37735 [D loss: 0.341246, acc.: 80.47%] [G loss: 0.416074]\n",
      "epoch:48 step:37736 [D loss: 0.039022, acc.: 97.66%] [G loss: 2.607358]\n",
      "epoch:48 step:37737 [D loss: 0.094571, acc.: 96.09%] [G loss: 0.003890]\n",
      "epoch:48 step:37738 [D loss: 0.021513, acc.: 100.00%] [G loss: 0.764658]\n",
      "epoch:48 step:37739 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.488226]\n",
      "epoch:48 step:37740 [D loss: 0.005004, acc.: 100.00%] [G loss: 0.008704]\n",
      "epoch:48 step:37741 [D loss: 0.004383, acc.: 100.00%] [G loss: 0.181802]\n",
      "epoch:48 step:37742 [D loss: 0.005653, acc.: 100.00%] [G loss: 0.004213]\n",
      "epoch:48 step:37743 [D loss: 0.007710, acc.: 100.00%] [G loss: 0.030554]\n",
      "epoch:48 step:37744 [D loss: 0.003213, acc.: 100.00%] [G loss: 0.001543]\n",
      "epoch:48 step:37745 [D loss: 0.005504, acc.: 100.00%] [G loss: 0.031284]\n",
      "epoch:48 step:37746 [D loss: 0.003125, acc.: 100.00%] [G loss: 0.216665]\n",
      "epoch:48 step:37747 [D loss: 0.008250, acc.: 100.00%] [G loss: 0.049668]\n",
      "epoch:48 step:37748 [D loss: 0.006437, acc.: 100.00%] [G loss: 0.005318]\n",
      "epoch:48 step:37749 [D loss: 0.030617, acc.: 99.22%] [G loss: 0.012774]\n",
      "epoch:48 step:37750 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.015711]\n",
      "epoch:48 step:37751 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.030598]\n",
      "epoch:48 step:37752 [D loss: 0.000835, acc.: 100.00%] [G loss: 0.086922]\n",
      "epoch:48 step:37753 [D loss: 0.008042, acc.: 100.00%] [G loss: 0.044574]\n",
      "epoch:48 step:37754 [D loss: 0.003510, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:48 step:37755 [D loss: 0.018141, acc.: 100.00%] [G loss: 0.020908]\n",
      "epoch:48 step:37756 [D loss: 0.003336, acc.: 100.00%] [G loss: 0.002975]\n",
      "epoch:48 step:37757 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.005939]\n",
      "epoch:48 step:37758 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.009100]\n",
      "epoch:48 step:37759 [D loss: 0.001440, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:48 step:37760 [D loss: 0.001560, acc.: 100.00%] [G loss: 0.002831]\n",
      "epoch:48 step:37761 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000846]\n",
      "epoch:48 step:37762 [D loss: 0.001687, acc.: 100.00%] [G loss: 0.064731]\n",
      "epoch:48 step:37763 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:48 step:37764 [D loss: 0.001225, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:48 step:37765 [D loss: 0.009036, acc.: 100.00%] [G loss: 0.003783]\n",
      "epoch:48 step:37766 [D loss: 0.005234, acc.: 100.00%] [G loss: 0.004883]\n",
      "epoch:48 step:37767 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.005770]\n",
      "epoch:48 step:37768 [D loss: 0.003454, acc.: 100.00%] [G loss: 0.000786]\n",
      "epoch:48 step:37769 [D loss: 0.001233, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:48 step:37770 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.000736]\n",
      "epoch:48 step:37771 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.090275]\n",
      "epoch:48 step:37772 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000638]\n",
      "epoch:48 step:37773 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.001825]\n",
      "epoch:48 step:37774 [D loss: 0.007214, acc.: 99.22%] [G loss: 0.001223]\n",
      "epoch:48 step:37775 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.011300]\n",
      "epoch:48 step:37776 [D loss: 0.003809, acc.: 100.00%] [G loss: 0.002394]\n",
      "epoch:48 step:37777 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.004913]\n",
      "epoch:48 step:37778 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.002450]\n",
      "epoch:48 step:37779 [D loss: 0.013336, acc.: 100.00%] [G loss: 0.000770]\n",
      "epoch:48 step:37780 [D loss: 0.003135, acc.: 100.00%] [G loss: 0.001863]\n",
      "epoch:48 step:37781 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.003899]\n",
      "epoch:48 step:37782 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.004347]\n",
      "epoch:48 step:37783 [D loss: 0.007181, acc.: 100.00%] [G loss: 0.004455]\n",
      "epoch:48 step:37784 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.002355]\n",
      "epoch:48 step:37785 [D loss: 0.001188, acc.: 100.00%] [G loss: 0.001349]\n",
      "epoch:48 step:37786 [D loss: 0.005240, acc.: 100.00%] [G loss: 0.005179]\n",
      "epoch:48 step:37787 [D loss: 0.015242, acc.: 100.00%] [G loss: 0.001804]\n",
      "epoch:48 step:37788 [D loss: 0.002973, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:48 step:37789 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.005342]\n",
      "epoch:48 step:37790 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.003758]\n",
      "epoch:48 step:37791 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.012354]\n",
      "epoch:48 step:37792 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.020330]\n",
      "epoch:48 step:37793 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.002179]\n",
      "epoch:48 step:37794 [D loss: 0.001850, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:48 step:37795 [D loss: 0.057724, acc.: 98.44%] [G loss: 0.004822]\n",
      "epoch:48 step:37796 [D loss: 0.000922, acc.: 100.00%] [G loss: 0.059088]\n",
      "epoch:48 step:37797 [D loss: 0.001852, acc.: 100.00%] [G loss: 0.064102]\n",
      "epoch:48 step:37798 [D loss: 0.001838, acc.: 100.00%] [G loss: 0.025460]\n",
      "epoch:48 step:37799 [D loss: 0.004773, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:48 step:37800 [D loss: 0.011146, acc.: 100.00%] [G loss: 0.007751]\n",
      "epoch:48 step:37801 [D loss: 0.019377, acc.: 100.00%] [G loss: 0.426077]\n",
      "epoch:48 step:37802 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.057449]\n",
      "epoch:48 step:37803 [D loss: 0.084540, acc.: 96.88%] [G loss: 0.002442]\n",
      "epoch:48 step:37804 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:48 step:37805 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:48 step:37806 [D loss: 0.004366, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:48 step:37807 [D loss: 0.069655, acc.: 97.66%] [G loss: 0.001831]\n",
      "epoch:48 step:37808 [D loss: 0.008298, acc.: 100.00%] [G loss: 0.008080]\n",
      "epoch:48 step:37809 [D loss: 0.000955, acc.: 100.00%] [G loss: 0.181617]\n",
      "epoch:48 step:37810 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.152488]\n",
      "epoch:48 step:37811 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.005428]\n",
      "epoch:48 step:37812 [D loss: 0.057033, acc.: 99.22%] [G loss: 0.025845]\n",
      "epoch:48 step:37813 [D loss: 0.004633, acc.: 100.00%] [G loss: 0.106801]\n",
      "epoch:48 step:37814 [D loss: 0.015109, acc.: 100.00%] [G loss: 0.139379]\n",
      "epoch:48 step:37815 [D loss: 0.018981, acc.: 99.22%] [G loss: 0.014680]\n",
      "epoch:48 step:37816 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.056987]\n",
      "epoch:48 step:37817 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.005589]\n",
      "epoch:48 step:37818 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.026312]\n",
      "epoch:48 step:37819 [D loss: 0.008924, acc.: 100.00%] [G loss: 0.012774]\n",
      "epoch:48 step:37820 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.002081]\n",
      "epoch:48 step:37821 [D loss: 0.003545, acc.: 100.00%] [G loss: 0.005022]\n",
      "epoch:48 step:37822 [D loss: 0.001733, acc.: 100.00%] [G loss: 0.056944]\n",
      "epoch:48 step:37823 [D loss: 0.001685, acc.: 100.00%] [G loss: 0.019367]\n",
      "epoch:48 step:37824 [D loss: 0.004846, acc.: 100.00%] [G loss: 0.046781]\n",
      "epoch:48 step:37825 [D loss: 0.006872, acc.: 100.00%] [G loss: 0.006924]\n",
      "epoch:48 step:37826 [D loss: 0.043569, acc.: 96.88%] [G loss: 0.000191]\n",
      "epoch:48 step:37827 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.004489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37828 [D loss: 0.002548, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:48 step:37829 [D loss: 0.002907, acc.: 100.00%] [G loss: 0.000900]\n",
      "epoch:48 step:37830 [D loss: 0.001748, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:48 step:37831 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.003299]\n",
      "epoch:48 step:37832 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:48 step:37833 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:48 step:37834 [D loss: 0.033917, acc.: 99.22%] [G loss: 0.000809]\n",
      "epoch:48 step:37835 [D loss: 0.003864, acc.: 100.00%] [G loss: 0.021260]\n",
      "epoch:48 step:37836 [D loss: 0.008723, acc.: 99.22%] [G loss: 0.013670]\n",
      "epoch:48 step:37837 [D loss: 0.001704, acc.: 100.00%] [G loss: 0.012605]\n",
      "epoch:48 step:37838 [D loss: 0.013311, acc.: 100.00%] [G loss: 0.001932]\n",
      "epoch:48 step:37839 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.001580]\n",
      "epoch:48 step:37840 [D loss: 0.000892, acc.: 100.00%] [G loss: 0.003791]\n",
      "epoch:48 step:37841 [D loss: 0.016996, acc.: 100.00%] [G loss: 0.035887]\n",
      "epoch:48 step:37842 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:48 step:37843 [D loss: 0.037210, acc.: 99.22%] [G loss: 0.001873]\n",
      "epoch:48 step:37844 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.005177]\n",
      "epoch:48 step:37845 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.037453]\n",
      "epoch:48 step:37846 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.006371]\n",
      "epoch:48 step:37847 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.014371]\n",
      "epoch:48 step:37848 [D loss: 0.001165, acc.: 100.00%] [G loss: 0.026575]\n",
      "epoch:48 step:37849 [D loss: 0.014152, acc.: 99.22%] [G loss: 0.004033]\n",
      "epoch:48 step:37850 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.002493]\n",
      "epoch:48 step:37851 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:48 step:37852 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.001623]\n",
      "epoch:48 step:37853 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:48 step:37854 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000840]\n",
      "epoch:48 step:37855 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.002256]\n",
      "epoch:48 step:37856 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:48 step:37857 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:48 step:37858 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.003576]\n",
      "epoch:48 step:37859 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.006037]\n",
      "epoch:48 step:37860 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.003313]\n",
      "epoch:48 step:37861 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000634]\n",
      "epoch:48 step:37862 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.001449]\n",
      "epoch:48 step:37863 [D loss: 0.001212, acc.: 100.00%] [G loss: 0.071095]\n",
      "epoch:48 step:37864 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:48 step:37865 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:48 step:37866 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:48 step:37867 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.001777]\n",
      "epoch:48 step:37868 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.083820]\n",
      "epoch:48 step:37869 [D loss: 0.009660, acc.: 100.00%] [G loss: 0.002003]\n",
      "epoch:48 step:37870 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000911]\n",
      "epoch:48 step:37871 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.072941]\n",
      "epoch:48 step:37872 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001904]\n",
      "epoch:48 step:37873 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.001252]\n",
      "epoch:48 step:37874 [D loss: 0.003652, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:48 step:37875 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.003484]\n",
      "epoch:48 step:37876 [D loss: 0.009255, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:48 step:37877 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002792]\n",
      "epoch:48 step:37878 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000857]\n",
      "epoch:48 step:37879 [D loss: 0.003369, acc.: 100.00%] [G loss: 0.000469]\n",
      "epoch:48 step:37880 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:48 step:37881 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000643]\n",
      "epoch:48 step:37882 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:48 step:37883 [D loss: 0.002594, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:48 step:37884 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:48 step:37885 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:48 step:37886 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.001214]\n",
      "epoch:48 step:37887 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:48 step:37888 [D loss: 0.147217, acc.: 93.75%] [G loss: 0.266609]\n",
      "epoch:48 step:37889 [D loss: 0.012529, acc.: 100.00%] [G loss: 1.127662]\n",
      "epoch:48 step:37890 [D loss: 0.086363, acc.: 96.09%] [G loss: 0.284252]\n",
      "epoch:48 step:37891 [D loss: 0.049240, acc.: 98.44%] [G loss: 0.476569]\n",
      "epoch:48 step:37892 [D loss: 0.002547, acc.: 100.00%] [G loss: 0.323499]\n",
      "epoch:48 step:37893 [D loss: 0.039893, acc.: 99.22%] [G loss: 0.063500]\n",
      "epoch:48 step:37894 [D loss: 0.492223, acc.: 76.56%] [G loss: 5.157816]\n",
      "epoch:48 step:37895 [D loss: 0.102670, acc.: 95.31%] [G loss: 6.562707]\n",
      "epoch:48 step:37896 [D loss: 0.463558, acc.: 77.34%] [G loss: 2.840933]\n",
      "epoch:48 step:37897 [D loss: 0.066807, acc.: 98.44%] [G loss: 1.746014]\n",
      "epoch:48 step:37898 [D loss: 0.331823, acc.: 84.38%] [G loss: 4.618730]\n",
      "epoch:48 step:37899 [D loss: 2.020449, acc.: 46.88%] [G loss: 3.962254]\n",
      "epoch:48 step:37900 [D loss: 0.030279, acc.: 100.00%] [G loss: 5.719104]\n",
      "epoch:48 step:37901 [D loss: 0.024589, acc.: 100.00%] [G loss: 6.469588]\n",
      "epoch:48 step:37902 [D loss: 0.012909, acc.: 100.00%] [G loss: 0.012466]\n",
      "epoch:48 step:37903 [D loss: 0.081248, acc.: 98.44%] [G loss: 2.264679]\n",
      "epoch:48 step:37904 [D loss: 0.096266, acc.: 96.09%] [G loss: 4.301448]\n",
      "epoch:48 step:37905 [D loss: 0.101310, acc.: 96.88%] [G loss: 5.487026]\n",
      "epoch:48 step:37906 [D loss: 0.008395, acc.: 100.00%] [G loss: 4.515274]\n",
      "epoch:48 step:37907 [D loss: 0.024986, acc.: 100.00%] [G loss: 2.750731]\n",
      "epoch:48 step:37908 [D loss: 0.030761, acc.: 100.00%] [G loss: 2.271110]\n",
      "epoch:48 step:37909 [D loss: 0.034144, acc.: 100.00%] [G loss: 2.343168]\n",
      "epoch:48 step:37910 [D loss: 0.015978, acc.: 100.00%] [G loss: 1.873472]\n",
      "epoch:48 step:37911 [D loss: 0.236850, acc.: 86.72%] [G loss: 3.997209]\n",
      "epoch:48 step:37912 [D loss: 0.090848, acc.: 95.31%] [G loss: 3.239815]\n",
      "epoch:48 step:37913 [D loss: 0.042138, acc.: 99.22%] [G loss: 1.654384]\n",
      "epoch:48 step:37914 [D loss: 0.054647, acc.: 99.22%] [G loss: 1.368303]\n",
      "epoch:48 step:37915 [D loss: 0.262166, acc.: 88.28%] [G loss: 3.245265]\n",
      "epoch:48 step:37916 [D loss: 0.641107, acc.: 72.66%] [G loss: 1.643232]\n",
      "epoch:48 step:37917 [D loss: 0.075265, acc.: 97.66%] [G loss: 1.848850]\n",
      "epoch:48 step:37918 [D loss: 0.158781, acc.: 90.62%] [G loss: 0.505024]\n",
      "epoch:48 step:37919 [D loss: 0.102685, acc.: 97.66%] [G loss: 0.202376]\n",
      "epoch:48 step:37920 [D loss: 0.031799, acc.: 99.22%] [G loss: 0.049376]\n",
      "epoch:48 step:37921 [D loss: 0.054911, acc.: 100.00%] [G loss: 2.260872]\n",
      "epoch:48 step:37922 [D loss: 0.019839, acc.: 100.00%] [G loss: 0.925628]\n",
      "epoch:48 step:37923 [D loss: 0.024106, acc.: 99.22%] [G loss: 0.959492]\n",
      "epoch:48 step:37924 [D loss: 0.066340, acc.: 97.66%] [G loss: 1.984141]\n",
      "epoch:48 step:37925 [D loss: 0.066997, acc.: 98.44%] [G loss: 2.045866]\n",
      "epoch:48 step:37926 [D loss: 0.046369, acc.: 99.22%] [G loss: 3.140839]\n",
      "epoch:48 step:37927 [D loss: 0.126396, acc.: 93.75%] [G loss: 4.334573]\n",
      "epoch:48 step:37928 [D loss: 0.038967, acc.: 98.44%] [G loss: 5.131294]\n",
      "epoch:48 step:37929 [D loss: 0.068078, acc.: 98.44%] [G loss: 0.813950]\n",
      "epoch:48 step:37930 [D loss: 0.007065, acc.: 100.00%] [G loss: 0.302409]\n",
      "epoch:48 step:37931 [D loss: 0.008193, acc.: 100.00%] [G loss: 0.433169]\n",
      "epoch:48 step:37932 [D loss: 0.265395, acc.: 85.16%] [G loss: 1.760528]\n",
      "epoch:48 step:37933 [D loss: 0.303658, acc.: 89.06%] [G loss: 1.671194]\n",
      "epoch:48 step:37934 [D loss: 0.009562, acc.: 100.00%] [G loss: 0.689375]\n",
      "epoch:48 step:37935 [D loss: 0.001209, acc.: 100.00%] [G loss: 0.175401]\n",
      "epoch:48 step:37936 [D loss: 0.008399, acc.: 100.00%] [G loss: 0.110144]\n",
      "epoch:48 step:37937 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.110543]\n",
      "epoch:48 step:37938 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.039435]\n",
      "epoch:48 step:37939 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.049712]\n",
      "epoch:48 step:37940 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.015665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37941 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.003138]\n",
      "epoch:48 step:37942 [D loss: 0.000905, acc.: 100.00%] [G loss: 0.009958]\n",
      "epoch:48 step:37943 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.003603]\n",
      "epoch:48 step:37944 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.460100]\n",
      "epoch:48 step:37945 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.004518]\n",
      "epoch:48 step:37946 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.076499]\n",
      "epoch:48 step:37947 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.005570]\n",
      "epoch:48 step:37948 [D loss: 0.001577, acc.: 100.00%] [G loss: 0.014175]\n",
      "epoch:48 step:37949 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.004188]\n",
      "epoch:48 step:37950 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.004646]\n",
      "epoch:48 step:37951 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.006630]\n",
      "epoch:48 step:37952 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.001893]\n",
      "epoch:48 step:37953 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000684]\n",
      "epoch:48 step:37954 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.075900]\n",
      "epoch:48 step:37955 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.002028]\n",
      "epoch:48 step:37956 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.006843]\n",
      "epoch:48 step:37957 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.001404]\n",
      "epoch:48 step:37958 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.008160]\n",
      "epoch:48 step:37959 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.002130]\n",
      "epoch:48 step:37960 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.004610]\n",
      "epoch:48 step:37961 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:48 step:37962 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.000876]\n",
      "epoch:48 step:37963 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:48 step:37964 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.048975]\n",
      "epoch:48 step:37965 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.000599]\n",
      "epoch:48 step:37966 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:48 step:37967 [D loss: 0.009951, acc.: 100.00%] [G loss: 0.003551]\n",
      "epoch:48 step:37968 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.008177]\n",
      "epoch:48 step:37969 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.005330]\n",
      "epoch:48 step:37970 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.003335]\n",
      "epoch:48 step:37971 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.059139]\n",
      "epoch:48 step:37972 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.007055]\n",
      "epoch:48 step:37973 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.004318]\n",
      "epoch:48 step:37974 [D loss: 0.003990, acc.: 100.00%] [G loss: 0.001289]\n",
      "epoch:48 step:37975 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:48 step:37976 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.027808]\n",
      "epoch:48 step:37977 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.001283]\n",
      "epoch:48 step:37978 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:48 step:37979 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.002355]\n",
      "epoch:48 step:37980 [D loss: 0.000766, acc.: 100.00%] [G loss: 0.003363]\n",
      "epoch:48 step:37981 [D loss: 0.001206, acc.: 100.00%] [G loss: 0.002211]\n",
      "epoch:48 step:37982 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.004025]\n",
      "epoch:48 step:37983 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.042262]\n",
      "epoch:48 step:37984 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.011421]\n",
      "epoch:48 step:37985 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.008318]\n",
      "epoch:48 step:37986 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.058391]\n",
      "epoch:48 step:37987 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.001753]\n",
      "epoch:48 step:37988 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.008285]\n",
      "epoch:48 step:37989 [D loss: 0.001732, acc.: 100.00%] [G loss: 0.040275]\n",
      "epoch:48 step:37990 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:48 step:37991 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.005031]\n",
      "epoch:48 step:37992 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.003856]\n",
      "epoch:48 step:37993 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.000477]\n",
      "epoch:48 step:37994 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:48 step:37995 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.001120]\n",
      "epoch:48 step:37996 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001064]\n",
      "epoch:48 step:37997 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:48 step:37998 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.015265]\n",
      "epoch:48 step:37999 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:48 step:38000 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000557]\n",
      "epoch:48 step:38001 [D loss: 0.002219, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:48 step:38002 [D loss: 0.001215, acc.: 100.00%] [G loss: 0.034584]\n",
      "epoch:48 step:38003 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.005682]\n",
      "epoch:48 step:38004 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.000978]\n",
      "epoch:48 step:38005 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.001933]\n",
      "epoch:48 step:38006 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000816]\n",
      "epoch:48 step:38007 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.005244]\n",
      "epoch:48 step:38008 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.008234]\n",
      "epoch:48 step:38009 [D loss: 0.001174, acc.: 100.00%] [G loss: 0.024503]\n",
      "epoch:48 step:38010 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.001651]\n",
      "epoch:48 step:38011 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000922]\n",
      "epoch:48 step:38012 [D loss: 0.005408, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:48 step:38013 [D loss: 0.006547, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:48 step:38014 [D loss: 0.039784, acc.: 99.22%] [G loss: 0.000819]\n",
      "epoch:48 step:38015 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.002030]\n",
      "epoch:48 step:38016 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.001510]\n",
      "epoch:48 step:38017 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.004116]\n",
      "epoch:48 step:38018 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.006562]\n",
      "epoch:48 step:38019 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:48 step:38020 [D loss: 0.023978, acc.: 100.00%] [G loss: 0.011701]\n",
      "epoch:48 step:38021 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.005765]\n",
      "epoch:48 step:38022 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.001286]\n",
      "epoch:48 step:38023 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.005308]\n",
      "epoch:48 step:38024 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001905]\n",
      "epoch:48 step:38025 [D loss: 0.050415, acc.: 97.66%] [G loss: 0.000107]\n",
      "epoch:48 step:38026 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.001000]\n",
      "epoch:48 step:38027 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:48 step:38028 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:48 step:38029 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:48 step:38030 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000713]\n",
      "epoch:48 step:38031 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:48 step:38032 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:48 step:38033 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:48 step:38034 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000982]\n",
      "epoch:48 step:38035 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:48 step:38036 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:48 step:38037 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:48 step:38038 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:48 step:38039 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000866]\n",
      "epoch:48 step:38040 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000732]\n",
      "epoch:48 step:38041 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.008996]\n",
      "epoch:48 step:38042 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:48 step:38043 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001241]\n",
      "epoch:48 step:38044 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:48 step:38045 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:48 step:38046 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000750]\n",
      "epoch:48 step:38047 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:48 step:38048 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:48 step:38049 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:48 step:38050 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:48 step:38051 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:48 step:38052 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:48 step:38053 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:48 step:38054 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:48 step:38055 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38056 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.010104]\n",
      "epoch:48 step:38057 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.002453]\n",
      "epoch:48 step:38058 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:48 step:38059 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:48 step:38060 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:48 step:38061 [D loss: 0.001775, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:48 step:38062 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:48 step:38063 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000975]\n",
      "epoch:48 step:38064 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:48 step:38065 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002510]\n",
      "epoch:48 step:38066 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:48 step:38067 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:48 step:38068 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:48 step:38069 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:48 step:38070 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:48 step:38071 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.021343]\n",
      "epoch:48 step:38072 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:48 step:38073 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:48 step:38074 [D loss: 0.007902, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:48 step:38075 [D loss: 0.017357, acc.: 100.00%] [G loss: 0.000886]\n",
      "epoch:48 step:38076 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.001093]\n",
      "epoch:48 step:38077 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:48 step:38078 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:48 step:38079 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.004372]\n",
      "epoch:48 step:38080 [D loss: 0.367003, acc.: 77.34%] [G loss: 7.434668]\n",
      "epoch:48 step:38081 [D loss: 0.175216, acc.: 92.97%] [G loss: 6.042177]\n",
      "epoch:48 step:38082 [D loss: 0.329744, acc.: 84.38%] [G loss: 1.346651]\n",
      "epoch:48 step:38083 [D loss: 0.011485, acc.: 100.00%] [G loss: 0.660142]\n",
      "epoch:48 step:38084 [D loss: 0.012214, acc.: 100.00%] [G loss: 0.473876]\n",
      "epoch:48 step:38085 [D loss: 0.336249, acc.: 82.81%] [G loss: 6.435996]\n",
      "epoch:48 step:38086 [D loss: 0.059236, acc.: 96.09%] [G loss: 7.201195]\n",
      "epoch:48 step:38087 [D loss: 0.640178, acc.: 74.22%] [G loss: 2.048371]\n",
      "epoch:48 step:38088 [D loss: 0.062089, acc.: 96.09%] [G loss: 1.813717]\n",
      "epoch:48 step:38089 [D loss: 0.016733, acc.: 100.00%] [G loss: 1.430678]\n",
      "epoch:48 step:38090 [D loss: 0.209406, acc.: 89.06%] [G loss: 0.147704]\n",
      "epoch:48 step:38091 [D loss: 0.015697, acc.: 99.22%] [G loss: 7.500295]\n",
      "epoch:48 step:38092 [D loss: 0.240575, acc.: 88.28%] [G loss: 0.058293]\n",
      "epoch:48 step:38093 [D loss: 0.010552, acc.: 100.00%] [G loss: 0.001533]\n",
      "epoch:48 step:38094 [D loss: 0.021297, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:48 step:38095 [D loss: 0.009483, acc.: 100.00%] [G loss: 5.612111]\n",
      "epoch:48 step:38096 [D loss: 0.001393, acc.: 100.00%] [G loss: 4.163676]\n",
      "epoch:48 step:38097 [D loss: 0.018738, acc.: 100.00%] [G loss: 1.451999]\n",
      "epoch:48 step:38098 [D loss: 0.001925, acc.: 100.00%] [G loss: 4.266950]\n",
      "epoch:48 step:38099 [D loss: 0.151427, acc.: 92.19%] [G loss: 4.244212]\n",
      "epoch:48 step:38100 [D loss: 0.005809, acc.: 100.00%] [G loss: 0.204629]\n",
      "epoch:48 step:38101 [D loss: 2.615193, acc.: 21.09%] [G loss: 1.838032]\n",
      "epoch:48 step:38102 [D loss: 0.005729, acc.: 100.00%] [G loss: 6.595675]\n",
      "epoch:48 step:38103 [D loss: 0.297498, acc.: 85.94%] [G loss: 5.127203]\n",
      "epoch:48 step:38104 [D loss: 0.082117, acc.: 96.88%] [G loss: 2.521902]\n",
      "epoch:48 step:38105 [D loss: 0.082653, acc.: 96.88%] [G loss: 2.606025]\n",
      "epoch:48 step:38106 [D loss: 0.005703, acc.: 100.00%] [G loss: 1.089511]\n",
      "epoch:48 step:38107 [D loss: 0.122131, acc.: 96.09%] [G loss: 1.577004]\n",
      "epoch:48 step:38108 [D loss: 0.008514, acc.: 100.00%] [G loss: 0.425141]\n",
      "epoch:48 step:38109 [D loss: 0.038844, acc.: 100.00%] [G loss: 0.662475]\n",
      "epoch:48 step:38110 [D loss: 0.194833, acc.: 91.41%] [G loss: 1.910820]\n",
      "epoch:48 step:38111 [D loss: 0.657061, acc.: 73.44%] [G loss: 0.307436]\n",
      "epoch:48 step:38112 [D loss: 0.132526, acc.: 98.44%] [G loss: 0.058493]\n",
      "epoch:48 step:38113 [D loss: 0.032645, acc.: 99.22%] [G loss: 0.127685]\n",
      "epoch:48 step:38114 [D loss: 0.015097, acc.: 100.00%] [G loss: 0.013927]\n",
      "epoch:48 step:38115 [D loss: 0.005834, acc.: 100.00%] [G loss: 0.044425]\n",
      "epoch:48 step:38116 [D loss: 0.111946, acc.: 93.75%] [G loss: 0.008809]\n",
      "epoch:48 step:38117 [D loss: 0.002479, acc.: 100.00%] [G loss: 0.566231]\n",
      "epoch:48 step:38118 [D loss: 0.014317, acc.: 100.00%] [G loss: 0.152914]\n",
      "epoch:48 step:38119 [D loss: 0.110181, acc.: 97.66%] [G loss: 0.043585]\n",
      "epoch:48 step:38120 [D loss: 0.008185, acc.: 100.00%] [G loss: 0.063535]\n",
      "epoch:48 step:38121 [D loss: 0.011252, acc.: 100.00%] [G loss: 0.006692]\n",
      "epoch:48 step:38122 [D loss: 0.002982, acc.: 100.00%] [G loss: 0.003099]\n",
      "epoch:48 step:38123 [D loss: 0.017429, acc.: 100.00%] [G loss: 0.613130]\n",
      "epoch:48 step:38124 [D loss: 0.046478, acc.: 98.44%] [G loss: 0.153766]\n",
      "epoch:48 step:38125 [D loss: 0.088274, acc.: 97.66%] [G loss: 0.736449]\n",
      "epoch:48 step:38126 [D loss: 0.077333, acc.: 98.44%] [G loss: 0.054243]\n",
      "epoch:48 step:38127 [D loss: 0.014565, acc.: 100.00%] [G loss: 0.780690]\n",
      "epoch:48 step:38128 [D loss: 0.004410, acc.: 100.00%] [G loss: 0.359282]\n",
      "epoch:48 step:38129 [D loss: 0.018797, acc.: 100.00%] [G loss: 0.021254]\n",
      "epoch:48 step:38130 [D loss: 0.005516, acc.: 100.00%] [G loss: 0.128566]\n",
      "epoch:48 step:38131 [D loss: 0.001851, acc.: 100.00%] [G loss: 0.103447]\n",
      "epoch:48 step:38132 [D loss: 0.098607, acc.: 97.66%] [G loss: 0.100066]\n",
      "epoch:48 step:38133 [D loss: 0.017427, acc.: 100.00%] [G loss: 0.928509]\n",
      "epoch:48 step:38134 [D loss: 0.499368, acc.: 75.78%] [G loss: 0.001205]\n",
      "epoch:48 step:38135 [D loss: 0.044611, acc.: 100.00%] [G loss: 0.019841]\n",
      "epoch:48 step:38136 [D loss: 0.019739, acc.: 99.22%] [G loss: 0.010601]\n",
      "epoch:48 step:38137 [D loss: 0.002758, acc.: 100.00%] [G loss: 0.004299]\n",
      "epoch:48 step:38138 [D loss: 0.002652, acc.: 100.00%] [G loss: 0.144139]\n",
      "epoch:48 step:38139 [D loss: 0.002942, acc.: 100.00%] [G loss: 0.009825]\n",
      "epoch:48 step:38140 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.002645]\n",
      "epoch:48 step:38141 [D loss: 0.000665, acc.: 100.00%] [G loss: 0.084323]\n",
      "epoch:48 step:38142 [D loss: 0.002618, acc.: 100.00%] [G loss: 0.002987]\n",
      "epoch:48 step:38143 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.036504]\n",
      "epoch:48 step:38144 [D loss: 0.002891, acc.: 100.00%] [G loss: 0.022023]\n",
      "epoch:48 step:38145 [D loss: 0.005677, acc.: 100.00%] [G loss: 0.009832]\n",
      "epoch:48 step:38146 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.014985]\n",
      "epoch:48 step:38147 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.011993]\n",
      "epoch:48 step:38148 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.001907]\n",
      "epoch:48 step:38149 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.004365]\n",
      "epoch:48 step:38150 [D loss: 0.001518, acc.: 100.00%] [G loss: 0.008269]\n",
      "epoch:48 step:38151 [D loss: 0.012289, acc.: 100.00%] [G loss: 0.007347]\n",
      "epoch:48 step:38152 [D loss: 0.001246, acc.: 100.00%] [G loss: 0.087490]\n",
      "epoch:48 step:38153 [D loss: 0.027799, acc.: 99.22%] [G loss: 0.006698]\n",
      "epoch:48 step:38154 [D loss: 0.558586, acc.: 70.31%] [G loss: 2.377573]\n",
      "epoch:48 step:38155 [D loss: 0.227226, acc.: 87.50%] [G loss: 1.750190]\n",
      "epoch:48 step:38156 [D loss: 0.312532, acc.: 85.94%] [G loss: 0.001891]\n",
      "epoch:48 step:38157 [D loss: 0.015885, acc.: 100.00%] [G loss: 1.598105]\n",
      "epoch:48 step:38158 [D loss: 0.026179, acc.: 99.22%] [G loss: 0.590011]\n",
      "epoch:48 step:38159 [D loss: 0.040123, acc.: 100.00%] [G loss: 0.000392]\n",
      "epoch:48 step:38160 [D loss: 0.255017, acc.: 85.94%] [G loss: 0.306661]\n",
      "epoch:48 step:38161 [D loss: 0.140570, acc.: 92.97%] [G loss: 0.027897]\n",
      "epoch:48 step:38162 [D loss: 0.092025, acc.: 96.88%] [G loss: 0.007173]\n",
      "epoch:48 step:38163 [D loss: 0.013030, acc.: 100.00%] [G loss: 0.002470]\n",
      "epoch:48 step:38164 [D loss: 0.171758, acc.: 91.41%] [G loss: 1.190034]\n",
      "epoch:48 step:38165 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:48 step:38166 [D loss: 0.001229, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:48 step:38167 [D loss: 0.044799, acc.: 98.44%] [G loss: 0.000002]\n",
      "epoch:48 step:38168 [D loss: 0.001368, acc.: 100.00%] [G loss: 0.523592]\n",
      "epoch:48 step:38169 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38170 [D loss: 0.001723, acc.: 100.00%] [G loss: 0.752735]\n",
      "epoch:48 step:38171 [D loss: 0.003320, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:48 step:38172 [D loss: 0.004574, acc.: 100.00%] [G loss: 0.021838]\n",
      "epoch:48 step:38173 [D loss: 0.002124, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:48 step:38174 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:48 step:38175 [D loss: 0.000905, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:48 step:38176 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:48 step:38177 [D loss: 0.002129, acc.: 100.00%] [G loss: 0.020288]\n",
      "epoch:48 step:38178 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:48 step:38179 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.010509]\n",
      "epoch:48 step:38180 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.140528]\n",
      "epoch:48 step:38181 [D loss: 0.119335, acc.: 96.09%] [G loss: 0.000433]\n",
      "epoch:48 step:38182 [D loss: 0.000322, acc.: 100.00%] [G loss: 2.931651]\n",
      "epoch:48 step:38183 [D loss: 0.003155, acc.: 100.00%] [G loss: 0.091078]\n",
      "epoch:48 step:38184 [D loss: 0.101242, acc.: 95.31%] [G loss: 1.606478]\n",
      "epoch:48 step:38185 [D loss: 0.003654, acc.: 100.00%] [G loss: 0.000701]\n",
      "epoch:48 step:38186 [D loss: 0.001772, acc.: 100.00%] [G loss: 0.219423]\n",
      "epoch:48 step:38187 [D loss: 0.001428, acc.: 100.00%] [G loss: 0.002206]\n",
      "epoch:48 step:38188 [D loss: 0.000658, acc.: 100.00%] [G loss: 0.002358]\n",
      "epoch:48 step:38189 [D loss: 0.008708, acc.: 99.22%] [G loss: 0.000328]\n",
      "epoch:48 step:38190 [D loss: 0.004467, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:48 step:38191 [D loss: 0.002612, acc.: 100.00%] [G loss: 0.010058]\n",
      "epoch:48 step:38192 [D loss: 0.026768, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:48 step:38193 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.001294]\n",
      "epoch:48 step:38194 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:48 step:38195 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.002144]\n",
      "epoch:48 step:38196 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:48 step:38197 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.012989]\n",
      "epoch:48 step:38198 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:48 step:38199 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:48 step:38200 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.027132]\n",
      "epoch:48 step:38201 [D loss: 0.033893, acc.: 99.22%] [G loss: 0.000217]\n",
      "epoch:48 step:38202 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.001116]\n",
      "epoch:48 step:38203 [D loss: 0.002313, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:48 step:38204 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:48 step:38205 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001621]\n",
      "epoch:48 step:38206 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:48 step:38207 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:48 step:38208 [D loss: 0.001132, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:48 step:38209 [D loss: 0.000771, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:48 step:38210 [D loss: 0.001307, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:48 step:38211 [D loss: 0.006209, acc.: 99.22%] [G loss: 0.000049]\n",
      "epoch:48 step:38212 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:48 step:38213 [D loss: 0.029825, acc.: 99.22%] [G loss: 0.000043]\n",
      "epoch:48 step:38214 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.002269]\n",
      "epoch:48 step:38215 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:48 step:38216 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:48 step:38217 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.005224]\n",
      "epoch:48 step:38218 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:48 step:38219 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002480]\n",
      "epoch:48 step:38220 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:48 step:38221 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:48 step:38222 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:48 step:38223 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000716]\n",
      "epoch:48 step:38224 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:48 step:38225 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:48 step:38226 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:48 step:38227 [D loss: 0.000634, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:48 step:38228 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:48 step:38229 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:48 step:38230 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003203]\n",
      "epoch:48 step:38231 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.002491]\n",
      "epoch:48 step:38232 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.002247]\n",
      "epoch:48 step:38233 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.005537]\n",
      "epoch:48 step:38234 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:48 step:38235 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:48 step:38236 [D loss: 0.002336, acc.: 100.00%] [G loss: 0.002722]\n",
      "epoch:48 step:38237 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:48 step:38238 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:48 step:38239 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:48 step:38240 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:48 step:38241 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.040777]\n",
      "epoch:48 step:38242 [D loss: 0.002452, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:48 step:38243 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.005000]\n",
      "epoch:48 step:38244 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:48 step:38245 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:48 step:38246 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:48 step:38247 [D loss: 0.018086, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:48 step:38248 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:48 step:38249 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:48 step:38250 [D loss: 0.001581, acc.: 100.00%] [G loss: 0.000691]\n",
      "epoch:48 step:38251 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.002853]\n",
      "epoch:48 step:38252 [D loss: 0.027364, acc.: 100.00%] [G loss: 0.010597]\n",
      "epoch:48 step:38253 [D loss: 0.000868, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:48 step:38254 [D loss: 0.001128, acc.: 100.00%] [G loss: 0.038747]\n",
      "epoch:48 step:38255 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.002405]\n",
      "epoch:48 step:38256 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:48 step:38257 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.112321]\n",
      "epoch:48 step:38258 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.001030]\n",
      "epoch:48 step:38259 [D loss: 0.064641, acc.: 98.44%] [G loss: 0.000008]\n",
      "epoch:48 step:38260 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:48 step:38261 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000751]\n",
      "epoch:48 step:38262 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:48 step:38263 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:48 step:38264 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:48 step:38265 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:48 step:38266 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:48 step:38267 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.008326]\n",
      "epoch:48 step:38268 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.004282]\n",
      "epoch:48 step:38269 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38270 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:49 step:38271 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:49 step:38272 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.007572]\n",
      "epoch:49 step:38273 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002029]\n",
      "epoch:49 step:38274 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38275 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38276 [D loss: 0.004260, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:49 step:38277 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:49 step:38278 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:49 step:38279 [D loss: 0.010482, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38280 [D loss: 0.003960, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:49 step:38281 [D loss: 0.000991, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:49 step:38282 [D loss: 0.002010, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:49 step:38283 [D loss: 0.013680, acc.: 100.00%] [G loss: 0.000684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38284 [D loss: 0.000955, acc.: 100.00%] [G loss: 0.001034]\n",
      "epoch:49 step:38285 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:49 step:38286 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001401]\n",
      "epoch:49 step:38287 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:49 step:38288 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.004103]\n",
      "epoch:49 step:38289 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.001380]\n",
      "epoch:49 step:38290 [D loss: 0.001262, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:49 step:38291 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:49 step:38292 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:49 step:38293 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.001825]\n",
      "epoch:49 step:38294 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.020369]\n",
      "epoch:49 step:38295 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000492]\n",
      "epoch:49 step:38296 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.003787]\n",
      "epoch:49 step:38297 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:49 step:38298 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:49 step:38299 [D loss: 0.018054, acc.: 98.44%] [G loss: 0.004915]\n",
      "epoch:49 step:38300 [D loss: 0.003609, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:49 step:38301 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.031157]\n",
      "epoch:49 step:38302 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000424]\n",
      "epoch:49 step:38303 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.001185]\n",
      "epoch:49 step:38304 [D loss: 0.002448, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:49 step:38305 [D loss: 0.001056, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:49 step:38306 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.000650]\n",
      "epoch:49 step:38307 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.017030]\n",
      "epoch:49 step:38308 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.150745]\n",
      "epoch:49 step:38309 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.004260]\n",
      "epoch:49 step:38310 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.005528]\n",
      "epoch:49 step:38311 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001219]\n",
      "epoch:49 step:38312 [D loss: 0.003327, acc.: 100.00%] [G loss: 0.005517]\n",
      "epoch:49 step:38313 [D loss: 0.007126, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38314 [D loss: 0.002222, acc.: 100.00%] [G loss: 0.002581]\n",
      "epoch:49 step:38315 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:49 step:38316 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.001052]\n",
      "epoch:49 step:38317 [D loss: 0.004883, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:49 step:38318 [D loss: 0.002993, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:49 step:38319 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:49 step:38320 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:49 step:38321 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:49 step:38322 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000933]\n",
      "epoch:49 step:38323 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002303]\n",
      "epoch:49 step:38324 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38325 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:49 step:38326 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:49 step:38327 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001649]\n",
      "epoch:49 step:38328 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:49 step:38329 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:49 step:38330 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002118]\n",
      "epoch:49 step:38331 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:49 step:38332 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002665]\n",
      "epoch:49 step:38333 [D loss: 0.001068, acc.: 100.00%] [G loss: 0.002171]\n",
      "epoch:49 step:38334 [D loss: 0.002421, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:49 step:38335 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:49 step:38336 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:49 step:38337 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:49 step:38338 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:49 step:38339 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38340 [D loss: 0.000964, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:49 step:38341 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38342 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:49 step:38343 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:49 step:38344 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:49 step:38345 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:49 step:38346 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:49 step:38347 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:49 step:38348 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:49 step:38349 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:49 step:38350 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:49 step:38351 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:49 step:38352 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38353 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:49 step:38354 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:49 step:38355 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:49 step:38356 [D loss: 0.001934, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:49 step:38357 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000435]\n",
      "epoch:49 step:38358 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:49 step:38359 [D loss: 0.018357, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38360 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:49 step:38361 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001029]\n",
      "epoch:49 step:38362 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:49 step:38363 [D loss: 0.009869, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38364 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:49 step:38365 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:49 step:38366 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001175]\n",
      "epoch:49 step:38367 [D loss: 0.003840, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:49 step:38368 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:49 step:38369 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:49 step:38370 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:49 step:38371 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38372 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:49 step:38373 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:49 step:38374 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:49 step:38375 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:49 step:38376 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38377 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:49 step:38378 [D loss: 0.571006, acc.: 75.00%] [G loss: 2.728928]\n",
      "epoch:49 step:38379 [D loss: 0.315360, acc.: 83.59%] [G loss: 3.244359]\n",
      "epoch:49 step:38380 [D loss: 1.145797, acc.: 59.38%] [G loss: 0.027239]\n",
      "epoch:49 step:38381 [D loss: 0.015784, acc.: 100.00%] [G loss: 0.002455]\n",
      "epoch:49 step:38382 [D loss: 0.157929, acc.: 92.19%] [G loss: 0.013278]\n",
      "epoch:49 step:38383 [D loss: 0.018666, acc.: 100.00%] [G loss: 0.018613]\n",
      "epoch:49 step:38384 [D loss: 0.011513, acc.: 100.00%] [G loss: 0.019431]\n",
      "epoch:49 step:38385 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.112611]\n",
      "epoch:49 step:38386 [D loss: 0.043086, acc.: 100.00%] [G loss: 0.027165]\n",
      "epoch:49 step:38387 [D loss: 0.081231, acc.: 98.44%] [G loss: 0.267726]\n",
      "epoch:49 step:38388 [D loss: 0.046870, acc.: 99.22%] [G loss: 0.635568]\n",
      "epoch:49 step:38389 [D loss: 0.009171, acc.: 99.22%] [G loss: 0.009469]\n",
      "epoch:49 step:38390 [D loss: 0.002287, acc.: 100.00%] [G loss: 0.010577]\n",
      "epoch:49 step:38391 [D loss: 0.005779, acc.: 100.00%] [G loss: 0.003237]\n",
      "epoch:49 step:38392 [D loss: 0.002794, acc.: 100.00%] [G loss: 0.002617]\n",
      "epoch:49 step:38393 [D loss: 0.006387, acc.: 100.00%] [G loss: 0.007856]\n",
      "epoch:49 step:38394 [D loss: 0.001519, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:49 step:38395 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:49 step:38396 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.004946]\n",
      "epoch:49 step:38397 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.001184]\n",
      "epoch:49 step:38398 [D loss: 0.001710, acc.: 100.00%] [G loss: 1.988451]\n",
      "epoch:49 step:38399 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.001487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38400 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.002971]\n",
      "epoch:49 step:38401 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.003190]\n",
      "epoch:49 step:38402 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.003002]\n",
      "epoch:49 step:38403 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.001207]\n",
      "epoch:49 step:38404 [D loss: 0.012563, acc.: 100.00%] [G loss: 0.001340]\n",
      "epoch:49 step:38405 [D loss: 0.001230, acc.: 100.00%] [G loss: 0.003611]\n",
      "epoch:49 step:38406 [D loss: 0.093790, acc.: 98.44%] [G loss: 0.061266]\n",
      "epoch:49 step:38407 [D loss: 0.005316, acc.: 100.00%] [G loss: 0.059449]\n",
      "epoch:49 step:38408 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.648651]\n",
      "epoch:49 step:38409 [D loss: 0.012501, acc.: 100.00%] [G loss: 0.006218]\n",
      "epoch:49 step:38410 [D loss: 0.009546, acc.: 100.00%] [G loss: 0.187217]\n",
      "epoch:49 step:38411 [D loss: 0.112513, acc.: 96.09%] [G loss: 0.163378]\n",
      "epoch:49 step:38412 [D loss: 0.024898, acc.: 99.22%] [G loss: 0.310705]\n",
      "epoch:49 step:38413 [D loss: 0.021778, acc.: 99.22%] [G loss: 0.161256]\n",
      "epoch:49 step:38414 [D loss: 0.033902, acc.: 98.44%] [G loss: 0.044055]\n",
      "epoch:49 step:38415 [D loss: 0.004573, acc.: 100.00%] [G loss: 0.031046]\n",
      "epoch:49 step:38416 [D loss: 0.023460, acc.: 100.00%] [G loss: 0.016966]\n",
      "epoch:49 step:38417 [D loss: 0.054571, acc.: 98.44%] [G loss: 0.041509]\n",
      "epoch:49 step:38418 [D loss: 0.023957, acc.: 100.00%] [G loss: 0.014991]\n",
      "epoch:49 step:38419 [D loss: 0.002690, acc.: 100.00%] [G loss: 0.258035]\n",
      "epoch:49 step:38420 [D loss: 0.011577, acc.: 100.00%] [G loss: 0.003088]\n",
      "epoch:49 step:38421 [D loss: 0.001744, acc.: 100.00%] [G loss: 0.004562]\n",
      "epoch:49 step:38422 [D loss: 0.018956, acc.: 100.00%] [G loss: 0.007601]\n",
      "epoch:49 step:38423 [D loss: 0.010860, acc.: 100.00%] [G loss: 0.515673]\n",
      "epoch:49 step:38424 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.044526]\n",
      "epoch:49 step:38425 [D loss: 0.001557, acc.: 100.00%] [G loss: 0.016696]\n",
      "epoch:49 step:38426 [D loss: 0.005337, acc.: 100.00%] [G loss: 0.029869]\n",
      "epoch:49 step:38427 [D loss: 0.020434, acc.: 99.22%] [G loss: 0.006385]\n",
      "epoch:49 step:38428 [D loss: 0.101982, acc.: 97.66%] [G loss: 0.089043]\n",
      "epoch:49 step:38429 [D loss: 0.142633, acc.: 96.88%] [G loss: 0.190935]\n",
      "epoch:49 step:38430 [D loss: 0.006206, acc.: 100.00%] [G loss: 0.205224]\n",
      "epoch:49 step:38431 [D loss: 0.003187, acc.: 100.00%] [G loss: 0.058748]\n",
      "epoch:49 step:38432 [D loss: 0.014333, acc.: 100.00%] [G loss: 0.047773]\n",
      "epoch:49 step:38433 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.021000]\n",
      "epoch:49 step:38434 [D loss: 0.001283, acc.: 100.00%] [G loss: 0.036433]\n",
      "epoch:49 step:38435 [D loss: 0.025696, acc.: 100.00%] [G loss: 0.046160]\n",
      "epoch:49 step:38436 [D loss: 0.007773, acc.: 100.00%] [G loss: 0.087778]\n",
      "epoch:49 step:38437 [D loss: 0.004755, acc.: 100.00%] [G loss: 0.039603]\n",
      "epoch:49 step:38438 [D loss: 0.040881, acc.: 100.00%] [G loss: 0.615983]\n",
      "epoch:49 step:38439 [D loss: 0.003095, acc.: 100.00%] [G loss: 3.145063]\n",
      "epoch:49 step:38440 [D loss: 0.035576, acc.: 100.00%] [G loss: 0.063767]\n",
      "epoch:49 step:38441 [D loss: 0.007365, acc.: 100.00%] [G loss: 0.233756]\n",
      "epoch:49 step:38442 [D loss: 0.036177, acc.: 98.44%] [G loss: 0.460676]\n",
      "epoch:49 step:38443 [D loss: 0.030520, acc.: 99.22%] [G loss: 0.102088]\n",
      "epoch:49 step:38444 [D loss: 0.004373, acc.: 100.00%] [G loss: 0.004731]\n",
      "epoch:49 step:38445 [D loss: 0.101589, acc.: 96.09%] [G loss: 0.001207]\n",
      "epoch:49 step:38446 [D loss: 0.002523, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:49 step:38447 [D loss: 0.001749, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:49 step:38448 [D loss: 0.020099, acc.: 100.00%] [G loss: 0.000896]\n",
      "epoch:49 step:38449 [D loss: 0.006064, acc.: 100.00%] [G loss: 0.001192]\n",
      "epoch:49 step:38450 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.005864]\n",
      "epoch:49 step:38451 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.002854]\n",
      "epoch:49 step:38452 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000611]\n",
      "epoch:49 step:38453 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.010355]\n",
      "epoch:49 step:38454 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.002219]\n",
      "epoch:49 step:38455 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.001082]\n",
      "epoch:49 step:38456 [D loss: 0.002446, acc.: 100.00%] [G loss: 0.008944]\n",
      "epoch:49 step:38457 [D loss: 0.005624, acc.: 100.00%] [G loss: 0.001846]\n",
      "epoch:49 step:38458 [D loss: 0.052110, acc.: 99.22%] [G loss: 0.003603]\n",
      "epoch:49 step:38459 [D loss: 0.019025, acc.: 100.00%] [G loss: 0.008766]\n",
      "epoch:49 step:38460 [D loss: 0.003983, acc.: 100.00%] [G loss: 0.091863]\n",
      "epoch:49 step:38461 [D loss: 0.000352, acc.: 100.00%] [G loss: 1.979019]\n",
      "epoch:49 step:38462 [D loss: 0.009745, acc.: 100.00%] [G loss: 0.023343]\n",
      "epoch:49 step:38463 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.008879]\n",
      "epoch:49 step:38464 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.007361]\n",
      "epoch:49 step:38465 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.014522]\n",
      "epoch:49 step:38466 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.017029]\n",
      "epoch:49 step:38467 [D loss: 0.010339, acc.: 99.22%] [G loss: 0.011906]\n",
      "epoch:49 step:38468 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.001361]\n",
      "epoch:49 step:38469 [D loss: 0.000778, acc.: 100.00%] [G loss: 0.010628]\n",
      "epoch:49 step:38470 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.005391]\n",
      "epoch:49 step:38471 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001293]\n",
      "epoch:49 step:38472 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.003044]\n",
      "epoch:49 step:38473 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.009068]\n",
      "epoch:49 step:38474 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.014197]\n",
      "epoch:49 step:38475 [D loss: 0.001985, acc.: 100.00%] [G loss: 0.001060]\n",
      "epoch:49 step:38476 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.007204]\n",
      "epoch:49 step:38477 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.001610]\n",
      "epoch:49 step:38478 [D loss: 0.076337, acc.: 95.31%] [G loss: 0.000098]\n",
      "epoch:49 step:38479 [D loss: 0.032792, acc.: 99.22%] [G loss: 0.000446]\n",
      "epoch:49 step:38480 [D loss: 0.001697, acc.: 100.00%] [G loss: 0.011531]\n",
      "epoch:49 step:38481 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.004285]\n",
      "epoch:49 step:38482 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.108840]\n",
      "epoch:49 step:38483 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.000991]\n",
      "epoch:49 step:38484 [D loss: 0.002895, acc.: 100.00%] [G loss: 0.134849]\n",
      "epoch:49 step:38485 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.003758]\n",
      "epoch:49 step:38486 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.002922]\n",
      "epoch:49 step:38487 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.002131]\n",
      "epoch:49 step:38488 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.006414]\n",
      "epoch:49 step:38489 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000409]\n",
      "epoch:49 step:38490 [D loss: 0.003174, acc.: 100.00%] [G loss: 0.028700]\n",
      "epoch:49 step:38491 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.002046]\n",
      "epoch:49 step:38492 [D loss: 0.005408, acc.: 100.00%] [G loss: 0.001017]\n",
      "epoch:49 step:38493 [D loss: 0.000678, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:49 step:38494 [D loss: 0.028423, acc.: 99.22%] [G loss: 0.011743]\n",
      "epoch:49 step:38495 [D loss: 0.001379, acc.: 100.00%] [G loss: 0.018749]\n",
      "epoch:49 step:38496 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.014398]\n",
      "epoch:49 step:38497 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.185830]\n",
      "epoch:49 step:38498 [D loss: 0.015682, acc.: 100.00%] [G loss: 0.007199]\n",
      "epoch:49 step:38499 [D loss: 0.092800, acc.: 95.31%] [G loss: 0.000178]\n",
      "epoch:49 step:38500 [D loss: 0.434824, acc.: 85.16%] [G loss: 3.022406]\n",
      "epoch:49 step:38501 [D loss: 0.043369, acc.: 96.88%] [G loss: 7.586341]\n",
      "epoch:49 step:38502 [D loss: 0.490821, acc.: 76.56%] [G loss: 0.031732]\n",
      "epoch:49 step:38503 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.552544]\n",
      "epoch:49 step:38504 [D loss: 0.002403, acc.: 100.00%] [G loss: 0.003894]\n",
      "epoch:49 step:38505 [D loss: 0.014918, acc.: 100.00%] [G loss: 0.256578]\n",
      "epoch:49 step:38506 [D loss: 0.005893, acc.: 100.00%] [G loss: 0.108217]\n",
      "epoch:49 step:38507 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.035055]\n",
      "epoch:49 step:38508 [D loss: 0.005439, acc.: 100.00%] [G loss: 0.016115]\n",
      "epoch:49 step:38509 [D loss: 0.823809, acc.: 70.31%] [G loss: 7.358729]\n",
      "epoch:49 step:38510 [D loss: 0.339877, acc.: 86.72%] [G loss: 5.008728]\n",
      "epoch:49 step:38511 [D loss: 0.022496, acc.: 99.22%] [G loss: 0.701390]\n",
      "epoch:49 step:38512 [D loss: 0.007558, acc.: 100.00%] [G loss: 2.692874]\n",
      "epoch:49 step:38513 [D loss: 0.008491, acc.: 100.00%] [G loss: 2.118119]\n",
      "epoch:49 step:38514 [D loss: 0.004129, acc.: 100.00%] [G loss: 0.003253]\n",
      "epoch:49 step:38515 [D loss: 0.050341, acc.: 96.88%] [G loss: 0.826759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38516 [D loss: 0.015756, acc.: 99.22%] [G loss: 0.000080]\n",
      "epoch:49 step:38517 [D loss: 0.018162, acc.: 100.00%] [G loss: 0.008140]\n",
      "epoch:49 step:38518 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:49 step:38519 [D loss: 0.003421, acc.: 100.00%] [G loss: 0.677755]\n",
      "epoch:49 step:38520 [D loss: 0.026381, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:49 step:38521 [D loss: 0.007023, acc.: 100.00%] [G loss: 0.082370]\n",
      "epoch:49 step:38522 [D loss: 0.021408, acc.: 99.22%] [G loss: 0.345224]\n",
      "epoch:49 step:38523 [D loss: 0.025165, acc.: 99.22%] [G loss: 0.003837]\n",
      "epoch:49 step:38524 [D loss: 0.002851, acc.: 100.00%] [G loss: 0.005132]\n",
      "epoch:49 step:38525 [D loss: 0.003395, acc.: 100.00%] [G loss: 0.075203]\n",
      "epoch:49 step:38526 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.004320]\n",
      "epoch:49 step:38527 [D loss: 0.094237, acc.: 96.88%] [G loss: 1.205240]\n",
      "epoch:49 step:38528 [D loss: 0.203996, acc.: 93.75%] [G loss: 0.036863]\n",
      "epoch:49 step:38529 [D loss: 0.056232, acc.: 98.44%] [G loss: 0.118769]\n",
      "epoch:49 step:38530 [D loss: 0.119406, acc.: 94.53%] [G loss: 0.000013]\n",
      "epoch:49 step:38531 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38532 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:49 step:38533 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38534 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:49 step:38535 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38536 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38537 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:49 step:38538 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38539 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.003272]\n",
      "epoch:49 step:38540 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.051856]\n",
      "epoch:49 step:38541 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:49 step:38542 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:49 step:38543 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:49 step:38544 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38545 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38546 [D loss: 0.010446, acc.: 99.22%] [G loss: 0.000001]\n",
      "epoch:49 step:38547 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38548 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:49 step:38549 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:49 step:38550 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38551 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:49 step:38552 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:49 step:38553 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.001403]\n",
      "epoch:49 step:38554 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38555 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38556 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38557 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38558 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38559 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:49 step:38560 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38561 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:49 step:38562 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:49 step:38563 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38564 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38565 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000965]\n",
      "epoch:49 step:38566 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38567 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38568 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:49 step:38569 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:49 step:38570 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000423]\n",
      "epoch:49 step:38571 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:49 step:38572 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:49 step:38573 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:49 step:38574 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:49 step:38575 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:49 step:38576 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:49 step:38577 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38578 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:49 step:38579 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38580 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:49 step:38581 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:49 step:38582 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:49 step:38583 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:49 step:38584 [D loss: 0.000942, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:49 step:38585 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:49 step:38586 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38587 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38588 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000520]\n",
      "epoch:49 step:38589 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38590 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38591 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:49 step:38592 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:49 step:38593 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.003396]\n",
      "epoch:49 step:38594 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38595 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38596 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:49 step:38597 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38598 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:49 step:38599 [D loss: 0.002215, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38600 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.003817]\n",
      "epoch:49 step:38601 [D loss: 0.002195, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38602 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38603 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:49 step:38604 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:49 step:38605 [D loss: 0.006701, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:49 step:38606 [D loss: 0.000723, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:49 step:38607 [D loss: 0.433798, acc.: 83.59%] [G loss: 2.253076]\n",
      "epoch:49 step:38608 [D loss: 0.000552, acc.: 100.00%] [G loss: 5.371789]\n",
      "epoch:49 step:38609 [D loss: 2.777216, acc.: 47.66%] [G loss: 0.153111]\n",
      "epoch:49 step:38610 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.306214]\n",
      "epoch:49 step:38611 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.036584]\n",
      "epoch:49 step:38612 [D loss: 0.002950, acc.: 100.00%] [G loss: 0.119784]\n",
      "epoch:49 step:38613 [D loss: 0.003740, acc.: 100.00%] [G loss: 0.019257]\n",
      "epoch:49 step:38614 [D loss: 0.003736, acc.: 100.00%] [G loss: 0.004764]\n",
      "epoch:49 step:38615 [D loss: 0.003388, acc.: 100.00%] [G loss: 0.019952]\n",
      "epoch:49 step:38616 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.017784]\n",
      "epoch:49 step:38617 [D loss: 0.008588, acc.: 100.00%] [G loss: 0.029320]\n",
      "epoch:49 step:38618 [D loss: 0.052338, acc.: 99.22%] [G loss: 0.014152]\n",
      "epoch:49 step:38619 [D loss: 0.002303, acc.: 100.00%] [G loss: 0.029671]\n",
      "epoch:49 step:38620 [D loss: 0.003407, acc.: 100.00%] [G loss: 0.004495]\n",
      "epoch:49 step:38621 [D loss: 0.002818, acc.: 100.00%] [G loss: 0.007361]\n",
      "epoch:49 step:38622 [D loss: 0.002396, acc.: 100.00%] [G loss: 0.001143]\n",
      "epoch:49 step:38623 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.015164]\n",
      "epoch:49 step:38624 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.007798]\n",
      "epoch:49 step:38625 [D loss: 0.005681, acc.: 100.00%] [G loss: 0.005999]\n",
      "epoch:49 step:38626 [D loss: 0.002049, acc.: 100.00%] [G loss: 0.010430]\n",
      "epoch:49 step:38627 [D loss: 0.008203, acc.: 100.00%] [G loss: 0.048681]\n",
      "epoch:49 step:38628 [D loss: 0.005976, acc.: 100.00%] [G loss: 0.009163]\n",
      "epoch:49 step:38629 [D loss: 0.006442, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:49 step:38630 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.018039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38631 [D loss: 0.001075, acc.: 100.00%] [G loss: 0.113475]\n",
      "epoch:49 step:38632 [D loss: 0.196261, acc.: 95.31%] [G loss: 0.091463]\n",
      "epoch:49 step:38633 [D loss: 0.006154, acc.: 100.00%] [G loss: 1.374814]\n",
      "epoch:49 step:38634 [D loss: 0.072983, acc.: 96.88%] [G loss: 0.167130]\n",
      "epoch:49 step:38635 [D loss: 0.010957, acc.: 100.00%] [G loss: 0.054562]\n",
      "epoch:49 step:38636 [D loss: 0.002593, acc.: 100.00%] [G loss: 0.282827]\n",
      "epoch:49 step:38637 [D loss: 0.034319, acc.: 99.22%] [G loss: 0.028366]\n",
      "epoch:49 step:38638 [D loss: 0.005404, acc.: 100.00%] [G loss: 0.093534]\n",
      "epoch:49 step:38639 [D loss: 0.057384, acc.: 100.00%] [G loss: 0.101142]\n",
      "epoch:49 step:38640 [D loss: 0.001764, acc.: 100.00%] [G loss: 0.142621]\n",
      "epoch:49 step:38641 [D loss: 0.002408, acc.: 100.00%] [G loss: 0.064063]\n",
      "epoch:49 step:38642 [D loss: 0.002294, acc.: 100.00%] [G loss: 0.018720]\n",
      "epoch:49 step:38643 [D loss: 0.019621, acc.: 100.00%] [G loss: 0.013691]\n",
      "epoch:49 step:38644 [D loss: 0.067200, acc.: 97.66%] [G loss: 0.031537]\n",
      "epoch:49 step:38645 [D loss: 0.001554, acc.: 100.00%] [G loss: 0.972141]\n",
      "epoch:49 step:38646 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:49 step:38647 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:49 step:38648 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:49 step:38649 [D loss: 0.001120, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:49 step:38650 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:49 step:38651 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:49 step:38652 [D loss: 0.033649, acc.: 99.22%] [G loss: 0.000150]\n",
      "epoch:49 step:38653 [D loss: 0.003883, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:49 step:38654 [D loss: 0.008442, acc.: 100.00%] [G loss: 0.001039]\n",
      "epoch:49 step:38655 [D loss: 0.028782, acc.: 99.22%] [G loss: 0.001541]\n",
      "epoch:49 step:38656 [D loss: 0.006979, acc.: 100.00%] [G loss: 0.005013]\n",
      "epoch:49 step:38657 [D loss: 1.463323, acc.: 46.09%] [G loss: 4.588138]\n",
      "epoch:49 step:38658 [D loss: 0.487784, acc.: 77.34%] [G loss: 1.164584]\n",
      "epoch:49 step:38659 [D loss: 0.077582, acc.: 96.88%] [G loss: 4.956052]\n",
      "epoch:49 step:38660 [D loss: 0.157456, acc.: 93.75%] [G loss: 0.898271]\n",
      "epoch:49 step:38661 [D loss: 0.029563, acc.: 98.44%] [G loss: 0.431280]\n",
      "epoch:49 step:38662 [D loss: 0.001467, acc.: 100.00%] [G loss: 0.040503]\n",
      "epoch:49 step:38663 [D loss: 0.147633, acc.: 92.97%] [G loss: 0.692857]\n",
      "epoch:49 step:38664 [D loss: 0.016387, acc.: 100.00%] [G loss: 0.741352]\n",
      "epoch:49 step:38665 [D loss: 0.014168, acc.: 100.00%] [G loss: 0.402108]\n",
      "epoch:49 step:38666 [D loss: 0.028817, acc.: 99.22%] [G loss: 0.270160]\n",
      "epoch:49 step:38667 [D loss: 0.028983, acc.: 99.22%] [G loss: 0.182595]\n",
      "epoch:49 step:38668 [D loss: 0.154597, acc.: 93.75%] [G loss: 0.987485]\n",
      "epoch:49 step:38669 [D loss: 0.008906, acc.: 100.00%] [G loss: 1.386919]\n",
      "epoch:49 step:38670 [D loss: 0.034466, acc.: 99.22%] [G loss: 0.597176]\n",
      "epoch:49 step:38671 [D loss: 0.051443, acc.: 98.44%] [G loss: 0.233015]\n",
      "epoch:49 step:38672 [D loss: 0.018056, acc.: 100.00%] [G loss: 0.155086]\n",
      "epoch:49 step:38673 [D loss: 0.130814, acc.: 96.88%] [G loss: 0.271338]\n",
      "epoch:49 step:38674 [D loss: 0.127021, acc.: 94.53%] [G loss: 0.048154]\n",
      "epoch:49 step:38675 [D loss: 0.078750, acc.: 96.88%] [G loss: 0.105945]\n",
      "epoch:49 step:38676 [D loss: 0.002579, acc.: 100.00%] [G loss: 0.007645]\n",
      "epoch:49 step:38677 [D loss: 0.006917, acc.: 100.00%] [G loss: 0.110955]\n",
      "epoch:49 step:38678 [D loss: 0.008273, acc.: 100.00%] [G loss: 0.030217]\n",
      "epoch:49 step:38679 [D loss: 0.035211, acc.: 99.22%] [G loss: 0.000031]\n",
      "epoch:49 step:38680 [D loss: 0.039333, acc.: 98.44%] [G loss: 0.135975]\n",
      "epoch:49 step:38681 [D loss: 0.001911, acc.: 100.00%] [G loss: 0.035979]\n",
      "epoch:49 step:38682 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:49 step:38683 [D loss: 0.012319, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:49 step:38684 [D loss: 0.005166, acc.: 100.00%] [G loss: 0.020091]\n",
      "epoch:49 step:38685 [D loss: 0.019539, acc.: 100.00%] [G loss: 0.021482]\n",
      "epoch:49 step:38686 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:49 step:38687 [D loss: 0.005082, acc.: 100.00%] [G loss: 0.008182]\n",
      "epoch:49 step:38688 [D loss: 0.017098, acc.: 100.00%] [G loss: 0.027325]\n",
      "epoch:49 step:38689 [D loss: 0.056701, acc.: 98.44%] [G loss: 0.000015]\n",
      "epoch:49 step:38690 [D loss: 0.001688, acc.: 100.00%] [G loss: 0.003837]\n",
      "epoch:49 step:38691 [D loss: 0.002327, acc.: 100.00%] [G loss: 0.001143]\n",
      "epoch:49 step:38692 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.005725]\n",
      "epoch:49 step:38693 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.015543]\n",
      "epoch:49 step:38694 [D loss: 0.001128, acc.: 100.00%] [G loss: 0.008483]\n",
      "epoch:49 step:38695 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38696 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:49 step:38697 [D loss: 0.008035, acc.: 100.00%] [G loss: 0.003022]\n",
      "epoch:49 step:38698 [D loss: 0.014596, acc.: 100.00%] [G loss: 0.003835]\n",
      "epoch:49 step:38699 [D loss: 0.258011, acc.: 88.28%] [G loss: 0.000091]\n",
      "epoch:49 step:38700 [D loss: 0.005722, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:49 step:38701 [D loss: 0.000975, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:49 step:38702 [D loss: 0.003752, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:49 step:38703 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:49 step:38704 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:49 step:38705 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.002171]\n",
      "epoch:49 step:38706 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:49 step:38707 [D loss: 0.001464, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:49 step:38708 [D loss: 0.040230, acc.: 99.22%] [G loss: 0.000136]\n",
      "epoch:49 step:38709 [D loss: 0.001996, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38710 [D loss: 0.003091, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38711 [D loss: 0.005927, acc.: 100.00%] [G loss: 0.036278]\n",
      "epoch:49 step:38712 [D loss: 0.132742, acc.: 96.88%] [G loss: 0.007016]\n",
      "epoch:49 step:38713 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.086770]\n",
      "epoch:49 step:38714 [D loss: 0.086989, acc.: 96.09%] [G loss: 0.034155]\n",
      "epoch:49 step:38715 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.006451]\n",
      "epoch:49 step:38716 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.003817]\n",
      "epoch:49 step:38717 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.001729]\n",
      "epoch:49 step:38718 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001054]\n",
      "epoch:49 step:38719 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.009580]\n",
      "epoch:49 step:38720 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001079]\n",
      "epoch:49 step:38721 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003802]\n",
      "epoch:49 step:38722 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.839366]\n",
      "epoch:49 step:38723 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.028368]\n",
      "epoch:49 step:38724 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.032076]\n",
      "epoch:49 step:38725 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.116269]\n",
      "epoch:49 step:38726 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001249]\n",
      "epoch:49 step:38727 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.002067]\n",
      "epoch:49 step:38728 [D loss: 0.003490, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:49 step:38729 [D loss: 0.227239, acc.: 88.28%] [G loss: 0.093010]\n",
      "epoch:49 step:38730 [D loss: 0.001013, acc.: 100.00%] [G loss: 0.325676]\n",
      "epoch:49 step:38731 [D loss: 0.031097, acc.: 98.44%] [G loss: 0.804847]\n",
      "epoch:49 step:38732 [D loss: 0.231306, acc.: 90.62%] [G loss: 0.001535]\n",
      "epoch:49 step:38733 [D loss: 0.001557, acc.: 100.00%] [G loss: 1.366825]\n",
      "epoch:49 step:38734 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.005180]\n",
      "epoch:49 step:38735 [D loss: 0.018467, acc.: 100.00%] [G loss: 0.294273]\n",
      "epoch:49 step:38736 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.058504]\n",
      "epoch:49 step:38737 [D loss: 0.002456, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:49 step:38738 [D loss: 0.003087, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:49 step:38739 [D loss: 0.012444, acc.: 100.00%] [G loss: 0.062296]\n",
      "epoch:49 step:38740 [D loss: 0.001644, acc.: 100.00%] [G loss: 0.379257]\n",
      "epoch:49 step:38741 [D loss: 0.065635, acc.: 99.22%] [G loss: 0.015427]\n",
      "epoch:49 step:38742 [D loss: 0.048351, acc.: 97.66%] [G loss: 0.146949]\n",
      "epoch:49 step:38743 [D loss: 0.026357, acc.: 98.44%] [G loss: 0.022455]\n",
      "epoch:49 step:38744 [D loss: 0.003344, acc.: 100.00%] [G loss: 0.023078]\n",
      "epoch:49 step:38745 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.008651]\n",
      "epoch:49 step:38746 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.433661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38747 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.792529]\n",
      "epoch:49 step:38748 [D loss: 0.001010, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:49 step:38749 [D loss: 0.001314, acc.: 100.00%] [G loss: 0.002687]\n",
      "epoch:49 step:38750 [D loss: 0.001379, acc.: 100.00%] [G loss: 0.027264]\n",
      "epoch:49 step:38751 [D loss: 0.003483, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:49 step:38752 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000824]\n",
      "epoch:49 step:38753 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.051811]\n",
      "epoch:49 step:38754 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.149017]\n",
      "epoch:49 step:38755 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.002873]\n",
      "epoch:49 step:38756 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001360]\n",
      "epoch:49 step:38757 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.003417]\n",
      "epoch:49 step:38758 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:49 step:38759 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.042535]\n",
      "epoch:49 step:38760 [D loss: 0.004457, acc.: 100.00%] [G loss: 0.001808]\n",
      "epoch:49 step:38761 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000595]\n",
      "epoch:49 step:38762 [D loss: 0.001230, acc.: 100.00%] [G loss: 0.001800]\n",
      "epoch:49 step:38763 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.003019]\n",
      "epoch:49 step:38764 [D loss: 0.001062, acc.: 100.00%] [G loss: 0.001383]\n",
      "epoch:49 step:38765 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:49 step:38766 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:49 step:38767 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002412]\n",
      "epoch:49 step:38768 [D loss: 0.001467, acc.: 100.00%] [G loss: 0.005644]\n",
      "epoch:49 step:38769 [D loss: 0.004541, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:49 step:38770 [D loss: 0.002888, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:49 step:38771 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.005534]\n",
      "epoch:49 step:38772 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:49 step:38773 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.001937]\n",
      "epoch:49 step:38774 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:49 step:38775 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:49 step:38776 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:49 step:38777 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000947]\n",
      "epoch:49 step:38778 [D loss: 0.004535, acc.: 100.00%] [G loss: 0.003494]\n",
      "epoch:49 step:38779 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.001503]\n",
      "epoch:49 step:38780 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:49 step:38781 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:49 step:38782 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:49 step:38783 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:49 step:38784 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:49 step:38785 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:49 step:38786 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:49 step:38787 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:49 step:38788 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:49 step:38789 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001375]\n",
      "epoch:49 step:38790 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:49 step:38791 [D loss: 0.002115, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:49 step:38792 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000756]\n",
      "epoch:49 step:38793 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.001236]\n",
      "epoch:49 step:38794 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:49 step:38795 [D loss: 0.007659, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:49 step:38796 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:49 step:38797 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.016521]\n",
      "epoch:49 step:38798 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:49 step:38799 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:49 step:38800 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:49 step:38801 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38802 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000998]\n",
      "epoch:49 step:38803 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:49 step:38804 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:49 step:38805 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:49 step:38806 [D loss: 0.002990, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38807 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38808 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000513]\n",
      "epoch:49 step:38809 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:49 step:38810 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:49 step:38811 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:49 step:38812 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:49 step:38813 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:49 step:38814 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:49 step:38815 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38816 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:49 step:38817 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38818 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:49 step:38819 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:49 step:38820 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:49 step:38821 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:49 step:38822 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:49 step:38823 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:49 step:38824 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:49 step:38825 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:49 step:38826 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38827 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:49 step:38828 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:49 step:38829 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:49 step:38830 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38831 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:49 step:38832 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:49 step:38833 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:49 step:38834 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:49 step:38835 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:49 step:38836 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:49 step:38837 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:49 step:38838 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:49 step:38839 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:49 step:38840 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:49 step:38841 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:49 step:38842 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:49 step:38843 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:49 step:38844 [D loss: 0.000892, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38845 [D loss: 0.033864, acc.: 99.22%] [G loss: 0.002641]\n",
      "epoch:49 step:38846 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.004166]\n",
      "epoch:49 step:38847 [D loss: 0.002700, acc.: 100.00%] [G loss: 0.001689]\n",
      "epoch:49 step:38848 [D loss: 0.003874, acc.: 100.00%] [G loss: 0.005032]\n",
      "epoch:49 step:38849 [D loss: 1.206995, acc.: 54.69%] [G loss: 5.493826]\n",
      "epoch:49 step:38850 [D loss: 1.608374, acc.: 55.47%] [G loss: 1.297439]\n",
      "epoch:49 step:38851 [D loss: 0.374892, acc.: 84.38%] [G loss: 1.104230]\n",
      "epoch:49 step:38852 [D loss: 0.044018, acc.: 97.66%] [G loss: 0.001352]\n",
      "epoch:49 step:38853 [D loss: 0.005819, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:49 step:38854 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.228331]\n",
      "epoch:49 step:38855 [D loss: 0.002553, acc.: 100.00%] [G loss: 0.001784]\n",
      "epoch:49 step:38856 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:49 step:38857 [D loss: 0.018109, acc.: 99.22%] [G loss: 0.000334]\n",
      "epoch:49 step:38858 [D loss: 0.024893, acc.: 99.22%] [G loss: 0.086102]\n",
      "epoch:49 step:38859 [D loss: 0.133179, acc.: 94.53%] [G loss: 0.709743]\n",
      "epoch:49 step:38860 [D loss: 0.015599, acc.: 100.00%] [G loss: 0.742511]\n",
      "epoch:49 step:38861 [D loss: 0.008925, acc.: 100.00%] [G loss: 0.457936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38862 [D loss: 0.043182, acc.: 98.44%] [G loss: 0.252847]\n",
      "epoch:49 step:38863 [D loss: 0.005272, acc.: 100.00%] [G loss: 0.003351]\n",
      "epoch:49 step:38864 [D loss: 0.010746, acc.: 100.00%] [G loss: 0.001547]\n",
      "epoch:49 step:38865 [D loss: 0.055651, acc.: 98.44%] [G loss: 0.038084]\n",
      "epoch:49 step:38866 [D loss: 0.007281, acc.: 100.00%] [G loss: 0.096063]\n",
      "epoch:49 step:38867 [D loss: 0.007307, acc.: 100.00%] [G loss: 0.198340]\n",
      "epoch:49 step:38868 [D loss: 0.006914, acc.: 100.00%] [G loss: 0.083136]\n",
      "epoch:49 step:38869 [D loss: 0.001277, acc.: 100.00%] [G loss: 0.147774]\n",
      "epoch:49 step:38870 [D loss: 0.076696, acc.: 97.66%] [G loss: 0.000465]\n",
      "epoch:49 step:38871 [D loss: 0.002478, acc.: 100.00%] [G loss: 0.003957]\n",
      "epoch:49 step:38872 [D loss: 0.001775, acc.: 100.00%] [G loss: 0.008300]\n",
      "epoch:49 step:38873 [D loss: 0.009301, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:49 step:38874 [D loss: 0.003532, acc.: 100.00%] [G loss: 0.006021]\n",
      "epoch:49 step:38875 [D loss: 0.004022, acc.: 100.00%] [G loss: 0.002571]\n",
      "epoch:49 step:38876 [D loss: 0.007073, acc.: 100.00%] [G loss: 0.001068]\n",
      "epoch:49 step:38877 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.003583]\n",
      "epoch:49 step:38878 [D loss: 0.001714, acc.: 100.00%] [G loss: 0.001910]\n",
      "epoch:49 step:38879 [D loss: 0.001116, acc.: 100.00%] [G loss: 0.001530]\n",
      "epoch:49 step:38880 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.001091]\n",
      "epoch:49 step:38881 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:49 step:38882 [D loss: 0.009634, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:49 step:38883 [D loss: 0.005749, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:49 step:38884 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:49 step:38885 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.001525]\n",
      "epoch:49 step:38886 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.001004]\n",
      "epoch:49 step:38887 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.006741]\n",
      "epoch:49 step:38888 [D loss: 0.012975, acc.: 100.00%] [G loss: 0.001191]\n",
      "epoch:49 step:38889 [D loss: 0.187218, acc.: 92.19%] [G loss: 0.059271]\n",
      "epoch:49 step:38890 [D loss: 0.007738, acc.: 100.00%] [G loss: 0.638221]\n",
      "epoch:49 step:38891 [D loss: 0.153224, acc.: 92.97%] [G loss: 0.521599]\n",
      "epoch:49 step:38892 [D loss: 0.230985, acc.: 89.06%] [G loss: 0.022272]\n",
      "epoch:49 step:38893 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38894 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:49 step:38895 [D loss: 0.006129, acc.: 100.00%] [G loss: 0.003746]\n",
      "epoch:49 step:38896 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:49 step:38897 [D loss: 0.014966, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:49 step:38898 [D loss: 0.859297, acc.: 65.62%] [G loss: 1.431108]\n",
      "epoch:49 step:38899 [D loss: 0.152625, acc.: 92.19%] [G loss: 2.383828]\n",
      "epoch:49 step:38900 [D loss: 1.164863, acc.: 56.25%] [G loss: 0.325221]\n",
      "epoch:49 step:38901 [D loss: 0.002795, acc.: 100.00%] [G loss: 0.392948]\n",
      "epoch:49 step:38902 [D loss: 0.121176, acc.: 96.09%] [G loss: 0.240432]\n",
      "epoch:49 step:38903 [D loss: 0.002174, acc.: 100.00%] [G loss: 0.325577]\n",
      "epoch:49 step:38904 [D loss: 0.013978, acc.: 99.22%] [G loss: 1.172597]\n",
      "epoch:49 step:38905 [D loss: 0.005003, acc.: 100.00%] [G loss: 0.080411]\n",
      "epoch:49 step:38906 [D loss: 0.011008, acc.: 100.00%] [G loss: 0.279416]\n",
      "epoch:49 step:38907 [D loss: 0.002424, acc.: 100.00%] [G loss: 0.070069]\n",
      "epoch:49 step:38908 [D loss: 0.002188, acc.: 100.00%] [G loss: 0.580161]\n",
      "epoch:49 step:38909 [D loss: 0.019771, acc.: 100.00%] [G loss: 0.062903]\n",
      "epoch:49 step:38910 [D loss: 0.007986, acc.: 100.00%] [G loss: 0.038680]\n",
      "epoch:49 step:38911 [D loss: 0.002800, acc.: 100.00%] [G loss: 0.031458]\n",
      "epoch:49 step:38912 [D loss: 0.001917, acc.: 100.00%] [G loss: 0.205437]\n",
      "epoch:49 step:38913 [D loss: 0.002927, acc.: 100.00%] [G loss: 0.426200]\n",
      "epoch:49 step:38914 [D loss: 0.002085, acc.: 100.00%] [G loss: 0.021139]\n",
      "epoch:49 step:38915 [D loss: 0.011832, acc.: 100.00%] [G loss: 0.021924]\n",
      "epoch:49 step:38916 [D loss: 0.001105, acc.: 100.00%] [G loss: 0.263558]\n",
      "epoch:49 step:38917 [D loss: 0.000993, acc.: 100.00%] [G loss: 0.028367]\n",
      "epoch:49 step:38918 [D loss: 0.015392, acc.: 99.22%] [G loss: 0.005719]\n",
      "epoch:49 step:38919 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.007025]\n",
      "epoch:49 step:38920 [D loss: 0.002758, acc.: 100.00%] [G loss: 0.071972]\n",
      "epoch:49 step:38921 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.016449]\n",
      "epoch:49 step:38922 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.008353]\n",
      "epoch:49 step:38923 [D loss: 0.025929, acc.: 100.00%] [G loss: 0.003247]\n",
      "epoch:49 step:38924 [D loss: 0.002774, acc.: 100.00%] [G loss: 0.100793]\n",
      "epoch:49 step:38925 [D loss: 0.003353, acc.: 100.00%] [G loss: 0.012939]\n",
      "epoch:49 step:38926 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.003089]\n",
      "epoch:49 step:38927 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.005110]\n",
      "epoch:49 step:38928 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.003381]\n",
      "epoch:49 step:38929 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.005669]\n",
      "epoch:49 step:38930 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.010106]\n",
      "epoch:49 step:38931 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.008729]\n",
      "epoch:49 step:38932 [D loss: 0.002021, acc.: 100.00%] [G loss: 0.004715]\n",
      "epoch:49 step:38933 [D loss: 0.002563, acc.: 100.00%] [G loss: 0.004447]\n",
      "epoch:49 step:38934 [D loss: 0.029749, acc.: 99.22%] [G loss: 0.000762]\n",
      "epoch:49 step:38935 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.015007]\n",
      "epoch:49 step:38936 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:49 step:38937 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:49 step:38938 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.004028]\n",
      "epoch:49 step:38939 [D loss: 0.005109, acc.: 100.00%] [G loss: 0.001321]\n",
      "epoch:49 step:38940 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000640]\n",
      "epoch:49 step:38941 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.002156]\n",
      "epoch:49 step:38942 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.000950]\n",
      "epoch:49 step:38943 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.001394]\n",
      "epoch:49 step:38944 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.002658]\n",
      "epoch:49 step:38945 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:49 step:38946 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:49 step:38947 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.009478]\n",
      "epoch:49 step:38948 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.001804]\n",
      "epoch:49 step:38949 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000702]\n",
      "epoch:49 step:38950 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:49 step:38951 [D loss: 0.058275, acc.: 99.22%] [G loss: 0.005901]\n",
      "epoch:49 step:38952 [D loss: 0.002185, acc.: 100.00%] [G loss: 0.007290]\n",
      "epoch:49 step:38953 [D loss: 0.002358, acc.: 100.00%] [G loss: 0.011925]\n",
      "epoch:49 step:38954 [D loss: 0.005000, acc.: 100.00%] [G loss: 0.004973]\n",
      "epoch:49 step:38955 [D loss: 0.021591, acc.: 100.00%] [G loss: 0.019954]\n",
      "epoch:49 step:38956 [D loss: 0.007934, acc.: 100.00%] [G loss: 0.008493]\n",
      "epoch:49 step:38957 [D loss: 0.001605, acc.: 100.00%] [G loss: 0.005408]\n",
      "epoch:49 step:38958 [D loss: 0.002795, acc.: 100.00%] [G loss: 0.004823]\n",
      "epoch:49 step:38959 [D loss: 0.005638, acc.: 100.00%] [G loss: 0.010458]\n",
      "epoch:49 step:38960 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.002885]\n",
      "epoch:49 step:38961 [D loss: 0.008214, acc.: 100.00%] [G loss: 0.006207]\n",
      "epoch:49 step:38962 [D loss: 0.005254, acc.: 100.00%] [G loss: 0.007958]\n",
      "epoch:49 step:38963 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.005652]\n",
      "epoch:49 step:38964 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.003131]\n",
      "epoch:49 step:38965 [D loss: 0.001216, acc.: 100.00%] [G loss: 0.001931]\n",
      "epoch:49 step:38966 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.004775]\n",
      "epoch:49 step:38967 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.001318]\n",
      "epoch:49 step:38968 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.011015]\n",
      "epoch:49 step:38969 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.002865]\n",
      "epoch:49 step:38970 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.006385]\n",
      "epoch:49 step:38971 [D loss: 0.004473, acc.: 100.00%] [G loss: 0.001919]\n",
      "epoch:49 step:38972 [D loss: 0.001617, acc.: 100.00%] [G loss: 0.000891]\n",
      "epoch:49 step:38973 [D loss: 0.015911, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:49 step:38974 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.002148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38975 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.015806]\n",
      "epoch:49 step:38976 [D loss: 0.003531, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:49 step:38977 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:49 step:38978 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.000573]\n",
      "epoch:49 step:38979 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.001213]\n",
      "epoch:49 step:38980 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:49 step:38981 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:49 step:38982 [D loss: 0.006526, acc.: 100.00%] [G loss: 0.001728]\n",
      "epoch:49 step:38983 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:49 step:38984 [D loss: 0.007382, acc.: 100.00%] [G loss: 0.000922]\n",
      "epoch:49 step:38985 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:49 step:38986 [D loss: 0.001542, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:49 step:38987 [D loss: 0.002390, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:49 step:38988 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.007490]\n",
      "epoch:49 step:38989 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:49 step:38990 [D loss: 0.001594, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:49 step:38991 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:49 step:38992 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.001669]\n",
      "epoch:49 step:38993 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.001047]\n",
      "epoch:49 step:38994 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:49 step:38995 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:49 step:38996 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:49 step:38997 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:49 step:38998 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:49 step:38999 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:49 step:39000 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:49 step:39001 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:49 step:39002 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:49 step:39003 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:49 step:39004 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:49 step:39005 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:49 step:39006 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:49 step:39007 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.002932]\n",
      "epoch:49 step:39008 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:49 step:39009 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:49 step:39010 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:49 step:39011 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:49 step:39012 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.004704]\n",
      "epoch:49 step:39013 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000838]\n",
      "epoch:49 step:39014 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:49 step:39015 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:49 step:39016 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:49 step:39017 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:49 step:39018 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:49 step:39019 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:49 step:39020 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:49 step:39021 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:49 step:39022 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:49 step:39023 [D loss: 0.003383, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:49 step:39024 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:49 step:39025 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:49 step:39026 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:49 step:39027 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:49 step:39028 [D loss: 0.002919, acc.: 100.00%] [G loss: 0.000451]\n",
      "epoch:49 step:39029 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:49 step:39030 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:49 step:39031 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:49 step:39032 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:49 step:39033 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:49 step:39034 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000699]\n",
      "epoch:49 step:39035 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002413]\n",
      "epoch:49 step:39036 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:49 step:39037 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:49 step:39038 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:49 step:39039 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:49 step:39040 [D loss: 0.001317, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:49 step:39041 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001655]\n",
      "epoch:49 step:39042 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:49 step:39043 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:49 step:39044 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:49 step:39045 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:49 step:39046 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:49 step:39047 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:49 step:39048 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:49 step:39049 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:49 step:39050 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000509]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as Data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        img=img.reshape([3,32,32])\n",
    "        return img\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import model\n",
    "import torch.nn.functional as F\n",
    "model = model.cifar10(128)\n",
    "model.load_state_dict(torch.load('./log/default/best-85.pth'))\n",
    "model.cuda()\n",
    "def get_mean_var(y_logit):\n",
    "    y_logit=np.abs(y_logit-0.1)\n",
    "    return np.mean(y_logit,axis=1)\n",
    "\n",
    "def get_possibility(images):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=100, shuffle=True)\n",
    "    y_logits = []\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "        y_logits=np.array(y_logits)\n",
    "    return y_logits\n",
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('cgan')):\n",
    "    os.mkdir('saved_models_{}'.format('cgan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('cgan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "import cv2\n",
    "\n",
    "\n",
    "from keras.datasets import fashion_mnist,cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import Concatenate, GaussianNoise,Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(2 * 2 * 512, activation='relu',input_dim=self.latent_dim))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Reshape((2, 2, 512)))\n",
    "\n",
    "        model.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "        model.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        model.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add( BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "        model.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Conv 1: 16x16x64\n",
    "        model.add(Conv2D(64, kernel_size=5, strides=2, padding='same' ,input_shape=self.img_shape))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 2:\n",
    "        model.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 3:\n",
    "        model.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add( BatchNormalization(momentum=0.9))\n",
    "        model.add( LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 4:\n",
    "        model.add(Conv2D(512, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add( LeakyReLU(alpha=0.1))\n",
    "        model.summary()\n",
    "\n",
    "        # FC\n",
    "        model.add(Flatten())\n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        labels = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        discriminator =model(img)\n",
    "\n",
    "        # Concatenate\n",
    "        merged_layer = Concatenate()([discriminator, labels])\n",
    "        discriminator = Dense(512, activation='relu')(merged_layer)\n",
    "\n",
    "        # Output\n",
    "        discriminator = Dense(1, activation='sigmoid')(discriminator)\n",
    "\n",
    "        return Model([img, label], discriminator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (X_test, _) = cifar10.load_data()\n",
    "\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "        steps = []\n",
    "        values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict([noise, labels])\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on labels\n",
    "                sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                if global_step % sample_interval == 0:\n",
    "                    self.mode_drop(epoch, global_step)\n",
    "                    \n",
    "\n",
    "    def mode_drop(self, epoch, global_step):\n",
    "        r, c = 10, 1000\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise,sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        y_logits = get_possibility(gen_imgs)\n",
    "        metrics = get_mean_var(y_logits)\n",
    "\n",
    "\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('global_step:' + str(global_step))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in metrics)\n",
    "        f.writelines('\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cgan = CGAN()\n",
    "    cgan.train(epochs=50, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
